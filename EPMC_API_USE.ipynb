{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NLP notebook for idenitfying ML methods papers in life science jorunal \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 2. Load Data\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load the DOME abstract and title data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlisdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Ensures that all required NLP libraries (NLTK, SpaCy, Scikit-learn, etc.) are available.  \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 2. Text Cleaning & Normalization  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 1B. DOME full text \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess DOME data & get insights from the literature data for use with developing ML lit triage model \n",
    "\n",
    "# 1A. DOME abstract and title\n",
    "\n",
    "# 1. Import Necessary Libraries  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# import spacy - depedncy issues avoid for now \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 2. Load Data\n",
    "# Load the DOME abstract and title data\n",
    "os.lisir('.')\n",
    "\n",
    "# Ensures that all required NLP libraries (NLTK, SpaCy, Scikit-learn, etc.) are available.  \n",
    "\n",
    "# 2. Text Cleaning & Normalization  \n",
    "# ✔ Lowercasing (Step 1.1) → Enables case-insensitive matching.  \n",
    "# ✔ Text Normalization (Step 1.8, moved earlier) → Expands contractions, converts numbers, standardizes abbreviations.  \n",
    "\n",
    "# 3. Tokenization & Basic Cleaning  \n",
    "# ✔ Tokenization (Step 1.2) → Splits text into individual words or subwords.  \n",
    "# ✔ Removing Punctuation (Step 1.3) → Ensures proper word separation.  \n",
    "# ✔ Removing Stopwords (Step 1.4) → Eliminates commonly occurring but uninformative words.  \n",
    "\n",
    "# 4. Lemmatization & Stemming  \n",
    "# ✔ Lemmatization OR Stemming (Step 1.5) → Converts words to their root form.  \n",
    "# (Lemmatization is preferable for accuracy; stemming is faster but less precise.)  \n",
    "\n",
    "# 5. Feature Extraction (Reorganized for clarity)  \n",
    "# ✔ Part-of-Speech (POS) Tagging & Counts (Step 1.6.1)  \n",
    "# ✔ Named Entity Recognition (NER) (Step 1.6.2)  \n",
    "# ✔ Term Frequency - Inverse Document Frequency (TF-IDF) (Step 1.6.3)  \n",
    "# ✔ Word Embeddings (Word2Vec, GloVe, BERT, etc.) (Step 1.6.4 - remove “FREQUENCIES” since embeddings are dense vectors, not simple word counts.)  \n",
    "\n",
    "# 6. Vectorization (Final Step)  \n",
    "# ✔ TF-IDF OR Embeddings (Step 1.9) → Converts text into a numerical representation suitable for ML models.  \n",
    "# (Vectorization is technically part of feature extraction, so this step can be merged with 1.6 if preferred.)\n",
    "\n",
    "\n",
    "# 1B. DOME full text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify using ml ontology and others rleevant ML words not within - eg: model types etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine if wider corpus of ML papers needed - automatically find some papers and then also preprocess\n",
    "# could dtermine using text word mining\n",
    "#random papers form lit suggest or negatiev search of terms - eg noo model/ml etc \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess all ML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download all papers mentioning machine learning and AI from EPMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy ML model to predict if a paper is about ML or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyse top papers and journals insights from the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPMC-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
