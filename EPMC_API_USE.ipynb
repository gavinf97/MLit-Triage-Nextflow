{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NLP notebook for idenitfying ML methods papers in life science jorunal \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mImportError: /home/gavinfarrell/anaconda3/envs/EPMC-NLP/lib/python3.11/lib-dynload/_sqlite3.cpython-311-x86_64-linux-gnu.so: undefined symbol: sqlite3_deserialize. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 1. Preprocess DOME data & get insights from the literature data for use with developing ML lit triage model \n",
    "\n",
    "# 1A. DOME abstract and title\n",
    "\n",
    "# 1. Import Necessary Libraries \n",
    "# Ensures that all required NLP libraries (NLTK, Scikit-learn, Gensim, etc.) are available.  \n",
    "# import spacy # depedncy issues avoided for now \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download('all')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 2. Load Data\n",
    "# Load the DOME abstract and title data text files \n",
    "# Read in names of PMC files in title and abstract folder + put into variable \n",
    "title_abstract_names = os.listdir('./DOME_Registry_PMC_Title_Abstract')\n",
    "# Read in the text from each file and put into a new list var for usage\n",
    "title_abstract_names_list = []\n",
    "for file in title_abstract_names:\n",
    "    with open('./DOME_Registry_PMC_Title_Abstract/' + file, 'r') as f:\n",
    "        title_abstract_names_list.append(f.read())\n",
    "        \n",
    "# 2. Text Cleaning & Normalization  \n",
    "# 2.1 Lowercasing function and implementation to enable case-insensitive matching.  \n",
    "def lowercasing(text):\n",
    "    return text.lower()\n",
    "\n",
    "clean1_title_abstract = []\n",
    "for title_abstract in title_abstract_names_list:\n",
    "    clean1_title_abstract.append(lowercasing(title_abstract))\n",
    "\n",
    "# print(lower_title_abstract[4])\n",
    "\n",
    "# 2.2 Remove Special Characters & Numbers function and implementation --> do not do for full text\n",
    "def remove_special_chars_numbers(text):\n",
    "    # Remove numbers and special characters, keeping only letters and spaces using regex replace\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "clean2_title_abstract = []\n",
    "for title_abstract in clean1_title_abstract:\n",
    "    clean2_title_abstract.append(remove_special_chars_numbers(title_abstract))\n",
    "    \n",
    "#print(clean2_title_abstract[0])\n",
    "\n",
    "# 2.3 Remove any double white spaces\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "clean3_title_abstract = []\n",
    "for title_abstract in clean2_title_abstract:\n",
    "    clean3_title_abstract.append(remove_extra_whitespace(title_abstract))\n",
    "\n",
    "#print(clean3_title_abstract[0])\n",
    "\n",
    "# 3. Tokenization & Basic Cleaning  \n",
    "# 3.1 Tokenization - Splits text into individual words (we can also try subwords or sentence level for full text). \n",
    "# Tokenizer choice - to be explained\n",
    "clean4_title_abstract = []\n",
    "\n",
    "for title_abstract in clean3_title_abstract:\n",
    "    clean4_title_abstract.append(word_tokenize(title_abstract))\n",
    "\n",
    "#print(clean4_title_abstract[180])\n",
    "\n",
    "# 3.2 Ensure proper word separation - ignore for the moment, address if tokenisation issues \n",
    "\n",
    "# 3.3 Removing Stopwords - Eliminates commonly occurring but uninformative words.  \n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "clean5_title_abstract = []\n",
    "for title_abstract in clean4_title_abstract:\n",
    "    clean5_title_abstract.append(remove_stopwords(title_abstract))\n",
    "\n",
    "# print(clean5_title_abstract[180])\n",
    "\n",
    "# 4. Lemmatization (or Stemming - where would be preferred)  \n",
    "# Lemmatization → Converts words to their root form.  \n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "clean6_title_abstract = []\n",
    "for title_abstract in clean5_title_abstract:\n",
    "    clean6_title_abstract.append(lemmatize(title_abstract))\n",
    "\n",
    "#print(clean5_title_abstract[180])\n",
    "#print(clean6_title_abstract[180])\n",
    "\n",
    "# 5. Feature Extraction  \n",
    "# 5.1 Part-of-Speech (POS) Tagging & Counts  \n",
    "def pos_tagging(text):\n",
    "    nltk.pos_tag(text)\n",
    "    return pos_tag(text)\n",
    "\n",
    "pos_tags = []\n",
    "for title_abstract in clean6_title_abstract:\n",
    "    pos_tags.append(pos_tagging(title_abstract))\n",
    "\n",
    "print(pos_tags[180])\n",
    "\n",
    "# 5.2 Named Entity Recognition (NER)  \n",
    "\n",
    "\n",
    "# 5.3 Term Frequency - Inverse Document Frequency - (TF-IDF)  \n",
    "# To try on doc basis and also on full corpus basis\n",
    "\n",
    "# 5.3.1 Term frequency → Measures how frequently a term occurs in a document.\n",
    "def compute_term_frequency(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    tf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# 5.3.2 IDF → Measures how important a term is within a corpus.\n",
    "def compute_inverse_document_frequency(tf_matrix):\n",
    "    transformer = TfidfTransformer(norm=None, use_idf=True)\n",
    "    transformer.fit(tf_matrix)\n",
    "    idf = transformer.idf_\n",
    "    return idf\n",
    "\n",
    "# 5.3.3 TF-IDF → Combines the above two metrics to determine the importance of a term in a document relative to a corpus.\n",
    "def compute_tf_idf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_idf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage:\n",
    "documents = [\" \".join(doc) for doc in clean6_title_abstract]\n",
    "tf_matrix, terms = compute_term_frequency(documents)\n",
    "idf = compute_inverse_document_frequency(tf_matrix)\n",
    "tf_idf_matrix, tf_idf_terms = compute_tf_idf(documents)\n",
    "\n",
    "print(\"TF Matrix Shape:\", tf_matrix.shape)\n",
    "print(\"IDF Shape:\", idf.shape)\n",
    "print(\"TF-IDF Matrix Shape:\", tf_idf_matrix.shape)\n",
    "\n",
    "# Save the matrices and terms\n",
    "output_dir = './DOME_Registry_PMC_Title_Abstract_Analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert sparse matrix to dense format for saving as CSV\n",
    "tf_matrix_dense = tf_matrix.toarray()\n",
    "tf_idf_matrix_dense = tf_idf_matrix.toarray()\n",
    "\n",
    "# Save as CSV\n",
    "pd.DataFrame(tf_matrix_dense, columns=terms).to_csv(os.path.join(output_dir, 'tf_matrix.csv'), index=False)\n",
    "pd.DataFrame({'term': terms, 'idf': idf}).to_csv(os.path.join(output_dir, 'idf.csv'), index=False)\n",
    "pd.DataFrame(tf_idf_matrix_dense, columns=tf_idf_terms).to_csv(os.path.join(output_dir, 'tf_idf_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Bag of Words (BoW)\n",
    "def compute_bag_of_words(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    return bow_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# BOW usage:\n",
    "bow_matrix, bow_terms = compute_bag_of_words(documents)\n",
    "print(\"BoW Matrix Shape:\", bow_matrix.shape)\n",
    "\n",
    "# Save BoW matrix as CSV\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "pd.DataFrame(bow_matrix_dense, columns=bow_terms).to_csv(os.path.join(output_dir, 'bow_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Word Embeddings (Word2Vec, GloVe, BERT, etc.)\n",
    "def compute_word2vec_gensim(documents):\n",
    "    model = Word2Vec(sentences=documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "word2vec_model = compute_word2vec_gensim(clean6_title_abstract)\n",
    "print(\"Word2Vec Model Vocabulary Size:\", len(word2vec_model.wv))\n",
    "\n",
    "# Display the vector for a sample word\n",
    "sample_word = 'machine'\n",
    "if sample_word in word2vec_model.wv:\n",
    "    print(f\"Vector for '{sample_word}':\", word2vec_model.wv[sample_word])\n",
    "else:\n",
    "    print(f\"'{sample_word}' not in vocabulary\")\n",
    "\n",
    "# Visualize Word2Vec word clusters\n",
    "def visualize_word_clusters(model, words):\n",
    "    word_vectors = np.array([model.wv[word] for word in words if word in model.wv])\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=5)  # Set perplexity to a value less than the number of words\n",
    "    word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in model.wv:\n",
    "            plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n",
    "            plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "sample_words = ['machine', 'learning', 'data', 'science', 'model', 'algorithm']\n",
    "visualize_word_clusters(word2vec_model, sample_words)\n",
    "\n",
    "# 6. Vectorization (Final Step)\n",
    "# #n-Grams – Captures sequences of words. Example: Bi-gram: \"machine learning\" → ([\"machine\", \"learning\"]).  \n",
    "# ✔ TF-IDF OR Embeddings (Step 1.9) → Converts text into a numerical representation suitable for ML models.  \n",
    "# (Vectorization is technically part of feature extraction, so this step can be merged with 1.6 if preferred.)\n",
    "\n",
    "# Try title only and then abstarct only\n",
    "\n",
    "\n",
    "# 1B. DOME full text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do\n",
    "# topic modelling\n",
    "# analyse the oooutputs\n",
    "# use outputs for identifying more relavnt papers \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify using ml ontology and others rleevant ML words not within - eg: model types etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine if wider corpus of ML papers needed - automatically find some papers and then also preprocess\n",
    "# could dtermine using text word mining\n",
    "#random papers form lit suggest or negatiev search of terms - eg noo model/ml etc \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess all ML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download all papers mentioning machine learning and AI from EPMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy ML model to predict if a paper is about ML or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyse top papers and journals insights from the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPMC-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
