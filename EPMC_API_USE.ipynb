{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NLP notebook for idenitfying ML methods papers in life science jorunal \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Matrix Shape: (186, 4813)\n",
      "IDF Shape: (4813,)\n",
      "TF-IDF Matrix Shape: (186, 4813)\n",
      "BoW Matrix Shape: (186, 4813)\n",
      "Word2Vec Model Vocabulary Size: 4830\n",
      "Vector for 'machine': [-2.07052842e-01  2.29632020e-01  1.69503182e-01  6.81412891e-02\n",
      "  3.91420424e-02 -5.49060941e-01  1.50891185e-01  5.50810993e-01\n",
      " -2.24079937e-01 -2.26501480e-01 -1.44010678e-01 -4.49575156e-01\n",
      " -5.75649589e-02  4.28074747e-02 -4.90287691e-02 -1.35430828e-01\n",
      "  6.46141395e-02 -3.73235047e-01 -2.56669745e-02 -5.13415635e-01\n",
      "  7.68738016e-02  1.74924389e-01  2.45191067e-01 -1.32091582e-01\n",
      " -1.20081730e-01 -4.64878753e-02 -2.56311327e-01 -1.98453560e-01\n",
      " -2.62828141e-01 -2.60042660e-02  4.18170393e-01  1.05281964e-01\n",
      "  5.78797534e-02 -1.71213642e-01 -6.27136528e-02  3.11336517e-01\n",
      "  1.07119381e-01 -2.95320153e-01 -1.71882644e-01 -5.02573669e-01\n",
      "  1.09168537e-01 -2.94993192e-01 -2.13022664e-01 -7.99899325e-02\n",
      "  2.79208392e-01 -6.98935539e-02 -2.55676031e-01 -1.65186450e-02\n",
      "  1.37909442e-01  2.14212939e-01  2.10443392e-01 -1.29872173e-01\n",
      " -1.01063244e-01  1.97392143e-02 -2.26015866e-01  2.24414587e-01\n",
      "  1.67680308e-01 -4.56288047e-02 -2.81879246e-01  5.40602915e-02\n",
      " -1.59447908e-03  1.28352597e-01 -7.07014501e-02  5.53884208e-02\n",
      " -3.14434379e-01  1.48926631e-01  2.21878663e-02  8.58023539e-02\n",
      " -3.40900987e-01  2.72704214e-01 -1.76772073e-01  1.11556999e-01\n",
      "  3.97871405e-01 -6.11255690e-02  2.23484278e-01  1.74332589e-01\n",
      " -7.45993457e-05 -5.44823594e-02 -2.75693417e-01  6.62105158e-02\n",
      " -1.67155012e-01  7.15115219e-02 -2.11501211e-01  3.83014292e-01\n",
      " -3.34971286e-02 -4.68251435e-03  3.17221247e-02  3.28166932e-01\n",
      "  3.64878893e-01  7.56291151e-02  2.89047241e-01  1.56017557e-01\n",
      "  5.65972179e-02  7.16048032e-02  5.28564453e-01  2.32968241e-01\n",
      "  7.72120655e-02 -3.08452815e-01  8.17243531e-02  4.26388793e-02]\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess DOME data & get insights from the literature data for use with developing ML lit triage model \n",
    "\n",
    "# 1A. DOME abstract and title\n",
    "\n",
    "# 1. Import Necessary Libraries \n",
    "# # Ensures that all required NLP libraries (NLTK, SpaCy, Scikit-learn, etc.) are available.  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# import spacy - depedncy issues avoid for now \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import scipy.sparse\n",
    "import joblib\n",
    "\n",
    "# 2. Load Data\n",
    "# Load the DOME abstract and title data\n",
    "# Read in names of PMC files in title and abstract folder + put into variable \n",
    "title_abstract_names = os.listdir('./DOME_Registry_PMC_Title_Abstract')\n",
    "# Read in the text from each file and put into a new list\n",
    "title_abstract_names_list = []\n",
    "for file in title_abstract_names:\n",
    "    with open('./DOME_Registry_PMC_Title_Abstract/' + file, 'r') as f:\n",
    "        title_abstract_names_list.append(f.read())\n",
    "        \n",
    "# 2. Text Cleaning & Normalization  \n",
    "# 2.1 Lowercasing to enable case-insensitive matching.  \n",
    "def lowercasing(text):\n",
    "    return text.lower()\n",
    "\n",
    "clean1_title_abstract = []\n",
    "for title_abstract in title_abstract_names_list:\n",
    "    clean1_title_abstract.append(lowercasing(title_abstract))\n",
    "\n",
    "# print(lower_title_abstract[4])\n",
    "\n",
    "# 2.2 Remove Special Characters & Numbers --> do not do for full text\n",
    "def remove_special_chars_numbers(text):\n",
    "    # Remove numbers and special characters, keeping only letters and spaces\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "clean2_title_abstract = []\n",
    "for title_abstract in clean1_title_abstract:\n",
    "    clean2_title_abstract.append(remove_special_chars_numbers(title_abstract))\n",
    "    \n",
    "#print(clean2_title_abstract[0])\n",
    "\n",
    "# 2.3 Remove double white spaces\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "clean3_title_abstract = []\n",
    "for title_abstract in clean2_title_abstract:\n",
    "    clean3_title_abstract.append(remove_extra_whitespace(title_abstract))\n",
    "\n",
    "#print(clean3_title_abstract[0])\n",
    "\n",
    "# 3. Tokenization & Basic Cleaning  \n",
    "# 3.1 Tokenization - Splits text into individual words can also try subwords.). \n",
    "# #Tokenizer chosen also removes punctuation\n",
    "clean4_title_abstract = []\n",
    "\n",
    "for title_abstract in clean3_title_abstract:\n",
    "    clean4_title_abstract.append(word_tokenize(title_abstract))\n",
    "\n",
    "#print(clean4_title_abstract[180])\n",
    "\n",
    "# #3.2 R (Step 1.3) → Ensures proper word separation. - ignore for the moment, address if tokenisation issues \n",
    "\n",
    "# 3.3 Removing Stopwords - Eliminates commonly occurring but uninformative words.  \n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "clean5_title_abstract = []\n",
    "for title_abstract in clean4_title_abstract:\n",
    "    clean5_title_abstract.append(remove_stopwords(title_abstract))\n",
    "\n",
    "# print(clean5_title_abstract[180])\n",
    "\n",
    "# 4. Lemmatization (& Stemming - where would be preferred)  \n",
    "# Lemmatization → Converts words to their root form.  \n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "clean6_title_abstract = []\n",
    "for title_abstract in clean5_title_abstract:\n",
    "    clean6_title_abstract.append(lemmatize(title_abstract))\n",
    "\n",
    "#print(clean5_title_abstract[180])\n",
    "#print(clean6_title_abstract[180])\n",
    "\n",
    "# 5. Feature Extraction (Reorganized for clarity)  \n",
    "# 5.1 Part-of-Speech (POS) Tagging & Counts  \n",
    "\n",
    "\n",
    "# 5.2 Named Entity Recognition (NER)  \n",
    "\n",
    "# 5.3 Term Frequency - Inverse Document Frequency - (TF-IDF) (Step 1.6.3)  \n",
    "# Try on doc basis and also on full corpus basis\n",
    "\n",
    "# 5.3.1 Term frequency → Measures how frequently a term occurs in a document.\n",
    "def compute_term_frequency(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    tf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# 5.3.2 IDF → Measures how important a term is within a corpus.\n",
    "def compute_inverse_document_frequency(tf_matrix):\n",
    "    transformer = TfidfTransformer(norm=None, use_idf=True)\n",
    "    transformer.fit(tf_matrix)\n",
    "    idf = transformer.idf_\n",
    "    return idf\n",
    "\n",
    "# 5.3.3 TF-IDF → Combines the above two metrics to determine the importance of a term in a document relative to a corpus.\n",
    "def compute_tf_idf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_idf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage:\n",
    "documents = [\" \".join(doc) for doc in clean6_title_abstract]\n",
    "tf_matrix, terms = compute_term_frequency(documents)\n",
    "idf = compute_inverse_document_frequency(tf_matrix)\n",
    "tf_idf_matrix, tf_idf_terms = compute_tf_idf(documents)\n",
    "\n",
    "print(\"TF Matrix Shape:\", tf_matrix.shape)\n",
    "print(\"IDF Shape:\", idf.shape)\n",
    "print(\"TF-IDF Matrix Shape:\", tf_idf_matrix.shape)\n",
    "\n",
    "# Save the matrices and terms\n",
    "output_dir = './DOME_Registry_PMC_Title_Abstract_Analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert sparse matrix to dense format for saving as CSV\n",
    "tf_matrix_dense = tf_matrix.toarray()\n",
    "tf_idf_matrix_dense = tf_idf_matrix.toarray()\n",
    "\n",
    "# Save as CSV\n",
    "pd.DataFrame(tf_matrix_dense, columns=terms).to_csv(os.path.join(output_dir, 'tf_matrix.csv'), index=False)\n",
    "pd.DataFrame({'term': terms, 'idf': idf}).to_csv(os.path.join(output_dir, 'idf.csv'), index=False)\n",
    "pd.DataFrame(tf_idf_matrix_dense, columns=tf_idf_terms).to_csv(os.path.join(output_dir, 'tf_idf_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Bag of Words (BoW)\n",
    "def compute_bag_of_words(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    return bow_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# BOW usage:\n",
    "bow_matrix, bow_terms = compute_bag_of_words(documents)\n",
    "print(\"BoW Matrix Shape:\", bow_matrix.shape)\n",
    "\n",
    "# Save BoW matrix as CSV\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "pd.DataFrame(bow_matrix_dense, columns=bow_terms).to_csv(os.path.join(output_dir, 'bow_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Word Embeddings (Word2Vec, GloVe, BERT, etc.)\n",
    "def compute_word2vec_gensim(documents):\n",
    "    model = Word2Vec(sentences=documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "word2vec_model = compute_word2vec_gensim(clean6_title_abstract)\n",
    "print(\"Word2Vec Model Vocabulary Size:\", len(word2vec_model.wv))\n",
    "\n",
    "# Display the vector for a sample word\n",
    "sample_word = 'machine'\n",
    "if sample_word in word2vec_model.wv:\n",
    "    print(f\"Vector for '{sample_word}':\", word2vec_model.wv[sample_word])\n",
    "else:\n",
    "    print(f\"'{sample_word}' not in vocabulary\")\n",
    "\n",
    "# 6. Vectorization (Final Step)  \n",
    "# ✔ TF-IDF OR Embeddings (Step 1.9) → Converts text into a numerical representation suitable for ML models.  \n",
    "# (Vectorization is technically part of feature extraction, so this step can be merged with 1.6 if preferred.)\n",
    "\n",
    "# Try title only and then abstarct only\n",
    "\n",
    "\n",
    "# 1B. DOME full text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify using ml ontology and others rleevant ML words not within - eg: model types etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine if wider corpus of ML papers needed - automatically find some papers and then also preprocess\n",
    "# could dtermine using text word mining\n",
    "#random papers form lit suggest or negatiev search of terms - eg noo model/ml etc \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess all ML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download all papers mentioning machine learning and AI from EPMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy ML model to predict if a paper is about ML or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyse top papers and journals insights from the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPMC-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
