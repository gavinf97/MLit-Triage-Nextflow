{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NLP notebook for idenitfying ML methods papers in life science jorunal \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'word2vec' from 'nltk' (/home/gavinfarrell/anaconda3/envs/EPMC-NLP/lib/python3.13/site-packages/nltk/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word2vec\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Download required datasets\u001b[39;00m\n\u001b[1;32m     33\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'word2vec' from 'nltk' (/home/gavinfarrell/anaconda3/envs/EPMC-NLP/lib/python3.13/site-packages/nltk/__init__.py)"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess DOME data & get insights from the literature data for use with developing ML lit triage model \n",
    "\n",
    "# 1A. DOME abstract and title\n",
    "\n",
    "# 1. Import Necessary Libraries \n",
    "# # Ensures that all required NLP libraries (NLTK, SpaCy, Scikit-learn, etc.) are available.  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# import spacy - depedncy issues avoid for now \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import scipy.sparse\n",
    "import joblib\n",
    "\n",
    "# 2. Load Data\n",
    "# Load the DOME abstract and title data\n",
    "# Read in names of PMC files in title and abstract folder + put into variable \n",
    "title_abstract_names = os.listdir('./DOME_Registry_PMC_Title_Abstract')\n",
    "# Read in the text from each file and put into a new list\n",
    "title_abstract_names_list = []\n",
    "for file in title_abstract_names:\n",
    "    with open('./DOME_Registry_PMC_Title_Abstract/' + file, 'r') as f:\n",
    "        title_abstract_names_list.append(f.read())\n",
    "        \n",
    "# 2. Text Cleaning & Normalization  \n",
    "# 2.1 Lowercasing to enable case-insensitive matching.  \n",
    "def lowercasing(text):\n",
    "    return text.lower()\n",
    "\n",
    "clean1_title_abstract = []\n",
    "for title_abstract in title_abstract_names_list:\n",
    "    clean1_title_abstract.append(lowercasing(title_abstract))\n",
    "\n",
    "# print(lower_title_abstract[4])\n",
    "\n",
    "# 2.2 Remove Special Characters & Numbers --> do not do for full text\n",
    "def remove_special_chars_numbers(text):\n",
    "    # Remove numbers and special characters, keeping only letters and spaces\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "clean2_title_abstract = []\n",
    "for title_abstract in clean1_title_abstract:\n",
    "    clean2_title_abstract.append(remove_special_chars_numbers(title_abstract))\n",
    "    \n",
    "#print(clean2_title_abstract[0])\n",
    "\n",
    "# 2.3 Remove double white spaces\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "clean3_title_abstract = []\n",
    "for title_abstract in clean2_title_abstract:\n",
    "    clean3_title_abstract.append(remove_extra_whitespace(title_abstract))\n",
    "\n",
    "#print(clean3_title_abstract[0])\n",
    "\n",
    "# 3. Tokenization & Basic Cleaning  \n",
    "# 3.1 Tokenization - Splits text into individual words can also try subwords.). \n",
    "# #Tokenizer chosen also removes punctuation\n",
    "clean4_title_abstract = []\n",
    "\n",
    "for title_abstract in clean3_title_abstract:\n",
    "    clean4_title_abstract.append(word_tokenize(title_abstract))\n",
    "\n",
    "#print(clean4_title_abstract[180])\n",
    "\n",
    "# #3.2 R (Step 1.3) → Ensures proper word separation. - ignore for the moment, address if tokenisation issues \n",
    "\n",
    "# 3.3 Removing Stopwords - Eliminates commonly occurring but uninformative words.  \n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "clean5_title_abstract = []\n",
    "for title_abstract in clean4_title_abstract:\n",
    "    clean5_title_abstract.append(remove_stopwords(title_abstract))\n",
    "\n",
    "# print(clean5_title_abstract[180])\n",
    "\n",
    "# 4. Lemmatization (& Stemming - where would be preferred)  \n",
    "# Lemmatization → Converts words to their root form.  \n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "clean6_title_abstract = []\n",
    "for title_abstract in clean5_title_abstract:\n",
    "    clean6_title_abstract.append(lemmatize(title_abstract))\n",
    "\n",
    "#print(clean5_title_abstract[180])\n",
    "#print(clean6_title_abstract[180])\n",
    "\n",
    "# 5. Feature Extraction (Reorganized for clarity)  \n",
    "# 5.1 Part-of-Speech (POS) Tagging & Counts  \n",
    "\n",
    "\n",
    "# 5.2 Named Entity Recognition (NER)  \n",
    "\n",
    "# 5.3 Term Frequency - Inverse Document Frequency - (TF-IDF) (Step 1.6.3)  \n",
    "# Try on doc basis and also on full corpus basis\n",
    "\n",
    "# 5.3.1 Term frequency → Measures how frequently a term occurs in a document.\n",
    "def compute_term_frequency(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    tf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# 5.3.2 IDF → Measures how important a term is within a corpus.\n",
    "def compute_inverse_document_frequency(tf_matrix):\n",
    "    transformer = TfidfTransformer(norm=None, use_idf=True)\n",
    "    transformer.fit(tf_matrix)\n",
    "    idf = transformer.idf_\n",
    "    return idf\n",
    "\n",
    "# 5.3.3 TF-IDF → Combines the above two metrics to determine the importance of a term in a document relative to a corpus.\n",
    "def compute_tf_idf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tf_idf_matrix = vectorizer.fit_transform(documents)\n",
    "    return tf_idf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Example usage:\n",
    "documents = [\" \".join(doc) for doc in clean6_title_abstract]\n",
    "tf_matrix, terms = compute_term_frequency(documents)\n",
    "idf = compute_inverse_document_frequency(tf_matrix)\n",
    "tf_idf_matrix, tf_idf_terms = compute_tf_idf(documents)\n",
    "\n",
    "print(\"TF Matrix Shape:\", tf_matrix.shape)\n",
    "print(\"IDF Shape:\", idf.shape)\n",
    "print(\"TF-IDF Matrix Shape:\", tf_idf_matrix.shape)\n",
    "\n",
    "# Save the matrices and terms\n",
    "output_dir = './DOME_Registry_PMC_Title_Abstract_Analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert sparse matrix to dense format for saving as CSV\n",
    "tf_matrix_dense = tf_matrix.toarray()\n",
    "tf_idf_matrix_dense = tf_idf_matrix.toarray()\n",
    "\n",
    "# Save as CSV\n",
    "pd.DataFrame(tf_matrix_dense, columns=terms).to_csv(os.path.join(output_dir, 'tf_matrix.csv'), index=False)\n",
    "pd.DataFrame({'term': terms, 'idf': idf}).to_csv(os.path.join(output_dir, 'idf.csv'), index=False)\n",
    "pd.DataFrame(tf_idf_matrix_dense, columns=tf_idf_terms).to_csv(os.path.join(output_dir, 'tf_idf_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Bag of Words (BoW)\n",
    "def compute_bag_of_words(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    return bow_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# BOW usage:\n",
    "bow_matrix, bow_terms = compute_bag_of_words(documents)\n",
    "print(\"BoW Matrix Shape:\", bow_matrix.shape)\n",
    "\n",
    "# Save BoW matrix as CSV\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "pd.DataFrame(bow_matrix_dense, columns=bow_terms).to_csv(os.path.join(output_dir, 'bow_matrix.csv'), index=False)\n",
    "\n",
    "# 5.4 Word Embeddings (Word2Vec, GloVe, BERT, etc.)\n",
    "def compute_word2vec_gensim(documents):\n",
    "    model = Word2Vec(sentences=documents, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "word2vec_model = compute_word2vec_gensim(clean6_title_abstract)\n",
    "print(\"Word2Vec Model Vocabulary Size:\", len(word2vec_model.wv))\n",
    "\n",
    "# Display the vector for a sample word\n",
    "sample_word = 'machine'\n",
    "if sample_word in word2vec_model.wv:\n",
    "    print(f\"Vector for '{sample_word}':\", word2vec_model.wv[sample_word])\n",
    "else:\n",
    "    print(f\"'{sample_word}' not in vocabulary\")\n",
    "\n",
    "# 6. Vectorization (Final Step)  \n",
    "# ✔ TF-IDF OR Embeddings (Step 1.9) → Converts text into a numerical representation suitable for ML models.  \n",
    "# (Vectorization is technically part of feature extraction, so this step can be merged with 1.6 if preferred.)\n",
    "\n",
    "# Try title only and then abstarct only\n",
    "\n",
    "\n",
    "# 1B. DOME full text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify using ml ontology and others rleevant ML words not within - eg: model types etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine if wider corpus of ML papers needed - automatically find some papers and then also preprocess\n",
    "# could dtermine using text word mining\n",
    "#random papers form lit suggest or negatiev search of terms - eg noo model/ml etc \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess all ML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download all papers mentioning machine learning and AI from EPMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy ML model to predict if a paper is about ML or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyse top papers and journals insights from the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPMC-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
