{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning NLP notebook for idenitfying ML methods papers in life science jorunal \n",
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/gavinfarrell/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'classification', 'pallidal', 'oscillations', 'increasing', 'parkinsonian', 'severity', 'abstract', 'firing', 'patterns', 'neurons', 'basal', 'ganglia', 'known', 'become', 'oscillatory', 'synchronized', 'healthy', 'parkinsonian', 'conditions', 'similar', 'changes', 'observed', 'local', 'field', 'potentials', 'lfps', 'study', 'used', 'unbiased', 'machine', 'learning', 'approach', 'investigate', 'utility', 'pallidal', 'lfps', 'discriminating', 'stages', 'progressive', 'parkinsonian', 'model', 'feature', 'selection', 'algorithm', 'used', 'identify', 'subsets', 'lfp', 'features', 'provided', 'discriminatory', 'information', 'severity', 'parkinsonian', 'motor', 'signs', 'prediction', 'errors', 'achievable', 'using', 'possible', 'features', 'tested', 'subjects', 'spectral', 'feature', 'within', 'beta', 'band', 'chosen', 'feature', 'selection', 'algorithm', 'combination', 'features', 'including', 'alpha', 'band', 'power', 'phase', 'amplitude', 'coupling', 'necessary', 'achieve', 'minimal', 'prediction', 'errors', 'large', 'variability', 'discriminatory', 'features', 'individual', 'subjects', 'testing', 'classifiers', 'subjects', 'yielded', 'prediction', 'errors', 'results', 'suggest', 'pallidal', 'oscillations', 'predictive', 'biomarkers', 'parkinsonian', 'severity', 'features', 'complex', 'spectral', 'power', 'individual', 'frequency', 'bands', 'beta', 'band', 'additionally', 'best', 'feature', 'set', 'subject', 'specific', 'highlights', 'pathophysiological', 'heterogeneity', 'parkinsonism', 'importance', 'subject', 'specificity', 'designing', 'closed', 'loop', 'system', 'controllers', 'dependent', 'features']\n",
      "['title', 'classification', 'pallidal', 'oscillation', 'increasing', 'parkinsonian', 'severity', 'abstract', 'firing', 'pattern', 'neuron', 'basal', 'ganglion', 'known', 'become', 'oscillatory', 'synchronized', 'healthy', 'parkinsonian', 'condition', 'similar', 'change', 'observed', 'local', 'field', 'potential', 'lfps', 'study', 'used', 'unbiased', 'machine', 'learning', 'approach', 'investigate', 'utility', 'pallidal', 'lfps', 'discriminating', 'stage', 'progressive', 'parkinsonian', 'model', 'feature', 'selection', 'algorithm', 'used', 'identify', 'subset', 'lfp', 'feature', 'provided', 'discriminatory', 'information', 'severity', 'parkinsonian', 'motor', 'sign', 'prediction', 'error', 'achievable', 'using', 'possible', 'feature', 'tested', 'subject', 'spectral', 'feature', 'within', 'beta', 'band', 'chosen', 'feature', 'selection', 'algorithm', 'combination', 'feature', 'including', 'alpha', 'band', 'power', 'phase', 'amplitude', 'coupling', 'necessary', 'achieve', 'minimal', 'prediction', 'error', 'large', 'variability', 'discriminatory', 'feature', 'individual', 'subject', 'testing', 'classifier', 'subject', 'yielded', 'prediction', 'error', 'result', 'suggest', 'pallidal', 'oscillation', 'predictive', 'biomarkers', 'parkinsonian', 'severity', 'feature', 'complex', 'spectral', 'power', 'individual', 'frequency', 'band', 'beta', 'band', 'additionally', 'best', 'feature', 'set', 'subject', 'specific', 'highlight', 'pathophysiological', 'heterogeneity', 'parkinsonism', 'importance', 'subject', 'specificity', 'designing', 'closed', 'loop', 'system', 'controller', 'dependent', 'feature']\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess DOME data & get insights from the literature data for use with developing ML lit triage model \n",
    "\n",
    "# 1A. DOME abstract and title\n",
    "\n",
    "# 1. Import Necessary Libraries \n",
    "# # Ensures that all required NLP libraries (NLTK, SpaCy, Scikit-learn, etc.) are available.  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# import spacy - depedncy issues avoid for now \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download required datasets\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# 2. Load Data\n",
    "# Load the DOME abstract and title data\n",
    "# Read in names of PMC files in title and abstract folder + put into variable \n",
    "title_abstract_names = os.listdir('./DOME_Registry_PMC_Title_Abstract')\n",
    "# Read in the text from each file and put into a new list\n",
    "title_abstract_names_list = []\n",
    "for file in title_abstract_names:\n",
    "    with open('./DOME_Registry_PMC_Title_Abstract/' + file, 'r') as f:\n",
    "        title_abstract_names_list.append(f.read())\n",
    "        \n",
    "# 2. Text Cleaning & Normalization  \n",
    "# 2.1 Lowercasing to enable case-insensitive matching.  \n",
    "def lowercasing(text):\n",
    "    return text.lower()\n",
    "\n",
    "clean1_title_abstract = []\n",
    "for title_abstract in title_abstract_names_list:\n",
    "    clean1_title_abstract.append(lowercasing(title_abstract))\n",
    "\n",
    "# print(lower_title_abstract[4])\n",
    "\n",
    "# 2.2 Remove Special Characters & Numbers --> do not do for full text\n",
    "def remove_special_chars_numbers(text):\n",
    "    # Remove numbers and special characters, keeping only letters and spaces\n",
    "    cleaned_text = re.sub(r'[^A-Za-z\\s]', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "clean2_title_abstract = []\n",
    "for title_abstract in clean1_title_abstract:\n",
    "    clean2_title_abstract.append(remove_special_chars_numbers(title_abstract))\n",
    "    \n",
    "#print(clean2_title_abstract[0])\n",
    "\n",
    "# 2.3 Remove double white spaces\n",
    "def remove_extra_whitespace(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "clean3_title_abstract = []\n",
    "for title_abstract in clean2_title_abstract:\n",
    "    clean3_title_abstract.append(remove_extra_whitespace(title_abstract))\n",
    "\n",
    "#print(clean3_title_abstract[0])\n",
    "\n",
    "# 3. Tokenization & Basic Cleaning  \n",
    "# 3.1 Tokenization - Splits text into individual words can also try subwords.). \n",
    "# #Tokenizer chosen also removes punctuation\n",
    "clean4_title_abstract = []\n",
    "\n",
    "for title_abstract in clean3_title_abstract:\n",
    "    clean4_title_abstract.append(word_tokenize(title_abstract))\n",
    "\n",
    "#print(clean4_title_abstract[180])\n",
    "\n",
    "# #3.2 R (Step 1.3) → Ensures proper word separation. - ignore for the moment, address if tokenisation issues \n",
    "\n",
    "# 3.3 Removing Stopwords - Eliminates commonly occurring but uninformative words.  \n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "clean5_title_abstract = []\n",
    "for title_abstract in clean4_title_abstract:\n",
    "    clean5_title_abstract.append(remove_stopwords(title_abstract))\n",
    "\n",
    "# print(clean5_title_abstract[180])\n",
    "\n",
    "# 4. Lemmatization (& Stemming - where would be preferred)  \n",
    "# Lemmatization → Converts words to their root form.  \n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "clean6_title_abstract = []\n",
    "for title_abstract in clean5_title_abstract:\n",
    "    clean6_title_abstract.append(lemmatize(title_abstract))\n",
    "\n",
    "#print(clean5_title_abstract[180])\n",
    "#print(clean6_title_abstract[180])\n",
    "\n",
    "# 5. Feature Extraction (Reorganized for clarity)  \n",
    "# 5.1 Part-of-Speech (POS) Tagging & Counts  \n",
    "\n",
    "\n",
    "# 5.2 Named Entity Recognition (NER)  \n",
    "\n",
    "# 5.3 Term Frequency - Inverse Document Frequency - (TF-IDF) (Step 1.6.3)  \n",
    "\n",
    "# 5.4 Bag of Words (BoW) \n",
    "\n",
    "# 5.4 Word Embeddings (Word2Vec, GloVe, BERT, etc.) (Step 1.6.4 - remove “FREQUENCIES” since embeddings are dense vectors, not simple word counts.)  \n",
    "\n",
    "# 6. Vectorization (Final Step)  \n",
    "# ✔ TF-IDF OR Embeddings (Step 1.9) → Converts text into a numerical representation suitable for ML models.  \n",
    "# (Vectorization is technically part of feature extraction, so this step can be merged with 1.6 if preferred.)\n",
    "\n",
    "# Try title only and then abstarct only\n",
    "\n",
    "\n",
    "# 1B. DOME full text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify using ml ontology and others rleevant ML words not within - eg: model types etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Determine if wider corpus of ML papers needed - automatically find some papers and then also preprocess\n",
    "# could dtermine using text word mining\n",
    "#random papers form lit suggest or negatiev search of terms - eg noo model/ml etc \n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocess all ML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Download all papers mentioning machine learning and AI from EPMC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Deploy ML model to predict if a paper is about ML or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Analyse top papers and journals insights from the literature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EPMC-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
