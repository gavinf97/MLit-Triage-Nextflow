{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Python script to download latest DOME Registry contents, related full text papers & provide DOME Registry entries metadata read out (20241202)**\n",
    "1. DOME Registry contents will be downloaded by API call proividng the json file of DOME Registry data\n",
    "2. DOME Registry data json will be flattened and converted into CSV for working with entries data (row based data)\n",
    "3. DOME Registry CSV will be checked and used to produce a metadata readout file (+ graphs)\n",
    "4. DOME Registry DOIs of articles will be convrted to PMCIDs for full text retrieval \n",
    "5. DOME Registry entries will be downloaded as full XML files using PMCIDs list and NCBI Entrez service (Replace with EPMC when API works [^1] )\n",
    "\n",
    "[^1]: EPMC full text XL API module issues on 20241204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DOME Registry contents will be downloaded by API call proividng the json file of DOME Registry data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists for today's date, do you want to overwrite? (y/n)\n",
      "Overwriting file\n",
      "DOME Registry data downloaded and saved to 'DOME_Registry_Contents_2024-12-05.json'\n"
     ]
    }
   ],
   "source": [
    "# 1. Use the DOME API to download all entries of the DOME Registry and store this in a json file \n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the URL for the API call: check the API documentation for the correct URL on the DOME Registry website\n",
    "url = \"https://registry.dome-ml.org/api/review?skip=0&limit=250&text=%20&public=true&sort=publication.year&asc=true\"\n",
    "\n",
    "# Make an API request to the URL\n",
    "response = requests.get(url, headers={'accept': '*/*'})\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Get the current date in ISO format for file naming\n",
    "    # Potentially update fiel datetime granularity if needing to run more regularly than daily, DOME Registry contents unlikely to be more regular than this\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "\n",
    "    # Create the output file name \n",
    "    file_name = f\"DOME_Registry_Contents_{current_date}.json\"\n",
    "\n",
    "    # Check if the file pathway already exists\n",
    "    if os.path.exists(file_name):\n",
    "        print(f\"File already exists for today's date, do you want to overwrite? (y/n)\")\n",
    "        overwrite = input('Do you want to overwrite the file? (y/n): ') \n",
    "        if overwrite == 'n':\n",
    "            print('Exiting without overwriting file')\n",
    "            exit()\n",
    "        elif overwrite == 'y':\n",
    "            print('Overwriting file')\n",
    "        else:\n",
    "            print('Invalid input, exiting')\n",
    "            exit()\n",
    "\n",
    "    \n",
    "    # Save the content to a file\n",
    "    with open(file_name, 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "    \n",
    "    print(f\"DOME Registry data downloaded and saved to '{file_name}'\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the data. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DOME Registry data json will be flattened and converted into CSV for working with entries data (row based data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened JSON data saved to 'flattened_DOME_Registry_Contents_2024-12-05.json'\n",
      "JSON data written to 'flattened_DOME_Registry_Contents_2024-12-05.csv'\n"
     ]
    }
   ],
   "source": [
    "# 2. Produce DOME Registry contents metadata .csv file and data visualisation\n",
    "import json\n",
    "\n",
    "# 2.1 Pretty print DOME Registry contents JSON file for inspection to ensure all looks as expected\n",
    "\n",
    "# Function to read and pretty-print the JSON file sample entry\n",
    "def pretty_print_json(file_name):\n",
    "    try:\n",
    "        # Open and read the JSON file\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        print(json.dumps(data, indent=4))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "\n",
    "# Call the function to pretty-print the JSON file\n",
    "# pretty_print_json(file_name)\n",
    "\n",
    "\n",
    "# 2.2 Flatten the JSON for easier data processing and write to a new .json file \n",
    "# Function to read JSON data\n",
    "def read_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to flatten JSON\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "# Function to save flattened JSON to a file\n",
    "def save_flattened_json(flattened_data, output_file_name):\n",
    "    try:\n",
    "        with open(output_file_name, 'w', encoding='utf-8') as file:\n",
    "            json.dump(flattened_data, file, indent=4)\n",
    "        print(f\"Flattened JSON data saved to '{output_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the flattened JSON file: {e}\")\n",
    "\n",
    "# Read JSON data\n",
    "data = read_json(file_name)\n",
    "\n",
    "# Flatten JSON data and save to a new file\n",
    "if data:\n",
    "    flattened_data = [flatten_json(entry) for entry in data]\n",
    "    flattened_file_name = (\"flattened_\"+file_name)\n",
    "    save_flattened_json(flattened_data, flattened_file_name)\n",
    "    \n",
    "    # Print the flattened JSON data to view it\n",
    "    # to add a print of file output name and sucess ftatement - print(flattened_file_name)\n",
    "    #print(json.dumps(flattened_data, indent=4))\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "\n",
    "\n",
    "\n",
    "#2.3 Convert flattened json to csv \n",
    "# Function to read flattened JSON data\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the path to the flattened JSON file\n",
    "#flattened_file_name = 'flattened_DOME_Registry_Contents.json'  # Replace with your actual file name\n",
    "\n",
    "# Function to read flattened JSON data\n",
    "def read_flattened_json(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the flattened JSON file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to write JSON data to a CSV file\n",
    "def write_json_to_csv(json_data, csv_file_name):\n",
    "    try:\n",
    "        # Determine all possible headers from the entire dataset\n",
    "        headers = set()\n",
    "        for entry in json_data:\n",
    "            headers.update(entry.keys())\n",
    "        headers = list(headers)\n",
    "        \n",
    "        # Write data to CSV file\n",
    "        with open(csv_file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            for entry in json_data:\n",
    "                writer.writerow(entry)\n",
    "        \n",
    "        print(f\"JSON data written to '{csv_file_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to the CSV file: {e}\")\n",
    "\n",
    "# Read flattened JSON data\n",
    "flattened_data = read_flattened_json(flattened_file_name)\n",
    "\n",
    "# Process JSON data into CSV\n",
    "if flattened_data:\n",
    "    csv_file_name = flattened_file_name[:-5]+'.csv'\n",
    "    write_json_to_csv(flattened_data, csv_file_name)\n",
    "else:\n",
    "    print(\"No data to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DOME Registry CSV will be analysed for entry compliance and used to produce a metadata readout file (+ graphs - TBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Production of the DOME Registry fields validity data & subsequent metadata csv file \\nimport csv\\nimport re\\nimport os\\n\\n# 3.1  Simple print of all DOME fields & simple explainer put into text file \\n# for assisting with use of DOME Regsitry entry data\\n# for header in csv_file_name:\\n#    print(header)\\n\\n# Define regexes to check various CSV header field entries\\n# Define the EPMC regex pattern for PMIDs\\npmid_pattern = re.compile(r\\'^\\\\d{8}$\\')\\n\\n# Define the regex pattern for DOIs\\ndoi_pattern = re.compile(r\\'^10.\\\\d{4,9}/[-._;()/:A-Z0-9]+$\\', re.IGNORECASE)\\n\\n# Function to read CSV data\\ndef read_csv(file_name):\\n    try:\\n        with open(file_name, \\'r\\', encoding=\\'utf-8\\') as csvfile:\\n            reader = csv.DictReader(csvfile)\\n            data = [row for row in reader]\\n        return data\\n    except Exception as e:\\n        print(f\"Error reading the CSV file: {e}\")\\n        return None\\n\\n# Function to check PMIDs and DOIs and generate metadata\\ndef check_pmids_and_dois_and_generate_metadata(data, csv_file_name):\\n    pmid_valid = 0\\n    pmid_invalid = 0\\n    doi_valid = 0\\n    doi_invalid = 0\\n    total_entries = len(data)\\n    \\n    for row in data:\\n        pmid = row.get(\\'publication_pmid\\', \\'\\')\\n        doi = row.get(\\'publication_doi\\', \\'\\')\\n        \\n        if pmid_pattern.match(pmid):\\n            pmid_valid += 1\\n        else:\\n            pmid_invalid += 1\\n        \\n        if doi_pattern.match(doi):\\n            doi_valid += 1\\n        else:\\n            doi_invalid += 1\\n    \\n    # Print the results\\n    print(f\"{pmid_valid} of {total_entries} PMIDs valid\")\\n    print(f\"{pmid_invalid} of {total_entries} PMIDs invalid\")\\n    print(f\"{doi_valid} of {total_entries} DOIs valid\")\\n    print(f\"{doi_invalid} of {total_entries} DOIs invalid\")\\n    \\n    # Create metadata CSV file\\n    metadata_file_name = f\"Metadata_{os.path.basename(csv_file_name)}\"\\n    with open(metadata_file_name, \\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as csvfile:\\n        writer = csv.writer(csvfile)\\n        writer.writerow([\\'Metric\\', \\'Value\\'])\\n        writer.writerow([\\'Total Entries\\', total_entries])\\n        writer.writerow([\\'Valid PMIDs\\', pmid_valid])\\n        writer.writerow([\\'Invalid PMIDs\\', pmid_invalid])\\n        writer.writerow([\\'Valid DOIs\\', doi_valid])\\n        writer.writerow([\\'Invalid DOIs\\', doi_invalid])\\n    \\n    print(f\"Metadata written to \\'{metadata_file_name}\\'\")\\n\\n# Read CSV data\\ncsv_data = read_csv(csv_file_name)\\n\\n# Check PMIDs and DOIs and generate metadata\\nif csv_data:\\n    check_pmids_and_dois_and_generate_metadata(csv_data, csv_file_name)\\nelse:\\n    print(\"No data to process.\")\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Production of the DOME Registry fields validity data & subsequent metadata csv file \n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 3.1  Simple print of all DOME fields & simple explainer put into text file \n",
    "# for assisting with use of DOME Regsitry entry data\n",
    "# for header in csv_file_name:\n",
    "#    print(header)\n",
    "\n",
    "# Define regexes to check various CSV header field entries\n",
    "# Define the EPMC regex pattern for PMIDs\n",
    "pmid_pattern = re.compile(r'^\\d{8}$')\n",
    "\n",
    "# Define the regex pattern for DOIs\n",
    "doi_pattern = re.compile(r'^10.\\d{4,9}/[-._;()/:A-Z0-9]+$', re.IGNORECASE)\n",
    "\n",
    "# Function to read CSV data\n",
    "def read_csv(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            data = [row for row in reader]\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to check PMIDs and DOIs and generate metadata\n",
    "def check_pmids_and_dois_and_generate_metadata(data, csv_file_name):\n",
    "    pmid_valid = 0\n",
    "    pmid_invalid = 0\n",
    "    doi_valid = 0\n",
    "    doi_invalid = 0\n",
    "    total_entries = len(data)\n",
    "    \n",
    "    for row in data:\n",
    "        pmid = row.get('publication_pmid', '')\n",
    "        doi = row.get('publication_doi', '')\n",
    "        \n",
    "        if pmid_pattern.match(pmid):\n",
    "            pmid_valid += 1\n",
    "        else:\n",
    "            pmid_invalid += 1\n",
    "        \n",
    "        if doi_pattern.match(doi):\n",
    "            doi_valid += 1\n",
    "        else:\n",
    "            doi_invalid += 1\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{pmid_valid} of {total_entries} PMIDs valid\")\n",
    "    print(f\"{pmid_invalid} of {total_entries} PMIDs invalid\")\n",
    "    print(f\"{doi_valid} of {total_entries} DOIs valid\")\n",
    "    print(f\"{doi_invalid} of {total_entries} DOIs invalid\")\n",
    "    \n",
    "    # Create metadata CSV file\n",
    "    metadata_file_name = f\"Metadata_{os.path.basename(csv_file_name)}\"\n",
    "    with open(metadata_file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Metric', 'Value'])\n",
    "        writer.writerow(['Total Entries', total_entries])\n",
    "        writer.writerow(['Valid PMIDs', pmid_valid])\n",
    "        writer.writerow(['Invalid PMIDs', pmid_invalid])\n",
    "        writer.writerow(['Valid DOIs', doi_valid])\n",
    "        writer.writerow(['Invalid DOIs', doi_invalid])\n",
    "    \n",
    "    print(f\"Metadata written to '{metadata_file_name}'\")\n",
    "\n",
    "# Read CSV data\n",
    "csv_data = read_csv(csv_file_name)\n",
    "\n",
    "# Check PMIDs and DOIs and generate metadata\n",
    "if csv_data:\n",
    "    check_pmids_and_dois_and_generate_metadata(csv_data, csv_file_name)\n",
    "else:\n",
    "    print(\"No data to process.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 3.1  Simple print of all DOME fields & simple explainer put into text file \\n# for assisting with use of DOME Registry entry data\\n# reformat using dataframe the DOME entries csv columns into more logical format\\nimport csv\\nimport pandas as pd #because who doesn't love a panda \\n\\n#Read in DOME Entries CSV as dataframe via pandas library functions\\nprint(csv_file_name)\\nDOME_Entries_dataframe = pd.read_csv(csv_file_name)\\n\\n# View data frame to inspect all data appears ok\\nDOME_Entries_dataframe.head()\\nDOME_Entries_dataframe.shape\\n\\n#Get number of entries in header row correpsonding to DOME Registry entries fields \\n# from a given entry related to its originating JSON file\\ni=0\\nheader_entries_for_text_file = []\\nfor header_entry in (DOME_Entries_dataframe.columns):\\n     i = i+1\\n     header_entries_for_text_file.append(header_entry)\\nprint('Number of DOME Registry field entries: ' + str(i))\\nprint(header_entries_for_text_file)\\n\\n# to remove redundant fields and check over these - TBC\\n#df = DOME_Entries_dataframe.reindex(sorted(DOME_Entries_dataframe.columns), axis=1)\\n#df.head()\\n# to create more metadata and graph of entries (to work and think on)\\n# to add and choose main ID for rows header\\n# to do DOI checks and regex\\n# to get PMC full text from DOI and store in local folder\\n\\n#row_names = df['uuid']\\n#print(row_names)\\n\\n# Set row names as shortid whichh corresponds to DOME Registry unique short id \\ndf = pd.DataFrame(df).set_index('shortid')\\ndf.to_csv(csv_file_name, sep=',', index=True, encoding='utf-8')\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 3.1  Simple print of all DOME fields & simple explainer put into text file \n",
    "# for assisting with use of DOME Registry entry data\n",
    "# reformat using dataframe the DOME entries csv columns into more logical format\n",
    "import csv\n",
    "import pandas as pd #because who doesn't love a panda \n",
    "\n",
    "#Read in DOME Entries CSV as dataframe via pandas library functions\n",
    "print(csv_file_name)\n",
    "DOME_Entries_dataframe = pd.read_csv(csv_file_name)\n",
    "\n",
    "# View data frame to inspect all data appears ok\n",
    "DOME_Entries_dataframe.head()\n",
    "DOME_Entries_dataframe.shape\n",
    "\n",
    "#Get number of entries in header row correpsonding to DOME Registry entries fields \n",
    "# from a given entry related to its originating JSON file\n",
    "i=0\n",
    "header_entries_for_text_file = []\n",
    "for header_entry in (DOME_Entries_dataframe.columns):\n",
    "     i = i+1\n",
    "     header_entries_for_text_file.append(header_entry)\n",
    "print('Number of DOME Registry field entries: ' + str(i))\n",
    "print(header_entries_for_text_file)\n",
    "\n",
    "# to remove redundant fields and check over these - TBC\n",
    "#df = DOME_Entries_dataframe.reindex(sorted(DOME_Entries_dataframe.columns), axis=1)\n",
    "#df.head()\n",
    "# to create more metadata and graph of entries (to work and think on)\n",
    "# to add and choose main ID for rows header\n",
    "# to do DOI checks and regex\n",
    "# to get PMC full text from DOI and store in local folder\n",
    "\n",
    "#row_names = df['uuid']\n",
    "#print(row_names)\n",
    "\n",
    "# Set row names as shortid whichh corresponds to DOME Registry unique short id \n",
    "df = pd.DataFrame(df).set_index('shortid')\n",
    "df.to_csv(csv_file_name, sep=',', index=True, encoding='utf-8')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 3.2\\n\\n# Reorder metadata to start of columns list\\n# Define the prefixes to match and group csv data\\nprefix_matches_cols = 'matches_'\\nprefix_publications_cols= 'matches_publication'\\nprefix_data_cols= 'matches_data'\\nprefix_optimization_cols= 'matches_optimization'\\nprefix_model_cols= 'matches_model'\\nprefix_evaluation_cols= 'matches_evaluation'\\n\\n# Separate columns based on whether they start with the prefix\\nmatches_publication_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\\nmatches_data_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\\nmatches_optimization_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\\nmatches_model_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\\nmatches_evaluation_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\\nother_columns = [col for col in df.columns if not col.startswith(prefix_matches_cols)]\\n\\n# Reorder columns\\nreordered_columns = other_columns + matches_data_columns + matches_optimization_columns + matches_model_columns + matches_evaluation_columns\\ndf = df[reordered_columns]\\n\\nprint(df.head())\\n\\ndf.to_csv(csv_file_name, sep=',', index=True, encoding='utf-8')\\n\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 3.2\n",
    "\n",
    "# Reorder metadata to start of columns list\n",
    "# Define the prefixes to match and group csv data\n",
    "prefix_matches_cols = 'matches_'\n",
    "prefix_publications_cols= 'matches_publication'\n",
    "prefix_data_cols= 'matches_data'\n",
    "prefix_optimization_cols= 'matches_optimization'\n",
    "prefix_model_cols= 'matches_model'\n",
    "prefix_evaluation_cols= 'matches_evaluation'\n",
    "\n",
    "# Separate columns based on whether they start with the prefix\n",
    "matches_publication_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "matches_data_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "matches_optimization_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "matches_model_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "matches_evaluation_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "other_columns = [col for col in df.columns if not col.startswith(prefix_matches_cols)]\n",
    "\n",
    "# Reorder columns\n",
    "reordered_columns = other_columns + matches_data_columns + matches_optimization_columns + matches_model_columns + matches_evaluation_columns\n",
    "df = df[reordered_columns]\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(csv_file_name, sep=',', index=True, encoding='utf-8')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n# View data frame to inspect all data appears ok\\nDOME_Entries_dataframe.head()\\nDOME_Entries_dataframe.shape\\n\\n#Get number of entries in header row correpsonding to DOME Registry entries fields \\n# from a given entry related to its originating JSON file\\ni=0\\nheader_entries_for_text_file = []\\nfor header_entry in (DOME_Entries_dataframe.columns):\\n     i = i+1\\n     header_entries_for_text_file.append(header_entry)\\nprint('Number of DOME Registry field entries: ' + str(i))\\nprint(header_entries_for_text_file)\\n\\n# to remove redundant fields and check over these - TBC\\ndf = DOME_Entries_dataframe.reindex(sorted(DOME_Entries_dataframe.columns), axis=1)\\ndf.head()\\n# to create more metadata and graph of entries (to work and think on)\\n# to add and choose main ID for rows header\\n# to do DOI checks and regex\\n# to get PMC full text from DOI and store in local folder\\n\\nrow_names = df['uuid']\\n#print(row_names)\\n\\n# Set row names as shortid whichh corresponds to DOME Registry unique short id \\ndf = pd.DataFrame(df).set_index('shortid')\\ndf.head()\\n\\n\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "# View data frame to inspect all data appears ok\n",
    "DOME_Entries_dataframe.head()\n",
    "DOME_Entries_dataframe.shape\n",
    "\n",
    "#Get number of entries in header row correpsonding to DOME Registry entries fields \n",
    "# from a given entry related to its originating JSON file\n",
    "i=0\n",
    "header_entries_for_text_file = []\n",
    "for header_entry in (DOME_Entries_dataframe.columns):\n",
    "     i = i+1\n",
    "     header_entries_for_text_file.append(header_entry)\n",
    "print('Number of DOME Registry field entries: ' + str(i))\n",
    "print(header_entries_for_text_file)\n",
    "\n",
    "# to remove redundant fields and check over these - TBC\n",
    "df = DOME_Entries_dataframe.reindex(sorted(DOME_Entries_dataframe.columns), axis=1)\n",
    "df.head()\n",
    "# to create more metadata and graph of entries (to work and think on)\n",
    "# to add and choose main ID for rows header\n",
    "# to do DOI checks and regex\n",
    "# to get PMC full text from DOI and store in local folder\n",
    "\n",
    "row_names = df['uuid']\n",
    "#print(row_names)\n",
    "\n",
    "# Set row names as shortid whichh corresponds to DOME Registry unique short id \n",
    "df = pd.DataFrame(df).set_index('shortid')\n",
    "df.head()\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.2 reorder data frame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Define the path to your CSV file\n",
    "# csv_file_name = 'path_to_your_csv_file.csv'  # Replace with your actual file name\n",
    "\n",
    "# Read in DOME Entries CSV as dataframe via pandas library functions\n",
    "# df = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Define the prefixes to match and group csv data\n",
    "prefix_publications_cols = 'publication_'\n",
    "prefix_data_cols = 'matches_data'\n",
    "prefix_optimization_cols = 'matches_optimization'\n",
    "prefix_model_cols = 'matches_model'\n",
    "prefix_evaluation_cols = 'matches_evaluation'\n",
    "\n",
    "# Separate columns based on whether they start with the prefix\n",
    "publication_columns = [col for col in df.columns if col.startswith(prefix_publications_cols)]\n",
    "matches_data_columns = [col for col in df.columns if col.startswith(prefix_data_cols)]\n",
    "matches_optimization_columns = [col for col in df.columns if col.startswith(prefix_optimization_cols)]\n",
    "matches_model_columns = [col for col in df.columns if col.startswith(prefix_model_cols)]\n",
    "matches_evaluation_columns = [col for col in df.columns if col.startswith(prefix_evaluation_cols)]\n",
    "other_columns = [col for col in df.columns if not col.startswith('matches_') and not col.startswith('publication_')]\n",
    "\n",
    "# Reorder columns\n",
    "reordered_columns = (other_columns + publication_columns + matches_data_columns +\n",
    "                     matches_optimization_columns + matches_model_columns + matches_evaluation_columns)\n",
    "df = df[reordered_columns]\n",
    "\n",
    "# Print the reordered DataFrame\n",
    "#print(df.head())\n",
    "\n",
    "df = pd.DataFrame(df).set_index('shortid')\n",
    "df.to_csv(csv_file_name, sep=',', index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(matches_publication_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      shortid                       _id                   created  public  \\\n",
      "0  6i0xepuivt  63516fedb9c880af1f305b5c  2022-09-01T15:16:05.444Z    True   \n",
      "1  nlj5x3dld8  63516fedb9c880af1f305b93  2022-09-01T15:16:05.445Z    True   \n",
      "2  ysqyy92zyr  66030aaa1502715bfe53d65c  2024-03-26T17:49:30.048Z    True   \n",
      "3  qx3ex71jye  66041e5d1502715bfe53d70a  2024-03-27T13:25:49.790Z    True   \n",
      "4  v536tc3b5t  63516fedb9c880af1f305b1c  2022-09-01T15:16:05.443Z    True   \n",
      "\n",
      "                                 publication_authors  publication_created  \\\n",
      "0               Wang H, Zheng H, Simpson D, Azuaje F                  NaN   \n",
      "1               Al-Shahib A, Breitling R, Gilbert DR                  NaN   \n",
      "2  Hui Lan, Rachel Carson , Nicholas J Provart an...                  NaN   \n",
      "3  Blaise Gassend, Charles W O'Donnell, William T...                  NaN   \n",
      "4                  Tsai RT, Dai HJ, Huang CH, Hsu WL                  NaN   \n",
      "\n",
      "               publication_doi  publication_done publication_journal  \\\n",
      "0      10.1186/1471-2105-7-116               0.0  BMC Bioinformatics   \n",
      "1       10.1186/1471-2164-8-78               0.0        BMC Genomics   \n",
      "2      10.1186/1471-2105-8-358               NaN  BMC Bioinformatics   \n",
      "3    10.1186/1471-2105-8-S5-S3               NaN  BMC Bioinformatics   \n",
      "4  10.1186/1471-2105-9-S12-S18               0.0  BMC Bioinformatics   \n",
      "\n",
      "  publication_pmid  ...                         matches_model/availability  \\\n",
      "0         16524483  ...                                                NaN   \n",
      "1         17374164  ...                                                NaN   \n",
      "2         17888165  ...                                                NaN   \n",
      "3         17570862  ...                                                NaN   \n",
      "4         19091017  ...  The code to reproduce the results can be downl...   \n",
      "\n",
      "                              matches_model/duration  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2                                         not stated   \n",
      "3                                                NaN   \n",
      "4  Training time is expressed as milliseconds per...   \n",
      "\n",
      "                      matches_model/interpretability  \\\n",
      "0                                          Black box   \n",
      "1                                          Black box   \n",
      "2  Model is partially interpretable, since classi...   \n",
      "3  Model is interpreatable, since paramenters lea...   \n",
      "4  Transparent : The attribution recall score for...   \n",
      "\n",
      "                                matches_model/output  \\\n",
      "0                                                NaN   \n",
      "1      Classification prediction of residue contact.   \n",
      "2                                                NaN   \n",
      "3                              binary classification   \n",
      "4  Regression is the output by neural networks, i...   \n",
      "\n",
      "                   matches_evaluation/availability  \\\n",
      "0                                              NaN   \n",
      "1                                              NaN   \n",
      "2                                              NaN   \n",
      "3                                    Not available   \n",
      "4  Yes. (https://github.com/ggonzalezp/hyperfoods)   \n",
      "\n",
      "                       matches_evaluation/comparison  \\\n",
      "0                                                NaN   \n",
      "1  Performance achieved with methods based on aut...   \n",
      "2       no comparison with other approaches perfomed   \n",
      "3  Cmparison with other approaches is missing. No...   \n",
      "4  A baseline input is used, in which all drug ta...   \n",
      "\n",
      "                       matches_evaluation/confidence  \\\n",
      "0                                                NaN   \n",
      "1                                                NaN   \n",
      "2  no confidence interval reported. No statistica...   \n",
      "3                                       Not reported   \n",
      "4  Confidence intervals and statistical significa...   \n",
      "\n",
      "                          matches_evaluation/measure  \\\n",
      "0                                                NaN   \n",
      "1  Precision as a function of effective aligned s...   \n",
      "2                                          ROC curve   \n",
      "3  accuracy and segment-overlap value for alpha h...   \n",
      "4  Balanced accuracy, F1 score, AUPR. The last tw...   \n",
      "\n",
      "                           matches_evaluation/method mapped_pmcid  \n",
      "0                         five-fold cross-validation   PMC1421439  \n",
      "1                    Independent dataset form CASP11   PMC1847686  \n",
      "2                                   cross validation   PMC2213690  \n",
      "3                  Repeated random traing/test split   PMC1892091  \n",
      "4  Cross-validation. The model was also tested on...   PMC2638158  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "#DOIs to PMCIDs\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_name = 'path_to_your_csv_file.csv'  # Replace with your actual file name\n",
    "\n",
    "# Read in DOME Entries CSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Extract DOIs from the DataFrame\n",
    "dois = df['publication_doi'].dropna().unique()\n",
    "\n",
    "# Function to map DOIs to PMCIDs using NCBI E-utilities API\n",
    "def map_dois_to_pmcids(dois):\n",
    "    pmcid_mapping = {}\n",
    "    for doi in dois:\n",
    "        url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?tool=my_tool&email=my_email@example.com&ids={doi}&format=json\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            records = data.get('records', [])\n",
    "            if records:\n",
    "                for record in records:\n",
    "                    if 'pmcid' in record:\n",
    "                        pmcid_mapping[doi] = record['pmcid']\n",
    "                    else:\n",
    "                        pmcid_mapping[doi] = None\n",
    "        else:\n",
    "            pmcid_mapping[doi] = None\n",
    "    return pmcid_mapping\n",
    "\n",
    "# Map DOIs to PMCIDs\n",
    "doi_to_pmcid_mapping = map_dois_to_pmcids(dois)\n",
    "\n",
    "# Add the mapped PMCIDs to the DataFrame\n",
    "df['mapped_pmcid'] = df['publication_doi'].map(doi_to_pmcid_mapping)\n",
    "\n",
    "# TO UPDATE FILE NAMING TO CORRELATE BETTER\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_csv_file_name = 'updated_DOME_Registry_Contents.csv'\n",
    "df.to_csv(output_csv_file_name, index=False)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df.head())\n",
    "\n",
    "#TO FIX REQUEST INTO SMALLER BATCHES VS SINGLE REQUESTS FOR SPEED\n",
    "#to add number of pmcids missing readout for metadata file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metadata file readout as a csv and text file to explain contents - TO ADD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text for PMCID PMC1421439 saved to 'PMC_Full_Texts/PMC1421439.xml'\n",
      "Full text for PMCID PMC1847686 saved to 'PMC_Full_Texts/PMC1847686.xml'\n",
      "Full text for PMCID PMC2213690 saved to 'PMC_Full_Texts/PMC2213690.xml'\n",
      "Full text for PMCID PMC1892091 saved to 'PMC_Full_Texts/PMC1892091.xml'\n",
      "Full text for PMCID PMC2638158 saved to 'PMC_Full_Texts/PMC2638158.xml'\n",
      "Full text for PMCID PMC2665034 saved to 'PMC_Full_Texts/PMC2665034.xml'\n",
      "Full text for PMCID PMC2275242 saved to 'PMC_Full_Texts/PMC2275242.xml'\n",
      "Full text for PMCID PMC2561051 saved to 'PMC_Full_Texts/PMC2561051.xml'\n",
      "Full text for PMCID PMC2660303 saved to 'PMC_Full_Texts/PMC2660303.xml'\n",
      "Full text for PMCID PMC2752621 saved to 'PMC_Full_Texts/PMC2752621.xml'\n",
      "Full text for PMCID PMC3009519 saved to 'PMC_Full_Texts/PMC3009519.xml'\n",
      "Failed to retrieve full text for PMCID PMC3169429. Status code: 404\n",
      "Full text for PMCID PMC3542245 saved to 'PMC_Full_Texts/PMC3542245.xml'\n",
      "Full text for PMCID PMC3396452 saved to 'PMC_Full_Texts/PMC3396452.xml'\n",
      "Full text for PMCID PMC3292016 saved to 'PMC_Full_Texts/PMC3292016.xml'\n",
      "Full text for PMCID PMC3340366 saved to 'PMC_Full_Texts/PMC3340366.xml'\n",
      "Full text for PMCID PMC4289375 saved to 'PMC_Full_Texts/PMC4289375.xml'\n",
      "Full text for PMCID PMC3967921 saved to 'PMC_Full_Texts/PMC3967921.xml'\n",
      "Full text for PMCID PMC4058174 saved to 'PMC_Full_Texts/PMC4058174.xml'\n",
      "Full text for PMCID PMC3912131 saved to 'PMC_Full_Texts/PMC3912131.xml'\n",
      "Full text for PMCID PMC4706063 saved to 'PMC_Full_Texts/PMC4706063.xml'\n",
      "Failed to retrieve full text for PMCID PMC4507953. Status code: 404\n",
      "Full text for PMCID PMC4388847 saved to 'PMC_Full_Texts/PMC4388847.xml'\n",
      "Full text for PMCID PMC4606520 saved to 'PMC_Full_Texts/PMC4606520.xml'\n",
      "Full text for PMCID PMC4466774 saved to 'PMC_Full_Texts/PMC4466774.xml'\n",
      "Full text for PMCID PMC4589233 saved to 'PMC_Full_Texts/PMC4589233.xml'\n",
      "Full text for PMCID PMC4315323 saved to 'PMC_Full_Texts/PMC4315323.xml'\n",
      "Full text for PMCID PMC4315436 saved to 'PMC_Full_Texts/PMC4315436.xml'\n",
      "Full text for PMCID PMC4834164 saved to 'PMC_Full_Texts/PMC4834164.xml'\n",
      "Full text for PMCID PMC4773135 saved to 'PMC_Full_Texts/PMC4773135.xml'\n",
      "Full text for PMCID PMC5042084 saved to 'PMC_Full_Texts/PMC5042084.xml'\n",
      "Full text for PMCID PMC4931851 saved to 'PMC_Full_Texts/PMC4931851.xml'\n",
      "Failed to retrieve full text for PMCID PMC5079830. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC5034704. Status code: 404\n",
      "Full text for PMCID PMC5104375 saved to 'PMC_Full_Texts/PMC5104375.xml'\n",
      "Full text for PMCID PMC5113897 saved to 'PMC_Full_Texts/PMC5113897.xml'\n",
      "Full text for PMCID PMC4894951 saved to 'PMC_Full_Texts/PMC4894951.xml'\n",
      "Full text for PMCID PMC5120500 saved to 'PMC_Full_Texts/PMC5120500.xml'\n",
      "Failed to retrieve full text for PMCID PMC5775817. Status code: 404\n",
      "Full text for PMCID PMC5738356 saved to 'PMC_Full_Texts/PMC5738356.xml'\n",
      "Full text for PMCID PMC5517062 saved to 'PMC_Full_Texts/PMC5517062.xml'\n",
      "Failed to retrieve full text for PMCID PMC5650527. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC5656045. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC5821114. Status code: 404\n",
      "Full text for PMCID PMC5773889 saved to 'PMC_Full_Texts/PMC5773889.xml'\n",
      "Failed to retrieve full text for PMCID PMC5610945. Status code: 404\n",
      "Full text for PMCID PMC5550971 saved to 'PMC_Full_Texts/PMC5550971.xml'\n",
      "Full text for PMCID PMC5870574 saved to 'PMC_Full_Texts/PMC5870574.xml'\n",
      "Failed to retrieve full text for PMCID PMC5860171. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC5860114. Status code: 404\n",
      "Full text for PMCID PMC5688026 saved to 'PMC_Full_Texts/PMC5688026.xml'\n",
      "Failed to retrieve full text for PMCID PMC6172579. Status code: 404\n",
      "Full text for PMCID PMC6036855 saved to 'PMC_Full_Texts/PMC6036855.xml'\n",
      "Full text for PMCID PMC6237755 saved to 'PMC_Full_Texts/PMC6237755.xml'\n",
      "Full text for PMCID PMC5930664 saved to 'PMC_Full_Texts/PMC5930664.xml'\n",
      "Full text for PMCID PMC5976622 saved to 'PMC_Full_Texts/PMC5976622.xml'\n",
      "Full text for PMCID PMC6214550 saved to 'PMC_Full_Texts/PMC6214550.xml'\n",
      "Full text for PMCID PMC6214495 saved to 'PMC_Full_Texts/PMC6214495.xml'\n",
      "Full text for PMCID PMC6036478 saved to 'PMC_Full_Texts/PMC6036478.xml'\n",
      "Full text for PMCID PMC6242780 saved to 'PMC_Full_Texts/PMC6242780.xml'\n",
      "Failed to retrieve full text for PMCID PMC5821274. Status code: 404\n",
      "Full text for PMCID PMC6091426 saved to 'PMC_Full_Texts/PMC6091426.xml'\n",
      "Full text for PMCID PMC5910428 saved to 'PMC_Full_Texts/PMC5910428.xml'\n",
      "Full text for PMCID PMC6277570 saved to 'PMC_Full_Texts/PMC6277570.xml'\n",
      "Full text for PMCID PMC5487762 saved to 'PMC_Full_Texts/PMC5487762.xml'\n",
      "Full text for PMCID PMC9328381 saved to 'PMC_Full_Texts/PMC9328381.xml'\n",
      "Full text for PMCID PMC6394031 saved to 'PMC_Full_Texts/PMC6394031.xml'\n",
      "Full text for PMCID PMC6495231 saved to 'PMC_Full_Texts/PMC6495231.xml'\n",
      "Failed to retrieve full text for PMCID PMC6708480. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC6851483. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC6902683. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC6715517. Status code: 404\n",
      "Failed to retrieve full text for PMCID PMC6657583. Status code: 404\n",
      "Full text for PMCID PMC6908647 saved to 'PMC_Full_Texts/PMC6908647.xml'\n",
      "Failed to retrieve full text for PMCID PMC6478501. Status code: 404\n",
      "Full text for PMCID PMC6459551 saved to 'PMC_Full_Texts/PMC6459551.xml'\n",
      "Full text for PMCID PMC6690680 saved to 'PMC_Full_Texts/PMC6690680.xml'\n",
      "Full text for PMCID PMC6664791 saved to 'PMC_Full_Texts/PMC6664791.xml'\n",
      "Full text for PMCID PMC6457539 saved to 'PMC_Full_Texts/PMC6457539.xml'\n",
      "Full text for PMCID PMC6436896 saved to 'PMC_Full_Texts/PMC6436896.xml'\n",
      "Full text for PMCID PMC6743778 saved to 'PMC_Full_Texts/PMC6743778.xml'\n",
      "Full text for PMCID PMC6929456 saved to 'PMC_Full_Texts/PMC6929456.xml'\n",
      "Full text for PMCID PMC6923882 saved to 'PMC_Full_Texts/PMC6923882.xml'\n",
      "Full text for PMCID PMC6629660 saved to 'PMC_Full_Texts/PMC6629660.xml'\n",
      "Failed to retrieve full text for PMCID PMC6679781. Status code: 404\n",
      "Full text for PMCID PMC6732622 saved to 'PMC_Full_Texts/PMC6732622.xml'\n",
      "Full text for PMCID PMC6817842 saved to 'PMC_Full_Texts/PMC6817842.xml'\n",
      "Failed to retrieve full text for PMCID PMC6510637. Status code: 404\n",
      "Full text for PMCID PMC6386402 saved to 'PMC_Full_Texts/PMC6386402.xml'\n",
      "Full text for PMCID PMC6737184 saved to 'PMC_Full_Texts/PMC6737184.xml'\n",
      "Failed to retrieve full text for PMCID PMC6548586. Status code: 404\n",
      "Full text for PMCID PMC6982787 saved to 'PMC_Full_Texts/PMC6982787.xml'\n",
      "Full text for PMCID PMC6594643 saved to 'PMC_Full_Texts/PMC6594643.xml'\n",
      "Full text for PMCID PMC7297119 saved to 'PMC_Full_Texts/PMC7297119.xml'\n",
      "Failed to retrieve full text for PMCID PMC7073919. Status code: 404\n",
      "Full text for PMCID PMC7648120 saved to 'PMC_Full_Texts/PMC7648120.xml'\n",
      "Full text for PMCID PMC7333383 saved to 'PMC_Full_Texts/PMC7333383.xml'\n",
      "Full text for PMCID PMC7520974 saved to 'PMC_Full_Texts/PMC7520974.xml'\n",
      "Full text for PMCID PMC7054390 saved to 'PMC_Full_Texts/PMC7054390.xml'\n",
      "Full text for PMCID PMC7442807 saved to 'PMC_Full_Texts/PMC7442807.xml'\n",
      "Full text for PMCID PMC7142336 saved to 'PMC_Full_Texts/PMC7142336.xml'\n",
      "Full text for PMCID PMC7162491 saved to 'PMC_Full_Texts/PMC7162491.xml'\n",
      "Full text for PMCID PMC7569858 saved to 'PMC_Full_Texts/PMC7569858.xml'\n",
      "Full text for PMCID PMC7493359 saved to 'PMC_Full_Texts/PMC7493359.xml'\n",
      "Full text for PMCID PMC7721480 saved to 'PMC_Full_Texts/PMC7721480.xml'\n",
      "Full text for PMCID PMC7725002 saved to 'PMC_Full_Texts/PMC7725002.xml'\n",
      "Full text for PMCID PMC7591939 saved to 'PMC_Full_Texts/PMC7591939.xml'\n",
      "Full text for PMCID PMC7237030 saved to 'PMC_Full_Texts/PMC7237030.xml'\n",
      "Full text for PMCID PMC7734183 saved to 'PMC_Full_Texts/PMC7734183.xml'\n",
      "Failed to retrieve full text for PMCID PMC8545175. Status code: 404\n",
      "Full text for PMCID PMC7680913 saved to 'PMC_Full_Texts/PMC7680913.xml'\n",
      "Full text for PMCID PMC7068237 saved to 'PMC_Full_Texts/PMC7068237.xml'\n",
      "Full text for PMCID PMC7406221 saved to 'PMC_Full_Texts/PMC7406221.xml'\n",
      "Full text for PMCID PMC7596958 saved to 'PMC_Full_Texts/PMC7596958.xml'\n",
      "Failed to retrieve full text for PMCID PMC7718328. Status code: 404\n",
      "Full text for PMCID PMC7735824 saved to 'PMC_Full_Texts/PMC7735824.xml'\n",
      "Full text for PMCID PMC7352871 saved to 'PMC_Full_Texts/PMC7352871.xml'\n",
      "Full text for PMCID PMC7933593 saved to 'PMC_Full_Texts/PMC7933593.xml'\n",
      "Full text for PMCID PMC7751093 saved to 'PMC_Full_Texts/PMC7751093.xml'\n",
      "Full text for PMCID PMC7156652 saved to 'PMC_Full_Texts/PMC7156652.xml'\n",
      "Full text for PMCID PMC7794018 saved to 'PMC_Full_Texts/PMC7794018.xml'\n",
      "Full text for PMCID PMC7446623 saved to 'PMC_Full_Texts/PMC7446623.xml'\n",
      "Full text for PMCID PMC7602301 saved to 'PMC_Full_Texts/PMC7602301.xml'\n",
      "Full text for PMCID PMC7682087 saved to 'PMC_Full_Texts/PMC7682087.xml'\n",
      "Full text for PMCID PMC7035778 saved to 'PMC_Full_Texts/PMC7035778.xml'\n",
      "Full text for PMCID PMC8336795 saved to 'PMC_Full_Texts/PMC8336795.xml'\n",
      "Full text for PMCID PMC8225676 saved to 'PMC_Full_Texts/PMC8225676.xml'\n",
      "Full text for PMCID PMC8843059 saved to 'PMC_Full_Texts/PMC8843059.xml'\n",
      "Full text for PMCID PMC8351329 saved to 'PMC_Full_Texts/PMC8351329.xml'\n",
      "Full text for PMCID PMC7816647 saved to 'PMC_Full_Texts/PMC7816647.xml'\n",
      "Full text for PMCID PMC8100175 saved to 'PMC_Full_Texts/PMC8100175.xml'\n",
      "Full text for PMCID PMC8185002 saved to 'PMC_Full_Texts/PMC8185002.xml'\n",
      "Full text for PMCID PMC8067080 saved to 'PMC_Full_Texts/PMC8067080.xml'\n",
      "Full text for PMCID PMC8230313 saved to 'PMC_Full_Texts/PMC8230313.xml'\n",
      "Full text for PMCID PMC8259419 saved to 'PMC_Full_Texts/PMC8259419.xml'\n",
      "Full text for PMCID PMC8182908 saved to 'PMC_Full_Texts/PMC8182908.xml'\n",
      "Full text for PMCID PMC8485143 saved to 'PMC_Full_Texts/PMC8485143.xml'\n",
      "Full text for PMCID PMC7860026 saved to 'PMC_Full_Texts/PMC7860026.xml'\n",
      "Full text for PMCID PMC8204819 saved to 'PMC_Full_Texts/PMC8204819.xml'\n",
      "Full text for PMCID PMC8469072 saved to 'PMC_Full_Texts/PMC8469072.xml'\n",
      "Full text for PMCID PMC7845959 saved to 'PMC_Full_Texts/PMC7845959.xml'\n",
      "Full text for PMCID PMC8385175 saved to 'PMC_Full_Texts/PMC8385175.xml'\n",
      "Full text for PMCID PMC8328792 saved to 'PMC_Full_Texts/PMC8328792.xml'\n",
      "Full text for PMCID PMC8125376 saved to 'PMC_Full_Texts/PMC8125376.xml'\n",
      "Full text for PMCID PMC8100172 saved to 'PMC_Full_Texts/PMC8100172.xml'\n",
      "Full text for PMCID PMC8042551 saved to 'PMC_Full_Texts/PMC8042551.xml'\n",
      "Full text for PMCID PMC8019900 saved to 'PMC_Full_Texts/PMC8019900.xml'\n",
      "Full text for PMCID PMC8192578 saved to 'PMC_Full_Texts/PMC8192578.xml'\n",
      "Full text for PMCID PMC8093828 saved to 'PMC_Full_Texts/PMC8093828.xml'\n",
      "Full text for PMCID PMC8175075 saved to 'PMC_Full_Texts/PMC8175075.xml'\n",
      "Full text for PMCID PMC8261512 saved to 'PMC_Full_Texts/PMC8261512.xml'\n",
      "Full text for PMCID PMC8295265 saved to 'PMC_Full_Texts/PMC8295265.xml'\n",
      "Full text for PMCID PMC8554859 saved to 'PMC_Full_Texts/PMC8554859.xml'\n",
      "Full text for PMCID PMC7876317 saved to 'PMC_Full_Texts/PMC7876317.xml'\n",
      "Full text for PMCID PMC8553134 saved to 'PMC_Full_Texts/PMC8553134.xml'\n",
      "Full text for PMCID PMC8162250 saved to 'PMC_Full_Texts/PMC8162250.xml'\n",
      "Full text for PMCID PMC8352508 saved to 'PMC_Full_Texts/PMC8352508.xml'\n",
      "Full text for PMCID PMC9681730 saved to 'PMC_Full_Texts/PMC9681730.xml'\n",
      "Full text for PMCID PMC9252801 saved to 'PMC_Full_Texts/PMC9252801.xml'\n",
      "Full text for PMCID PMC9329459 saved to 'PMC_Full_Texts/PMC9329459.xml'\n",
      "Full text for PMCID PMC9757591 saved to 'PMC_Full_Texts/PMC9757591.xml'\n",
      "Full text for PMCID PMC10087011 saved to 'PMC_Full_Texts/PMC10087011.xml'\n",
      "Full text for PMCID PMC10776382 saved to 'PMC_Full_Texts/PMC10776382.xml'\n",
      "Full text for PMCID PMC10730818 saved to 'PMC_Full_Texts/PMC10730818.xml'\n",
      "Full text for PMCID PMC9805592 saved to 'PMC_Full_Texts/PMC9805592.xml'\n",
      "Full text for PMCID PMC10684096 saved to 'PMC_Full_Texts/PMC10684096.xml'\n",
      "Full text for PMCID PMC10653424 saved to 'PMC_Full_Texts/PMC10653424.xml'\n",
      "Full text for PMCID PMC10441000 saved to 'PMC_Full_Texts/PMC10441000.xml'\n",
      "Full text for PMCID PMC10541796 saved to 'PMC_Full_Texts/PMC10541796.xml'\n",
      "Full text for PMCID PMC10603766 saved to 'PMC_Full_Texts/PMC10603766.xml'\n",
      "Full text for PMCID PMC10600917 saved to 'PMC_Full_Texts/PMC10600917.xml'\n",
      "Full text for PMCID PMC10673642 saved to 'PMC_Full_Texts/PMC10673642.xml'\n",
      "Full text for PMCID PMC10646871 saved to 'PMC_Full_Texts/PMC10646871.xml'\n",
      "Full text for PMCID PMC10716825 saved to 'PMC_Full_Texts/PMC10716825.xml'\n",
      "Full text for PMCID PMC10821710 saved to 'PMC_Full_Texts/PMC10821710.xml'\n",
      "Full text for PMCID PMC11340644 saved to 'PMC_Full_Texts/PMC11340644.xml'\n",
      "Full text for PMCID PMC10783149 saved to 'PMC_Full_Texts/PMC10783149.xml'\n",
      "Full text for PMCID PMC10940896 saved to 'PMC_Full_Texts/PMC10940896.xml'\n",
      "Failed to retrieve full text for PMCID PMC11034027. Status code: 404\n",
      "Full text for PMCID PMC11258913 saved to 'PMC_Full_Texts/PMC11258913.xml'\n",
      "Full text for PMCID PMC11238428 saved to 'PMC_Full_Texts/PMC11238428.xml'\n",
      "Full text for PMCID PMC11299106 saved to 'PMC_Full_Texts/PMC11299106.xml'\n",
      "Full text for PMCID PMC11423353 saved to 'PMC_Full_Texts/PMC11423353.xml'\n"
     ]
    }
   ],
   "source": [
    "# Download using epmc api the full text using pmcids into folder\n",
    " \n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_name = 'updated_DOME_Registry_Contents.csv'  # Replace with your actual file name\n",
    "\n",
    "# Read in DOME Entries CSV as dataframe via pandas library functions\n",
    "df = pd.read_csv(csv_file_name)\n",
    "\n",
    "# Extract PMCIDs from the DataFrame\n",
    "pmcids = df['mapped_pmcid'].dropna().unique()\n",
    "\n",
    "# Define the output folder for full text files\n",
    "output_folder = 'PMC_Full_Texts'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to download full text for each PMCID using Europe PMC API\n",
    "def download_full_text(pmcids):\n",
    "    for pmcid in pmcids:\n",
    "        url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            full_text = response.text\n",
    "            output_file = os.path.join(output_folder, f\"{pmcid}.xml\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as file:\n",
    "                file.write(full_text)\n",
    "            print(f\"Full text for PMCID {pmcid} saved to '{output_file}'\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve full text for PMCID {pmcid}. Status code: {response.status_code}\")\n",
    "\n",
    "# Download full text for each PMCID\n",
    "download_full_text(pmcids)\n",
    "\n",
    "# print how many successfully downloaded and how many failed to download - TO ADD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import csv\\nimport requests\\nimport os\\n\\n# Define the path to the \"Valid DOME Registry\" CSV file\\nvalid_csv_file_name = \\'valid_DOME_Registry_Contents.csv\\'  # Replace with your actual file name\\n\\n# Define the output folder for the mapped identifiers\\noutput_folder = \\'Mapped_Identifiers\\'\\nos.makedirs(output_folder, exist_ok=True)\\n\\n# Function to read CSV data\\ndef read_csv(file_name):\\n    try:\\n        with open(file_name, \\'r\\', encoding=\\'utf-8\\') as csvfile:\\n            reader = csv.DictReader(csvfile)\\n            data = [row for row in reader]\\n        return data\\n    except Exception as e:\\n        print(f\"Error reading the CSV file: {e}\")\\n        return None\\n\\n# Function to map PMIDs to PMCIDs using NCBI E-utilities API\\ndef map_pmids_to_pmcids(pmids):\\n    pmid_str = \\',\\'.join(pmids)\\n    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&db=pmc&id={pmid_str}&retmode=json\"\\n    response = requests.get(url)\\n    if response.status_code == 200:\\n        data = response.json()\\n        return data\\n    else:\\n        print(f\"Failed to retrieve data. Status code: {response.status_code}\")\\n        return None\\n\\n# Function to save mapped identifiers to a CSV file\\ndef save_mapped_identifiers(mapped_data, output_file_name):\\n    with open(output_file_name, \\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as csvfile:\\n        writer = csv.writer(csvfile)\\n        writer.writerow([\\'PMID\\', \\'PMCID\\'])\\n        for linkset in mapped_data[\\'linksets\\']:\\n            pmid = linkset[\\'ids\\'][0]\\n            if \\'linksetdbs\\' in linkset:\\n                for linksetdb in linkset[\\'linksetdbs\\']:\\n                    if linksetdb[\\'dbto\\'] == \\'pmc\\':\\n                        for link in linksetdb[\\'links\\']:\\n                            writer.writerow([pmid, link])\\n            else:\\n                writer.writerow([pmid, \\'\\'])\\n\\n# Read CSV data\\ncsv_data = read_csv(valid_csv_file_name)\\n\\n# Extract PMIDs from the CSV data\\npmids = [row.get(\\'publication_pmid\\', \\'\\') for row in csv_data if row.get(\\'publication_pmid\\', \\'\\')]\\n\\n# Map PMIDs to PMCIDs\\nmapped_data = map_pmids_to_pmcids(pmids)\\n\\n# Save the mapped identifiers to a CSV file\\nif mapped_data:\\n    output_file_name = os.path.join(output_folder, \\'mapped_identifiers.csv\\')\\n    save_mapped_identifiers(mapped_data, output_file_name)\\n    print(f\"Mapped identifiers saved to \\'{output_file_name}\\'\")\\nelse:\\n    print(\"No data to process.\")\\n    '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metadata file readout as CSV and text file to explain contents and graph visualisation of data validation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
