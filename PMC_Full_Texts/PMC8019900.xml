<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Oncol</journal-id><journal-id journal-id-type="iso-abbrev">Front Oncol</journal-id><journal-id journal-id-type="publisher-id">Front. Oncol.</journal-id><journal-title-group><journal-title>Frontiers in Oncology</journal-title></journal-title-group><issn pub-type="epub">2234-943X</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">8019900</article-id><article-id pub-id-type="pmid">33828982</article-id><article-id pub-id-type="doi">10.3389/fonc.2021.629321</article-id><article-categories><subj-group subj-group-type="heading"><subject>Oncology</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Improving the Prediction of Benign or Malignant Breast Masses Using a Combination of Image Biomarkers and Clinical Parameters</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cui</surname><given-names>Yanhua</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1137043/overview"/></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yun</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="author-notes" rid="fn002"><sup>&#x02020;</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1259909/overview"/></contrib><contrib contrib-type="author"><name><surname>Xing</surname><given-names>Dong</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1259929/overview"/></contrib><contrib contrib-type="author"><name><surname>Bai</surname><given-names>Tong</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Jiwen</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/729796/overview"/></contrib><contrib contrib-type="author"><name><surname>Zhu</surname><given-names>Jian</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref><xref ref-type="aff" rid="aff6"><sup>6</sup></xref><xref ref-type="aff" rid="aff7"><sup>7</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Department of Radiation Oncology Physics and Technology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences</institution>, <addr-line>Jinan</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>Department of Radiology, Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences</institution>, <addr-line>Jinan</addr-line>, <country>China</country></aff><aff id="aff3"><sup>3</sup><institution>Department of Radiology, Yantai Yuhuangding Hospital</institution>, <addr-line>Yantai</addr-line>, <country>China</country></aff><aff id="aff4"><sup>4</sup><institution>Shandong Provincial Key Laboratory of Network based Intelligent Computing, School of Information Science and Engineering, University of Jinan</institution>, <addr-line>Jinan</addr-line>, <country>China</country></aff><aff id="aff5"><sup>5</sup><institution>Shandong Medical Imaging and Radiotherapy Engineering Technology Research Center</institution>, <addr-line>Jinan</addr-line>, <country>China</country></aff><aff id="aff6"><sup>6</sup><institution>Shandong College Collaborative Innovation Center of Digital Medicine Clinical Treatment and Nutrition Health</institution>, <addr-line>Qingdao</addr-line>, <country>China</country></aff><aff id="aff7"><sup>7</sup><institution>Shandong Provincial Key Laboratory of Digital Medicine and Computer-Assisted Surgery</institution>, <addr-line>Qingdao</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Hong Huang, Chongqing University, China</p></fn><fn fn-type="edited-by"><p>Reviewed by: Surendiran Balasubramanian, National Institute of Technology Puducherry, India; Guolin Ma, China-Japan Friendship Hospital, China</p></fn><corresp id="c001">*Correspondence: Jian Zhu <email>zhujian.cn@163.com</email></corresp><fn fn-type="other" id="fn001"><p>This article was submitted to Cancer Imaging and Image-directed Interventions, a section of the journal Frontiers in Oncology</p></fn><fn fn-type="other" id="fn002"><p>&#x02020;These authors have contributed equally to this work</p></fn></author-notes><pub-date pub-type="epub"><day>22</day><month>3</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>11</volume><elocation-id>629321</elocation-id><history><date date-type="received"><day>14</day><month>11</month><year>2020</year></date><date date-type="accepted"><day>22</day><month>2</month><year>2021</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2021 Cui, Li, Xing, Bai, Dong and Zhu.</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Cui, Li, Xing, Bai, Dong and Zhu</copyright-holder><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p><bold>Background:</bold> Breast cancer is one of the leading causes of death in female cancer patients. The disease can be detected early using Mammography, an effective X-ray imaging technology. The most important step in mammography is the classification of mammogram patches as benign or malignant. Classically, benign or malignant breast tumors are diagnosed by radiologists' interpretation of mammograms based on clinical parameters. However, because masses are heterogeneous, clinical parameters supply limited information on mammography mass. Therefore, this study aimed to predict benign or malignant breast masses using a combination of image biomarkers and clinical parameters.</p><p><bold>Methods:</bold> We trained a deep learning (DL) fusion network of VGG16 and Inception-V3 network in 5,996 mammography images from the training cohort; DL features were extracted from the second fully connected layer of the DL fusion network. We then developed a combined model incorporating DL features, hand-crafted features, and clinical parameters to predict benign or malignant breast masses. The prediction performance was compared between clinical parameters and the combination of the above features. The strengths of the clinical model and the combined model were subsequently validated in a test cohort (<italic>n</italic> = 244) and an external validation cohort (<italic>n</italic> = 100), respectively.</p><p><bold>Results:</bold> Extracted features comprised 30 hand-crafted features, 27 DL features, and 5 clinical features (shape, margin type, breast composition, age, mass size). The model combining the three feature types yielded the best performance in predicting benign or malignant masses (AUC = 0.961) in the test cohort. A significant difference in the predictive performance between the combined model and the clinical model was observed in an independent external validation cohort (AUC: 0.973 vs. 0.911, p = 0.019).</p><p><bold>Conclusion:</bold> The prediction of benign or malignant breast masses improves when image biomarkers and clinical parameters are combined; the combined model was more robust than clinical parameters alone.</p></abstract><kwd-group><kwd>mammography</kwd><kwd>image feature</kwd><kwd>deep learning</kwd><kwd>clinical prediction</kwd><kwd>radiomics</kwd></kwd-group><funding-group><award-group><funding-source id="cn001">National Key Research and Development Program of China<named-content content-type="fundref-id">10.13039/501100012166</named-content></funding-source></award-group><award-group><funding-source id="cn002">Natural Science Foundation of Shandong Province<named-content content-type="fundref-id">10.13039/501100007129</named-content></funding-source></award-group></funding-group><counts><fig-count count="4"/><table-count count="5"/><equation-count count="0"/><ref-count count="34"/><page-count count="8"/><word-count count="4960"/></counts></article-meta></front><body><sec sec-type="intro" id="s1"><title>Introduction</title><p>Breast cancer is one of the leading causes of death in female cancer patients. Early diagnosis of the condition is crucial to improve the survival rate and relieve suffering in patients (<xref rid="B1" ref-type="bibr">1</xref>). Mammography is an effective X-ray imaging technology that detects breast cancer early. Classically, benign or malignant breast tumors are diagnosed by radiologists' interpretation of mammograms based on clinical parameters. However, because masses are heterogeneous, clinical parameters supply limited information on mammography mass (<xref rid="B2" ref-type="bibr">2</xref>). There is, therefore, an urgent need to find new tools that can identify patients with breast cancer.</p><p>Machine learning (<xref rid="B3" ref-type="bibr">3</xref>) from artificial intelligence (AI) has made progress in automatically quantifying the characteristics of masses (<xref rid="B4" ref-type="bibr">4</xref>). Radiomics is an emerging field in quantitative imaging; it is a method that uses machine learning to transform images into high-dimensional and minable feature data (<xref rid="B5" ref-type="bibr">5</xref>, <xref rid="B6" ref-type="bibr">6</xref>). With radiomics, clinical decision support can be improved. Exploratory research using this method has shown great promise in the diagnosis of breast masses (<xref rid="B7" ref-type="bibr">7</xref>). Radiomics can quantify large-scale information extracted from mammography images, which makes it a tool with better diagnostic capabilities for benign and malignant breast masses, and this method also provides radiologists with supplementary data (<xref rid="B8" ref-type="bibr">8</xref>). Analysis by radiomics requires machine learning methods with high levels of robustness and statistical power. This extraction method continues to be developed to improve its performance in evaluating masses, and this improvement, in turn, assists radiologists in accurately interpreting mammography imaging.</p><p>Hand-crafted-based radiomics extracts low-level features (texture features and shape features) as image biomarkers and estimate the likelihood of malignant masses based on extracted image biomarkers (<xref rid="B9" ref-type="bibr">9</xref>&#x02013;<xref rid="B11" ref-type="bibr">11</xref>). In recent years, there has been significant progress on the subject of deep learning (<xref rid="B12" ref-type="bibr">12</xref>) (DL) and computer vision, with DL radiomics attaining remarkable heights in various medical imaging applications (<xref rid="B13" ref-type="bibr">13</xref>&#x02013;<xref rid="B15" ref-type="bibr">15</xref>); DL directly learns unintuitive hidden features from images. DL features acquire more information and superior performance than hand-crafted image features (<xref rid="B16" ref-type="bibr">16</xref>). DL has only been used in a few studies in the field of mammography automatic diagnosis (<xref rid="B17" ref-type="bibr">17</xref>). Classifying benign or malignant masses, as compared to normal and abnormal areas, for the lack of obvious features is more complex. With the shift from hand-crafted to DL-based radiomics, combining deep learning and hand-crafted features have become more popular in radiomics most recently (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B19" ref-type="bibr">19</xref>).</p><p>In this study, we explore a DL fusion network of two different transfer-learning models combined with data augmentation, aimed at improving the classification accuracy. We hypothesized that image biomarkers (DL features and hand-crafted features) and clinical parameters could express intrinsic information on mass thoroughly when combined. We built a classification model that combines image biomarkers with clinical parameters and called it a combined model. Using clinical characteristics as the diagnostic information from mammography, we sought to determine the classification performance of the combined model and its clinical predictor. We evaluated predictive performance in two validation cohorts: the absence of mammography in the training cohort as the test cohort (inner-validation) and mammography from other hospitals as the external validation cohort.</p></sec><sec sec-type="materials and methods" id="s2"><title>Materials and Methods</title><sec><title>Patients</title><p>Mammography produces two images on both Cranio-Caudal (CC) views and the Medio-Lateral Oblique (MLO) (<xref ref-type="fig" rid="F1">Figure 1</xref>). Five hundred and twenty-four patients were enrolled prospectively (confirmed by pathology) with digital mammography masses, including 988 mammography images (malignant: 494, benign: 494). Inclusion criteria: mammography images classified as Breast imaging reporting and data system (BI-RADS) 3, 4, and 5; BI-RADS 3 means probably benign, BI-RADS 4 means suspected malignancy, and BI-RADS 5 means highly suspected malignancy (<xref rid="B20" ref-type="bibr">20</xref>). Mass areas were labeled on MLO and CC views, respectively, in rectangular frames. Images were saved as 2-dimensional Digital Imaging and Communications in Medicine (DICOM) files with a 16-bit gray level.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>(A)</bold> Example cases on CC and MLO views. The yellow square represents the suspicious area labeled by the radiologist. <bold>(B)</bold> 8 benign and 8 malignant masses.</p></caption><graphic xlink:href="fonc-11-629321-g0001"/></fig></sec><sec><title>Data Preprocessing</title><p>All mammography images were preprocessed per the steps below:</p><p>Step 1: background removal. Regions of interest (ROIs) were obtained using a cropping operation on the mammography in order to remove the unnecessary black background.</p><p>Step 2: image normalization. ROIs were converted to a range [0, 1] with the linear function below (Func.1), which revealed that the original data were scaled in proportion. X_(norm) normalized data, X is the original data of mammography image, X_(max) and X_(min) are the maximum and minimum values of the original data, respectively.</p><p>X_(norm) = (X-X_(min))/(X_(max)-X_(min)) (Func.1).</p><p>Step 3: ROI size normalization. To meet standard input dimension requirements for most CNN, zero-filled images were achieved under no deformation conditions, and adjusted to 224 &#x000d7; 224 (The right of <xref ref-type="fig" rid="F1">Figure 1</xref>).</p><p>Step 4: data sets separation. Training cohort (<italic>n</italic> = 744) and test cohort (<italic>n</italic> = 244) were created <italic>via</italic> random splitting. ROIs of test cohort were not enrolled into the training cohort.</p><p>Step 5: data augmentation. For each ROI in the training cohort, we used a combination of flipping and rotation transformations (90, 180, and 270 degrees), aiming at generating seven new label-preserving samples.</p></sec><sec><title>Transfer-Learning and DL Fusion Network</title><p>DL architectures have three main components including convolutional layer, pooling layer, and fully connected (FC) layer. It is assumed that transfer of such sets with some fine tuning for the target network would be robust. Therefore, Vgg16 (<xref rid="B21" ref-type="bibr">21</xref>) network-based transfer-learning was used for this study. The VGG16 network has been pre-trained on the ImageNet dataset (<xref rid="B22" ref-type="bibr">22</xref>). Learned weights of the network gained during pre-training were applied to the target network. We proposed a DL fusion network combining the Vgg16 and Inception-V3 (<xref rid="B23" ref-type="bibr">23</xref>) networks based on transfer-learning aimed at strengthening the ability of transfer-learning. The learned weights of the network were transferred to the DL fusion network shown in <xref ref-type="fig" rid="F2">Figure 2A</xref>. GlobalMaxPooling was used separately on the two networks to retain more information. The two networks were connected, and three FC layers were added to the fusion network. Additionally, the robustness of the DL fusion network was compared with that of the Vgg16 network.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p>Framework of the proposed model structure. <bold>(A)</bold> DL fusion network, <bold>(B)</bold> combined model. FC, fully connected; SVM, Support Vector Machine; DL, deep learning.</p></caption><graphic xlink:href="fonc-11-629321-g0002"/></fig><p>The architecture of the Vgg16 fine-tuned network is shown in <xref rid="T1" ref-type="table">Table 1</xref>. The input layer of the image consisted of three parts: width, height, and channel. The input image size was 224 &#x000d7; 224 &#x000d7; 3. The number of layers after the first 12 layers were used for training. The epoch and the learning rate of the network were set to 200 and 1e-4, respectively. The Stochastic Gradient Descent (SGD) was used as the optimization algorithm (<xref rid="B24" ref-type="bibr">24</xref>). The momentum was set at 0.9, and the weight decay was set at 5 &#x000d7; 10<sup>&#x02212;4</sup>. The fully connected layer was regularized using the dropout (<xref rid="B25" ref-type="bibr">25</xref>), with the last layer corresponding to the soft-max classifier. DL fusion network parameter settings referred to the VGG16 fine-tuned network. We performed a simulation of the python environment. The DL network training was performed on one GeForce GTX 1080Ti GPU.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>CNN network structure parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Vgg16 fine tuning</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Out</bold></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>learning layer type</bold></th><th rowspan="1" colspan="1"/></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Conv1-2</td><td valign="top" align="center" rowspan="1" colspan="1">(224 &#x000d7; 224 &#x000d7; 64)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Max_Pooling</td><td valign="top" align="center" rowspan="1" colspan="1">(112 &#x000d7; 112 &#x000d7; 64)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Conv3-4</td><td valign="top" align="center" rowspan="1" colspan="1">(112 &#x000d7; 112 &#x000d7; 128)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Max_Pooling</td><td valign="top" align="center" rowspan="1" colspan="1">(56 &#x000d7; 56 &#x000d7; 128)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Conv5-7</td><td valign="top" align="center" rowspan="1" colspan="1">(56 &#x000d7; 56 &#x000d7; 256)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Max_Pooling</td><td valign="top" align="center" rowspan="1" colspan="1">(28 &#x000d7; 28 &#x000d7; 256)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Conv8-10</td><td valign="top" align="center" rowspan="1" colspan="1">(28 &#x000d7; 28 &#x000d7; 512)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Max_Pooling</td><td valign="top" align="center" rowspan="1" colspan="1">(14 &#x000d7; 14 &#x000d7; 512)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Conv11-13</td><td valign="top" align="center" rowspan="1" colspan="1">(14 &#x000d7; 14 &#x000d7; 512)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Max_Pooling</td><td valign="top" align="center" rowspan="1" colspan="1">(7 &#x000d7; 7 &#x000d7; 512)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">GAP</td><td valign="top" align="center" rowspan="1" colspan="1">(1 &#x000d7; 1 &#x000d7; 512)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FC_1</td><td valign="top" align="center" rowspan="1" colspan="1">(1 &#x000d7; 1 &#x000d7; 1024)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Dropout</td><td valign="top" align="center" rowspan="1" colspan="1">(1 &#x000d7; 1 &#x000d7; 1024)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">FC_2</td><td valign="top" align="center" rowspan="1" colspan="1">(1 &#x000d7; 1 &#x000d7; 2)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Soft-max output</td><td valign="top" align="center" rowspan="1" colspan="1">P</td></tr></tbody></table><table-wrap-foot><p><italic>Conv, convolutional layer; GAP, Global Average Pooling; FC, fully connected; P, Probability</italic>.</p></table-wrap-foot></table-wrap></sec><sec><title>Feature Extraction</title><sec><title>Handcrafted-Based Features</title><p>Texture contains important information from many types of images (<xref rid="B26" ref-type="bibr">26</xref>), and this information was used for classification and analysis. Four different types of hand-crafted features are extracted separately from first-order histogram features, second-order texture features, Hu's moment invariants features, and high-order Gabor features (a total of 455 features). The second-order texture features include gray-level co-occurrence matrix (GLCM), gray-gradient co-occurrence matrix (GLGCM), gray-level difference statistics (GLDS), gray run-length matrix (GLRLM), local binary pattern (LBP), and Gaussian Markov random field (GMRF) features. Hand-crafted feature extraction algorithms were implemented on MatLab 2018a.</p></sec><sec><title>Clinical Features</title><p>The clinical features of the patients are shown in <xref rid="T2" ref-type="table">Table 2</xref>. Morphological descriptions of mass are encoded as numerical values to obtain true feature values.</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Clinical features description of patients.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Clinical features</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Feature coding</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Shape</td><td valign="top" align="left" rowspan="1" colspan="1">1-round, 2-oval, 3-irregular</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Margin type</td><td valign="top" align="left" rowspan="1" colspan="1">1-clear, 2-shadow, 3-differential leaf, 4-fuzzy, 5-glitch</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Breast composition</td><td valign="top" align="left" rowspan="1" colspan="1">1-The breasts are almost entirely fatty.</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">2-There are scattered areas of fibro glandular density.</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">3-The breasts are heterogeneously dense, which may obscure small masses.</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">4-The breasts are extremely dense, which lowers the sensitivity of mammography.</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Age</td><td valign="top" align="left" rowspan="1" colspan="1">20&#x02013;80 years old</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Mass size</td><td valign="top" align="left" rowspan="1" colspan="1">The diagonal pixel values of the ROI extracted can be roughly used as a method of measuring the size of the mass.</td></tr></tbody></table></table-wrap></sec><sec><title>DL-Based Features</title><p>A trained DL model can be used as a feature extractor to extract features of different layers in the model. We proposed a DL fusion network to extract deep feature information on masses. The DL fusion network converts the image of the mass into a 1024-dimensional feature vector. In this study, we referred to this high dimensional vector from the second FC layer of the network as the DL feature.</p></sec></sec><sec><title>Feature Selection</title><p>Feature selection is another key step in radiomics, which means selecting a subset of relevant features based on the evaluation criterion. To reduce the training time of the model and improve its robustness and reliability, we used the minimal-redundancy-maximal-relevance (mRMR) (<xref rid="B27" ref-type="bibr">27</xref>) method to select the most significant feature sets. Through feature selection, 30 hand-crafted features (shown in <xref rid="T3" ref-type="table">Table 3</xref>) and 27 DL features were selected for input into the classifier.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Hand crafted-based radiomics features after feature selection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Texture type</bold></th><th valign="top" align="left" rowspan="1" colspan="1"><bold>Texture descriptors</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Number of</bold></th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1"><bold>features selected</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Second-order</td><td valign="top" align="left" rowspan="1" colspan="1">Gray gradient co-occurrence</td><td valign="top" align="center" rowspan="1" colspan="1">1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">texture features</td><td valign="top" align="left" rowspan="1" colspan="1">matrix features</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Gray run-length matrix</td><td valign="top" align="center" rowspan="1" colspan="1">2</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Gaussian Markov random field</td><td valign="top" align="center" rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Gray-level difference statistics</td><td valign="top" align="center" rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1"/><td valign="top" align="left" rowspan="1" colspan="1">Local binary pattern</td><td valign="top" align="center" rowspan="1" colspan="1">1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Higher-order</td><td valign="top" align="left" rowspan="1" colspan="1">Gabor features</td><td valign="top" align="center" rowspan="1" colspan="1">21</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">features</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr></tbody></table></table-wrap></sec><sec><title>Model Construction</title><p>Because clinical features and mammography imaging express different types of information of a mass, we combined two types of information for exploratory analysis. Image biomarkers and clinical parameters were then processed using Min-Max normalization (as shown in <xref ref-type="fig" rid="F2">Figure 2B</xref>). The support vector machine (SVM) (<xref rid="B28" ref-type="bibr">28</xref>, <xref rid="B29" ref-type="bibr">29</xref>) with a linear kernel was used in the classification of breast masses. The SVM model aims to provide an efficient calculation method of learning by separating hyperplanes in a high dimensional feature space. A systematic review of machine learning techniques revealed that the SVM model is widely applied in breast tissue classification (<xref rid="B30" ref-type="bibr">30</xref>). In this study, the SVM hyper-parameters were fine-tuned through an internal grid search with 10-fold cross-validation.</p></sec><sec><title>Training, Testing, and External Validation</title><p>We trained the proposed model using data (image biomarkers and clinical biomarkers) from the training set (744 ROIs). The prediction performance and model stability of the clinical model and the combined model were evaluated in the test set (244 ROIs) and verified in the external validation set (100 ROIs from 58 patients). The 58 patients in the external validation set came from the Yantai Yuhuangding Hospital.</p></sec><sec><title>Evaluating Predictive Performance</title><p>Verifying the stability of the generated model using corresponding evaluation indicators is a key step to evaluating predictive performance. We established a confusion matrix to evaluate the proposed approach. We calculated the AUC (areas under the curve), accuracy, sensitivity, specificity, precision, and F_score from the confusion matrix to estimate the discriminant performance and stability of these models. Delong's test (<xref rid="B31" ref-type="bibr">31</xref>) was performed to evaluate the statistical significance of the AUC of the results. <italic>P</italic> &#x0003c; 0.05 was considered significant.</p></sec></sec><sec sec-type="results" id="s3"><title>Results</title><p>A total of 744 and 244 ROIs were randomly selected for the training cohort and test cohort, respectively. <xref ref-type="fig" rid="F3">Figure 3</xref> presents the convergence process and the training result of the Vgg16 fine-tuned network and the DL fusion network. The loss of the Vgg16 fine-tuned network fluctuated considerably for the worse convergence. The DL fusion network yielded better performances, as illustrated in <xref rid="T4" ref-type="table">Table 4</xref>. The accuracy of the DL fusion network improved to 87.30%, a 0.83% increase, compared to the Vgg16 fine-tuned network.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Loss and accuracy over epochs of training/ validation process. <bold>(A)</bold> Vgg16 fine-tuned network, <bold>(B)</bold> DL fusion network.</p></caption><graphic xlink:href="fonc-11-629321-g0003"/></fig><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Classification performance of Vgg16 fine-tuned network and DL fusion network.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Trained networks</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Sensitivity (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Specificity (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Accuracy (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>F-score</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Vgg16 fine-tuned</td><td valign="top" align="center" rowspan="1" colspan="1">82.79</td><td valign="top" align="center" rowspan="1" colspan="1">90.16</td><td valign="top" align="center" rowspan="1" colspan="1">86.47</td><td valign="top" align="center" rowspan="1" colspan="1">85.96</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">network</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">DL fusion</td><td valign="top" align="center" rowspan="1" colspan="1">84.43</td><td valign="top" align="center" rowspan="1" colspan="1">90.16</td><td valign="top" align="center" rowspan="1" colspan="1">87.30</td><td valign="top" align="center" rowspan="1" colspan="1">86.92</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">network</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr></tbody></table></table-wrap><p>Using the mRMR feature selection method, 30 hand-crafted features (9 texture features, 21 higher-order features) and 27 DL features (1024 reduced to 27 dimensions) were selected (<xref rid="T3" ref-type="table">Table 3</xref>). A comparative view of seven feature combination schemes used for the classification of SVM is illustrated in <xref rid="T5" ref-type="table">Table 5</xref>, while the ROC curves for the evaluated representations of the seven schemes in the test cohort are shown in <xref ref-type="fig" rid="F4">Figure 4A</xref>.</p><table-wrap id="T5" position="float"><label>Table 5</label><caption><p>Classification performance of different feature combination schemes in test cohort and validation cohort.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1"><bold>Feature combination schemes</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Sp (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Acc (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Sn (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>Pre (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>F-score (%)</bold></th><th valign="top" align="center" rowspan="1" colspan="1"><bold>AUC</bold></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Hcr</td><td valign="top" align="center" rowspan="1" colspan="1">83.61</td><td valign="top" align="center" rowspan="1" colspan="1">79.92</td><td valign="top" align="center" rowspan="1" colspan="1">76.23</td><td valign="top" align="center" rowspan="1" colspan="1">82.30</td><td valign="top" align="center" rowspan="1" colspan="1">79.15</td><td valign="top" align="center" rowspan="1" colspan="1">90.06</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Clinical</td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M1"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>88.52</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M2"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>88.93</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M3"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>89.34</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M4"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>89.61</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M5"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>88.98</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M6"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>94.48</mml:mtext></mml:mstyle></mml:math></inline-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Hcr+Clinical</td><td valign="top" align="center" rowspan="1" colspan="1">90.16</td><td valign="top" align="center" rowspan="1" colspan="1">89.34</td><td valign="top" align="center" rowspan="1" colspan="1">88.52</td><td valign="top" align="center" rowspan="1" colspan="1">90.00</td><td valign="top" align="center" rowspan="1" colspan="1">89.26</td><td valign="top" align="center" rowspan="1" colspan="1">95.99</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Deep 27</td><td valign="top" align="center" rowspan="1" colspan="1">90.16</td><td valign="top" align="center" rowspan="1" colspan="1">88.11</td><td valign="top" align="center" rowspan="1" colspan="1">86.07</td><td valign="top" align="center" rowspan="1" colspan="1">89.74</td><td valign="top" align="center" rowspan="1" colspan="1">87.87</td><td valign="top" align="center" rowspan="1" colspan="1">93.95</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Deep 27+Hcr</td><td valign="top" align="center" rowspan="1" colspan="1">90.98</td><td valign="top" align="center" rowspan="1" colspan="1">87.70</td><td valign="top" align="center" rowspan="1" colspan="1">84.43</td><td valign="top" align="center" rowspan="1" colspan="1">90.35</td><td valign="top" align="center" rowspan="1" colspan="1">87.29</td><td valign="top" align="center" rowspan="1" colspan="1">94.28</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Deep 27+Clinical</td><td valign="top" align="center" rowspan="1" colspan="1">91.45</td><td valign="top" align="center" rowspan="1" colspan="1">89.75</td><td valign="top" align="center" rowspan="1" colspan="1">87.70</td><td valign="top" align="center" rowspan="1" colspan="1">91.80</td><td valign="top" align="center" rowspan="1" colspan="1">89.54</td><td valign="top" align="center" rowspan="1" colspan="1">95.53</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Deep 27+Clinical+Hcr</td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M7"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>93.44</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M8"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>91.00</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M9"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>88.53</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M10"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>93.10</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M11"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>90.76</mml:mtext></mml:mstyle></mml:math></inline-formula></td><td valign="top" align="center" rowspan="1" colspan="1"><inline-formula><mml:math id="M12"><mml:mstyle mathvariant="monospace" class="text" mathcolor="#ee1c23"><mml:mtext>96.16</mml:mtext></mml:mstyle></mml:math></inline-formula></td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">clinical model</td><td valign="top" align="center" rowspan="1" colspan="1">94.00</td><td valign="top" align="center" rowspan="1" colspan="1">83.00</td><td valign="top" align="center" rowspan="1" colspan="1">72.00</td><td valign="top" align="center" rowspan="1" colspan="1">92.31</td><td valign="top" align="center" rowspan="1" colspan="1">80.90</td><td valign="top" align="center" rowspan="1" colspan="1">91.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">combined model</td><td valign="top" align="center" rowspan="1" colspan="1">100</td><td valign="top" align="center" rowspan="1" colspan="1">90.00</td><td valign="top" align="center" rowspan="1" colspan="1">80.00</td><td valign="top" align="center" rowspan="1" colspan="1">100</td><td valign="top" align="center" rowspan="1" colspan="1">88.89</td><td valign="top" align="center" rowspan="1" colspan="1">97.32</td></tr></tbody></table><table-wrap-foot><p><italic>Deep 27, 27 deep learning (DL) features; Sp, specificity; Acc, accuracy; Sn, Sensitivity; Pre, Precision; Hcr, hand crafted-based radiomics features; AUC, area under the receiver operating characteristic curve. Clinical model and combined model are validated in the validation cohort (n = 100)</italic>.</p></table-wrap-foot></table-wrap><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>(A)</bold> ROC curve for evaluated predictive performance of seven methods in test cohort. <bold>(B)</bold> ROC curve for evaluated predictive performance of the external validation (EV) set. Deep represents 27 deep learning (DL) features. Hcr, hand crafted-based radiomics features; Cli, Clinical.</p></caption><graphic xlink:href="fonc-11-629321-g0004"/></fig><p>In the test cohort, the clinical model attained a classification accuracy of 0.889, a specificity of 0.885, and an AUC of 0.944. Compared with clinical models or other models, the progress made by the combined model in discriminative performance was more significant (accuracy = 0.910, specificity = 0.934, AUC = 0.962). The accuracy of the combined model rose by 3&#x02013;11%, compared to models with a standalone image feature.</p><p>In the external validation set, the combined model was also proven to have better robustness and reliability (<xref rid="T5" ref-type="table">Table 5</xref>). The combined model yielded an improved accuracy and AUC, compared to the clinical model (accuracy: 0.900 vs. 0.830; specificity: 1.000 vs. 0.940; AUC: 0.973 vs. 0.911; <italic>P</italic> = 0.019). The ROC curves of the two models in the external validation cohort are shown in <xref ref-type="fig" rid="F4">Figure 4B</xref>.</p></sec><sec sec-type="discussion" id="s4"><title>Discussion</title><p>We conducted this study to develop an approach that combines radiomics features (Handcrafted-based and deep learning-based features) with clinical parameters for the assessment of the effects of classification in clinical practice. We also sought to reveal the classification performances of both the combined model and clinical parameters. As a result, we demonstrated the significance of combining image biomarkers with clinical parameters. Additionally, we observed a significant difference between the combined model and the clinical model; the former was more robust than the latter.</p><p>Interpreting the prediction performance of the combined model is not easy and must be done with caution to avoid drawing shallow conclusions. As shown by our results, the predictive performance improved when clinical data were added. The results of the combined model were improvements on those of the clinical model or other models, as illustrated in <xref rid="T5" ref-type="table">Table 5</xref>. Moreover, in the external validation cohort, the prediction performances of the combined model for benign and malignant masses were better than those for the clinical model (accuracy: 0.900 vs. 0.830; specificity: 1.000 vs. 0.940; AUC: 0.973 vs. 0.911; <italic>P</italic> = 0.019). There are potentially two major reasons for this outcome: first, the DL network design. The DL fusion network tries to encode breast mass images into deep features reflecting the internal information of masses. The neural network extracted abstract and complex features from the convolutional layers to the FC layers; second, clinical parameters are descriptive and distinguishable as a reference for BIRADS classification, which makes the results acceptable. But, because of the heterogeneity of breast masses, clinical parameters can indicate only limited mass information; the combined model carries information on intra-tumor heterogeneity, capturing the spatial relationships between neighboring pixels. Thus, performance largely depends on the ability of image biomarkers to distinguish between benign and malignant lesions.</p><p>Past studies have documented radiomics features' representation of valuable information from mass images (<xref rid="B9" ref-type="bibr">9</xref>&#x02013;<xref rid="B11" ref-type="bibr">11</xref>). Radiomics features have been widely identified as reliable and useful biomarkers in clinical practice (<xref rid="B8" ref-type="bibr">8</xref>). The final goal of this extraction method is to generate image biomarkers to build a model for the improvement of clinical decisions. With the shift from hand-crafted to DL-based radiomics, combining deep learning features with hand-crafted features has become a popular approach in radiomics most recently (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B19" ref-type="bibr">19</xref>). The importance of clinical parameters has been reported in an experimental study by Moura et al. (<xref rid="B32" ref-type="bibr">32</xref>). Our current findings are based on expanding these results and prior works. To do this, we quantified the characteristics of mass imaging from many aspects using data-characterization algorithms. We extracted five clinical parameters, 1024 DL, and 455 hand-crafted features from each ROI. For deep learning feature extraction, we established the DL fusion network by transfer-learning. We trained the network through a patch-based strategy. In the past, superior performances have been achieved in a pre-trained network, compared to training from scratch (<xref rid="B33" ref-type="bibr">33</xref>), primarily because network training from scratch is too complicated and prone to over-fitting for small datasets (<xref rid="B34" ref-type="bibr">34</xref>). Hand-crafted features likely played a role in texture characterization. Redundant features were removed using mRMR, and features that can reflect the essential meaningful features of masses were retained. The SVM classification method was also chosen for comparative analysis.</p><p>Our research had three main advantages vis-&#x000e0;-vis previous studies using radiomics (<xref rid="B18" ref-type="bibr">18</xref>, <xref rid="B32" ref-type="bibr">32</xref>, <xref rid="B33" ref-type="bibr">33</xref>). First, we used a DL fusion network for feature extraction. DL fusion network can learn the intrinsic characteristics of mass images automatically from imaging data. Therefore, the DL fusion network does not need hand-coded feature extraction. Second, we combined image biomarkers with clinical parameters to assess the effects of the classification in clinical practice. Finally, we used an external validation set, which allowed us to extend the experimental results to other institutions and environments, providing more credibility to our inference.</p><p>Despite the promising outcome of this investigation, we had some limitations. The specific characteristic difference between the convolutional layer and the FC layer was not explored. Furthermore, because of the few medical image datasets, the model validation cohort in this study did not reach an optimal level. In future work, we intend to use more samples from other publicly available datasets, such as Mammographic Image Analysis Society (MIAS) and Database for Screening Mammography (DDSM) datasets. This will provide data diversity in terms of feature representation and may also improve overall architecture and network performance. Additionally, we plan to explore the predictive performance of different layer features.</p><p>In conclusion, combining radiomics features with clinical parameters can potentially serve a role in the prediction of benign or malignant breast masses. Additionally, this combination has stronger prediction performance, compared with clinical parameters. This study, therefore, developed a strategy that combines deep learning with traditional machine learning approaches to assist radiologists in interpreting breast images.</p></sec><sec sec-type="data-availability" id="s5"><title>Data Availability Statement</title><p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p></sec><sec id="s6"><title>Author Contributions</title><p>JZ, YC, and JD conceived and designed the experiments. YC performed the experiments. JZ, YL, TB, and DX analyzed the data. YC, JZ, and YL participated in writing manuscript. The final version of the manuscript has been reviewed and approved for publication by all author.</p></sec><sec sec-type="COI-statement" id="conf1"><title>Conflict of Interest</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This work was supported by the Shandong Provincial Natural Science Foundation (ZR2016HQ09, ZR2020LZL001), the National Natural Science Foundation of China (Grant Numbers: 81671785, 81530060, and 81874224), the National Key Research and Develop Program of China (Grant Number: 2016YFC0105106), the Foundation of Taishan Scholars (No.tsqn201909140, ts20120505), the Academic promotion program of Shandong First Medical University (2020RC003 and 2019LJ004) and the Shandong Provincial Natural Science Foundation (ZR2016HQ09).</p></fn></fn-group><ref-list><title>References</title><ref id="B1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SJ</given-names></name><name><surname>Glassgow</surname><given-names>AE</given-names></name><name><surname>Watson</surname><given-names>KS</given-names></name><name><surname>Molina</surname><given-names>Y</given-names></name><name><surname>Calhoun</surname><given-names>EA</given-names></name></person-group>. <article-title>Gendered and racialized social expectations, barriers, and delayed breast cancer diagnosis</article-title>. <source>Cancer</source>. (<year>2018</year>) <volume>124</volume>:<fpage>4350</fpage>&#x02013;<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1002/cncr.31636</pub-id><?supplied-pmid 30246241?><pub-id pub-id-type="pmid">30246241</pub-id></mixed-citation></ref><ref id="B2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambin</surname><given-names>P</given-names></name><name><surname>Leijenaar</surname><given-names>RTH</given-names></name><name><surname>Deist</surname><given-names>TM</given-names></name><name><surname>Peerlings</surname><given-names>J</given-names></name><name><surname>De Jong</surname><given-names>EE</given-names></name><name><surname>Van Timmeren</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Radiomics: the bridge between medical imaging and personalized medicine</article-title>. <source>Nat Rev Clin Oncol</source>. (<year>2017</year>) <volume>14</volume>:<fpage>749</fpage>. <pub-id pub-id-type="doi">10.1038/nrclinonc.2017.141</pub-id><?supplied-pmid 28975929?><pub-id pub-id-type="pmid">28975929</pub-id></mixed-citation></ref><ref id="B3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badillo</surname><given-names>S</given-names></name><name><surname>Banfai</surname><given-names>B</given-names></name><name><surname>Birzele</surname><given-names>F</given-names></name><name><surname>Davydov</surname><given-names>II</given-names></name><name><surname>Hutchinson</surname><given-names>L</given-names></name><name><surname>Kam-Thong</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>An introduction to machine learning</article-title>. <source>Clin Pharmacol Ther</source>. (<year>2020</year>) <volume>107</volume>:<fpage>871</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1002/cpt.1796</pub-id><pub-id pub-id-type="pmid">32128792</pub-id></mixed-citation></ref><ref id="B4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosny</surname><given-names>A</given-names></name><name><surname>Parmar</surname><given-names>C</given-names></name><name><surname>Quackenbush</surname><given-names>J</given-names></name><name><surname>Schwartz</surname><given-names>LH</given-names></name><name><surname>Aerts</surname><given-names>HJ</given-names></name></person-group>. <article-title>Artificial intelligence in radiology</article-title>. <source>Nat Rev Cancer</source>. (<year>2018</year>) <volume>18</volume>:<fpage>500</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1038/s41568-018-0016-5</pub-id><pub-id pub-id-type="pmid">29777175</pub-id></mixed-citation></ref><ref id="B5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillies</surname><given-names>RJ</given-names></name><name><surname>Kinahan</surname><given-names>PE</given-names></name><name><surname>Hricak</surname><given-names>H</given-names></name></person-group>. <article-title>Radiomics: images are more than pictures, they are data</article-title>. <source>Radiology</source>. (<year>2016</year>) <volume>278</volume>:<fpage>563</fpage>&#x02013;<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1148/radiol.2015151169</pub-id><?supplied-pmid 26579733?><pub-id pub-id-type="pmid">26579733</pub-id></mixed-citation></ref><ref id="B6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mayerhoefer</surname><given-names>ME</given-names></name><name><surname>Materka</surname><given-names>A</given-names></name><name><surname>Langs</surname><given-names>G</given-names></name><name><surname>H&#x000e4;ggstr&#x000f6;m</surname><given-names>I</given-names></name><name><surname>Szczypi&#x00144;ski</surname><given-names>P</given-names></name><name><surname>Gibbs</surname><given-names>P</given-names></name><etal/></person-group>. <article-title>Introduction to radiomics</article-title>. <source>J Nucl Med</source>. (<year>2020</year>) <volume>61</volume>:<fpage>488</fpage>&#x02013;<lpage>95</lpage>. <pub-id pub-id-type="doi">10.2967/jnumed.118.222893</pub-id><pub-id pub-id-type="pmid">32060219</pub-id></mixed-citation></ref><ref id="B7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Burnside</surname><given-names>ES</given-names></name><name><surname>Huang</surname><given-names>E</given-names></name><name><surname>Drukker</surname><given-names>K</given-names></name><name><surname>Hoadley</surname><given-names>KA</given-names></name><etal/></person-group>. <article-title>Quantitative MRI radiomics in the prediction of molecular classifications of breast cancer subtypes in the TCGA/TCIA data set</article-title>. <source>NPJ Breast Cancer</source>. (<year>2016</year>) <volume>2</volume>:<fpage>1</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1038/npjbcancer.2016.12</pub-id><?supplied-pmid 27853751?><pub-id pub-id-type="pmid">27853751</pub-id></mixed-citation></ref><ref id="B8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>N</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Dong</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><etal/></person-group>. <article-title>Added value of radiomics on mammography for breast cancer diagnosis: a feasibility study</article-title>. <source>J Am Coll Radiol</source>. (<year>2019</year>) <volume>16</volume>:<fpage>485</fpage>&#x02013;<lpage>91</lpage>. <pub-id pub-id-type="doi">10.1016/j.jacr.2018.09.041</pub-id><?supplied-pmid 30528092?><pub-id pub-id-type="pmid">30528092</pub-id></mixed-citation></ref><ref id="B9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>S</given-names></name><name><surname>Khanna</surname><given-names>P</given-names></name></person-group>. <article-title>Computer-aided diagnosis of malignant mammograms using Zernike moments and SVM</article-title>. <source>J Digit Imaging</source>. (<year>2015</year>) <volume>28</volume>:<fpage>77</fpage>&#x02013;<lpage>90</lpage>. <pub-id pub-id-type="doi">10.1007/s10278-014-9719-7</pub-id><?supplied-pmid 25005867?><pub-id pub-id-type="pmid">25005867</pub-id></mixed-citation></ref><ref id="B10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdel-Nasser</surname><given-names>M</given-names></name><name><surname>Moreno</surname><given-names>A</given-names></name><name><surname>Puig</surname><given-names>D</given-names></name></person-group>. <article-title>Towards cost reduction of breast cancer diagnosis using mammography texture analysis</article-title>. <source>J Exp Theor Artif Intel</source>. (<year>2016</year>) <volume>28</volume>:<fpage>385</fpage>&#x02013;<lpage>402</lpage>. <pub-id pub-id-type="doi">10.1080/0952813X.2015.1024496</pub-id></mixed-citation></ref><ref id="B11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadad</surname><given-names>T</given-names></name><name><surname>Munir</surname><given-names>A</given-names></name><name><surname>Saba</surname><given-names>T</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name></person-group>. <article-title>Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature</article-title>. <source>J Comput Sci</source>. (<year>2018</year>) <volume>29</volume>:<fpage>34</fpage>&#x02013;<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1016/j.jocs.2018.09.015</pub-id></mixed-citation></ref><ref id="B12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group>. <article-title>Deep learning</article-title>. <source>Nature</source>. (<year>2015</year>) <volume>521</volume>:<fpage>436</fpage>&#x02013;<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="B13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Rong</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Wei</surname><given-names>W</given-names></name><etal/></person-group>. <article-title>Deep learning provides a new computed tomography-based prognostic biomarker for recurrence prediction in high-grade serous ovarian cancer</article-title>. <source>Radiother Oncol</source>. (<year>2019</year>) <volume>132</volume>:<fpage>171</fpage>&#x02013;<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1016/j.radonc.2018.10.019</pub-id><?supplied-pmid 30392780?><pub-id pub-id-type="pmid">30392780</pub-id></mixed-citation></ref><ref id="B14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afshar</surname><given-names>P</given-names></name><name><surname>Mohammadi</surname><given-names>A</given-names></name><name><surname>Plataniotis</surname><given-names>KN</given-names></name><name><surname>Oikonomou</surname><given-names>A</given-names></name><name><surname>Benali</surname><given-names>H</given-names></name></person-group>. <article-title>From handcrafted to deep-learning-based cancer radiomics: challenges and opportunities</article-title>. <source>IEEE Signal Proc Mag</source>. (<year>2019</year>) <volume>36</volume>:<fpage>132</fpage>&#x02013;<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1109/MSP.2019.2900993</pub-id></mixed-citation></ref><ref id="B15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswas</surname><given-names>M</given-names></name><name><surname>Kuppili</surname><given-names>V</given-names></name><name><surname>Saba</surname><given-names>L</given-names></name><name><surname>Edla</surname><given-names>DR</given-names></name><name><surname>Suri</surname><given-names>HS</given-names></name><name><surname>Cuadrado-Godia</surname><given-names>E</given-names></name><etal/></person-group>. <article-title>State-of-the-art review on deep learning in medical imaging</article-title>. <source>Front Biosci</source>. (<year>2019</year>) <volume>24</volume>:<fpage>392</fpage>&#x02013;<lpage>426</lpage>. <pub-id pub-id-type="doi">10.2741/4725</pub-id><?supplied-pmid 30468663?><pub-id pub-id-type="pmid">30468663</pub-id></mixed-citation></ref><ref id="B16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poplin</surname><given-names>R</given-names></name><name><surname>Varadarajan</surname><given-names>AV</given-names></name><name><surname>Blumer</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>McConnell</surname><given-names>MV</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><etal/></person-group>. <article-title>Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</article-title>. <source>Nat Biomed Eng</source>. (<year>2018</year>) <volume>2</volume>:<fpage>158</fpage>&#x02013;<lpage>64</lpage>. <pub-id pub-id-type="doi">10.1038/s41551-018-0195-0</pub-id><?supplied-pmid 31015713?><pub-id pub-id-type="pmid">31015713</pub-id></mixed-citation></ref><ref id="B17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jalalian</surname><given-names>A</given-names></name><name><surname>Mashohor</surname><given-names>SBT</given-names></name><name><surname>Mahmud</surname><given-names>HR</given-names></name><name><surname>Saripan</surname><given-names>MIB</given-names></name><name><surname>Ramli</surname><given-names>ARB</given-names></name><name><surname>Karasfi</surname><given-names>B</given-names></name></person-group>. <article-title>Computer-aided detection/diagnosis of breast cancer in mammography and ultrasound: a review</article-title>. <source>Clin Imag</source>. (<year>2013</year>) <volume>37</volume>:<fpage>420</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1016/j.clinimag.2012.09.024</pub-id><?supplied-pmid 23153689?><pub-id pub-id-type="pmid">23153689</pub-id></mixed-citation></ref><ref id="B18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huynh</surname><given-names>BQ</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Giger</surname><given-names>ML</given-names></name></person-group>. <article-title>Digital mammographic tumor classification using transfer learning from deep convolutional neural networks</article-title>. <source>J Med Imaging</source>. (<year>2016</year>) <volume>3</volume>:<fpage>034501</fpage>. <pub-id pub-id-type="doi">10.1117/1.JMI.3.3.034501</pub-id><?supplied-pmid 27610399?><pub-id pub-id-type="pmid">27610399</pub-id></mixed-citation></ref><ref id="B19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antropova</surname><given-names>N</given-names></name><name><surname>Huynh</surname><given-names>BQ</given-names></name><name><surname>Giger</surname><given-names>ML</given-names></name></person-group>. <article-title>A deep feature fusion methodology for breast cancer diagnosis demonstrated on three imaging modality datasets</article-title>. <source>Med Phys</source>. (<year>2017</year>) <volume>44</volume>:<fpage>5162</fpage>&#x02013;<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1002/mp.12453</pub-id><?supplied-pmid 28681390?><pub-id pub-id-type="pmid">28681390</pub-id></mixed-citation></ref><ref id="B20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raza</surname><given-names>S</given-names></name><name><surname>Goldkamp</surname><given-names>AL</given-names></name><name><surname>Chikarmane</surname><given-names>SA</given-names></name><name><surname>Birdwell</surname><given-names>RL</given-names></name></person-group>. <article-title>US of breast masses categorized as BI-RADS 3, 4, and 5: pictorial review of factors influencing clinical management</article-title>. <source>Radiographics</source>. (<year>2010</year>) <volume>30</volume>:<fpage>1199</fpage>. <pub-id pub-id-type="doi">10.1148/rg.305095144</pub-id><?supplied-pmid 20833845?><pub-id pub-id-type="pmid">20833845</pub-id></mixed-citation></ref><ref id="B21"><label>21.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. In: <source>CVPR</source>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2014</year>).</mixed-citation></ref><ref id="B22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Imagenet large scale visual recognition challenge</article-title>. <source>Int J Comput Vision</source>. (<year>2015</year>) <volume>115</volume>:<fpage>211</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation></ref><ref id="B23"><label>23.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Wojna</surname><given-names>Z</given-names></name></person-group>. <article-title>Rethinking the inception architecture for computer vision</article-title>. In: <source>CVPR</source>. <publisher-loc>Las Vegas, NV</publisher-loc> (<year>2016</year>). p. <fpage>2818</fpage>&#x02013;<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.308</pub-id></mixed-citation></ref><ref id="B24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Y</given-names></name><name><surname>Chao</surname><given-names>M</given-names></name><name><surname>Lo</surname><given-names>YC</given-names></name></person-group>. <article-title>Automatic skin lesion segmentation using deep fully convolutional networks with jaccard distance</article-title>. <source>IEEE Trans Med Imaging</source>. (<year>2017</year>) <volume>36</volume>:<fpage>1876</fpage>&#x02013;<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2017.2695227</pub-id><?supplied-pmid 28436853?><pub-id pub-id-type="pmid">28436853</pub-id></mixed-citation></ref><ref id="B25"><label>25.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group>. <article-title>Dropout as a bayesian approximation: representing model uncertainty in deep learning</article-title>. In: <source>ICML</source>. <publisher-loc>New York, NY</publisher-loc> (<year>2016</year>). p. <fpage>1050</fpage>&#x02013;<lpage>9</lpage>.</mixed-citation></ref><ref id="B26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Mao</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><etal/></person-group>. <article-title>Texture-based classification of different single liver lesion based on SPAIR T2W MRI images</article-title>. <source>BMC Med Imaging</source>. (<year>2017</year>) <volume>17</volume>:<fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1186/s12880-017-0212-x</pub-id><?supplied-pmid 28705145?><pub-id pub-id-type="pmid">28056868</pub-id></mixed-citation></ref><ref id="B27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Long</surname><given-names>F</given-names></name><name><surname>Ding</surname><given-names>C</given-names></name></person-group>. <article-title>Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy</article-title>. <source>IEEE Trans Pattern Anal</source>. (<year>2005</year>) <volume>27</volume>:<fpage>1226</fpage>&#x02013;<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2005.159</pub-id><?supplied-pmid 16119262?><pub-id pub-id-type="pmid">16119262</pub-id></mixed-citation></ref><ref id="B28"><label>28.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D</given-names></name></person-group>. <article-title>Support vector machine</article-title>. In: <source>Fundamentals of Image Data Mining</source>. <publisher-name>Springer</publisher-name> (<year>2019</year>). p. <fpage>179</fpage>&#x02013;<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-17989-2_8</pub-id></mixed-citation></ref><ref id="B29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ragab</surname><given-names>D. A</given-names></name><name><surname>Sharkas</surname><given-names>M</given-names></name><name><surname>Marshall</surname><given-names>S</given-names></name><name><surname>Ren</surname><given-names>J</given-names></name></person-group>. <article-title>Breast cancer detection using deep convolutional neural networks and support vector machines</article-title>. <source>PeerJ</source>. (<year>2019</year>) <volume>7</volume>:<fpage>e6201</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.6201</pub-id><?supplied-pmid 30713814?><pub-id pub-id-type="pmid">30713814</pub-id></mixed-citation></ref><ref id="B30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yassin</surname><given-names>NIR</given-names></name><name><surname>Omran</surname><given-names>S</given-names></name><name><surname>El Houby</surname><given-names>EMF</given-names></name><name><surname>Allam</surname><given-names>H</given-names></name></person-group>. <article-title>Machine learning techniques for breast cancer computer aided diagnosis using different image modalities: a systematic review</article-title>. <source>Comput Meth Prog Bio</source>. (<year>2018</year>) <volume>156</volume>:<fpage>25</fpage>&#x02013;<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2017.12.012</pub-id><?supplied-pmid 29428074?><pub-id pub-id-type="pmid">29428074</pub-id></mixed-citation></ref><ref id="B31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeLong</surname><given-names>ER</given-names></name><name><surname>DeLongb</surname><given-names>DM</given-names></name><name><surname>Clarke-Pearson</surname><given-names>DL</given-names></name></person-group>. <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>. (<year>1988</year>) <volume>44</volume>:<fpage>837</fpage>&#x02013;<lpage>45</lpage>. <pub-id pub-id-type="doi">10.2307/2531595</pub-id><?supplied-pmid 3203132?><pub-id pub-id-type="pmid">3203132</pub-id></mixed-citation></ref><ref id="B32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moura</surname><given-names>DC</given-names></name><name><surname>L&#x000f3;pez</surname><given-names>MAG</given-names></name></person-group>. <article-title>An evaluation of image descriptors combined with clinical data for breast cancer diagnosis</article-title>. <source>Int J Comput Ass Rad</source>. (<year>2013</year>) <volume>8</volume>:<fpage>561</fpage>&#x02013;<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1007/s11548-013-0838-2</pub-id><?supplied-pmid 23580025?><pub-id pub-id-type="pmid">23580025</pub-id></mixed-citation></ref><ref id="B33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsochatzidis</surname><given-names>L</given-names></name><name><surname>Costaridou</surname><given-names>L</given-names></name><name><surname>Pratikakis</surname><given-names>I</given-names></name></person-group>. <article-title>Deep learning for breast cancer diagnosis from mammograms&#x02014;a comparative study</article-title>. <source>J Imaging</source>. (<year>2019</year>) <volume>5</volume>:<fpage>37</fpage>. <pub-id pub-id-type="doi">10.3390/jimaging5030037</pub-id></mixed-citation></ref><ref id="B34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kooi</surname><given-names>T</given-names></name><name><surname>Litjens</surname><given-names>G</given-names></name><name><surname>Van Ginneken</surname><given-names>B</given-names></name><name><surname>Gubern-M&#x000e9;rida</surname><given-names>A</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>CI</given-names></name><name><surname>Mann</surname><given-names>R</given-names></name><etal/></person-group>. <article-title>Large scale deep learning for computer aided detection of mammographic lesions</article-title>. <source>Med Image Anal</source>. (<year>2017</year>) <volume>35</volume>:<fpage>303</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2016.07.007</pub-id><?supplied-pmid 27497072?><pub-id pub-id-type="pmid">27497072</pub-id></mixed-citation></ref></ref-list></back></article>