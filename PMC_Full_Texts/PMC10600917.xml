<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">10600917</article-id><article-id pub-id-type="doi">10.1093/gigascience/giad083</article-id><article-id pub-id-type="publisher-id">giad083</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject><subject>AcademicSubjects/SCI02254</subject></subj-group></article-categories><title-group><article-title>Machine learning&#x02013;based feature selection to search stable microbial biomarkers: application to inflammatory bowel disease</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9483-5898</contrib-id><name><surname>Lee</surname><given-names>Youngro</given-names></name><aff>
<institution>Department of Electrical and Computer Engineering, Seoul National University</institution>, <addr-line>Seoul, 08826</addr-line>, <country country="KP">Korea</country></aff><aff>
<institution>Institute of Engineering Research at Seoul National University</institution>, <addr-line>Seoul, 08826</addr-line>, <country country="KP">Korea</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1693-7792</contrib-id><name><surname>Cappellato</surname><given-names>Marco</given-names></name><aff>
<institution>Department of Information Engineering, University of Padova</institution>, <addr-line>Padova, 35122</addr-line>, <country country="IT">Italy</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8415-4688</contrib-id><name><surname>Di&#x000a0;Camillo</surname><given-names>Barbara</given-names></name><!--barbara.dicamillo@unipd.it--><aff>
<institution>Department of Information Engineering, University of Padova</institution>, <addr-line>Padova, 35122</addr-line>, <country country="IT">Italy</country></aff><xref rid="cor1" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">Correspondence address. Barbara Di Camillo, E-mail: <email>barbara.dicamillo@unipd.it</email></corresp></author-notes><pub-date pub-type="epub" iso-8601-date="2023-10-26"><day>26</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>26</day><month>10</month><year>2023</year></pub-date><volume>12</volume><elocation-id>giad083</elocation-id><history><date date-type="received"><day>21</day><month>6</month><year>2023</year></date><date date-type="rev-recd"><day>23</day><month>8</month><year>2023</year></date><date date-type="accepted"><day>17</day><month>9</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2023. Published by Oxford University Press GigaScience.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giad083.pdf"/><abstract><title>Abstract</title><sec id="abs1"><title>Background</title><p>Biomarker discovery exploiting feature importance of machine learning has risen recently in the microbiome landscape with its high predictive performance in several disease states. To have a concrete selection among a high number of features, recursive feature elimination (RFE) has been widely used in the bioinformatics field. However, machine learning&#x02013;based RFE has factors that decrease the stability of feature selection. In this article, we suggested methods to improve stability while sustaining performance.</p></sec><sec id="abs2"><title>Results</title><p>We exploited the abundance matrices of the gut microbiome (283 taxa at species level and 220 at genus level) to classify between patients with inflammatory bowel disease (IBD) and healthy control (1,569 samples). We found that applying an already published data transformation before RFE improves feature stability significantly. Moreover, we performed an in-depth evaluation of different variants of the data transformation and identify those that demonstrate better improvement in stability while not sacrificing classification performance. To ensure a robust comparison, we evaluated stability using various similarity metrics, distances, the common number of features, and the ability to filter out noise features. We were able to confirm that the mapping by the Bray&#x02013;Curtis similarity matrix before RFE consistently improves the stability while maintaining good performance. Multilayer perceptron algorithm exhibited the highest performance among 8 different machine learning algorithms when a large number of features (a few hundred) were considered based on the best performance across 100 bootstrapped internal test sets. Conversely, when utilizing only a limited number of biomarkers as a trade-off between optimal performance and method generalizability, the random forest algorithm demonstrated the best performance. Using the optimal pipeline we developed, we identified 14 biomarkers for IBD at the species level and analyzed their roles using Shapley additive explanations.</p></sec><sec id="abs3"><title>Conclusion</title><p>Taken together, our work not only showed how to improve biomarker discovery in the metataxonomic field without sacrificing classification performance but also provided useful insights for future comparative studies.</p></sec></abstract><kwd-group kwd-group-type="keywords"><kwd>microbiota</kwd><kwd>machine learning</kwd><kwd>feature selection</kwd><kwd>biomarkers discovery</kwd><kwd>Shapley values</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Ministry of Health &#x00026; Welfare, Republic of Korea</institution></institution-wrap>
</funding-source><award-id>HI21C1092</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Department of Information Engineering of the University of Padova</institution></institution-wrap>
</funding-source><award-id>DI_C_BIRD2020_01</award-id></award-group></funding-group><counts><page-count count="12"/></counts></article-meta></front><body><sec sec-type="intro" id="sec1"><title>Introduction</title><p>Next-generation sequencing technologies allow reconstructing the internal composition of the whole microbial community (microbiota) present in a sample possibly exploiting 2 different approaches: whole-genome shotgun sequencing (WGS) and targeted amplicon sequencing of 16S ribosomal RNA (16S rDNA-seq) [<xref rid="bib1" ref-type="bibr">1</xref>, <xref rid="bib2" ref-type="bibr">2</xref>]. The first focuses on all genomes, while the second only on a region of a single gene (i.e., 16S rRNA gene). Different bioinformatics preprocessing pipelines can be used to obtain the so-called abundance matrix and taxonomy matrix [<xref rid="bib3" ref-type="bibr">3</xref>, <xref rid="bib4" ref-type="bibr">4</xref>] from raw-read data. The abundance matrix describes the relative abundance of different operating taxonomic units (OTUs) or of different amplicon sequence variant (ASVs) [<xref rid="bib5" ref-type="bibr">5</xref>, <xref rid="bib6" ref-type="bibr">6</xref>] on each sample, while the taxonomy matrix contains information about the taxonomy of each OTU or ASV (i.e., kingdom, phylum, class, order, family, genus, species).</p><p>Machine learning (ML) can be used to classify samples based on their taxa composition, thus identifying a microbial signature that characterizes host phenotypes. Recently, several studies exploited different ML-based techniques to develop models for predicting disease states, such as inflammatory bowel disease (IBD) [<xref rid="bib7" ref-type="bibr">7</xref>, <xref rid="bib8" ref-type="bibr">8</xref>], colorectal cancer [<xref rid="bib9" ref-type="bibr">9</xref>, <xref rid="bib10" ref-type="bibr">10</xref>], and cardiovascular disease [<xref rid="bib11" ref-type="bibr">11</xref>], using microbial data. This growing interest is mainly due to the potential impact on diagnosis and therapeutic target identification.</p><p>However, the application of ML methods to microbiota might be challenging for a number of reasons [<xref rid="bib12" ref-type="bibr">12</xref>]. Despite the existence of several consortium studies [<xref rid="bib13" ref-type="bibr">13&#x02013;16</xref>], there is a lack of standards in data structures, metadata collection, and preprocessing. This leads to limited generalizability [<xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref>] of the results in terms of diagnostic and prognostic biomarkers&#x02014;that is, taxa that could be used as diagnostic or prognostic markers [<xref rid="bib19" ref-type="bibr">19</xref>] and that, as such, need to be robust and reproducible indicators of the biological state.</p><p>In this work, we performed a comprehensive evaluation of biomarker stability and classification approaches in the context of gut metataxonomic data used to identify a microbial signature of IBD-affected patients versus healthy controls. Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref> summarizes the whole analysis pipeline. Three different datasets (described in <italic toggle="yes">Methods, Data</italic>) were merged to increase the number of examples. The final merged dataset consists of 1,569 samples in total, where 702 samples were identified as IBD-affected patients and the others as healthy controls. The merged dataset was split in two, thus obtaining ensemble dataset 1 (ED1) and ensemble dataset 2 (ED2) by mixing the samples from the original studies. Note that, although the percentage of samples from different datasets is similar in ED1 versus ED2, we expected the abundance of each taxon to be different in the 2 datasets due to the high number of features and the high variability and sparsity of metagenomics datasets [<xref rid="bib20" ref-type="bibr">20</xref>]. This characteristic might potentially affect the performance and generalizability of methods, which is a property we wanted to check in our experiments.</p><fig position="float" id="fig1"><label>Figure 1:</label><caption><p>Diagram for the overall experiments. The figure reports the analysis performed on ensemble dataset 1 using ensemble dataset 2 only for testing (black: inputs and outputs; blue: analysis steps; green: results assessment). A symmetric analysis was performed on ensemble dataset 2 using ensemble dataset 1 for testing. The inset represents the details of the mapping transformation procedure.</p></caption><graphic xlink:href="giad083fig1" position="float"/></fig><p>Each dataset was given as input to a &#x0201c;typical&#x0201d; ML pipeline, including splitting the data into test and training sets, feature selection, and a final classification step. Since generalized biomarker discovery is a key point in this landscape, our effort focused on studying the feature selection step. In particular, we used recursive feature elimination (RFE) within a bootstrap embedding [<xref rid="bib21" ref-type="bibr">21</xref>] as described in <italic toggle="yes">Methods, Feature selection approach</italic> to identify and select robust features. Moreover, the integration of prior knowledge in the learning process was investigated as a method to achieve higher stability of the biomarker list [<xref rid="bib22" ref-type="bibr">22</xref>]. A fourth external dataset for which the samples&#x02019; labels were not available was used for this purpose, to compute feature similarity. We refer to this procedure as mapping strategy, since it is based on a kernel-based data transformation that projects data in a new space where the more similar 2 features are, the closer they are mapped in the new space. The basic idea is to take into account the correlation of different taxa: if some features are strongly correlated, then they likely have similar importance and are equally relevant for the classification task. More details about the mapping strategy are provided in <italic toggle="yes">Methods, Mapping</italic>. In <italic toggle="yes">Methods, Evaluation metrics</italic>, we introduce all metrics used to assess features stability (robustness of taxa selected).</p><p>It is worth noting that there are similar approaches that use a similarity matrix to map similar features into closer space [<xref rid="bib23" ref-type="bibr">23&#x02013;26</xref>]. Among others, AggMapNet utilizes a unique approach for data transformation, converting the original data into multichannel 2-dimensional (2D) spatial-correlated images through pairwise correlation distances. This is achieved by employing the manifold learning method called Uniform Manifold Approximation and Projection (UMAP) [<xref rid="bib27" ref-type="bibr">27</xref>]. Through a preliminary clustering step, various channels are selected based on the pairwise correlation distances among features. Subsequently, the feature maps are fed as input to machine learning models, such as convolutional neural networks, enabling effective classification tasks. However, traditional ML methods require as input 1-dimensional (1D) vectors. Our approach can map unordered features as 1D vectors for conventional ML models and is therefore somehow complementary to the AggMapNet approach.</p><p>With the selected set of features, we used different classification approaches to classify IBD-affected patients and healthy controls&#x02014;namely, logistic regression, support vector machines, random forests, extreme gradient boosting, and neural networks. In <italic toggle="yes">Methods, Classification model algorithms</italic>, we introduce all the prediction methods used in our study. To assess the generalizability of the classifiers and the robustness of the microbial signature, the models developed using ED1 were tested on test 1 (obtained by splitting ED1 in training and test in proportions 80% and 20%, respectively) and on the entire ED2. Similarly, models developed using ED2 were tested on test 2 and on the entire ED1.</p><p>Using the Bray&#x02013;Curtis similarity matrix to map the data provides the best, improved stability, without sacrificing the classification performance. Using this pipeline, we selected the top 14 features as a trade-off between optimal performance and method generalizability. The best-performing algorithm on this robust set of features was the random forest. We further investigated the role of these biomarkers using Shapley values [<xref rid="bib28" ref-type="bibr">28&#x02013;30</xref>].</p><p>To ensure the reproducibility of the results, datasets and all code written in support of this publication are publicly available in the <italic toggle="yes">GigaScience</italic> Database [<xref rid="bib31" ref-type="bibr">31</xref>].</p></sec><sec sec-type="materials|methods" id="sec2"><title>Methods</title><sec id="sec2-1"><title>Data</title><p>Four datasets were downloaded from Qiita, an open-source microbial study management platform (see Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref> for the reference Qiita study ID for reproducibility) [<xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib32" ref-type="bibr">32&#x02013;34</xref>]. Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref> summarizes the characteristics of the different datasets, such as 16S region sequenced, the specimen analyzed, the patient&#x02019;s geographic origin, and the abundance matrices&#x02019; dimensionality. Throughout the article, we will refer to each dataset with the ID indicated in the first column (Dataset#).</p><table-wrap position="float" id="tbl1"><label>Table 1:</label><caption><p>Summary of datasets used in the study. The table shows, in different columns, the ID of the dataset (Dataset#), the Qiita study ID, the sequenced hypervariable region (16S region), the specimen analyzed (Product), the patient&#x02019;s geographic origin (Geographic localization), and the number of samples labeled as IBD versus non-IBD. CD stands for Crohn&#x02019;s disease and UC for ulcerative colitis.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="8" align="center" rowspan="1">Dataset Information</th></tr><tr><th rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1"/><th rowspan="2" colspan="1"> Biological specimen</th><th rowspan="2" colspan="1"> Geographic localization</th><th colspan="3" align="center" rowspan="1">Selected samples for the study</th></tr><tr><th rowspan="1" colspan="1">&#x000a0;Dataset#</th><th rowspan="1" colspan="1">Qiita study ID</th><th rowspan="1" colspan="1">16S region</th><th align="center" rowspan="1" colspan="1">Total</th><th align="center" rowspan="1" colspan="1">IBD</th><th align="center" rowspan="1" colspan="1">Non-IBD</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Dataset 1 [<xref rid="bib16" ref-type="bibr">16</xref>]</td><td rowspan="1" colspan="1">11,484</td><td rowspan="1" colspan="1">V4</td><td rowspan="1" colspan="1">Feces</td><td rowspan="1" colspan="1">USA</td><td rowspan="1" colspan="1">96</td><td rowspan="1" colspan="1">95 CD:75/UC:20</td><td rowspan="1" colspan="1">1</td></tr><tr><td rowspan="1" colspan="1">Dataset 3 [<xref rid="bib32" ref-type="bibr">32</xref>]</td><td rowspan="1" colspan="1">2151</td><td rowspan="1" colspan="1">V4</td><td rowspan="1" colspan="1">Feces</td><td rowspan="1" colspan="1">USA</td><td rowspan="1" colspan="1">836</td><td rowspan="1" colspan="1">32 CD:10/UC:22</td><td rowspan="1" colspan="1">804</td></tr><tr><td rowspan="1" colspan="1">Dataset 2 [<xref rid="bib33" ref-type="bibr">33</xref>]</td><td rowspan="1" colspan="1">1629</td><td rowspan="1" colspan="1">V4</td><td rowspan="1" colspan="1">Feces</td><td rowspan="1" colspan="1">Sweden</td><td rowspan="1" colspan="1">637</td><td rowspan="1" colspan="1">575 CD:251/UC:324</td><td rowspan="1" colspan="1">62</td></tr><tr><td rowspan="1" colspan="1">Dataset 4 [<xref rid="bib34" ref-type="bibr">34</xref>]</td><td rowspan="1" colspan="1">10,317</td><td rowspan="1" colspan="1">V4</td><td rowspan="1" colspan="1">Feces</td><td rowspan="1" colspan="1">USA, UK, AU</td><td rowspan="1" colspan="1">444</td><td rowspan="1" colspan="1">N/A</td><td rowspan="1" colspan="1">N/A</td></tr><tr><td colspan="8" align="left" rowspan="1">
<bold>Ensemble Dataset Information (Dataset 1 + Dataset 2 + Dataset 3)</bold>
</td></tr><tr><td colspan="5" align="left" rowspan="1">Ensemble Dataset 1&#x02014;Training Dataset</td><td rowspan="1" colspan="1">627</td><td rowspan="1" colspan="1">281</td><td rowspan="1" colspan="1">346</td></tr><tr><td colspan="5" align="left" rowspan="1">Ensemble Dataset 2&#x02014;Training Dataset</td><td rowspan="1" colspan="1">628</td><td rowspan="1" colspan="1">281</td><td rowspan="1" colspan="1">347</td></tr><tr><td colspan="5" align="left" rowspan="1">Ensemble Dataset 1&#x02014;Test Dataset</td><td rowspan="1" colspan="1">157</td><td rowspan="1" colspan="1">70</td><td rowspan="1" colspan="1">87</td></tr><tr><td colspan="5" align="left" rowspan="1">Ensemble Dataset 2&#x02014;Test Dataset</td><td rowspan="1" colspan="1">157</td><td rowspan="1" colspan="1">70</td><td rowspan="1" colspan="1">87</td></tr></tbody></table></table-wrap><p>We downloaded the metadata file, with all the available covariates (e.g., sample ID, age, sex, weight), the <italic toggle="yes">.biom</italic> file with the abundance matrices of the processed 16S rDNA sequences, and the associated taxonomy. For all the datasets, the abundance matrices were obtained by applying the same bioinformatics preprocessing steps&#x02014;that is, trimming (QIIMEq2 1.9.1) and Pick closed-reference OTUs (QIIMEq2 1.9.1) [<xref rid="bib35" ref-type="bibr">35</xref>] as implemented in Qiita.</p><p>Several preprocessing tools have been developed to handle with 16S sequencing data characteristics, such as log ratio&#x02013;based transformation, normalization, and zero imputation [<xref rid="bib36" ref-type="bibr">36&#x02013;38</xref>]. However, no standard approach has been identified yet. We preprocessed each dataset independently and then combined them in ED1 and ED2. First, all the taxa with the same taxonomy classification (i.e., same species level or genus level) were aggregated and their respective counts were summed, as usually done in microbiome studies [<xref rid="bib19" ref-type="bibr">19</xref>, <xref rid="bib3" ref-type="bibr">3</xref>, <xref rid="bib39" ref-type="bibr">39</xref>]. We considered both species and genus levels independently, thus obtaining 2 abundance matrices for each dataset. After that, we filtered out those taxa with more than 99% of abundances equal to 0. It is worth noting that usually, in the field of microbiota analysis, a threshold of 95% or 90% is used. However, lower thresholds can alter the composition of the abundance profiles (i.e., the abundances of all the features in a sample) and have an effect on the following normalization step [<xref rid="bib40" ref-type="bibr">40</xref>]. Then we divided the abundance profiles by the geometric mean and took the log (base 2) of the ratios leveraging on the idea of clr transformation, using a pseudocount value equal to the minimum observed data.</p><p>After finishing preprocessing Dataset 1 + Dataset 2 + Dataset 3 separately, we integrated the 3 datasets as shown in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>. To this purpose, from the entire set of features, only the ones common throughout all datasets were kept. In this way, we selected a core set of features with 283 taxa at the species level and 220 at the genus level. After dataset splitting in ED1 and ED2, min&#x02013;max scaling was performed to scale features into the same range. The preprocessing was performed separately to the external dataset (Dataset 4) following the same steps described above.</p></sec><sec id="sec2-2"><title>Feature selection approach</title><p>For each taxonomic level (genus, species) and each dataset ED1 and ED2, RFE was used as a feature ranking method to obtain the optimal number of features [<xref rid="bib21" ref-type="bibr">21</xref>] as described in the following. RFE was performed 100 times with bootstrapping. In each bootstrap, the training dataset was split into an internal training set and an internal test set. A prediction model was trained within the internal training set using a linear support vector machine (SVM). RFE is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Linear SVMs were chosen because of their cost-efficiency trade-off and their ability to deal with a high number of predictors [<xref rid="bib41" ref-type="bibr">41&#x02013;43</xref>]. The regularization parameter was tuned within each internal training set using grid search and 5-fold cross-validation with the Matthew correlation coefficient (MCC) [<xref rid="bib44" ref-type="bibr">44</xref>] as the performance index. Feature importance was measured based on the feature weight since data had been standardized in input. The least important feature was eliminated from the dataset iterating the process until only 1 feature was left. Sorting the features from the last eliminated, which will therefore have rank 1, and averaging the ranked lists across the 100 bootstraps, the global feature rank was calculated. Finally, MCC was computed independently on each bootstrap internal test sets for different numbers of features and then averaged across the 100 bootstraps. The number of features corresponding to the maximum MCC (MCC shows either a peak or a saturation effect) was taken as optimum.</p><p>RFE was performed (i) without any transformation, (ii) with mapping performed using Pearson correlation, and (iii) with mapping performed using Bray&#x02013;Curtis similarity, as explained in the following paragraph.</p></sec><sec id="sec2-3"><title>Mapping</title><sec id="sec2-3-1"><title>Similarity matrix</title><p>The similarity matrix is a symmetric matrix encoding the knowledge about the likeness between features. In this article, we used either Pearson correlation [<xref rid="bib45" ref-type="bibr">45</xref>] or Bray&#x02013;Curtis similarity [<xref rid="bib46" ref-type="bibr">46</xref>]. Pearson correlation is the linear correlation between 2 sets, dividing the covariance of 2 features by the product of each standard deviation. Bray&#x02013;Curtis similarity is the comparison of the composition between 2 different sites, dividing twice the sum of the lesser value for only those features in common between 2 sites by the total sum of values counted at both sites for all the features. Assume <inline-formula><tex-math id="TM0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${x}_{ij}$\end{document}</tex-math></inline-formula> as <inline-formula><tex-math id="TM0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${j}^{th}( &#x0003c; n)$\end{document}</tex-math></inline-formula> feature value of <inline-formula><tex-math id="TM0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${i}^{th}( &#x0003c; m)\ $\end{document}</tex-math></inline-formula>data sample. Then we can define the list of <inline-formula><tex-math id="TM0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${a}^{th}\ $\end{document}</tex-math></inline-formula>and <inline-formula><tex-math id="TM0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${b}^{th}$\end{document}</tex-math></inline-formula> feature value as <inline-formula><tex-math id="TM0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$A = [ {{x}_{1a},{x}_{2a}, \ldots \ {x}_{ma}} ]$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="TM0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$B = [ {{x}_{1b},{x}_{2b}, \ldots \ {x}_{mb}} ]$\end{document}</tex-math></inline-formula>. Pearson correlation and Bray&#x02013;Curtis similarity are defined as follows:</p><disp-formula id="equ1">
<label>(1)</label>
<tex-math id="TM0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
Pearson{\mathrm{\ }}Correlation = {\mathrm{\ }}\frac{{cov\left( {A,B} \right)}}{{{\sigma }_A{\sigma }_B}}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><disp-formula id="equ2">
<label>(2)</label>
<tex-math id="TM0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
Bray{\mathrm{\ }}Curtis{\mathrm{\ }}Similarity = \frac{{2\mathop \sum \nolimits_1^m min\left( {A\left[ i \right],B\left[ i \right]} \right)}}{{\left( {\sum A} \right) + \left( {\sum B} \right)}}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>It should be noted that, to avoid introducing any bias, the similarity matrices were calculated on dataset 4 (see Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref> and Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref>), an external dataset used only to calculate the similarity matrix to perform the mapping transformation (see <italic toggle="yes">Methods, Mapping, Similarity Matrix</italic>).</p></sec><sec id="sec2-3-2"><title>Mapping transformation and its theoretical advantages</title><p>Feature mapping is a crucial steps in machine learning that can significantly impact model performance. Feature mapping involves transforming raw input data into a format suitable for the learning algorithm, enabling the extraction of meaningful patterns and relationships. By converting complex and diverse features into a more structured representation, feature mapping empowers the model to discern relevant information, leading to more accurate predictions. In the context of omics data, due to the abundance of features, the problem of identifying relevant features for the predictive model becomes underconstrained, leading to numerous potential sets of relevant features that could achieve comparable accuracy. To address this, we leveraged <xref rid="sup9" ref-type="supplementary-material">supplementary data</xref> from an external dataset (dataset 4) to impose additional constraints during feature mapping. In essence, this approach aims to account for strong correlations among certain features, indicating their similar importance for the classification task. As a result, we ensure that these correlated features are equally relevant, enhancing the overall performance of the model.</p><p>Information about feature correlation is integrated by mapping data using a kernel transformation that has been shown to possibly alleviate feature instability [<xref rid="bib22" ref-type="bibr">22</xref>]. Transformation matrix <inline-formula><tex-math id="TM0010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{P}}$\end{document}</tex-math></inline-formula> is obtained using the equation <inline-formula><tex-math id="TM0011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{P}} = {{\mathrm{D}}}^{ - 1}( {{\mathrm{I}} + {\mathrm{\alpha }}( {{\mathrm{S}} - {\mathrm{I}}} )} )$\end{document}</tex-math></inline-formula>, where <inline-formula><tex-math id="TM0012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{S}}$\end{document}</tex-math></inline-formula> is the similarity matrix, <inline-formula><tex-math id="TM0013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{D}}$\end{document}</tex-math></inline-formula> is the diagonal matrix whose elements are the sum of the elements in the rows of the matrix <inline-formula><tex-math id="TM0014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{I}} + {\mathrm{\alpha }}( {{\mathrm{S}} - {\mathrm{I}}} )$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="TM0015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{\alpha }}$\end{document}</tex-math></inline-formula> is a tuning parameter. The value of <inline-formula><tex-math id="TM0016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{\alpha }}$\end{document}</tex-math></inline-formula> was decided for each experiment using 5-fold cross-validation within its internal training dataset using a grid of 0.01 and from 0.05 to 1 by step 0.05. In our approach, mapping was used only in the RFE step.</p></sec></sec><sec id="sec2-4"><title>Evaluation metrics</title><sec id="sec2-4-1"><title>Stability</title><p>Different rank-based stability indexes were used to evaluate the robustness of the feature selection algorithm: Spearman&#x02019;s rank correlation coefficient (SRCC), Hamming distance, Pearson correlation, and Bray&#x02013;Curtis dissimilarity [<xref rid="bib47" ref-type="bibr">47</xref>, <xref rid="bib48" ref-type="bibr">48</xref>].</p><p>Since the ranks are distinct integers, <italic toggle="yes"><bold>SRCC</bold></italic> between 2 rank sets can be calculated as follows:</p><disp-formula id="equ3">
<label>(3)</label>
<tex-math id="TM0017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
SRCC = 1 - 6{\mathrm{*}}\mathop \sum \limits_{each{\mathrm{\ }}feature} \frac{{{d}^2}}{{{n}^3 - n}}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>where <italic toggle="yes">d</italic> is the difference between the 2 ranks of each feature and <italic toggle="yes">n</italic> is the total number of ranked features.</p><p>
<bold>
<italic toggle="yes">Hamming distance</italic>
</bold> between 2 rank sets is calculated as the proportion of disagreeing components as follows:</p><disp-formula id="equ4">
<label>(4)</label>
<tex-math id="TM0018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
Hamming{\mathrm{\ }}Distance = \frac{1}{n}\mathop \sum \limits_{each{\mathrm{\ }}feature} \left\{ {\begin{array}{@{}*{1}{c}@{}} {1,\ if\ ranks\ are\ not\ the\ same}\\ {0,\ otherwise} \end{array}} \right.
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>The equation of <bold><italic toggle="yes">Pearson correlation and Bray&#x02013;Curtis dissimilarity</italic></bold> is identical to&#x000a0;Eq. (<xref rid="equ1" ref-type="disp-formula">1</xref>) and&#x000a0;Eq. (<xref rid="equ2" ref-type="disp-formula">2</xref>), but <italic toggle="yes">A</italic> and <italic toggle="yes">B</italic> now comprise rank of each feature at each bootstrap where <inline-formula><tex-math id="TM0019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\ {x}_{ij}\ $\end{document}</tex-math></inline-formula> represent the rank of <inline-formula><tex-math id="TM0020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${i}^{th}( {i = 1,\ \ldots ,\# of\ features} )$\end{document}</tex-math></inline-formula> feature of <inline-formula><tex-math id="TM0021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${j}^{th}( {j = 1,\ \ldots ,\ 100} )$\end{document}</tex-math></inline-formula> bootstrap. Bray&#x02013;Curtis dissimilarity is calculated by subtracting Bray&#x02013;Curtis similarity value from 1.</p><p>
<bold>
<italic toggle="yes">Euclidean distance</italic>
</bold> is the length of the line segment connecting 2 different points. The stability indexes described above were computed for each pair of bootstrap samples used for RFE and finally averaged across the 100 * (100 &#x02013; 1)/2 values.</p><p>The <bold><italic toggle="yes">number of commonly ranked features</italic></bold> was defined by counting the number of common features across all bootstraps considering a range that starts from the top-ranked single feature and expanding the range one by one. The condition can be varied by defining &#x0201c;common&#x0201d; as consistent across all 100 bootstraps or across at least 66 or 50 bootstraps.</p><p>To compare the ability to distinguish unimportant features from important features, a noise-filtering experiment was performed. We added 100 randomly generated noisy features (range 0 to 1 from a uniform distribution) to the original features (range 0 to 1 by min&#x02013;max scaler) before mapping and after mapping and RFE compared the average rank of noisy features from the average rank of true features (Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref>). With a good noise-filtering algorithm, noise features are expected to have big feature ranks.</p></sec><sec id="sec2-4-2"><title>Performance measurement</title><p>MCC was used for the step of recursive feature elimination and to compare the performance of the different models. MCC estimates the correlation between the predictive value and ground-truth value by using the following equation:</p><disp-formula id="equ5">
<label>(5)</label>
<tex-math id="TM0022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
MCC = \frac{{TN{\mathrm{*}}TP - FN{\mathrm{*}}FP}}{{\sqrt {\left( {TP + FP} \right)\left( {TP + FN} \right)\left( {TN + FP} \right)\left( {TN + FN} \right)} }}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>where TP is the number of true-positive, TN of true-negative, FN of false-negative, and FP of false-positive values. Chicco et al. [<xref rid="bib44" ref-type="bibr">44</xref>] showed the appropriateness of MCC for evaluating binary classification performance by considering all 4 confusion matrix categories.</p><p>AUC, accuracy, specificity, sensitivity, positive predictive value (PPV), and negative predictive value (NPV) were also measured to compare models&#x02019; performance. AUC stands for area under the ROC curve, and the ROC curve is drawn by placing the false-positive rate, <inline-formula><tex-math id="TM0023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\ \frac{{FP}}{{FN + TP}}$\end{document}</tex-math></inline-formula>, to the x-axis and the true-positive rate, <inline-formula><tex-math id="TM0024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TP}}{{TP + FN}}\ $\end{document}</tex-math></inline-formula>, to the y-axis. Accuracy stands for <inline-formula><tex-math id="TM0025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TP + TN}}{{TP + FP + FN + TN}}$\end{document}</tex-math></inline-formula>, specificity for <inline-formula><tex-math id="TM0026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TN}}{{TN + FP}}$\end{document}</tex-math></inline-formula>, sensitivity for <inline-formula><tex-math id="TM0027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TP}}{{TP + FN}}$\end{document}</tex-math></inline-formula>, PPV for <inline-formula><tex-math id="TM0028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TP}}{{TP + FP}}$\end{document}</tex-math></inline-formula>, and NPV for <inline-formula><tex-math id="TM0029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\frac{{TN}}{{TN + FN}}$\end{document}</tex-math></inline-formula>. Apart from linear SVM, which outputs the class, predictive values should be converted into binary values by establishing a threshold. Considering the positive to negative ratio was not high (with a value of 0.81 as indicated in Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref>), we set the threshold to 0.5.</p></sec></sec><sec id="sec2-5"><title>Classification model algorithms</title><p>Eight different types of prediction algorithms&#x02014;namely, logistic regression, linear SVM, random forest, XGBoost, perceptron, and multilayer perceptron (MLP) with 1, 2, or 3 hidden layers&#x02014;were used to classify samples in IBD versus healthy using the features selected within the RFE phase. Within each training dataset, a classification model that predicts the status of IBD was built. Hyperparameters in each model were tuned using 5-fold cross-validation within the training set with grid search.</p><p>
<bold>
<italic toggle="yes">Logistic regression</italic>
</bold> outputs prediction scores between 0 and 1 by applying logistic function to linear regression. In this study, regularized logistic regression, using L2 penalty (i.e., ridge regression) was used. For the optimization, lbfgs solver was used with the tolerance parameter of 1e-4 (which followed the default setting of the library sklearn.linear_model.LogisticRegression by scikit-learn) [<xref rid="bib49" ref-type="bibr">49</xref>]. Regularization parameter was tuned using a grid of 2 to the power of (&#x02212;5, &#x02212;3, &#x02212;1, 1, 3, 5).</p><p>
<bold>
<italic toggle="yes">Linear SVM</italic>
</bold> linearly separates data maximizing the margin between 2 classes. The squared L2 penalty was applied to prevent overfitting. The &#x0201c;rfb&#x0201d; kernel was used, and &#x0201c;gamma&#x0201d; was set as &#x0201c;scale&#x0201d; so that it used 1/(# of features * variation of features) as the value of gamma (which followed the default setting of the library sklearn.svm.LinearSVC by scikit-learn) [<xref rid="bib49" ref-type="bibr">49</xref>]. Regularization parameter was tuned with a grid of 2 to the power of (&#x02212;5 to 15 with the step of 2).</p><p>
<bold>
<italic toggle="yes">Random forest</italic>
</bold> builds many decision trees and uses bagging to make an uncorrelated forest of trees combined for better prediction. Gini impurity was used to decide the splits, while the maximum depth was unlimited so that nodes were expanded until all leaves were pure or contained less than the parameter &#x0201c;min samples split.&#x0201d; The parameter &#x0201c;min samples split,&#x0201d; which is the minimum number of samples for splitting an internal node, and the parameter &#x0201c;min samples leaf,&#x0201d; which is the minimum of samples to be at a leaf node, were set equal to 2. The number of features to search for the best split was set as <inline-formula><tex-math id="TM0030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\sqrt {\# {\mathrm{\ of\ features}}} $\end{document}</tex-math></inline-formula>. Finally, the number of trees in the forest was set as 100.</p><p>Extreme gradient boosting (<bold><italic toggle="yes">XGBoost</italic></bold>) is a tree ensemble model that utilizes gradient boosting to combine many trees. XGBRegressor was trained with 200 gradient boosted trees. Base learners had 20 maximum tree depth. Subsample ratio of the training instance was 0.2, while subsample ratio of columns when constructing each tree was 0.5. Minimum loss reduction, which is for the partition on a leaf node, was set to 1. Learning rate was tuned with the grid of 0.005, 0.01, 0.05, 0.1, 0.5. Regularization factor, alpha, which is the L1 regularization term, was with the grid of 1e-3, 1e-2, 0.1, 1, 10.</p><p>
<bold>
<italic toggle="yes">Perceptron</italic>
</bold> is a single-layer neural network predicting an output with combinations of input values, weights, and bias. <bold><italic toggle="yes">MLP</italic></bold> is a feedforward neural network with multiple hidden layers. In the experiments, MLP Regressor, which has a single output layer, was built. LBFGS optimizer minimizing the squared error function was used to optimize weight variables, while ReLu function was utilized for the activation function. When the loss score was under 1e-4, the optimization was considered as converged, and the training stopped. Learning rate and L2 regularization term were tuned with the same procedure with XGBoost. MLP with 1, 2, and 3 hidden layers were constructed with hidden layer size corresponding to the number of features.</p></sec></sec><sec sec-type="results" id="sec3"><title>Results</title><sec id="sec3-1"><title>Mapping transformation and recursive feature elimination</title><p>Hyperparameter <inline-formula><tex-math id="TM0031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> used in mapping decides how much we weigh the similarity matrix in data transformation when performing RFE and was tuned in cross-validation within each internal test set as explained in <italic toggle="yes">Methods, Mapping, Mapping transformation and its theoretical advantages</italic> (see Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2</xref>). Interestingly, the MCC values exhibit different patterns when comparing mapping performed using Pearson correlation and mapping performed using Bray&#x02013;Curtis similarity. For mapping with Pearson correlation, the MCC shows an unstable profile and a sudden drop as shown in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2A</xref>, <xref rid="fig2" ref-type="fig">C</xref>. The optimal &#x003b1; value appears as the smallest value, 0.01, within the grid range of 0.01 to 1. On the other hand, Bray&#x02013;Curtis similarity mappings demonstrate an initial increase in the performance followed by a stable pattern of decline, with optimal &#x003b1; values equal to 0.15 and 0.05 at the species level for ED1 and ED2, respectively, and equal to 0.05 at the genus level for both ED1 and ED2.</p><fig position="float" id="fig2"><label>Figure 2:</label><caption><p>Average MCC across bootstraps at different values of hyperparameter <inline-formula><tex-math id="TM0032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{\alpha }}$\end{document}</tex-math></inline-formula> in mapping transformation. Hyperparameter <inline-formula><tex-math id="TM0033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
${\mathrm{\alpha }}$\end{document}</tex-math></inline-formula> is used for controlling the degree of mapping transformation. (A) Species level, mapping by Pearson correlation. (B) Species level, mapping by Pearson correlation. (C) Genus level, mapping by Pearson correlation. (D) Genus level, mapping by Pearson correlation.</p></caption><graphic xlink:href="giad083fig2" position="float"/></fig><p>Performance dependence on the number of features is illustrated in Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref>, where MCC is shown (i) with not mapping, (ii) with mapping performed using Pearson correlation, and (iii) with mapping performed using Bray&#x02013;Curtis similarity. The pattern is similar in the 3 cases, with MCC close to 0.8 slightly increasing with the number of features and saturating only at the very right side of the curve. As a consequence, the optimal number of features is high (87.1% of features are selected on average) in most of the RFE experiments. The only exception is represented by mapping based on Pearson correlation on ED1 at the species level, which reaches the maximum MCC when using 22 features. The maximum value of the MCC (average calculated on the 100 bootstrap samples) is shown in the label of Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref> as &#x0201c;Maximum MCC,&#x0201d; with the corresponding number of features indicated as &#x0201c;Optimal feature number.&#x0201d;</p><fig position="float" id="fig3"><label>Figure 3:</label><caption><p>Average MCC across the 100 bootstrapped recursive feature elimination process. MCC (y-axis) at varying number of features (x-axis) averaged across the 100 bootstrap internal test sets. Maximum MCC with optimal feature number is written in each legend. Error bars represent the standard deviation of MCC. (A) Species level, ED1. (B) Species level, ED2. (C) Genus level, ED1. (D) Genus level, ED2.</p></caption><graphic xlink:href="giad083fig3" position="float"/></fig></sec><sec id="sec3-2"><title>Feature stability</title><sec id="sec3-2-1"><title>Stability indexes</title><p>Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref> illustrates the values of the stability metrics across the different bootstraps in each experiment. At the species level, mapping data using Bray&#x02013;Curtis similarity allows reaching better stability with every dataset and every metric. At the genus level, mapping data using Bray&#x02013;Curtis similarity still shows the better stability in the ED1 dataset (with the exception of Hamming distance and SRCC metrics) and in the ED2 dataset (with the exception of Hamming distance metric).</p><table-wrap position="float" id="tbl2"><label>Table 2:</label><caption><p>Stability metrics for each recursive feature elimination experiment. For each dataset, the best stability value is highlighted in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="6" align="center" rowspan="1">Level: Species</th></tr><tr><th rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1">SRCC</th><th align="center" rowspan="1" colspan="1">Pearson correlation</th><th align="center" rowspan="1" colspan="1">Hamming distance</th><th align="center" rowspan="1" colspan="1">Bray&#x02013;Curtis dissimilarity</th><th align="center" rowspan="1" colspan="1">Euclidean distance</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">ED1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">No mapping</td><td rowspan="1" colspan="1">0.662</td><td rowspan="1" colspan="1">0.653</td><td rowspan="1" colspan="1">0.919</td><td rowspan="1" colspan="1">0.085</td><td rowspan="1" colspan="1">1.402</td></tr><tr><td rowspan="1" colspan="1">Mapping: Pearson correlation</td><td rowspan="1" colspan="1">0.652</td><td rowspan="1" colspan="1">0.642</td><td rowspan="1" colspan="1">0.92</td><td rowspan="1" colspan="1">0.086</td><td rowspan="1" colspan="1">1.425</td></tr><tr><td rowspan="1" colspan="1">Mapping: Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">
<bold>0.667</bold>
</td><td rowspan="1" colspan="1">
<bold>0.689</bold>
</td><td rowspan="1" colspan="1">
<bold>0.918</bold>
</td><td rowspan="1" colspan="1">
<bold>0.08</bold>
</td><td rowspan="1" colspan="1">
<bold>1.325</bold>
</td></tr><tr><td rowspan="1" colspan="1">ED2</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">No mapping</td><td rowspan="1" colspan="1">0.617</td><td rowspan="1" colspan="1">0.639</td><td rowspan="1" colspan="1">0.925</td><td rowspan="1" colspan="1">0.088</td><td rowspan="1" colspan="1">1.423</td></tr><tr><td rowspan="1" colspan="1">Mapping: Pearson correlation</td><td rowspan="1" colspan="1">0.579</td><td rowspan="1" colspan="1">0.599</td><td rowspan="1" colspan="1">0.928</td><td rowspan="1" colspan="1">0.094</td><td rowspan="1" colspan="1">1.505</td></tr><tr><td rowspan="1" colspan="1">Mapping: Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">
<bold>0.643</bold>
</td><td rowspan="1" colspan="1">
<bold>0.662</bold>
</td><td rowspan="1" colspan="1">
<bold>0.921</bold>
</td><td rowspan="1" colspan="1">
<bold>0.084</bold>
</td><td rowspan="1" colspan="1">
<bold>1.355</bold>
</td></tr><tr><td colspan="6" align="left" rowspan="1">
<bold>Level: Genus</bold>
</td></tr><tr><td rowspan="1" colspan="1">ED1</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">No mapping</td><td rowspan="1" colspan="1">
<bold>0.67</bold>
</td><td rowspan="1" colspan="1">0.671</td><td rowspan="1" colspan="1">0.921</td><td rowspan="1" colspan="1">
<bold>0.1</bold>
</td><td rowspan="1" colspan="1">1.689</td></tr><tr><td rowspan="1" colspan="1">Mapping: Pearson correlation</td><td rowspan="1" colspan="1">0.607</td><td rowspan="1" colspan="1">0.618</td><td rowspan="1" colspan="1">
<bold>0.903</bold>
</td><td rowspan="1" colspan="1">0.108</td><td rowspan="1" colspan="1">1.819</td></tr><tr><td rowspan="1" colspan="1">Mapping: Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">0.667</td><td rowspan="1" colspan="1">
<bold>0.677</bold>
</td><td rowspan="1" colspan="1">0.919</td><td rowspan="1" colspan="1">
<bold>0.1</bold>
</td><td rowspan="1" colspan="1">
<bold>1.672</bold>
</td></tr><tr><td rowspan="1" colspan="1">ED2</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">No mapping</td><td rowspan="1" colspan="1">0.698</td><td rowspan="1" colspan="1">0.715</td><td rowspan="1" colspan="1">
<bold>0.916</bold>
</td><td rowspan="1" colspan="1">0.093</td><td rowspan="1" colspan="1">1.561</td></tr><tr><td rowspan="1" colspan="1">Mapping: Pearson correlation</td><td rowspan="1" colspan="1">0.647</td><td rowspan="1" colspan="1">0.662</td><td rowspan="1" colspan="1">0.922</td><td rowspan="1" colspan="1">0.102</td><td rowspan="1" colspan="1">1.705</td></tr><tr><td rowspan="1" colspan="1">Mapping: Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">
<bold>0.707</bold>
</td><td rowspan="1" colspan="1">
<bold>0.733</bold>
</td><td rowspan="1" colspan="1">0.917</td><td rowspan="1" colspan="1">
<bold>0.089</bold>
</td><td rowspan="1" colspan="1">
<bold>1.514</bold>
</td></tr></tbody></table></table-wrap><p>
<xref rid="sup9" ref-type="supplementary-material">Supplementary Figs. S1 and S2</xref> show the <italic toggle="yes">number of commonly ranked features</italic> across all 100 bootstraps or across at least 66 or 50 bootstraps. At small ranks, the difference between different mapping strategies is not clear as the number of common features across bootstraps is too small. However, as more features are considered, RFE with Bray&#x02013;Curtis similarity&#x02013;based mapping shows a greater number of common features in every dataset except ED1 at the genus level.</p></sec><sec id="sec3-2-2"><title>Noise filtering</title><p>To assess the noise-filtering ability of the various approaches (<italic toggle="yes">Methods, Evaluation metrics, Stability</italic>), we subtracted the average rank of noisy features from the average rank of original data features (283 features at the species level, 220 features at the genus level). In Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref>, except ED1 at the species level, mapping with Bray&#x02013;Curtis similarity shows a better noise-filtering score than the others. The superiority of mapping with Bray&#x02013;Curtis similarity is more apparent at the genus level, as the gap is 3 times bigger compared to the species level (22.87 to 77.66 and 26.06 to 80.49). While mapping with Bray&#x02013;Curtis similarity shows consistent noise-filtering ability, mapping with Pearson correlation shows unstable noise-filtering ability, which is sometimes worse than no mapping.</p><table-wrap position="float" id="tbl3"><label>Table 3:</label><caption><p>Noise-filtering result for each recursive feature elimination experiment. The margin between the rank of noise features and original features is calculated.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Noise-filtering score</th><th colspan="2" align="center" rowspan="1">avg(rank of noises)&#x02014;avg(rank of real) * rank by recursive feature elimination</th></tr><tr><th rowspan="1" colspan="1">Taxonomy level/algorithm</th><th align="center" rowspan="1" colspan="1">Species</th><th align="center" rowspan="1" colspan="1">Genus</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">ED 1: no mapping</td><td rowspan="1" colspan="1">11.30</td><td rowspan="1" colspan="1">34.61</td></tr><tr><td rowspan="1" colspan="1">ED 1: mapping with Pearson correlation</td><td rowspan="1" colspan="1">27.17</td><td rowspan="1" colspan="1">&#x02212;5.92</td></tr><tr><td rowspan="1" colspan="1">ED 1: mapping with Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">22.87</td><td rowspan="1" colspan="1">77.66</td></tr><tr><td rowspan="1" colspan="1">ED 2: no mapping</td><td rowspan="1" colspan="1">20.19</td><td rowspan="1" colspan="1">33.21</td></tr><tr><td rowspan="1" colspan="1">ED 2: mapping with Pearson correlation</td><td rowspan="1" colspan="1">14.10</td><td rowspan="1" colspan="1">33.42</td></tr><tr><td rowspan="1" colspan="1">ED 2: mapping with Bray&#x02013;Curtis similarity</td><td rowspan="1" colspan="1">26.06</td><td rowspan="1" colspan="1">80.49</td></tr></tbody></table></table-wrap></sec></sec><sec id="sec3-3"><title>Performance comparison by selected set of features</title><p>Prediction models were built based on the optimal set of features identified using linear SVM-based RFE and Bray&#x02013;Curtis similarity&#x02013;based mapping (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref>). To validate the methods, the models trained on ED1 were tested in both test 1&#x02013;ED1 and on the entire ED2, and models trained on ED2 were tested on test 2&#x02013;ED2 and on the entire ED1. Eight different prediction algorithms (see <italic toggle="yes">Methods, Classification Model Algorithms</italic>) were tested. Overall performance obtained by different methods using different metrics (AUC, accuracy, sensitivity, specificity, PPV, NPV, MCC) is illustrated in <xref rid="sup9" ref-type="supplementary-material">Supplementary Tables S1&#x02013;S4</xref>. <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S5</xref> summarizes the best MCC with the best-performing algorithm in each experiment. MLP with 1 or 2 hidden layers shows the best MCC in most of the experiments (23 among 24 results).</p><p>As the optimal number of features is high in most RFE experiments (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref>), prediction models were also trained and compared with a constant number of a few top features. We choose to consider the top 14 features (see <italic toggle="yes">Methods, Full Data Experiment and Biomarker Selection</italic>). Results obtained using different algorithms are shown in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S6</xref>. With a smaller number of selected features, the performance slightly decreases and the best-performing algorithm varies. Random forest shows the best performance in the most cases (15 among 24), followed by MLP-1,2 hidden layer, XGBoost, logistic regression, and linear SVM. RFE without mapping shows a better performance than RFE with mapping by Bray&#x02013;Curtis similarity with 0.0016 on average. Considering the small gap, mapping with Bray&#x02013;Curtis similarity, while improving feature stability, does not seem to affect method performance in terms of MCC, which overall, remains quite stable.</p></sec><sec id="sec3-4"><title>Full data experiment and biomarker selection</title><p>The best pipeline for biomarker selection (linear SVM-based RFE with mapping using Bray&#x02013;Curtis similarity) is applied to the full dataset at the species level without any split (i.e., training set is the combination of training sets in ED1 and ED2, test set of test sets in ED1 and ED2) to obtain the possible biomarkers for IBD. During the RFE process, we compute the average MCC across the 100 bootstrap internal test sets. Similar to Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref>, average MCC slightly increases with the number of features, and the maximum MCC is reached corresponding to 267 features among the 283 considered. The 8 different predictive models presented in <italic toggle="yes">Methods, Classification Model Algorithms</italic> are trained using the selected features, illustrated in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S7</xref>. MLP-1,2 hidden layers consistently show the highest performance in terms of MCC, confirming what was previously found.</p><p>As the optimal number of features is high in most RFE experiments (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3</xref>), prediction models were also trained and compared with a constant number of a few top features. We choose to consider the top 14 features as a tradeoff between optimal performance and generalizability potential, based on results shown in Fig.&#x000a0;<xref rid="fig4" ref-type="fig">4</xref> where we calculated the &#x000a0;&#x00394;MCC as the difference in MCC at different subsequent numbers of features and selected the value at which the &#x000a0;&#x00394;MCC starts to converge to zero. Compared to models with optimal number of features (267), overall performance decreases and the best model algorithm changes. While MLP with 1 hidden layer is the best algorithm with MCC 0.963 in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S7</xref>, it decreases to 0.826. Instead, random forest decreases to 0.845 from 0.854, which is the maximum MCC among the 8 algorithms.</p><fig position="float" id="fig4"><label>Figure 4:</label><caption><p>Differential of average MCC during RFE. Average MCC is calculated in each feature number, and each average MCC is subtracted by its former average MCC to calculate differentials. x-axis: the number of features, y-axis: differential of average MCC, horizontal red line: y = 0.</p></caption><graphic xlink:href="giad083fig4" position="float"/></fig><p>With the final random forest&#x02013;based predictive model, Shapley values are calculated on the training, test, and external datasets for the top 14 ranked features. Shapley values calculate the extent that each feature at each data sample contributes to the change of output of the prediction model. Since Shapley values are model agnostic, which provides local explanations, they are calculated on the training and test datasets and on dataset 4 (for which we do not know the sample labels but still can run the models and calculate the Shapley values). Shapley values on the training set are shown in Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5</xref> (Shapley values on the test and the external dataset 4 are shown in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S8</xref>). The rank of the 14 features based on Shapley values shows a high similarity across different datasets (training, test, and external dataset 4), and the directions, which increase or decrease the possibility of IBD, are identical. Random forest model with 14 selected features determines <italic toggle="yes">Lachnospiraceae (f) Butyrivibrio (g), Bacteroides (g) fragilis (s), Porphyromonadaceae (f) Dysgonomonas (g), Erysipelotrichaceae (f) cc_115 (g), Fusobacteriaceae (f) Fusobacterium (g), Alkalimonas (g) amylolytica (s), Lactobacillus (g) zeae (s), Peptococcaceae (f) Peptococcus (g)</italic> as indicators of IBD state, and <italic toggle="yes">Corynebacteriaceae (f) Corynebacterium (g), Tissierellaceae (f) WAL_1855D (g), Campylobacteraceae (f) Campylobacter (g), Lachnospiraceae (f) Ruminococcus (g), Porphyromonadaceae (f) Parabacteroides (g), Lactobacillus (g) iners (s)</italic> as indicators of negative of IBD (<italic toggle="yes">f</italic>: family, <italic toggle="yes">g</italic>: genus, <italic toggle="yes">s</italic>: species level).</p><fig position="float" id="fig5"><label>Figure 5:</label><caption><p>Shapley additive explanation (SHAP) summary plot of primary indicators of IBD and non-IBD. Random forest model trained by top 14 features in RFE (species level, trained by the combination of training sets in ED1 and ED2) is used to calculate SHAP values. SHAP values are measured by the training dataset. Total rank is the rank between 14 features used for training the model.</p></caption><graphic xlink:href="giad083fig5" position="float"/></fig></sec></sec><sec sec-type="discussion" id="sec4"><title>Discussion</title><p>Stable feature selection is a prerequisite to decide strong biomarkers. To prove the validity of a set of features as biomarkers, they should be observed consistently every time a feature selection algorithm is applied. In this article, we performed RFE multiple times using bootstrapping and checked the level of consistency of feature ranks between trials using stability indexes. Moreover, we investigated various strategies that could improve the stability of selected biomarkers.</p><p>First, we used bootstrap to cope with the scarcity of samples in the training set compared to the number of features [<xref rid="bib50" ref-type="bibr">50</xref>]. In this way, we generated various classifiers, selected various features on various data splits, and then averaged the results, preserving a high ranking only for those features that are consistently the most discriminating features across the splits. Second, since the high number of features makes the problem underconstrained (i.e., many possible sets of features can be considered relevant to the task and equally good in terms of accuracy), we used additional information from an external dataset (dataset 4) to include additional constraints in terms of feature mapping. In other words, if some features are strongly correlated, then they likely have similar importance, and we want them to be equally relevant for the classification task.</p><p>Three different datasets were merged to increase the number of examples and mitigate the potential batch effect and then split in two. As the division of the dataset was performed using bootstrapping, the ratio of non-IBD versis IBD was preserved, and the ratio of the data source was also preserved without significant difference, as shown in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S9</xref>. However, the distribution of each feature may differ as the size of the dataset was small compared to the number of features.</p><p>Mapping transformation with linear SVM-based RFE improved the stability without the loss of performance. We recommended using Bray&#x02013;Curtis similarity to improve the stability of the selected features since, in most cases, mapping using Bray&#x02013;Curtis similarity showed a significant increase of stability compared to other approaches and higher noise-filtering score, at both genus and species levels.</p><p>Moreover, as shown in <xref rid="sup9" ref-type="supplementary-material">Supplementary Figs. S1 and S2</xref>, mapping with Bray&#x02013;Curtis similarity presents a higher number of common features, already at an early stage of RFE.</p><p>Notably, the Bray&#x02013;Curtis similarity was designed for compositional data (a composition is represented by a vector of proportions with respect to a total sum on the sample under observation) and is therefore more robust to spurious correlations introduced by the fact that the taxonomic abundances are multivariate observations whose information content is closely linked to the relationship between the components.</p><p>A possible drawback of our method is that the mapping transformation compresses the information by shortening the distance between similar features. In case the degree of the transformation is high (<inline-formula><tex-math id="TM0034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha \ $\end{document}</tex-math></inline-formula> in the transformation matrix equation <inline-formula><tex-math id="TM0035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$P = {D}^{ - 1}( {I + \alpha ( {S - I} )} )$\end{document}</tex-math></inline-formula>), the loss of potential information can be important. Choosing <inline-formula><tex-math id="TM0036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> in cross-validation as done in this work might help mitigate this risk, although at a higher computational cost. Indeed, in our experiments, the optimal choice of <inline-formula><tex-math id="TM0037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> did not highlight a loss in classification performance.</p><p>There are similar approaches that use the similarity matrix to map similar features into closer space [<xref rid="bib23" ref-type="bibr">23&#x02013;26</xref>]. AggMapNet maps the original data into multichannel 2D spatial-correlated images based on their pairwise correlation distances using the manifold learning method UMAP [<xref rid="bib27" ref-type="bibr">27</xref>]. Different channels are chosen based on a preliminary clustering step, which is again based on pairwise correlation distances between features. Features maps are then given as input to machine learning methods such as convolutional neural networks to perform classification. Results obtained using the AggMapNet [<xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref>] approach on our dataset are shown in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S10</xref> and compared with our pipeline (Bray&#x02013;Curtis similarity&#x02013;based mapping + RFE + MLP). This comparison proves the strength of our pipeline in the situation of small data and a high number of features, which is typical of omics studies. Differently from AggMapNet [<xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref>], our pipeline does not project the data in multichannel 2D spatial-correlated images and thus is not suitable for 2D data representation and omics data integration.</p><p>To further assess the consistency of our results and the robustness of our approach, we calculated the False Discovery Rate (FDR)-adjusted <italic toggle="yes">P</italic> values using the Wilcoxon rank-sum test, fold change and average, and standard deviation information of the top 14 biomarkers we selected in <xref rid="sup9" ref-type="supplementary-material">Supplementary Table S11</xref>. We found that 13 of 14 biomarkers had a <italic toggle="yes">q</italic> value below 0.001 when comparing between IBD and non-IBD samples. However, despite the statistically significant differences in the markers, 52.3% of the features showed significant differences between IBD and non-IBD groups, which implies that relying solely on statistical tests may not be sufficient in determining good biomarkers. The robustness of our algorithm and the excellence of machine learning algorithms are shown by high-performing prediction for this sparse type of dataset.</p><p>It should be reported that we originally performed the overall experiment without logarithmic transformation (data not shown) and checked later that logarithmic transformation improved the performance significantly. We expect the logarithmic transformation has accounted for the highly skewed distribution of microbial abundances [<xref rid="bib51" ref-type="bibr">51</xref>]. Before using the logarithmic transformation, the best-performing algorithm was random forest instead of MLP. While tree-based ensembles utilize feature splitting based on a set of thresholds, neural network uses continuous values to build nonlinear relationships among variables, and logarithmic transformation is expected to advantage the performance in this aspect.</p><p>However, when fewer features are considered, random forest outperforms other methods, including MLP. In recent studies, random forest provides generally higher performance than other conventional algorithms for microbial data analysis [<xref rid="bib52" ref-type="bibr">52&#x02013;55</xref>]. We suppose the higher complexity of the MLP algorithm takes more advantage when a higher number of features is considered, whereas in case fewer features are considered, the &#x0201c;split&#x0201d; approach of tree-based ensemble models may fit to microbial data better.</p></sec><sec id="sec5"><title>Availability of Source Code and Requirements</title><p>Project name: MLonMicrobiome</p><p>Project homepage: <ext-link xlink:href="https://gitlab.com/sysbiobig/mlonmicrobiome" ext-link-type="uri">https://gitlab.com/sysbiobig/mlonmicrobiome</ext-link></p><p>Operating system(s): Platform independent</p><p>Programming language: Python, R</p><p>Other requirements: R 4.1.3, phyloseq 1.27.6, openxlsx 4.2.4, Python 3.85, scikit-learn 1.0.2</p><p>License: GNU GPL</p><p>
<ext-link xlink:href="https://scicrunch.org/resolver/https://scicrunch.org/resolver/RRID:" ext-link-type="uri">RRID:</ext-link> N/A</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>giad083_GIGA-D-23-00164_Original_Submission</label><media xlink:href="giad083_giga-d-23-00164_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup2" position="float" content-type="local-data"><label>giad083_GIGA-D-23-00164_Revision_1</label><media xlink:href="giad083_giga-d-23-00164_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup3" position="float" content-type="local-data"><label>giad083_GIGA-D-23-00164_Revision_2</label><media xlink:href="giad083_giga-d-23-00164_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup4" position="float" content-type="local-data"><label>giad083_Response_to_Reviewer_Comments_Original_Submission</label><media xlink:href="giad083_response_to_reviewer_comments_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup5" position="float" content-type="local-data"><label>giad083_Response_to_Reviewer_Comments_Revision_1</label><media xlink:href="giad083_response_to_reviewer_comments_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup6" position="float" content-type="local-data"><label>giad083_Reviewer_1_Report_Original_Submission</label><caption><p>Wan Xiang Shen -- 7/10/2023 Reviewed</p></caption><media xlink:href="giad083_reviewer_1_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup7" position="float" content-type="local-data"><label>giad083_Reviewer_2_Report_Original_Submission</label><caption><p>Jakob Wirbel -- 7/16/2023 Reviewed</p></caption><media xlink:href="giad083_reviewer_2_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup8" position="float" content-type="local-data"><label>giad083_Reviewer_2_Report_Revision_1</label><caption><p>Jakob Wirbel -- 8/29/2023 Reviewed</p></caption><media xlink:href="giad083_reviewer_2_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup9" position="float" content-type="local-data"><label>giad083_Supplemental_File</label><media xlink:href="giad083_supplemental_file.docx"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgement</title><p>This research was supported by a grant of the Korea Health Technology R&#x00026;D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health &#x00026; Welfare, Republic of Korea(grant HI21C1092) and by the SEED Project &#x0201c;tRajectoriEs of baCtErial NeTwoRks from hEalthy to disease state and back (RECENTRE)&#x0201d; funded by the Department of Information Engineering of the University of Padova, grant DI_C_BIRD2020_01.</p></ack><sec sec-type="data-availability" id="sec7"><title>Data Availability</title><p>Four datasets were downloaded from Qiita, an open-source microbial study management platform (see Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref> for the reference Qiita study ID for reproducibility) [<xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib32" ref-type="bibr">32&#x02013;34</xref>]. Data after each preprocessing stage can be accessed by our code repository. Our code and other data further supporting this work, including DOME-ML annotations, are openly available in the <italic toggle="yes">GigaScience</italic> repository GigaDB [<xref rid="bib31" ref-type="bibr">31</xref>].</p></sec><sec id="sec8"><title>Abbreviations</title><p>AUC: area under the receiver operating characteristic curve; CD: Canberra distance; IBD: inflammatory bowel disease; MCC: Matthew correlation coefficient; MLP: multilayer perceptron; RF: random forest; RFE: recursive feature elimination; SRCC: Spearman&#x02019;s rank correlation coefficient; SVM: support vector machine; WGS: whole-genome shotgun sequencing; XGBoost: Extreme Gradient Boosting.</p></sec><sec sec-type="COI-statement" id="sec9"><title>Competing Interests</title><p>The authors declare that they have no competing interests.</p></sec><ref-list id="ref1"><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Quince</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Walker</surname><given-names>AW</given-names></string-name>, <string-name><surname>Simpson</surname><given-names>JT</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Shotgun metagenomics, from sampling to analysis</article-title>. <source>Nat Biotechnol</source>. <year>2017</year>;<volume>35</volume>(<issue>9</issue>):<fpage>833</fpage>&#x02013;<lpage>44</lpage>. <comment>Erratum in: Nat Biotechnol. 2017;35(12):1211. </comment><pub-id pub-id-type="doi">10.1038/nbt.3935</pub-id>.<pub-id pub-id-type="pmid">28898207</pub-id></mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kamble</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Sawant</surname><given-names>S</given-names></string-name>, <string-name><surname>Singh</surname><given-names>H</given-names></string-name></person-group>. <article-title>16S ribosomal RNA gene-based metagenomics: a review</article-title>. <source>Biomed Res J</source>. <year>2020</year>;<volume>7</volume>:<fpage>5</fpage>&#x02013;<lpage>11</lpage>. <pub-id pub-id-type="doi">10.4103/BMRJ.BMRJ_4_20</pub-id>.</mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Breitwieser</surname>
<given-names>FP</given-names>
</string-name>, <string-name><surname>Lu</surname><given-names>J</given-names></string-name>, <string-name><surname>Salzberg</surname><given-names>SL</given-names></string-name></person-group>. <article-title>A review of methods and databases for metagenomic classification and assembly</article-title>. <source>Brief Bioinform</source>. <year>2019</year>;<volume>20</volume>(<issue>4</issue>):<fpage>1125</fpage>&#x02013;<lpage>36</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbx120</pub-id>.<pub-id pub-id-type="pmid">29028872</pub-id></mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bharti</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Grimm</surname><given-names>DG</given-names></string-name></person-group>. <article-title>Current challenges and best-practice protocols for microbiome analysis</article-title>. <source>Brief Bioinform</source>. <year>2021</year>;<volume>22</volume>(<issue>1</issue>):<fpage>178</fpage>&#x02013;<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbz155</pub-id>.<pub-id pub-id-type="pmid">31848574</pub-id></mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Blaxter</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Mann</surname><given-names>J</given-names></string-name>, <string-name><surname>Chapman</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Defining operational taxonomic units using DNA barcode data</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2005</year>;<volume>360</volume>(<issue>1462</issue>):<fpage>1935</fpage>&#x02013;<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1098/rstb.2005.1725</pub-id>.<pub-id pub-id-type="pmid">16214751</pub-id></mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Callahan</surname>
<given-names>BJ</given-names>
</string-name>, <string-name><surname>McMurdie</surname><given-names>PJ</given-names></string-name>, <string-name><surname>Holmes</surname><given-names>SP</given-names></string-name></person-group>. <article-title>Exact sequence variants should replace operational taxonomic units in marker-gene data analysis</article-title>. <source>ISME J</source>. <year>2017</year>;<volume>11</volume>(<issue>12</issue>):<fpage>2639</fpage>&#x02013;<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1038/ismej.2017.119</pub-id>.<pub-id pub-id-type="pmid">28731476</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Manandhar</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Alimadadi</surname><given-names>A</given-names></string-name>, <string-name><surname>Aryal</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Gut microbiome-based supervised machine learning for clinical diagnosis of inflammatory bowel diseases</article-title>. <source>Am J Physiol Gastrointest Liver Physiol</source>. <year>2021</year>;<volume>320</volume>(<issue>3</issue>):<fpage>G328</fpage>&#x02013;<lpage>37</lpage>. <pub-id pub-id-type="doi">10.1152/ajpgi.00360.2020</pub-id>.<pub-id pub-id-type="pmid">33439104</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>X</given-names>
</string-name>, <string-name><surname>Xiao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Xu</surname><given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Characteristics of fecal microbiota and machine learning strategy for fecal invasive biomarkers in pediatric inflammatory bowel disease</article-title>. <source>Front Cell Infect Microbiol</source>. <year>2021</year>;<volume>11</volume>:<fpage>711884</fpage>. <pub-id pub-id-type="doi">10.3389/fcimb.2021.711884</pub-id>.<pub-id pub-id-type="pmid">34950604</pub-id></mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thomas</surname>
<given-names>AM</given-names>
</string-name>, <string-name><surname>Manghi</surname><given-names>P</given-names></string-name>, <string-name><surname>Asnicar</surname><given-names>F</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Metagenomic analysis of colorectal cancer datasets identifies cross-cohort microbial diagnostic signatures and a link with choline degradation</article-title>. <source>Nat Med</source>. <year>2019</year>;<volume>25</volume>(<issue>4</issue>):<fpage>667</fpage>&#x02013;<lpage>78</lpage>. <comment>Erratum in: Nat Med. 2019;25(12):1948. </comment><pub-id pub-id-type="doi">10.1038/s41591-019-0405-7</pub-id>.<pub-id pub-id-type="pmid">30936548</pub-id></mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gao</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Zhu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Sun</surname><given-names>F</given-names></string-name></person-group>. <article-title>Increasing prediction performance of colorectal cancer disease status using random forests classification based on metagenomic shotgun sequencing data</article-title>. <source>Synth Syst Biotechnol</source>. <year>2022</year>;<volume>7</volume>(<issue>1</issue>):<fpage>574</fpage>&#x02013;<lpage>85</lpage>. <pub-id pub-id-type="doi">10.1016/j.synbio.2022.01.005</pub-id>.<pub-id pub-id-type="pmid">35155839</pub-id></mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Aryal</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Alimadadi</surname><given-names>A</given-names></string-name>, <string-name><surname>Manandhar</surname><given-names>I</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Machine learning strategy for gut microbiome-based diagnostic screening of cardiovascular disease</article-title>. <source>Hypertension</source>. <year>2020</year>;<volume>76</volume>(<issue>5</issue>):<fpage>1555</fpage>&#x02013;<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1161/HYPERTENSIONAHA.120.15885</pub-id>.<pub-id pub-id-type="pmid">32909848</pub-id></mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Marcos-Zambrano</surname>
<given-names>LJ</given-names>
</string-name>, <string-name><surname>Karaduzovic-Hadziabdic</surname><given-names>K</given-names></string-name>, <string-name><surname>Loncar&#x000a0;Turukalo</surname><given-names>T</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Applications of machine learning in Human microbiome studies: a review on feature selection, biomarker identification, disease prediction and treatment</article-title>. <source>Front Microbiol</source>. <year>2021</year>;<volume>12</volume>:<fpage>634511</fpage>. <pub-id pub-id-type="doi">10.3389/fmicb.2021.634511</pub-id>.<pub-id pub-id-type="pmid">33737920</pub-id></mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<collab>Human Microbiome Project Consortium</collab>
</person-group>. <article-title>Structure, function and diversity of the healthy human microbiome</article-title>. <source>Nature</source>. <year>2012</year>;<volume>486</volume>(<issue>7402</issue>):<fpage>207</fpage>&#x02013;<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1038/nature11234</pub-id>.<pub-id pub-id-type="pmid">22699609</pub-id></mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thompson</surname>
<given-names>LR</given-names>
</string-name>, <string-name><surname>Sanders</surname><given-names>JG</given-names></string-name>, <string-name><surname>McDonald</surname><given-names>D</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Earth Microbiome Project Consortium. A communal catalogue reveals Earth's multiscale microbial diversity</article-title>. <source>Nature</source>. <year>2017</year>;<volume>551</volume>(<issue>7681</issue>):<fpage>457</fpage>&#x02013;<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1038/nature24621</pub-id>.<pub-id pub-id-type="pmid">29088705</pub-id></mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<collab>Integrative HMP (iHMP) Research Network Consortium</collab>
</person-group>. <article-title>The Integrative Human Microbiome Project</article-title>. <source>Nature</source>. <year>2019</year>;<volume>569</volume>(<issue>7758</issue>):<fpage>641</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-019-1238-8</pub-id>.<pub-id pub-id-type="pmid">31142853</pub-id></mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lloyd-Price</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Arze</surname><given-names>C</given-names></string-name>, <string-name><surname>Ananthakrishnan</surname><given-names>AN</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases</article-title>. <source>Nature</source>. <year>2019</year>;<volume>569</volume>(<issue>7758</issue>):<fpage>655</fpage>&#x02013;<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-019-1237-9</pub-id>.<pub-id pub-id-type="pmid">31142855</pub-id></mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hornung</surname>
<given-names>BVH</given-names>
</string-name>, <string-name><surname>Zwittink</surname><given-names>RD</given-names></string-name>, <string-name><surname>Kuijper</surname><given-names>EJ</given-names></string-name></person-group>. <article-title>Issues and current standards of controls in microbiome research</article-title>. <source>FEMS Microbiol Ecol</source>. <year>2019</year>;<volume>95</volume>(<issue>5</issue>):<fpage>fiz045</fpage>. <pub-id pub-id-type="doi">10.1093/femsec/fiz045</pub-id>.<pub-id pub-id-type="pmid">30997495</pub-id></mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Cernava</surname>
<given-names>T</given-names>
</string-name>, <string-name><surname>Rybakova</surname><given-names>D</given-names></string-name>, <string-name><surname>Buscot</surname><given-names>F</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Metadata harmonization-standards are the key for a better usage of omics data for integrative microbiome analysis</article-title>. <source>Environ Microbiome</source>. <year>2022</year>;<volume>17</volume>(<issue>1</issue>):<fpage>33</fpage>. <pub-id pub-id-type="doi">10.1186/s40793-022-00425-1</pub-id>.<pub-id pub-id-type="pmid">35751093</pub-id></mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Duvallet</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Gibbons</surname><given-names>SM</given-names></string-name>, <string-name><surname>Gurry</surname><given-names>T</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Meta-analysis of gut microbiome studies identifies disease-specific and shared responses</article-title>. <source>Nat Commun</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>1784</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-017-01973-8</pub-id>.<pub-id pub-id-type="pmid">29209090</pub-id></mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gloor</surname>
<given-names>GB</given-names>
</string-name>, <string-name><surname>Macklaim</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pawlowsky-Glahn</surname><given-names>V</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Microbiome datasets are compositional: and this is not optional</article-title>. <source>Front Microbiol</source>. <year>2017</year>;<volume>8</volume>:<fpage>2224</fpage>. <pub-id pub-id-type="doi">10.3389/fmicb.2017.02224</pub-id>.<pub-id pub-id-type="pmid">29187837</pub-id></mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Guyon</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Weston</surname><given-names>J</given-names></string-name>, <string-name><surname>Barnhill</surname><given-names>S</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Gene selection for cancer classification using support vector machines</article-title>. <source>Machine Learning</source>. <year>2002</year>;<volume>46</volume>(<issue>1/3</issue>):<fpage>389</fpage>&#x02013;<lpage>422</lpage>. <pub-id pub-id-type="doi">10.1023/A:1012487302797</pub-id>.</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sanavia</surname>
<given-names>T</given-names>
</string-name>, <string-name><surname>Aiolli</surname><given-names>F</given-names></string-name>, <string-name><surname>Da&#x000a0;San&#x000a0;Martino</surname><given-names>G</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Improving biomarker list stability by integration of biological knowledge in the learning process</article-title>. <source>BMC Bioinf</source>. <year>2012</year>;<volume>13</volume>(<issue>Suppl 4</issue>):<fpage>S22</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2105-13-S4-S22</pub-id>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shen</surname>
<given-names>WX</given-names>
</string-name>, <string-name><surname>Liang</surname><given-names>SR</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>YY</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Enhanced metagenomic deep learning for disease prediction and consistent signature recognition by restructured microbiome 2D representations</article-title>. <source>Patterns</source>. <year>2023</year>;<volume>4</volume>(<issue>1</issue>):<fpage>100658</fpage>. <pub-id pub-id-type="doi">10.1016/j.patter.2022.100658</pub-id>.<pub-id pub-id-type="pmid">36699735</pub-id></mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shen</surname>
<given-names>WX</given-names>
</string-name>, <string-name><surname>Liu</surname><given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname><given-names>Y</given-names></string-name></person-group>. <article-title>AggMapNet: enhanced and explainable low-sample omics deep learning with feature-aggregated multi-channel networks</article-title>. <source>Nucleic Acids Res</source>. <year>2022</year>;<volume>50</volume>(<issue>8</issue>):<fpage>e45</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gkac010</pub-id>.<pub-id pub-id-type="pmid">35100418</pub-id></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Ma</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name></person-group>. <article-title>OmicsMapNet: transforming omics data to take advantage of deep convolutional neural network for discovery</article-title>. <year>2018</year>. <comment>arXiv. <ext-link xlink:href="https://arxiv.org/abs/1804.05283" ext-link-type="uri">https://doi.org/10.48550/arXiv.1804.05283</ext-link></comment></mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bazgir</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Zhang</surname><given-names>R</given-names></string-name>, <string-name><surname>Dhruba</surname><given-names>SR</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Representation of features as images with neighborhood dependencies for compatibility with convolutional neural networks</article-title>. <source>Nat Commun</source>. <year>2020</year>;<volume>11</volume>:<fpage>4391</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-18197-y</pub-id>.<pub-id pub-id-type="pmid">32873806</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>McInnes</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Healy</surname><given-names>J</given-names></string-name>, <string-name><surname>Melville</surname><given-names>J</given-names></string-name></person-group>. <article-title>UMAP: Uniform Manifold Approximation and Projection</article-title>. <source>J Open Source Softw</source>. <year>2018</year>;<volume>3</volume>(<issue>29</issue>):<fpage>861</fpage>. <pub-id pub-id-type="doi">10.21105/joss.00861</pub-id>.</mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lundberg</surname>
<given-names>SM</given-names>
</string-name>, <string-name><surname>Lee</surname><given-names>S-I</given-names></string-name></person-group>. <article-title>A unified approach to interpreting model predictions</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2017</year>, <fpage>4765</fpage>&#x02013;<lpage>4774</lpage>.. <ext-link xlink:href="https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html" ext-link-type="uri">https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</ext-link>.</mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gou</surname>
<given-names>W</given-names>
</string-name>, <string-name><surname>Ling</surname><given-names>CW</given-names></string-name>, <string-name><surname>He</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Interpretable machine learning framework reveals robust gut microbiome features associated with type 2 diabetes</article-title>. <source>Diabetes Care</source>. <year>2021</year>;<volume>44</volume>(<issue>2</issue>):<fpage>358</fpage>&#x02013;<lpage>66</lpage>. <pub-id pub-id-type="doi">10.2337/dc20-1536</pub-id>.<pub-id pub-id-type="pmid">33288652</pub-id></mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gan</surname>
<given-names>RW</given-names>
</string-name>, <string-name><surname>Sun</surname><given-names>D</given-names></string-name>, <string-name><surname>Tatro</surname><given-names>AR</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Replicating prediction algorithms for hospitalization and corticosteroid use in patients with inflammatory bowel disease</article-title>. <source>PLoS One</source>. <year>2021</year>;<volume>16</volume>(<issue>9</issue>):<fpage>e0257520</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0257520</pub-id>.<pub-id pub-id-type="pmid">34543353</pub-id></mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Cappellato</surname><given-names>M</given-names></string-name>, <string-name><surname>Camillo</surname><given-names>BD</given-names></string-name></person-group>. <article-title>Supporting data for &#x0201c;Machine Learning&#x02013;Based Feature Selection to Search Stable Microbial Biomarkers: Application to Inflammatory Bowel Disease</article-title>.&#x0201d; <source>GigaScience Database.</source><year>2023</year>. <pub-id pub-id-type="doi">10.5524/102450</pub-id>.</mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Flores</surname>
<given-names>GE</given-names>
</string-name>, <string-name><surname>Caporaso</surname><given-names>JG</given-names></string-name>, <string-name><surname>Henley</surname><given-names>JB</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Temporal variability is a personalized feature of the human microbiome</article-title>. <source>Genome Biol</source>. <year>2014</year>;<volume>15</volume>(<issue>12</issue>):<fpage>531</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-014-0531-y</pub-id>.<pub-id pub-id-type="pmid">25517225</pub-id></mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Halfvarson</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Brislawn</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Lamendella</surname><given-names>R</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Dynamics of the human gut microbiome in inflammatory bowel disease</article-title>. <source>Nat Microbiol</source>. <year>2017</year>;<volume>2</volume>:<fpage>17004</fpage>. <pub-id pub-id-type="doi">10.1038/nmicrobiol.2017.4</pub-id>.<pub-id pub-id-type="pmid">28191884</pub-id></mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>McDonald</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Hyde</surname><given-names>E</given-names></string-name>, <string-name><surname>Debelius</surname><given-names>JW</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>American gut: an open platform for citizen science microbiome research</article-title>. <source>mSystems</source>. <year>2018</year>;<volume>3</volume>(<issue>3</issue>):<fpage>e00031</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1128/mSystems.00031-18</pub-id>.<pub-id pub-id-type="pmid">29795809</pub-id></mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Caporaso</surname>
<given-names>JG</given-names>
</string-name>, <string-name><surname>Kuczynski</surname><given-names>J</given-names></string-name>, <string-name><surname>Stombaugh</surname><given-names>J</given-names></string-name><etal>et al.</etal></person-group>
<article-title>QIIME allows analysis of high-throughput community sequencing data</article-title>. <source>Nat Methods</source>. <year>2010</year>;<volume>7</volume>(<issue>5</issue>):<fpage>335</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.f.303</pub-id>.<pub-id pub-id-type="pmid">20383131</pub-id></mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lin</surname>
<given-names>H</given-names>
</string-name>, <string-name><surname>Peddada</surname><given-names>SD.</given-names></string-name></person-group>
<article-title>Analysis of microbial compositions: a review of normalization and differential abundance analysis</article-title>. <source>NPJ Biofilms Microbiomes</source>. <year>2020</year>;<volume>6</volume>(<issue>1</issue>):<fpage>60</fpage>. <pub-id pub-id-type="doi">10.1038/s41522-020-00160-w</pub-id>.<pub-id pub-id-type="pmid">33268781</pub-id></mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Llor&#x000e9;ns-Rico</surname>
<given-names>V</given-names>
</string-name>, <string-name><surname>Vieira-Silva</surname><given-names>S</given-names></string-name>, <string-name><surname>Gon&#x000e7;alves</surname><given-names>PJ</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Benchmarking microbiome transformations favors experimental quantitative approaches to address compositionality and sampling depth biases</article-title>. <source>Nat Commun</source>. <year>2021</year>;<volume>12</volume>(<issue>1</issue>):<fpage>3562</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-23821-6</pub-id>.<pub-id pub-id-type="pmid">34117246</pub-id></mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baruzzo</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Patuzzi</surname><given-names>I</given-names></string-name>, <string-name><surname>Di&#x000a0;Camillo</surname><given-names>B.</given-names></string-name></person-group>
<article-title>Beware to ignore the rare: how imputing zero-values can improve the quality of 16S rRNA gene studies results</article-title>. <source>BMC Bioinf</source>. <year>2022</year>;<volume>22</volume>(<issue>Suppl 15</issue>):<fpage>618</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-022-04587-0</pub-id>.</mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kubinski</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Djamen-Kepaou</surname><given-names>JY</given-names></string-name>, <string-name><surname>Zhanabaev</surname><given-names>T</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Benchmark of data processing methods and machine learning models for gut microbiome-based diagnosis of inflammatory bowel disease</article-title>. <source>Front Genet</source>. <year>2022</year>;<volume>13</volume>:<fpage>784397</fpage>. <pub-id pub-id-type="doi">10.3389/fgene.2022.784397</pub-id>.<pub-id pub-id-type="pmid">35251123</pub-id></mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Lahti</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Shetty</surname><given-names>S</given-names></string-name>, <string-name><surname>Ernst</surname><given-names>FM</given-names></string-name><etal>et al.</etal></person-group>
<source>Orchestrating Microbiome Analysis with Bioconductor [Beta Version]</source>. <year>2021</year>, <ext-link xlink:href="https://github.com/microbiome/OMA" ext-link-type="uri">https://github.com/microbiome/OMA</ext-link>.</mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sanz</surname>
<given-names>H</given-names>
</string-name>, <string-name><surname>Valim</surname><given-names>C</given-names></string-name>, <string-name><surname>Vegas</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>SVM-RFE: selection and visualization of the most relevant features through non-linear kernels</article-title>. <source>BMC Bioinf</source>. <year>2018</year>;<volume>19</volume>:<fpage>432</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-018-2451-4</pub-id>.</mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lin</surname>
<given-names>X</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Selecting feature subsets based on SVM-RFE and the overlapping ratio with applications in bioinformatics</article-title>. <source>Molecules</source>. <year>2017</year>;<volume>23</volume>(<issue>1</issue>):<fpage>52</fpage>. <pub-id pub-id-type="doi">10.3390/molecules23010052</pub-id>.<pub-id pub-id-type="pmid">29278382</pub-id></mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Racedo</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Portnoy</surname><given-names>I</given-names></string-name>, <string-name><surname>V&#x000e9;lez</surname><given-names>J,I</given-names></string-name><etal>et al.</etal></person-group>
<article-title>A new pipeline for structural characterization and classification of RNA-seq microbiome data</article-title>. <source>BioData Min</source>. <year>2021</year>;<volume>14</volume>(<issue>1</issue>):<fpage>31</fpage>. <pub-id pub-id-type="doi">10.1186/s13040-021-00266-7</pub-id>.<pub-id pub-id-type="pmid">34243809</pub-id></mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chicco</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Jurman</surname><given-names>G</given-names></string-name></person-group>. <article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title>. <source>BMC Genomics</source>. <year>2020</year>;<volume>21</volume>:<fpage>6</fpage>. <pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id>.<pub-id pub-id-type="pmid">31898477</pub-id></mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Freedman</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Pisani</surname><given-names>R</given-names></string-name>, <string-name><surname>Purves</surname><given-names>R</given-names></string-name></person-group>. <source>Statistics (International Student Edition)</source>. <edition>4th ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>WW Norton &#x00026; Company</publisher-name>; <year>2007</year>. <comment>ISBN: 978-0-393-92972-0</comment>.</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bray</surname>
<given-names>JR</given-names>
</string-name>, <string-name><surname>Curtis</surname><given-names>JT</given-names></string-name></person-group>. <article-title>An ordination of upland forest communities of southern Wisconsin</article-title>. <source>Ecological Monographs</source>. <year>1957</year>;<volume>27</volume>:<fpage>325</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.2307/1942268</pub-id>.</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mohana</surname>
<given-names>CP</given-names>
</string-name>, <string-name><surname>Perumal</surname><given-names>K</given-names></string-name></person-group>. <article-title>A survey on feature selection stability measures</article-title>. <source>Int J Comput Sci Info Technol</source>. <year>2016</year>;<volume>5</volume>(<issue>1</issue>):<fpage>ISSN: 2279</fpage>&#x02013;<lpage>0764</lpage>.. <ext-link xlink:href="https://www.ijcit.com/archives/volume5/issue1/Paper050114.pdf" ext-link-type="uri">https://www.ijcit.com/archives/volume5/issue1/Paper050114.pdf</ext-link>.</mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Khaire</surname>
<given-names>UM</given-names>
</string-name>, <string-name><surname>Dhanalakshmi</surname><given-names>R</given-names></string-name></person-group>. <article-title>Stability of feature selection algorithm: a review</article-title>. <source>J King Saud Univs</source>. <year>2022</year>;<volume>34</volume>(<issue>4</issue>):<fpage>1060</fpage>&#x02013;<lpage>73</lpage>. <pub-id pub-id-type="doi">10.1016/j.jksuci.2019.06.012</pub-id>.</mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pedregosa</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Varoquaux</surname><given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Scikit-learn: machine learning in Python</article-title>. <source>JMLR</source>. <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>&#x02013;<lpage>30</lpage>.. <ext-link xlink:href="https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf" ext-link-type="uri">https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf</ext-link>.</mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Di&#x000a0;Camillo</surname>
<given-names>B</given-names>
</string-name>, <string-name><surname>Sanavia</surname><given-names>T</given-names></string-name>, <string-name><surname>Martini</surname><given-names>M</given-names></string-name></person-group>. <article-title>Effect of size and heterogeneity of samples on biomarker discovery: synthetic and real data assessment</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>3</issue>):<fpage>e32200</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0032200</pub-id>.<pub-id pub-id-type="pmid">22403633</pub-id></mixed-citation></ref><ref id="bib51"><label>51.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>West</surname>
<given-names>RM</given-names>
</string-name>
</person-group>. <article-title>Best practice in statistics: the use of log transformation</article-title>. <source>Ann Clin Biochem</source>. <year>2022</year>;<volume>59</volume>(<issue>3</issue>):<fpage>162</fpage>&#x02013;<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1177/00045632211050531</pub-id>.<pub-id pub-id-type="pmid">34666549</pub-id></mixed-citation></ref><ref id="bib52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pasolli</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Truong</surname><given-names>DT</given-names></string-name>, <string-name><surname>Malik</surname><given-names>F</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Machine learning meta-analysis of large metagenomic datasets: tools and biological insights</article-title>. <source>PLoS Comput Biol</source>. <year>2016</year>;<volume>12</volume>(<issue>7</issue>):<fpage>e1004977</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004977</pub-id>.<pub-id pub-id-type="pmid">27400279</pub-id></mixed-citation></ref><ref id="bib53"><label>53.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Giliberti</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Cavaliere</surname><given-names>S</given-names></string-name>, <string-name><surname>Mauriello</surname><given-names>IE</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Host phenotype classification from human microbiome data is mainly driven by the presence of microbial taxa</article-title>. <source>PLoS Comput Biol</source>. <year>2022</year>;<volume>18</volume>(<issue>4</issue>):<fpage>e1010066</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010066</pub-id>.<pub-id pub-id-type="pmid">35446845</pub-id></mixed-citation></ref><ref id="bib54"><label>54.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bakir-Gungor</surname>
<given-names>B</given-names>
</string-name>, <string-name><surname>Hac&#x00131;lar</surname><given-names>H</given-names></string-name>, <string-name><surname>Jabeer</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Inflammatory bowel disease biomarkers of human gut microbiota selected via different feature selection methods</article-title>. <source>PeerJ</source>. <year>2022</year>;<volume>10</volume>:<fpage>e13205</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.13205</pub-id>.<pub-id pub-id-type="pmid">35497193</pub-id></mixed-citation></ref><ref id="bib55"><label>55.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Li&#x000f1;ares-Blanco</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Fernandez-Lozano</surname><given-names>C</given-names></string-name>, <string-name><surname>Seoane</surname><given-names>JA</given-names></string-name><etal>et al.</etal></person-group>
<article-title>Machine learning based microbiome signature to predict inflammatory bowel disease subtypes</article-title>. <source>Front Microbiol</source>. <year>2022</year>;<volume>13</volume>:<fpage>872671</fpage>. <pub-id pub-id-type="doi">10.3389/fmicb.2022.872671</pub-id>.<pub-id pub-id-type="pmid">35663898</pub-id></mixed-citation></ref></ref-list></back></article>