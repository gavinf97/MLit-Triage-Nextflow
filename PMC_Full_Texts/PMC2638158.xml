<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">2638158</article-id><article-id pub-id-type="publisher-id">1471-2105-9-S12-S18</article-id><article-id pub-id-type="pmid">19091017</article-id><article-id pub-id-type="doi">10.1186/1471-2105-9-S12-S18</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title>Semi-automatic conversion of BioProp semantic annotation to PASBio annotation</article-title></title-group><contrib-group><contrib id="A1" corresp="yes" contrib-type="author"><name><surname>Tsai</surname><given-names>Richard Tzong-Han</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>thtsai@saturn.yzu.edu.tw</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Dai</surname><given-names>Hong-Jie</given-names></name><xref ref-type="aff" rid="I2">2</xref><xref ref-type="aff" rid="I3">3</xref><email>hongjie@iis.sinica.edu.tw</email></contrib><contrib id="A3" contrib-type="author"><name><surname>Huang</surname><given-names>Chi-Hsin</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>sinyuhgs@iis.sinica.edu.tw</email></contrib><contrib id="A4" corresp="yes" contrib-type="author"><name><surname>Hsu</surname><given-names>Wen-Lian</given-names></name><xref ref-type="aff" rid="I2">2</xref><xref ref-type="aff" rid="I3">3</xref><email>hsu@iis.sinica.edu.tw</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Computer Science &#x00026; Engineering, Yuan Ze University, Chung-Li, Taiwan, R.O.C</aff><aff id="I2"><label>2</label>Institute of Information Science, Academia Sinica, Nankang, Taipei, Taiwan, R.O.C</aff><aff id="I3"><label>3</label>Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan, R.O.C</aff><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>12</day><month>12</month><year>2008</year></pub-date><volume>9</volume><issue>Suppl 12</issue><supplement><named-content content-type="supplement-title">Seventh International Conference on Bioinformatics (InCoB2008)</named-content><named-content content-type="supplement-editor">Shoba Ranganathan, Wen-Lian Hsu, Ueng-Cheng Yang and Tin Wee Tan</named-content></supplement><fpage>S18</fpage><lpage>S18</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/9/S12/S18"/><permissions><copyright-statement>Copyright &#x000a9; 2008 Tsai et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2008</copyright-year><copyright-holder>Tsai et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an open access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Tsai
               Tzong-Han
               Richard
               
               thtsai@saturn.yzu.edu.tw
            </dc:author><dc:title>
            Semi-automatic conversion of BioProp semantic annotation to PASBio annotation
         </dc:title><dc:date>2008</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 9(Suppl 12): S18-. (2008)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2008)9:Suppl 12&#x0003c;S18&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>Semantic role labeling (SRL) is an important text analysis technique. In SRL, sentences are represented by one or more predicate-argument structures (PAS). Each PAS is composed of a predicate (verb) and several arguments (noun phrases, adverbial phrases, etc.) with different semantic roles, including main arguments (agent or patient) as well as adjunct arguments (time, manner, or location). PropBank is the most widely used PAS corpus and annotation format in the newswire domain. In the biomedical field, however, more detailed and restrictive PAS annotation formats such as PASBio are popular. Unfortunately, due to the lack of an annotated PASBio corpus, no publicly available machine-learning (ML) based SRL systems based on PASBio have been developed. In previous work, we constructed a biomedical corpus based on the PropBank standard called BioProp, on which we developed an ML-based SRL system, BIOSMILE. In this paper, we aim to build a system to convert BIOSMILE's BioProp annotation output to PASBio annotation. Our system consists of BIOSMILE in combination with a BioProp-PASBio rule-based converter, and an additional semi-automatic rule generator.</p></sec><sec><title>Results</title><p>Our first experiment evaluated our rule-based converter's performance independently from BIOSMILE performance. The converter achieved an F-score of 85.29%. The second experiment evaluated combined system (BIOSMILE + rule-based converter). The system achieved an F-score of 69.08% for PASBio's 29 verbs.</p></sec><sec><title>Conclusion</title><p>Our approach allows PAS conversion between BioProp and PASBio annotation using BIOSMILE alongside our newly developed semi-automatic rule generator and rule-based converter. Our system can match the performance of other state-of-the-art domain-specific ML-based SRL systems and can be easily customized for PASBio application development.</p></sec></abstract><conference><conf-date>20&#x02013;23 October 2008</conf-date><conf-name>Asia Pacific Bioinformatics Network (APBioNet) Seventh International Conference on Bioinformatics (InCoB2008)</conf-name><conf-loc>Taipei, Taiwan</conf-loc></conference></article-meta></front><body><sec><title>Background</title><p>The amount of biomedical literature available online continues to grow rapidly today, creating a need for automatic processing using bioinformatics tools. Many information extraction (IE) systems incorporating natural language processing (NLP) techniques have been developed for use in the biomedical field. A key IE task in this field is the extraction of relations between named entities (NEs), such as protein-protein and gene-disease interactions.</p><p><italic>Semantic role labeling </italic>(<italic>SRL</italic>), also called shallow semantic parsing [<xref ref-type="bibr" rid="B1">1</xref>], is a popular semantic analysis technique for extracting relations. In SRL, sentences are represented by one or more <italic>predicate-argument structures </italic>(<italic>PAS</italic>), also known as propositions [<xref ref-type="bibr" rid="B2">2</xref>]. Each PAS is composed of a predicate (e.g., a verb) and several arguments (e.g., noun phrases) that have different semantic roles, including main arguments such as an agent that deliberately performs an action (e.g., <bold>Bill </bold>drank his soup quietly) and a patient that experiences an action (e.g., the falling rocks crushed <bold>the car</bold>), as well as adjunct arguments, such as time, manner, and location. Here, the term <italic>argument </italic>refers to a syntactic constituent of the sentence related to the predicate; and the term <italic>semantic role </italic>refers to the semantic relationship between a predicate (e.g., a verb) and an argument (e.g., a noun phrase) of a sentence. For example, in Figure <xref ref-type="fig" rid="F1">1</xref>, the sentence "IL4 and IL13 receptors activate STAT6, STAT3, and STAT5 proteins in the human B cells" describes a molecular activation process. It can be represented by a PAS in which "activate" is the predicate, "IL4 and IL13 receptors" comprises the agent, "STAT6, STAT3, and STAT5 proteins" comprises the patient, and "in the human B cells" is the location. Thus, the agent, patient, and location are the arguments of the predicate.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p>A parse tree annotated with semantic roles.</p></caption><graphic xlink:href="1471-2105-9-S12-S18-1"/></fig><p>An important preliminary task in SRL is to define the set of possible semantic roles for each verb sense, referred to as a <italic>roleset</italic>. A roleset can be paired with a set of syntactic frames that shows all the acceptable syntactic expressions of those roles. This is called a <italic>frameset </italic>[<xref ref-type="bibr" rid="B3">3</xref>]. In 2000, the Proposition Bank project (PropBank) [<xref ref-type="bibr" rid="B3">3</xref>] published a guide, PropBank I [<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B5">5</xref>], which defined a format for PAS annotation. Alongside PropBank I, the project also released a corpus of PAS's for 3,325 verbs in the newswire domain to facilitate ML-based SRL system development [<xref ref-type="bibr" rid="B6">6</xref>]. The semantic arguments of individual verbs in the PropBank I annotation are numbered from 0. For a specific verb, Arg0 is usually the argument corresponding to the agent [<xref ref-type="bibr" rid="B7">7</xref>], while Arg1 usually corresponds to the patient. However, higher-numbered arguments, which occupy about 10% of the total arguments, have no consistent role definitions. In addition to numbered arguments, there are also ArgMs, which refer to annotation of modifiers. (Detailed descriptions of all semantic role argument categories can be found in Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>.) The semi-regular and flexible assignment of numbered arguments to semantic roles found in PropBank I facilitates formulation of the SRL task as a classification problem with machine-learning (ML) based systems. That is, given a phrase, the sentence containing it, and the predicate, a system must classify the phrase's semantic role corresponding to the predicate.</p><p>For specific applications, however, the flexible argument assignment of PropBank I annotation may be a disadvantage. In some cases, developers may wish to limit the semantic roles of each argument. Take the frameset of "delete" for example. Table <xref ref-type="table" rid="T1">1</xref> shows the frameset definition.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Frameset of verb "delete" in PropBank I and PASBio</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Predicate: delete</td><td></td><td></td></tr></thead><tbody><tr><td align="left">Argument</td><td align="left">PropBank I</td><td align="left">PASBio</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">Arg0</td><td align="left">entity removing</td><td align="left">causer mechanism</td></tr><tr><td></td><td></td><td align="left">//mutation, alternative splicing//</td></tr><tr><td align="left">Arg1</td><td align="left">thing being removed</td><td align="left">entity being removed</td></tr><tr><td></td><td></td><td align="left">//exon, gene, chromosomal region, cell//</td></tr><tr><td align="left">Arg2</td><td align="left">removed from</td><td align="left">resultant product</td></tr><tr><td></td><td></td><td align="left">//transcripts//</td></tr></tbody></table></table-wrap><p>As you can see in Table <xref ref-type="table" rid="T1">1</xref>, the agent is defined as "entity removing", and the patient is defined as "thing being removed" in PropBank I. However, in certain biomedical events, a developer might want to limit the agent to being a certain causal mechanism such as a mutation or alternative splicing and the patient to being an "exon, gene, chromosomal region, [or] cell".</p><p>An alternative to PropBank, the PASBio [<xref ref-type="bibr" rid="B8">8</xref>] project provides more detailed and restrictive framesets for 29 biomedical verbs. The well-known biomedical text mining researchers Cohen and Hunter [<xref ref-type="bibr" rid="B9">9</xref>] have found the PASBio annotation viable for representing the PAS's of biomedical verbs. Several applications have been developed based on PASBio or following its spirit. For example, Shah et al. [<xref ref-type="bibr" rid="B10">10</xref>] used the frameset definitions of PASBio to construct semantic patterns which can extract information about tissue-specific gene expression from biomedical literature. Later, Shah and Bork applied this approach to construct the LSAT (Literature Support for Alternative Transcripts) database system [<xref ref-type="bibr" rid="B11">11</xref>]. Kogan et al. [<xref ref-type="bibr" rid="B12">12</xref>] followed the PASBio annotation to built a domain-specific set of PASs for the medical domain, which successfully extended PASBio to clinical texts. All these systems mainly use handcrafted rules to identify and classify arguments into semantic roles.</p><p>Unfortunately, due to the lack of an annotated corpus and inconsistent definitions between specific numbered arguments, no publicly available ML-based SRL systems based on the PASBio standard have been developed.</p><p>To be able to apply ML to the biomedical SRL problem, we constructed a biomedical domain specific proposition bank based on the more consistent PropBank I annotation format. The project, BioProp [<xref ref-type="bibr" rid="B13">13</xref>], defined roles for 30 common biomedical verbs and provided an annotated corpus on which we developed an ML-based SRL system, BIOSMILE [<xref ref-type="bibr" rid="B14">14</xref>]. This work was expanded upon with the release of our web-based search application, BIOSMILE web search [<xref ref-type="bibr" rid="B15">15</xref>], in February 2008.</p><p>In this paper, we aim to build a bridge between BioProp and PASBio to facilitate PASBio-based SRL system development. Using our system, one will first be able to roughly classify arguments' semantic roles according to BioProp, and then translate the PAS's into PASBio annotation using a rule-based converter.</p></sec><sec sec-type="methods"><title>Methods</title><p>The approaches applied in this work include: (1) named entity tagging, (2) semantic role labeling following BioProp's annotation format, and (3) rule-based conversion from BioProp to PASBio annotation.</p><sec><title>Named entity tagging</title><p>According to our observations, some BioProp arguments are equivalent to other PASBio arguments only under certain conditions, usually defined as the presence of a certain named entity (NE) in a certain argument. For example, Arg1 of the verb "express" must be a gene or gene product in PASBio. Therefore, it is necessary to first tag all NEs in the sentences. To do this, we employ our previously developed NE recognition software, NERBio [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B17">17</xref>], to tag five NE types: protein, DNA, RNA, cell line, and cell type. We use a dictionary to find other NE types, such as extron and intron.</p></sec><sec><title>Semantic role labeling</title><p>Before conversion to the PASBio annotation format, a fundamental step is to identify the PAS's of each sentence and annotate them using the BioProp format. Here, we briefly introduce how we constructed the BioProp-based SRL system, BIOSMILE, used for this task.</p><p>The first step was to construct a training corpus. In our previous work, Chou et al. [<xref ref-type="bibr" rid="B13">13</xref>], we annotated PAS's in GENIA's corpus of full parse trees, the GENIA Treebank (GTB) [<xref ref-type="bibr" rid="B18">18</xref>], using PropBank I framesets. We then defined and added framesets for biomedical verbs to fit specific usages in biomedical literature. However, all the new and modified framesets still conform strictly to the PropBank annotation format. A total of 2,304 PAS's were annotated for 49 biomedical verbs.</p><p>The second step we took was to formulate the SRL problem as an ML-based sentence tagging problem. The basic units of a sentence can be words, phrases, and constituents (nodes on a full parse tree). Punyakanok et al. [<xref ref-type="bibr" rid="B19">19</xref>] has shown that constituent-by-constituent (C-by-C, or node-by-node) tagging is the best formulation for the SRL problem; therefore, we adopted this formulation.</p><p>Finally, we constructed a biomedical full parser based on the Charniak parser [<xref ref-type="bibr" rid="B20">20</xref>] with GTB as its training data which could automatically generate parse trees for sentences. Its performance is reported in Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>.</p><p>Using BioProp as the training corpus, C-by-C formulation, and the parse trees generated by our biomedical full parser, we then constructed our SRL system, BIOSMILE, following the maximum entropy ML model [<xref ref-type="bibr" rid="B21">21</xref>]. Details of the features used in our SRL system can be found in [<xref ref-type="bibr" rid="B14">14</xref>].</p></sec><sec><title>Development of conversion rules</title><p>There are two main differences between BioProp and PASBio PAS framesets annotations: (1) PASBio developers usually define framesets to represent specific biological events. Therefore, for each argument, it is necessary to include information in addition to its semantic role, such as whether the argument should be a specific NE or contain specific keywords. (2) The order of arguments for a given verb sense in a BioProp frameset may not match that in a corresponding PASBio frameset. To deal with these two differences, we build conversion rules verb by verb using our semi-automatic rule-generation tool which describe under which conditions each mapping is valid. The algorithm used by the rule-generator compares corresponding framesets for a given verb sense, checks each argument in its PASBio frameset, and tries to find an argument in its BioProp frameset that has the same semantic role under a set of conditions. When a match is found, the algorithm maps a link between the two frameset arguments, which includes a description of required conditions (NEs and keywords).</p><p>Figure <xref ref-type="fig" rid="F2">2</xref> shows a screenshot from the tool. The user feeds the tool with sentences containing PASBio-based semantic role information. The information is placed in the "PASBio" column after loading. The sentences are pre-processed to generate full parse tree structures, BioProp-based SRL, POS's, as well as NEs information represented in the first, second, fourth and fifth columns, respectively. After pre-processing, the tool allows users to view, modify or create conversion rules by clicking on the "Generate Rules" button as shown in Figure <xref ref-type="fig" rid="F2">2</xref>. A conversion rule generated after clicking the button is shown in Figure <xref ref-type="fig" rid="F3">3</xref>.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p>Screenshot of the rule-generation tool.</p></caption><graphic xlink:href="1471-2105-9-S12-S18-2"/></fig><fig position="float" id="F3"><label>Figure 3</label><caption><p>Conversion rule for the verb "express" for Figure 2.</p></caption><graphic xlink:href="1471-2105-9-S12-S18-3"/></fig><p>Each conversion rule consists of two elements: predicates and transformations. The predicate is the target verb. The first part of each transformation is the condition, which specifies the criteria that the arguments should follow. These criteria are defined as the composition of one or more logical predicates, which are concatenated by logical operators, such as AND, and OR. Two most common predicates are ContainsNE(<italic>ne</italic>) and ContainsKeywords(<italic>kw</italic>). The former is true if the argument contains at least one instance of the NE type <italic>ne</italic>. The latter is true if the argument contains at least one specified keyword <italic>kw</italic>. If there are no conditions for a transformation, this part can be omitted.</p><p>The second part is the mapping between a BioProp argument and a PASBio argument. The mapping consists of three elements: the source argument, an arrow "&#x02192;", and the destination argument. For example, the transformation in Figure <xref ref-type="fig" rid="F3">3</xref> defines a mapping from ArgM-LOC to Arg3. All the arguments that are not defined in the transformation source field are dropped.</p><p>As shown in Figure <xref ref-type="fig" rid="F3">3</xref>, the condition of the transformation "ARG1 &#x02192; ARG1" is ContainsNE("protein"), which is interpreted as the mapping ARG1 &#x02192; ARG1 holds if ARG1 contains at least one protein. For a case in which arguments match, such as that in Figure <xref ref-type="fig" rid="F3">3</xref>, the conversion rules can be automatically generated as follows:</p><p>1. For each argument pair, (<italic>argument</italic><sub><italic>B</italic></sub>, <italic>argument</italic><sub><italic>P</italic></sub>), if the argument phrase does not contain any recognized NEs, a simple rule will be generated in the argument's "Rule Candidates" field: <italic>argument</italic><sub><italic>B </italic></sub>&#x02192; <italic>argument</italic><sub><italic>P</italic></sub></p><p>2. If the argument contains recognized NE types (<italic>NE</italic><sub><italic>type</italic></sub>), they will become the conditions imposed on the argument, and the following rule type will be generated: ContainsNE (<italic>NE</italic><sub><italic>type</italic></sub>)?<italic>argument</italic><sub><italic>B </italic></sub>&#x02192; <italic>argument</italic><sub><italic>P</italic></sub></p><p>Users can modify the generated rules by editing the "Rule Candidates" field.</p><p>In addition to defining simple conditions, such as ContainsNE, we also describe complex conditions using a format called the bracket form pattern, which can represent syntactic and semantic information as criteria. The pattern can be applied when two or more PASBio arguments are covered by only one BioProp argument (Figure <xref ref-type="fig" rid="F4">4</xref>), and vice versa. A bracket form [<xref ref-type="bibr" rid="B22">22</xref>] is a representation of a parse tree using brackets (Figure <xref ref-type="fig" rid="F4">4</xref>), to show the tree's structure.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p>Multiple overlap for the verb "express".</p></caption><graphic xlink:href="1471-2105-9-S12-S18-4"/></fig><p>A simplified bracket form for the parse tree shown in Figure <xref ref-type="fig" rid="F4">4</xref>, with some internal bracket divisions omitted for clarity: (NP (NP (Two equally abundant mRNAs for il8ra)) (,) (NP (2.0 and 2.4 kilobases in length))). </p><p>Each constituent and its daughters are enclosed with brackets. If we replace constituent words in the phrase with a wildcard symbol "(.*)", the above bracket form becomes:</p><p>(NP (NP (.*)) (.*) (NP (.*)))</p><p>We can then use the bracket form as a pattern to match parse trees with the same structures.</p><p>To make these patterns more precise, we can add restrictions on the phrase constituents, such as limiting their semantic roles, head words and head words' UPENN POS [<xref ref-type="bibr" rid="B23">23</xref>]. To restrict a constituent's semantic role, one would insert a hyphen followed by the semantic role after the constituent type. For example, (NP) might become (NP-Arg1). The head word can be defined as the most important word in a constituent [<xref ref-type="bibr" rid="B24">24</xref>], and we identify it using Collins' [<xref ref-type="bibr" rid="B25">25</xref>] rule-based method. Head words of constituents are marked with an ampersand followed by the head word &#x02013; e.g. (NP<sub>@kilobase</sub>). And the UPENN POS of the head word is placed directly after, separated by a forward slash &#x02013; e.g. (NP<sub>@kilobase/NNS</sub>). If we combine our above examples, we can make the pattern, "(NP-Arg1@<sub>mRNA/NNS </sub>(NP<sub>@mRNA/NNS </sub>(.*)) (NP<sub>@kilobase/NNS </sub>(.*)))", where the outside NP must be Arg1, and the inside NPs' head word must be "mRNA" and "kilobase" with POS's "NNS."</p><p>In our notation, a rule will appear as follows:</p><p>BracketFormPattern(<italic>x</italic>) ? <italic>C</italic><sub>0 </sub>&#x02192; <italic>argument</italic><sub>0</sub>, <italic>C</italic><sub>1 </sub>&#x02192; <italic>argument</italic><sub>1</sub>,..., <italic>C</italic><sub><italic>i </italic></sub>&#x02192; <italic>argument</italic><sub><italic>i</italic></sub>,..., <italic>C</italic><sub><italic>k </italic></sub>&#x02192; <italic>argument</italic><sub><italic>k</italic></sub>;</p><p>"BracketFormPattern" is a logical predicate which means the source argument, <italic>argument</italic><sub><italic>s</italic></sub>, must match the bracket form pattern <italic>x </italic>for the transformations "<italic>C</italic><sub><italic>i </italic></sub>&#x02192; <italic>argument</italic><sub><italic>i</italic></sub>" to occur, where <italic>C</italic><sub><italic>i </italic></sub>is any constituent of a source argument annotated by PASBio.</p><p>In the example in Figure <xref ref-type="fig" rid="F4">4</xref> for the verb "express", "ARG1" in the BioProp column does not directly match any one PASBio argument, but instead overlaps two arguments, Arg1 and Arg2. The rule-generation algorithm first generates two bracket forms for the unmatched noun phrase "Two equally abundant mRNAs for il8ra 2.0 and 2.4 kilobases in length", one for the "BioProp" column and the other for the "PASBio" column:</p><p>(NP-Arg1<sub>@mRNA/NNS </sub>(NP<sub>@mRNA/NNS </sub>(.*)) (.*) (NP<sub>@kilobase/NNS </sub>(.*))")</p><p>(NP<sub>@mRNA/NNS </sub>(NP-Arg1<sub>@mRNA/NNS </sub>(.*)) (.*) (NP-Arg2<sub>@kilobase/NNS </sub>(.*))")</p><p>Then, the first bracket form is merged with the second one as follows:</p><p>(NP-Arg1<sub>@mRNA/NNS </sub>(NP-<italic>C</italic><sub>0@mRNA/NNS </sub>(.*)) (.*) (NP-<italic>C</italic><sub>1@kilobase/NNS </sub>(.*))")</p><p>As you can see in the merged bracket form, all the PASBio constituents annotated with semantic roles are represented by the variable <italic>C</italic><sub><italic>i</italic></sub>. For example Arg1 becomes <italic>C</italic><sub>0</sub>.</p><p>Finally, the following three rules are automatically generated in the "Rule Candidates" field:</p><p>1. BracketFormPattern("(NP-Arg1 (NP-<italic>C</italic><sub>0 </sub>(.*)) (.*) (NP-<italic>C</italic><sub>1 </sub>(.*))") ? <italic>C</italic><sub>0 </sub>&#x02192; Arg1, <italic>C</italic><sub>1 </sub>&#x02192; Arg2</p><p>2. BracketFormPattern("(NP-Arg1<sub>@mRNA/</sub>(NP-<italic>C</italic><sub>0@mRNA/</sub>(.*)) (.*) (NP-<italic>C</italic><sub>1@kilobase/</sub>(.*))") ? <italic>C</italic><sub>0 </sub>&#x02192; Arg1, <italic>C</italic><sub>1 </sub>&#x02192; Arg2</p><p>3. BracketFormPattern("(NP-Arg1<sub>@/NNS </sub>(NP-<italic>C</italic><sub>0@/NNS </sub>(.*)) (.*) (NP-<italic>C</italic><sub>1@/NNS </sub>(.*))") ? <italic>C</italic><sub>0 </sub>&#x02192; Arg1, <italic>C</italic><sub>1 </sub>&#x02192; Arg2</p><p>The first rule is the loosest, only considering the parse tree structure and SRL tags. The second also considers the head word, and the third adds POS information as well. The user can check these rule candidates, and remove or modify the inappropriate ones.</p><p>Although these rules are semi-automatically generated, we have found from our observations that with slight human modification, they can be quite accurate. For the example in Figure <xref ref-type="fig" rid="F4">4</xref>, it is obvious that the first rule with no constraints on <italic>C</italic><sub>0 </sub>and <italic>C</italic><sub>1 </sub>is too loose. Likewise, the third rule, which limits <italic>C</italic><sub>0 </sub>and <italic>C</italic><sub>1</sub>'s POS to NNS, is too strict. However, the second rule is surprisingly accurate. If we look at the frameset definitions in BioProp and PASBio shown in Table <xref ref-type="table" rid="T2">2</xref>, we can see that PASBio defines Arg2 as a property of Arg1 and limits Arg1 to a gene or gene product name. Therefore, if we wish to annotate <italic>C</italic><sub>0 </sub>as Arg1 and <italic>C</italic><sub>1 </sub>as Arg2, they must match these two conditions. Rule two stipulates that <italic>C</italic><sub>1</sub>'s head word should be "kilobase" and <italic>C</italic><sub>0</sub>'s should be "mRNA", which matches PASBio's frameset definition for "express" because "kilobase" is a unit of mRNA. Therefore, the annotator could choose the second rule with head word information.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>The frameset of the verb "express" in BioProp and PASBio</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Predicate: express</td><td></td><td></td></tr></thead><tbody><tr><td align="left">Argument</td><td align="left">BioProp</td><td align="left">PASBio</td></tr><tr><td colspan="3"><hr></hr></td></tr><tr><td align="left">Arg0</td><td align="left">causer of expression</td><td align="left">no definition</td></tr><tr><td align="left">Arg1</td><td align="left">thing expressing</td><td align="left">named entity being expressed</td></tr><tr><td></td><td></td><td align="left">//gene or gene products//</td></tr><tr><td align="left">Arg2</td><td align="left">end state</td><td align="left">property of the existing named entity [Arg1]</td></tr><tr><td align="left">Arg3</td><td align="left">start state</td><td align="left">location referring to organelle, cell or tissue</td></tr></tbody></table></table-wrap></sec></sec><sec><title>Results</title><sec><title>Datasets</title><p>The training data of our SRL system, BIOSMILE, is an extended version of BioProp [<xref ref-type="bibr" rid="B13">13</xref>]. A total of 2,304 PAS's were annotated for 49 biomedical verbs. To evaluate BIOSMILE, the rule-based converter and the combined system, our in-lab biologists re-annotated the 313 annotated sentences available on PASBio's website according to the BioProp annotation format. The dataset from PASBio's website is hereafter referred to as PASBio<sub>P </sub>and the PASBio<sub>P </sub>dataset annotated using the BioProp format is referred to as PASBio<sub>B</sub>.</p></sec><sec><title>Evaluation metrics</title><p>Performance was evaluated in terms of three metrics: precision (P), recall (R) and F-scores (F), which are defined as follows:</p><p><disp-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-9-S12-S18-i1" overflow="scroll">
                     <mml:semantics>
                        <mml:mrow>
                           <mml:mtable>
                              <mml:mtr>
                                 <mml:mtd>
                                    <mml:mrow>
                                       <mml:mtext>Precision</mml:mtext>
                                       <mml:mo>=</mml:mo>
                                       <mml:mfrac>
                                          <mml:mrow>
                                             <mml:mtext>the&#x000a0;number&#x000a0;of&#x000a0;correctly&#x000a0;recognized&#x000a0;arguments</mml:mtext>
                                          </mml:mrow>
                                          <mml:mrow>
                                             <mml:mtext>the&#x000a0;number&#x000a0;of&#x000a0;recognized&#x000a0;arguments</mml:mtext>
                                          </mml:mrow>
                                       </mml:mfrac>
                                    </mml:mrow>
                                 </mml:mtd>
                              </mml:mtr>
                              <mml:mtr>
                                 <mml:mtd>
                                    <mml:mrow>
                                       <mml:mtext>Recall</mml:mtext>
                                       <mml:mo>=</mml:mo>
                                       <mml:mfrac>
                                          <mml:mrow>
                                             <mml:mtext>the&#x000a0;number&#x000a0;of&#x000a0;correctly&#x000a0;recognized&#x000a0;arguments</mml:mtext>
                                          </mml:mrow>
                                          <mml:mrow>
                                             <mml:mtext>the&#x000a0;number&#x000a0;of&#x000a0;true&#x000a0;arguments</mml:mtext>
                                          </mml:mrow>
                                       </mml:mfrac>
                                    </mml:mrow>
                                 </mml:mtd>
                              </mml:mtr>
                              <mml:mtr>
                                 <mml:mtd>
                                    <mml:mrow>
                                       <mml:mtext>F</mml:mtext>
                                       <mml:mo>&#x02212;</mml:mo>
                                       <mml:mtext>scores</mml:mtext>
                                       <mml:mo>=</mml:mo>
                                       <mml:mfrac>
                                          <mml:mrow>
                                             <mml:mn>2</mml:mn>
                                             <mml:mo>&#x000d7;</mml:mo>
                                             <mml:mtext>Precision</mml:mtext>
                                             <mml:mo>&#x000d7;</mml:mo>
                                             <mml:mtext>Recall</mml:mtext>
                                          </mml:mrow>
                                          <mml:mrow>
                                             <mml:mtext>Precision</mml:mtext>
                                             <mml:mo>+</mml:mo>
                                             <mml:mtext>Recall</mml:mtext>
                                          </mml:mrow>
                                       </mml:mfrac>
                                    </mml:mrow>
                                 </mml:mtd>
                              </mml:mtr>
                           </mml:mtable>
                        </mml:mrow>
                        
                     </mml:semantics>
                  </mml:math></disp-formula></p><p>For SRL and conversion evaluation, the official CoNLL-2004 [<xref ref-type="bibr" rid="B6">6</xref>] SRL evaluation script was used.</p></sec><sec><title>BIOSMILE performance</title><p>We followed the same experimental procedure that we used in [<xref ref-type="bibr" rid="B14">14</xref>] to evaluate BIOSMILE performance on the extended BioProp dataset, details about which can be found in Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>. The average results were an F-score of 72.67%, a precision of 81.72% and a recall of 65.42%.</p><p>To evaluate the actual performance on arbitrary sentences and verbs, we used PASBio<sub>B </sub>as an extra test data. BIOSMILE achieved an overall F-score of 67.31%, a precision of 76.28% and a recall of 60.22%. (More detailed performance data for each argument type can be found in Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>.) The drop in BIOSMILE's performance on PASBio<sub>B </sub>may be caused by the following factor: Even though BioProp contains all PASBio verbs, it contains very few PAS's for some verbs, which likely decreases the accuracy of ML-based SRL on those verbs. For example, there is only one PAS for "splice" and two for "begin".</p></sec><sec><title>Main system performance</title><p>We conducted two experiments &#x02013; the first to test the BioProp-PASBio converter independently of BIOSMILE SRL performance, and the second to evaluate combined system performance. For both, 3-fold cross validation (CV) was applied, which involved partitioning the PASBio<sub>p </sub>dataset into three subsets. A single subset is retained as the test data, and the remaining two subsets are used as training data for generating conversion rules. The CV process is then repeated three times, with each of the test sets being used exactly once.</p></sec><sec><title>Experiment 1: Evaluating the rule-based converter</title><p>In this experiment, we examined conversion performance using the PASBio<sub>P </sub>dataset, first feeding the PASBio<sub>B </sub>(gold-standard BioProp annotation) to the rule-based converter and then comparing the converted results with the PASBio<sub>P </sub>annotation to examine the precision, recall and F-scores. By using the PASBio<sub>B</sub>, we can effectively eliminate the influence of BIOSMILE SRL performance from this test. As shown in Table <xref ref-type="table" rid="T3">3</xref>, we achieved an average F-score of 85.29%. The high F-score demonstrates the feasibility of our proposed semi-automatic conversion method.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Rule-based converter performance (on PASBio<sub>p</sub>)</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Argument Type</td><td align="center">Precision</td><td align="center">Recall</td><td align="center">F-score</td></tr></thead><tbody><tr><td align="left">Arg0</td><td align="center">86.36</td><td align="center">92.36</td><td align="center">89.26</td></tr><tr><td align="left">Arg1</td><td align="center">90.04</td><td align="center">87.85</td><td align="center">88.93</td></tr><tr><td align="left">Arg2</td><td align="center">88.03</td><td align="center">70.55</td><td align="center">78.33</td></tr><tr><td align="left">Arg3</td><td align="center">90.00</td><td align="center">64.29</td><td align="center">75.00</td></tr><tr><td align="left">Arg4</td><td align="center">66.67</td><td align="center">54.54</td><td align="center">60.00</td></tr><tr><td align="left">ArgM-MNR</td><td align="center">88.89</td><td align="center">100.00</td><td align="center">94.12</td></tr><tr><td align="left">ArgM-MOD</td><td align="center">100.00</td><td align="center">100.00</td><td align="center">100.00</td></tr><tr><td align="left">ArgM-NEG</td><td align="center">100.00</td><td align="center">100.00</td><td align="center">100.00</td></tr><tr><td align="left">ArgR</td><td align="center">75.00</td><td align="center">33.33</td><td align="center">46.15</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">Overall</td><td align="center">88.55</td><td align="center">82.27</td><td align="center">85.29</td></tr></tbody></table></table-wrap></sec><sec><title>Experiment 2: Evaluating the combined system</title><p>In this experiment, we examined the combined performance of our system, as shown in Table <xref ref-type="table" rid="T4">4</xref>. Compared with Experiment 1, the recall of the combined system drops 23%; however, the precision only drops 6%. This may be due to the fact that BIOSMILE has a high precision on PASBio<sub>B </sub>(76.28%) but a low recall (60.22%). In addition, comparing the results in Table <xref ref-type="table" rid="T4">4</xref> to the BIOSMILE performance on PASBio<sub>B</sub>, we can see that the combined system's performance is higher. This might seem counterintuitive; however, if we take into account that some argument types with low accuracy, such as ArgM-TMP and ArgM-DIR, are not converted to PASBio since PASBio does not define those arguments, then we can explain this discrepancy.</p><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Combined system performance</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Argument Type</td><td align="right">Precision</td><td align="right">Recall</td><td align="right">F-score</td></tr></thead><tbody><tr><td align="left">Arg0</td><td align="right">79.49</td><td align="right">64.58</td><td align="right">71.26</td></tr><tr><td align="left">Arg1</td><td align="right">79.65</td><td align="right">63.89</td><td align="right">70.91</td></tr><tr><td align="left">Arg2</td><td align="right">87.80</td><td align="right">49.32</td><td align="right">63.16</td></tr><tr><td align="left">Arg3</td><td align="right">95.65</td><td align="right">39.29</td><td align="right">55.70</td></tr><tr><td align="left">Arg4</td><td align="right">100.00</td><td align="right">45.45</td><td align="right">62.50</td></tr><tr><td align="left">ArgM-MNR</td><td align="right">88.89</td><td align="right">100.0</td><td align="right">94.12</td></tr><tr><td align="left">ArgM-MOD</td><td align="right">100.00</td><td align="right">100.00</td><td align="right">100.00</td></tr><tr><td align="left">ArgM-NEG</td><td align="right">100.00</td><td align="right">100.00</td><td align="right">100.00</td></tr><tr><td align="left">ArgR</td><td align="right">100.00</td><td align="right">22.22</td><td align="right">36.36</td></tr><tr><td colspan="4"><hr></hr></td></tr><tr><td align="left">Overall</td><td align="right">82.85</td><td align="right">59.23</td><td align="right">69.08</td></tr></tbody></table></table-wrap></sec></sec><sec><title>Discussion</title><p>After examining the PAS's which were not labeled correctly in the experiments, we have concluded that the following two factors affected conversion performance most strongly:</p><sec><title>Absence of key terms for argument disambiguation</title><p>In cases where one BioProp argument can be divided into two or more PASBio arguments, our rules may be insufficient to disambiguate if NEs or keywords are absent. Consider the following example annotated by our system with BioProp/PASBio annotations both given concatenated by a forward slash:</p><p>... [protein extracts from the transfected COS cells <sub><bold>Arg0/Arg0</bold></sub>] [inhibited <sub><bold>V</bold></sub>] [both the C alpha and C beta isoforms of the PKA catalytic subunit with equal efficacy <sub><bold>Arg1/Arg2</bold></sub>].</p><p>The last argument is incorrectly converted from BioProp Arg1 to PASBio Arg2 by our system. To find out why, we must look at BioProp and PASBio's frameset definitions for "inhibit" shown in Table <xref ref-type="table" rid="T5">5</xref>.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>The frameset of the verb "inhibit" in PASBio and BioProp</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Argument</td><td align="left">BioProp</td></tr></thead><tbody><tr><td align="left">Arg0</td><td align="left">Inhibitor</td></tr><tr><td align="left">Arg1</td><td align="left">entity inhibited</td></tr><tr><td colspan="2"><hr></hr></td></tr><tr><td align="left">Argument</td><td align="left">PASBio</td></tr><tr><td colspan="2"><hr></hr></td></tr><tr><td align="left">Arg0</td><td align="left">agent</td></tr><tr><td align="left">Arg1</td><td align="left">the entity being inhibited by agent to get binding</td></tr><tr><td align="left">Arg2</td><td align="left">the action or property being inhibited</td></tr></tbody></table></table-wrap><p>We can see that PASBio defines both Arg1 and Arg2 as the objects being inhibited, but Arg1 is further constrained to being the entity bound by the agent. BioProp, which has no Arg2 definition, does not make this distinction. The automatically generated conversion rule for Arg1, therefore, will have the constraint ContainsKeywords("binding"). However, as the above example lacks any references to binding that would describe which entity "gets binding", the system converts to Arg2 instead of Arg1. In this case, simple NE-/keyword-based rules cannot distinguish Arg1 from Arg2.</p><p>According to our analysis, 3.83% of the PAS's in the PASBio<sub>P </sub>suffered from this problem, especially PAS's for verbs such as decrease, delete, inhibit, lost, mutate, transcribe and truncate.</p></sec><sec><title>Coordination ambiguity</title><p>Coordination ambiguity in the full parse information is another factor that affects conversion performance.</p><p>Figure <xref ref-type="fig" rid="F5">5</xref> shows two possible full parse structures for the following sentence:</p><p>NK cells express cell-surface receptors of the immunoglobulin and C-type lectin superfamilies that recognize MHC class I peptides and inhibit NK-cell-mediated cytotoxicity.</p><fig position="float" id="F5"><label>Figure 5</label><caption><p>Coordination ambiguity.</p></caption><graphic xlink:href="1471-2105-9-S12-S18-5"/></fig><p>The phrase "inhibit NK-cell-medidated cytotoxicity" can be coordinated with three different phrases, each with a different meaning. This syntactic ambiguity is referred to as "coordination ambiguity" [<xref ref-type="bibr" rid="B25">25</xref>] and is a major problem in parsing. As you can see in Figure <xref ref-type="fig" rid="F5">5(a)</xref>, our full parser coordinates the verb phrase "express cell-surface receptors of the ... class I peptides" with the verb phrase "inhibit NK-cell-mediated cytotoxicity." Therefore, BIOSMILE tags the noun phrase "NK cells" as "Arg0" for the verb "inhibit." However, in the gold standard annotation, the PASBio developers annotate the "cell-surface receptors of ... superfamilies" as "Arg0" for the verb "inhibit". The parse tree for the PASBio's annotation is illustrated in Figure <xref ref-type="fig" rid="F5">5(b)</xref>. It coordinates the verb phrase "recognize MHC class I peptides" with the verb phrase "inhibit NK-cell-mediated cytotoxicity." Although, both these parse trees were generated by our parser initially, in the end, it chose the incorrect one, Figure <xref ref-type="fig" rid="F5">5(a)</xref>, because, based on the training data, that one appeared to have the highest probability. In such cases it is impossible to distinguish the correct choice using syntactic parsing. Our results show that 1.92% PAS's in the PASBio<sub>P </sub>dataset suffered this problem.</p></sec><sec><title>Correlation between BIOSMILE and combined system performance</title><p>Figure <xref ref-type="fig" rid="F6">6</xref> shows a scatter diagram which plots BIOSMILE's SRL F-score against the combined system's. Each data point represents one PASBio verb. The correlation between these two F-scores is 0.52, which is in the range of moderately positive correlation (0.4&#x02013;0.7). We examined the outlying verbs with the greatest drops in F-score after conversion. These included "mutate", "truncate", "transcribe", and "modify". We found that the first three suffered from an absence of key terms. The last verb, modify, had less than five annotated sentences in the PASBio<sub>P </sub>corpus, making it difficult for our algorithm to generate effective transformation patterns.</p><fig position="float" id="F6"><label>Figure 6</label><caption><p>Correlation between BIOSMILE and combined system performance.</p></caption><graphic xlink:href="1471-2105-9-S12-S18-6"/></fig></sec></sec><sec><title>Conclusion</title><p>In this paper we have demonstrated the feasibility of converting between BioProp and PASBio annotation, which will hopefully facilitate and inspire further PASBio applications. Our approach has involved the use of our previous SRL system, BIOSMILE, as well as the development two new tools, a semi-automatic rule generator and a BioProp-PASBio converter. Our rule-generation tool can save considerable human effort by automatically generating conversion rules which only need fine tuning to be usable. Our BioProp-PASBio converter can achieve very high accuracy (85.29%) using the gold-standard BioProp dataset. Our combined system (BIOSMILE + rule-based converter) achieves an F-score of 69.08% for PASBio's 29 verbs. This performance is close to state-of-the-art ML-based SRL systems in other specific domains [<xref ref-type="bibr" rid="B26">26</xref>].</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors' contributions</title><p>RTH Tsai and HJ Dai designed the semi-automatic rule generation and rule-based conversion algorithms and wrote most of this paper. HJ Dai implemented the conversion algorithm and wrote the rule generator and rule-based converter program. CH Huang, the biologist in our laboratory, verified the generated rules and conducted all experiments. RTH Tsai and WL Hsu guided the whole project.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="S1"><caption><title>Additional file 1</title><p>Supplementary materials.</p></caption><media xlink:href="1471-2105-9-S12-S18-S1.pdf" mimetype="application" mime-subtype="pdf"><caption><p>Click here for file</p></caption></media></supplementary-material></sec></body><back><ack><sec><title>Acknowledgements</title><p>This research was supported in part by the National Science Council under grant NSC 97-2218-E-155-001, NSC96-2752-E-001-001-PAE and the thematic program of Academia Sinica under grant AS95ASIA02.</p><p>This article has been published as part of <italic>BMC Bioinformatics </italic>Volume 9 Supplement 12, 2008: Asia Pacific Bioinformatics Network (APBioNet) Seventh International Conference on Bioinformatics (InCoB2008). The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/9?issue=S12"/>.</p></sec></ack><ref-list><ref id="B1"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Pradhan</surname><given-names>S</given-names></name><name><surname>Ward</surname><given-names>W</given-names></name><name><surname>Hacioglu</surname><given-names>K</given-names></name><name><surname>Martin</surname><given-names>JH</given-names></name><name><surname>Jurafsky</surname><given-names>D</given-names></name></person-group><article-title>Shallow Semantic Parsing Using Support Vector Machines</article-title><source>Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL-2004) Boston, MA, USA</source><year>2004</year></citation></ref><ref id="B2"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hoernig</surname><given-names>R</given-names></name><name><surname>Rauh</surname><given-names>R</given-names></name><name><surname>Strube</surname><given-names>G</given-names></name><name><surname>Hoernig</surname><given-names>R</given-names></name><name><surname>Rauh</surname><given-names>R</given-names></name><name><surname>Strube</surname><given-names>G</given-names></name></person-group><person-group person-group-type="editor"><name><surname>G S</surname></name></person-group><article-title>Events-II: Modeling event recognition</article-title><source>The Cognitive Psychology of Knowledge</source><year>1993</year><publisher-name>Amsterdam WK: Elsevier Science</publisher-name><fpage>113</fpage><lpage>138</lpage></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>M</given-names></name><name><surname>Gildea</surname><given-names>D</given-names></name><name><surname>Kingsbury</surname><given-names>P</given-names></name></person-group><article-title>The proposition bank: An annotated corpus of semantic roles</article-title><source>Computational Linguistics</source><year>2005</year><volume>31</volume><fpage>71</fpage><lpage>106</lpage></citation></ref><ref id="B4"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Babko-Malaya</surname><given-names>O</given-names></name></person-group><article-title>PropBank Annotation Guidelines</article-title><year>2005</year></citation></ref><ref id="B5"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Babko-Malaya</surname><given-names>O</given-names></name></person-group><article-title>Guidelines for Propbank framers</article-title><year>2005</year></citation></ref><ref id="B6"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Carreras</surname><given-names>X</given-names></name><name><surname>M&#x000e0;rquez</surname><given-names>L</given-names></name></person-group><article-title>Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling</article-title><source>Proceedings of CoNLL-2004</source><year>2004</year><fpage>89</fpage><lpage>97</lpage></citation></ref><ref id="B7"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Dowty</surname><given-names>D</given-names></name></person-group><article-title>Thematic proto-roles and argument selection</article-title><source>Language</source><year>1991</year><volume>67</volume><publisher-name>Linguistic Society of America</publisher-name><fpage>547</fpage><lpage>619</lpage></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wattarujeekrit</surname><given-names>T</given-names></name><name><surname>Shah</surname><given-names>PK</given-names></name><name><surname>Collier</surname><given-names>N</given-names></name></person-group><article-title>PASBio: predicate-argument structures for event extraction in molecular biology</article-title><source>BMC Bioinformatics</source><year>2004</year><volume>5</volume><fpage>155</fpage><pub-id pub-id-type="pmid">15494078</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>KB</given-names></name><name><surname>Hunter</surname><given-names>L</given-names></name></person-group><article-title>A critical review of PASBio's argument structures for biomedical verbs</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>S5</fpage><pub-id pub-id-type="pmid">17134478</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>PK</given-names></name><name><surname>Jensen</surname><given-names>LJ</given-names></name><name><surname>Boue</surname><given-names>S</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name></person-group><article-title>Extraction of transcript diversity from scientific literature</article-title><source>PLoS Computational Biology</source><year>2005</year><pub-id pub-id-type="pmid">16103899</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>PK</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name></person-group><article-title>LSAT: learning about alternative transcripts in MEDLINE</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>857</fpage><lpage>865</lpage><pub-id pub-id-type="pmid">16410322</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kogan</surname><given-names>Y</given-names></name><name><surname>Collier</surname><given-names>N</given-names></name><name><surname>Pakhomov</surname><given-names>S</given-names></name><name><surname>Krauthammer</surname><given-names>M</given-names></name></person-group><article-title>Towards Semantic Role Labeling &#x00026; IE in the Medical Literature</article-title><source>AMIA Annual Symposium Proceedings</source><year>2005</year><volume>410</volume><fpage>4</fpage></citation></ref><ref id="B13"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>W-C</given-names></name><name><surname>Tsai</surname><given-names>RT-H</given-names></name><name><surname>Su</surname><given-names>Y-S</given-names></name><name><surname>Ku</surname><given-names>W</given-names></name><name><surname>Sung</surname><given-names>T-Y</given-names></name><name><surname>Hsu</surname><given-names>W-L</given-names></name></person-group><article-title>A Semi-Automatic Method for Annotating a Biomedical Proposition Bank</article-title><source>Proceedings of ACL Workshop on Frontiers in Linguistically Annotated Corpora</source><year>2006</year><fpage>5</fpage><lpage>12</lpage></citation></ref><ref id="B14"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>RT-H</given-names></name><name><surname>Chou</surname><given-names>W-C</given-names></name><name><surname>Su</surname><given-names>Y-S</given-names></name><name><surname>Lin</surname><given-names>Y-C</given-names></name><name><surname>Sung</surname><given-names>C-L</given-names></name><name><surname>Dai</surname><given-names>H-J</given-names></name><name><surname>Yeh</surname><given-names>IT</given-names></name><name><surname>Ku</surname><given-names>W</given-names></name><name><surname>Sung</surname><given-names>T-Y</given-names></name><name><surname>Hsu</surname><given-names>W-L</given-names></name></person-group><article-title>BIOSMILE: A semantic role labeling system for biomedical verbs using a maximum-entropy model with automatically generated template features</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>325</fpage><pub-id pub-id-type="pmid">17764570</pub-id></citation></ref><ref id="B15"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>H-J</given-names></name><name><surname>Huang</surname><given-names>C-H</given-names></name><name><surname>Lin</surname><given-names>RTK</given-names></name><name><surname>Tsai</surname><given-names>RT-H</given-names></name><name><surname>Hsu</surname><given-names>W-L</given-names></name></person-group><article-title>BIOSMILE web search: a web application for annotating biomedical entities and relations</article-title><source>Nucleic Acids Res</source><year>2008</year></citation></ref><ref id="B16"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>RT-H</given-names></name><name><surname>Sung</surname><given-names>C-L</given-names></name><name><surname>Dai</surname><given-names>H-J</given-names></name><name><surname>Hung</surname><given-names>H-C</given-names></name><name><surname>Sung</surname><given-names>T-Y</given-names></name><name><surname>Hsu</surname><given-names>W-L</given-names></name></person-group><article-title>NERBio: using selected word conjunctions, term normalization, and global patterns to improve biomedical named entity recognition</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>S11</fpage><pub-id pub-id-type="pmid">17254295</pub-id></citation></ref><ref id="B17"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>H-J</given-names></name><name><surname>Hung</surname><given-names>H-C</given-names></name><name><surname>Tsai</surname><given-names>RT-H</given-names></name><name><surname>Hsu</surname><given-names>W-L</given-names></name></person-group><article-title>IASL Systems in the Gene Mention Tagging Task and Protein Interaction Article Sub-task</article-title><source>Proceedings of Second BioCreAtIvE Challenge Evaluation Workshop: 2007; Madrid, Spain</source><year>2007</year><fpage>69</fpage><lpage>76</lpage></citation></ref><ref id="B18"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Tateisi</surname><given-names>Y</given-names></name><name><surname>Yakushiji</surname><given-names>A</given-names></name><name><surname>Ohta</surname><given-names>T</given-names></name><name><surname>Tsujii</surname><given-names>J</given-names></name></person-group><article-title>Syntax Annotation for the GENIA corpus</article-title><source>Proc IJCNLP Companion volume</source><year>2005</year><fpage>222</fpage><lpage>227</lpage></citation></ref><ref id="B19"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Punyakanok</surname><given-names>V</given-names></name><name><surname>Roth</surname><given-names>D</given-names></name><name><surname>Yih</surname><given-names>W</given-names></name><name><surname>Zimak</surname><given-names>D</given-names></name></person-group><article-title>Semantic role labeling via integer linear programming inference</article-title><source>Proceedings of the 20th international conference on Computational Linguistics</source><year>2004</year></citation></ref><ref id="B20"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Charniak</surname><given-names>E</given-names></name></person-group><article-title>A Maximum-Entropy-Inspired Parser</article-title><source>Proceedings of the first conference on North American chapter of the Association for Computational Linguistics</source><year>2000</year><publisher-name>Seattle, Washington: Morgan Kaufmann Publishers Inc</publisher-name><fpage>132</fpage><lpage>139</lpage></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>AL</given-names></name><name><surname>Della Pietra</surname><given-names>VJ</given-names></name><name><surname>Della Pietra</surname><given-names>SA</given-names></name></person-group><article-title>A maximum entropy approach to natural language processing</article-title><source>Computational Linguistics</source><year>1996</year><volume>22</volume><fpage>39</fpage><lpage>71</lpage></citation></ref><ref id="B22"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Warner</surname><given-names>C</given-names></name><name><surname>Bies</surname><given-names>A</given-names></name><name><surname>Brisson</surname><given-names>C</given-names></name><name><surname>Mott</surname><given-names>J</given-names></name></person-group><article-title>Addendum to the Penn Treebank II Style Bracketing Guidelines: BioMedical Treebank Annotation</article-title><year>2004</year></citation></ref><ref id="B23"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Santorini</surname><given-names>B</given-names></name></person-group><source>Part-of-speech tagging guidelines for the Penn Treebank Project (3rd revision)</source><year>1990</year><volume>178</volume><publisher-name>Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Tech Rep MS-CIS-90-47, Line Lab</publisher-name></citation></ref><ref id="B24"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Hudson</surname><given-names>RA</given-names></name></person-group><source>Word grammar</source><year>1984</year><publisher-name>B. Blackwell, Oxford, England; New York</publisher-name></citation></ref><ref id="B25"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>M</given-names></name></person-group><article-title>HEAD DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING</article-title><source>PhD Thesis</source><year>1999</year><publisher-name>Philadelphia: University of Pennsylvania</publisher-name></citation></ref><ref id="B26"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Carreras</surname><given-names>X</given-names></name><name><surname>M'arquez</surname><given-names>L&#x00131;</given-names></name></person-group><article-title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling</article-title><source>Proceedings of CoNLL</source><year>2005</year></citation></ref></ref-list></back></article>