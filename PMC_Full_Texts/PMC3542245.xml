<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" xml:lang="en"><?DTDIdentifier.IdentifierValue article.dtd?><?DTDIdentifier.IdentifierType system?><?SourceDTD.DTDName article.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName bmc2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id><journal-title-group><journal-title>BMC Bioinformatics</journal-title></journal-title-group><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">3542245</article-id><article-id pub-id-type="publisher-id">1471-2105-13-211</article-id><article-id pub-id-type="pmid">22913485</article-id><article-id pub-id-type="doi">10.1186/1471-2105-13-211</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>NetiNeti: discovery of scientific names from text using machine learning methods</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="A1"><name><surname>Akella</surname><given-names>Lakshmi Manohar</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>manohar.akella@gmail.com</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Norton</surname><given-names>Catherine N</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>cnorton@mbl.edu</email></contrib><contrib contrib-type="author" id="A3"><name><surname>Miller</surname><given-names>Holly</given-names></name><xref ref-type="aff" rid="I1">1</xref><email>hmiller@mbl.edu</email></contrib></contrib-group><aff id="I1"><label>1</label>MBLWHOI Library, Marine Biological Laboratory, Woods Hole, MA, USA</aff><aff id="I2"><label>2</label>Present address: Sears Holdings Corporation, Hoffman Estates, IL 60179, USA</aff><pub-date pub-type="collection"><year>2012</year></pub-date><pub-date pub-type="epub"><day>22</day><month>8</month><year>2012</year></pub-date><volume>13</volume><fpage>211</fpage><lpage>211</lpage><history><date date-type="received"><day>15</day><month>10</month><year>2010</year></date><date date-type="accepted"><day>6</day><month>8</month><year>2012</year></date></history><permissions><copyright-statement>Copyright &#x000a9;2012 Akella et al.; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2012</copyright-year><copyright-holder>Akella et al.; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (
<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="http://www.biomedcentral.com/1471-2105/13/211"/><abstract><sec><title>Background</title><p>A scientific name for an organism can be associated with almost all biological data. Name identification is an important step in many text mining tasks aiming to extract useful information from biological, biomedical and biodiversity text sources. A scientific name acts as an important metadata element to link biological information.</p></sec><sec><title>Results</title><p>We present NetiNeti (Name Extraction from Textual Information-Name Extraction for Taxonomic Indexing), a machine learning based approach for recognition of scientific names including the discovery of new species names from text that will also handle misspellings, OCR errors and other variations in names. The system generates candidate names using rules for scientific names and applies probabilistic machine learning methods to classify names based on structural features of candidate names and features derived from their contexts. NetiNeti can also disambiguate scientific names from other names using the contextual information. We evaluated NetiNeti on legacy biodiversity texts and biomedical literature (MEDLINE). NetiNeti performs better (precision&#x02009;=&#x02009;98.9% and recall&#x02009;=&#x02009;70.5%) compared to a popular dictionary based approach (precision&#x02009;=&#x02009;97.5% and recall&#x02009;=&#x02009;54.3%) on a 600-page biodiversity book that was manually marked by an annotator. On a small set of PubMed Central&#x02019;s full text articles annotated with scientific names, the precision and recall values are 98.5% and 96.2% respectively. NetiNeti found more than 190,000 unique binomial and trinomial names in more than 1,880,000 PubMed records when used on the full MEDLINE database. NetiNeti also successfully identifies almost all of the new species names mentioned within web pages.</p></sec><sec><title>Conclusions</title><p>We present NetiNeti, a machine learning based approach for identification and discovery of scientific names. The system implementing the approach can be accessed at
<ext-link ext-link-type="uri" xlink:href="http://namefinding.ubio.org.">http://namefinding.ubio.org.</ext-link></p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>There is a vast and ever growing amount of literature in biology, ecology, biomedicine, biodiversity, genomics and proteomics. The U.S National Library of Medicine&#x02019;s MEDLINE
[<xref ref-type="bibr" rid="B1">1</xref>] database is one such source with more than 18 million abstracts of journal articles in life sciences with focus in biomedicine. Major efforts to digitize legacy literature undertaken by consortiums like the Biodiversity Heritage Library (BHL)
[<xref ref-type="bibr" rid="B2">2</xref>] generate vast amounts of text data from the Optical Character Recognition (OCR) of scanned literature. Extraction of knowledge from sources like MEDLINE can significantly speed up biomedical research by providing access to relevant information about diseases, genes, gene-protein, protein-protein interactions, model organisms and drugs. While gene/protein identifications and binary interactions have been the focus of biomedical text mining, more ambitious tasks like identifying complex nested structures are also being pursued currently
[<xref ref-type="bibr" rid="B3">3</xref>].</p><p>Identification of species names and the normalization task of mapping them to identifiers in a database are considered essential sub-tasks for many text mining projects
[<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B5">5</xref>] like recognizing gene names
[<xref ref-type="bibr" rid="B6">6</xref>-<xref ref-type="bibr" rid="B8">8</xref>] or extracting organism-specific information like life history, geographic distribution and predator&#x02013;prey relationships from biodiversity and biomedical literature. A scientific name is a genus name or a species level name with genus followed by species or a name below the species level with genus, species and subspecies information. It can also be a higher order taxonomic name like family, order, etc. A scientific name is one of the named entities that can be connected with other entities like gene names, protein names, geographic locations, diseases, common names of organisms and names of people who first described the species. Recognition of named entities is frequently a first step in the process of performing more complex information extraction tasks like finding relations between the named entities or for question answering
[<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>]. The name of an organism is one of the few identifying elements associated with almost all biological data
[<xref ref-type="bibr" rid="B11">11</xref>]. A scientific name extraction system will be very useful in gathering all contexts in the form of sentences or paragraphs associated with organism names. These sentences and paragraphs can help enrich the existing content and add new content for projects like the Encyclopedia of Life (EOL), which aims to create a webpage for every single species on Earth
[<xref ref-type="bibr" rid="B12">12</xref>]. Natural language processing and machine learning methods can be applied to extract fine-grained, atomic information that can be used to populate biological databases and repositories. The organism name serves as an important metadata element for linking information from various biological sources
[<xref ref-type="bibr" rid="B13">13</xref>-<xref ref-type="bibr" rid="B16">16</xref>], so a species name identification system is an essential tool in information integration.</p><p>Most of the approaches in the literature addressing the problem of name finding from text sources primarily rely on dictionaries with a list of scientific and/or common names
[<xref ref-type="bibr" rid="B4">4</xref>,<xref ref-type="bibr" rid="B14">14</xref>,<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B18">18</xref>]. TaxonGrab
[<xref ref-type="bibr" rid="B17">17</xref>] is a dictionary-based approach that uses a dictionary generated by combining dictionaries of English words and biomedical terms instead of a list of scientific names. Words that do not appear in this dictionary (inverse lexicon) and that follow simple rules for capitalization, abbreviations, variants and subspecies mentions used in scientific names are considered as organism names. Approaches that primarily rely on this kind of an inverse lexicon tend to have low precision as this can gather many false positives from misspelled English words, OCR errors and non-English words that pass through the rule filters. The precision of the system can also vary significantly from one text source to another depending on the number of words covered by the inverse lexicon. Hence such a system is also likely to perform very poorly on non-English texts.</p><p>TaxonFinder
[<xref ref-type="bibr" rid="B14">14</xref>] is designed to find scientific names from text with the help of separate dictionaries for species and genus names. Though the approach is likely to have fewer false positives, the number of false negatives (the number of correct names missed) can be high as it cannot find anything that is not a genus and species combination from the dictionaries used in the approach. Such an approach cannot find misspelled names, names with OCR errors, new species names and other names not present in the dictionary. Such a system can also have false positives due to the presence of incorrect names, names that are spelled the same as some common English words and geo-location names (e.g. major, Atlanta).</p><p>The approach &#x0201c;Linnaeus&#x0201d;
[<xref ref-type="bibr" rid="B4">4</xref>] uses dictionaries for scientific and common names to construct a DFA (Deterministic Finite Automaton)
[<xref ref-type="bibr" rid="B19">19</xref>] to match species names. The system also tries to resolve acronyms for organisms (e.g. HIV, CMV) using the frequencies of most commonly used acronyms in MEDLINE calculated using Acromine
[<xref ref-type="bibr" rid="B20">20</xref>]. Linnaeus only focuses on finding species names and currently does not deal with genera or other higher-order taxonomic units. Inherently being a dictionary based approach, Linnaeus also will have issues that were discussed above for approaches like TaxonFinder. There are also other dictionary-based approaches that identify species names based on the NCBI taxonomy
[<xref ref-type="bibr" rid="B21">21</xref>,<xref ref-type="bibr" rid="B22">22</xref>]. FAT (Find All Taxon names)
[<xref ref-type="bibr" rid="B18">18</xref>] is another tool that uses a combination of rules, dictionaries of scientific names and non-names along with input from users to find scientific names. Wang et al.
[<xref ref-type="bibr" rid="B8">8</xref>,<xref ref-type="bibr" rid="B23">23</xref>,<xref ref-type="bibr" rid="B24">24</xref>] developed approaches to tag and disambiguate genes, proteins and protein-protein interaction with species names from the NCBI taxonomy, Uniprot
[<xref ref-type="bibr" rid="B25">25</xref>] and manually created dictionaries using a rule based approach and/or with a machine learning based classifier. Their main objective was to disambiguate gene/protein or protein-protein mentions in text using species tags.</p><p>Here we focus on recognition/discovery of scientific names of organisms from various text sources. The problem of discovery of binomial and trinomial scientific names along with genera and higher taxonomic units can be quite complex. For example, biodiversity literature and legacy text sources like BHL (Biodiversity Heritage Library) contain many names with OCR errors, alternative names and misclassified names. Thousands of new species are discovered every year and many are reclassified. Some names are spelled the same as geo-locations or people names and therefore disambiguation of names is required. We have developed approaches and built tools that address all of the above.</p><p>NetiNeti is a solution for scientific name recognition/discovery. This approach enables finding scientific names in literature from various domains like biomedicine and biodiversity. It can discover new scientific names and also find names with OCR errors and variations. The system is based on probabilistic machine learning methods where a given string has a certain probability of being a scientific name or not being a scientific name depending on the name string itself and the context in which it appears. NetiNeti builds a machine learning classifier from both the structural features of a string and its contextual features. In the process of classifying a string, the approach can differentiate between common words like names of places or people from scientific names based on the context in which a name appears. For example, <italic>Atlanta</italic> is a scientific name in the sentence, &#x0201c;Atlanta is a genus of pelagic marine gastropod molluscs&#x0201d;. However, in the sentence, &#x0201c;The city Atlanta is in the state of Georgia&#x0201d;, <italic>Atlanta</italic> is a geographic location and not a genus name. NetiNeti correctly recognizes the word <italic>Atlanta</italic> as a scientific name in the first context and does not recognize it as a scientific name in the second context. Simple rules for capitalization and abbreviations in species names are applied as a pre-filtering step to generate candidate names. Candidates with common English words were also removed in the pre-filtering process. The candidate names along with their contexts are then classified using a supervised machine learning classifier. While the system can disambiguate and discover what scientific names of organisms are mentioned in a document, the approach is not about discovering documents that are about specific organisms based on their presence in the document.</p><p>We evaluated NetiNeti on legacy biodiversity texts (BHL books) and biomedical literature (MEDLINE). We compared results of NetiNeti and a dictionary based scientific name finder with the results of manual annotation of a BHL book. A comparison of some of the probabilistic machine learning algorithms on our annotated dataset for scientific name finding is presented. We also present the results of running NetiNeti on other biological text sources.</p></sec><sec sec-type="methods"><title>Methods</title><sec><title>Pre-filtering and candidate generation</title><p>The input text is first tokenized using a tokenization scheme that breaks the characters in a stream of characters in natural language text into distinct meaningful units called tokens. We followed the conventions used by the Penn Treebank project
[<xref ref-type="bibr" rid="B26">26</xref>] to tokenize text. Word trigrams, which are groups of three tokens along the token-sequence are then generated from the tokenized text and each trigram is then passed through a simple rule filter which checks if the tokens in the trigram have the right capitalization, abbreviations, etc. and checks if the trigram has no common English words. Each trigram that passes through the rule filter is then classified by a machine learning classifier as &#x0201c;scientific-name&#x0201d; or &#x0201c;not-a-scientific-name&#x0201d; using the structural and contextual features of the trigram. The trigram that was classified as a scientific name corresponds to a trinomial name, which is a name below the species level with genus, species and usually a subspecies. If a trigram fails to pass though the rule filter, the first two tokens (word bigram) of the trigram are then tested to see if they can become a candidate for a binomial name, with genus followed by a species mention. The classifier then classifies such candidate bigrams. Similarly, the first token of a failed bigram is analysed if it can become a candidate for a uninominal name (genus or higher order taxonomic unit), which gets classified accordingly if it is deemed as a candidate. NetiNeti also resolves abbreviated species names by noting that an abbreviation can be used for a species after a mention of its genus or an abbreviation can follow a mention of a full name (genus-species combination) or an abbreviated name for a species can be used after a mention of another species name from the same genus.</p></sec><sec><title>Machine learning based classification</title><p>We applied probabilistic machine learning algorithms like Na&#x000ef;ve Bayes and Maximum Entropy to classify candidate names. The objective is to estimate the probability of a label (whether a name is scientific or not) given a candidate string along with its contextual information. Na&#x000ef;ve Bayes and Maximum Entropy classifiers learn or estimate the probabilities from a training set.</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math id="M1" name="1471-2105-13-211-i1" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi mathvariant="italic">P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math id="M2" name="1471-2105-13-211-i2" overflow="scroll"><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:msub><mml:mo>max</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mo>log</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mo>log</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We are primarily interested in the conditional probability of a class label,
<inline-formula><mml:math id="M3" name="1471-2105-13-211-i3" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02208;</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mo>'</mml:mo><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>'</mml:mo><mml:mo>,</mml:mo><mml:mo>'</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> given an input string and its contexts <italic>s</italic><sub><italic>j</italic></sub> as in Eq.1. The &#x02018;yes&#x02019; and &#x02018;<italic>no&#x02019;</italic> labels correspond to whether a string is a scientific name or not. Once we get these conditional probabilities, we simply choose the label with the highest probability for a given string. The Na&#x000ef;ve Bayes classifier
[<xref ref-type="bibr" rid="B27">27</xref>-<xref ref-type="bibr" rid="B29">29</xref>] as seen in Eq.1. actually models the joint probability
<inline-formula><mml:math id="M4" name="1471-2105-13-211-i4" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> of a class <italic>c</italic> and a string <italic>s</italic> and makes an assumption that all the features
<inline-formula><mml:math id="M5" name="1471-2105-13-211-i5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>for the string and its contexts given the class label are independent as in Eq.1 This independence assumption is strong, but it helps to easily estimate the probability
<inline-formula><mml:math id="M6" name="1471-2105-13-211-i6" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, of a string <italic>s</italic><sub><italic>j</italic></sub> given the class label <italic>c</italic><sub><italic>i</italic></sub> from a training set of labelled examples. Even with this independence assumption, the Na&#x000ef;ve Bayes classifier performs surprisingly well in many document classification tasks
[<xref ref-type="bibr" rid="B27">27</xref>,<xref ref-type="bibr" rid="B29">29</xref>].
<inline-formula><mml:math id="M7" name="1471-2105-13-211-i7" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> can be estimated from the number of training examples having the feature value <italic>f</italic><sub><italic>k</italic></sub>, and the number of examples with class label <italic>c</italic><sub><italic>i</italic></sub> and also having the feature value <italic>f</italic><sub><italic>k</italic></sub> We can then get the class label for a string (along with its contexts) from Eq.2 with probabilities taken in the log scale.</p><p>The Na&#x000ef;ve Bayes approach is called <italic>generative</italic> as it is based on a model of the joint distribution <italic>P</italic>(<italic>c</italic>, <italic>s</italic>). The maximum entropy classifier, also known as a logistic regression classifier, is called a <italic>discriminative</italic> approach as it is based on the model of the conditional distribution <italic>P</italic>(<italic>c</italic>|<italic>s</italic>) Maximum entropy is widely used for many natural language processing tasks like text segmentation
[<xref ref-type="bibr" rid="B30">30</xref>], parts-of-speech tagging
[<xref ref-type="bibr" rid="B31">31</xref>], language modelling
[<xref ref-type="bibr" rid="B32">32</xref>], text classification
[<xref ref-type="bibr" rid="B33">33</xref>] and Named Entity Recognition (NER)
[<xref ref-type="bibr" rid="B9">9</xref>,<xref ref-type="bibr" rid="B10">10</xref>]. The principle behind the maximum entropy approach is to model all that is known and assume nothing about what is unknown
[<xref ref-type="bibr" rid="B34">34</xref>]. Given a collection of facts (in the form of a training set), the approach chooses a model that is consistent with all facts with a distribution that is as uniform as possible i.e., the distribution that allocates its probability as evenly as possible obeying all the constraints derived from the training set. The conditional probability of a label
<inline-formula><mml:math id="M8" name="1471-2105-13-211-i8" overflow="scroll"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> given the string context
<inline-formula><mml:math id="M9" name="1471-2105-13-211-i9" overflow="scroll"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> takes the following exponential form
[<xref ref-type="bibr" rid="B35">35</xref>] in Eq.3.</p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math id="M10" name="1471-2105-13-211-i10" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>exp</mml:mo><mml:mfenced open="[" close="]"><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mi>g</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle></mml:mfenced></mml:mrow></mml:math></disp-formula></p><p>Where each
<inline-formula><mml:math id="M11" name="1471-2105-13-211-i11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula> is a binary valued feature function defined on the class label and the string context,
<inline-formula><mml:math id="M12" name="1471-2105-13-211-i12" overflow="scroll"><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>s are the weights to be learned from the training set for the feature functions and
<inline-formula><mml:math id="M13" name="1471-2105-13-211-i13" overflow="scroll"><mml:mrow><mml:mi>Z</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:munder><mml:mrow><mml:mo>exp</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle><mml:msub><mml:mi>g</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a normalizing factor that ensures that
<inline-formula><mml:math id="M14" name="1471-2105-13-211-i14" overflow="scroll"><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>&#x02211;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mrow><mml:mi>P</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The parameters
<inline-formula><mml:math id="M15" name="1471-2105-13-211-i15" overflow="scroll"><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> are estimated via hill climbing approaches like Improved Iterative Scaling (IIS)
[<xref ref-type="bibr" rid="B35">35</xref>] or Generalized Iterative Scaling (GIS)
[<xref ref-type="bibr" rid="B36">36</xref>]. Limited-Memory Variable Metric optimization methods like L-BFGS
[<xref ref-type="bibr" rid="B37">37</xref>] have been found to be effective for Maximum Entropy parameter estimation
[<xref ref-type="bibr" rid="B38">38</xref>]. In our scientific name recognition task, we have applied and compared the IIS, GIS and L-BFGS methods for parameter estimation on a corpus that was manually annotated with scientific names. For both Na&#x000ef;ve Bayes and the Maximum Entropy classifiers, we used the Python
[<xref ref-type="bibr" rid="B39">39</xref>] implementations in the NLTK
[<xref ref-type="bibr" rid="B40">40</xref>] package. MEGAM
[<xref ref-type="bibr" rid="B41">41</xref>] optimization package was used for L-BFGS optimization.</p></sec><sec><title>Training set generation</title><p>An initial set of about 5,000 names was used as a positive example set. Candidate strings from unigram, bigram and trigrams of a tokenized BHL book
[<xref ref-type="bibr" rid="B42">42</xref>], which does not contain any scientific names, was used as an initial negative example set. An initial maximum entropy classifier was trained with the initial training set using only the structural features of strings. A set of MEDLINE abstracts, a small portion of content from EOL
[<xref ref-type="bibr" rid="B12">12</xref>] and biodiversity texts from BHL were segmented into sentences using the sentence tokenizer in NLTK, pre-filtering and candidate generation steps were performed for each sentence, and the initial classifier was used to get scientific names that were identified with high confidence. The scientific names along with the sentences in which they occur together form the positive example set. Features were derived from the scientific names and a neighborhood of word contexts appearing around the scientific names in the sentences. We tokenized a geography book from the Internet archive
[<xref ref-type="bibr" rid="B42">42</xref>] and the strings derived from word unigrams, bigrams, and trigrams in the tokenized text of the book form the negative example set. About 10,000 positive examples with contextual information, another 10,000 examples from scientific names without contextual information were used as the positive example set. Abbreviated names from these examples were also added to the positive example set. A total of about 40,000 positive examples together with another set of about 43,000 negative examples were used to generate a training set of 83,000 examples for the two class labels. Features used include the last three, last two and the last characters along with the first and second characters of the unigram, bigram, and trigram candidates. Binary features like whether the last, second last, and third last characters are present in different partitions of the set, <italic>&#x02019;a&#x02019;,&#x02019;e&#x02019;,&#x02019;i&#x02019;,&#x02019;o&#x02019;,&#x02019;u&#x02019;,&#x02019;s&#x02019;,&#x02019;m&#x02019;</italic> were also used. Presence or absence of a particular word in unigram, bigram, and the trigram candidates in a dictionary of genus and species combinations were also part of the binary features. When a word token is part of the dictionary of names it contributes to the conditional probability of the candidate name given the structural and contextual features. Numerical features like the number of vowels in various parts of the candidate names were also used. For contextual features, words appearing in the neighborhood of candidate names and their parts-of-speech tags were used.</p></sec></sec><sec><title>Results and discussion</title><sec><title>Evaluation sets</title><p>NetiNeti focuses on discovering/identifying scientific names of organisms including names with spelling and OCR errors from text sources across domains like biodiversity and biomedicine. We present the results of running NetiNeti on three different text sources.</p><p>BHL is a rich source of biodiversity data with over 80,000 volumes corresponding to over 30 million scanned pages converted to text. A gold-standard biodiversity corpus marked with scientific names by an annotator was created, as there are no previously reported annotated corpora for biodiversity information. Also, the evaluation sets that were previously reported were not specifically annotated for scientific names of species along with errors and variations. All the scientific names, including names with OCR errors, occurring in a 600 page BHL book &#x0201c;American Seashells&#x0201d;
[<xref ref-type="bibr" rid="B43">43</xref>] were extracted manually by the annotator. We used NetiNeti to identify all names in this book and compared our results to the list of names that were manually extracted. We also compared our results with the results of the dictionary-based TaxonFinder
[<xref ref-type="bibr" rid="B44">44</xref>] and the FAT tool integrated into the GoldenGATE editor
[<xref ref-type="bibr" rid="B45">45</xref>] for finding scientific names The comparison results have been summarized in Table&#x02009;
<xref ref-type="table" rid="T1">1</xref>. We also ran NetiNeti on MEDLINE, which contains over 18 million bibliographic records from journal articles in life sciences with a concentration on biomedicine. We present the results of running two of the best performing algorithms against the MEDLINE database summarized in Table&#x02009;
<xref ref-type="table" rid="T2">2</xref>. We also evaluated NetiNeti on a small subset of 136 tagged PubMed Central&#x02019;s (PMC)
[<xref ref-type="bibr" rid="B46">46</xref>] open access full-text articles. These 136 articles were selected from the evaluation set used by Linnaeus species identification system
[<xref ref-type="bibr" rid="B4">4</xref>] with only scientific name tags, as their full PMC evaluation set consists of articles also tagged with common names.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Precision and recall values for NetiNeti, TaxonFinder and FAT on the american seashell book</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead valign="top"><tr><th align="left"><bold>APPROACH</bold></th><th align="left"><bold>PRECISION</bold></th><th align="left"><bold>RECALL</bold></th><th align="left"><bold>F-SCORE</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">NetiNeti<hr/></td><td align="left" valign="bottom">0.989<hr/></td><td align="left" valign="bottom">0.705<hr/></td><td align="left" valign="bottom">0.8231<hr/></td></tr><tr><td align="left" valign="bottom">TaxonFinder<hr/></td><td align="left" valign="bottom">0.975<hr/></td><td align="left" valign="bottom">0.543<hr/></td><td align="left" valign="bottom">0.6975<hr/></td></tr><tr><td align="left">FAT</td><td align="left">0.840</td><td align="left">0.402</td><td align="left">0.5437</td></tr></tbody></table></table-wrap><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Results of running NetiNeti with Na&#x000ef;ve Bayes and MaxEnt (GIS) on MEDLINE</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead valign="top"><tr><th align="left"><bold>Algorithm</bold></th><th align="left"><bold>Unique</bold></th><th align="left"><bold>Binomial and Trinomials</bold></th><th align="left"><bold>PMIDs covered</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">Na&#x000ef;ve Bayes<hr/></td><td align="left" valign="bottom">227796<hr/></td><td align="left" valign="bottom">193596<hr/></td><td align="left" valign="bottom">1883750<hr/></td></tr><tr><td align="left">MaxEnt</td><td align="left">214352</td><td align="left">188606</td><td align="left">1551176</td></tr></tbody></table></table-wrap></sec><sec><title>Comparison of machine learning classifiers</title><p>We performed a series of training experiments with the Na&#x000ef;ve Bayes classifier using different neighbourhoods for contextual features, different sizes of positive and negative training examples and evaluated the resulting classifiers with the precision and recall measures on the &#x0201c;American Seashells&#x0201d; book
[<xref ref-type="bibr" rid="B43">43</xref>] using the manually extracted set of names from it. Precision is the fraction of the retrieved names that are relevant scientific names and recall is the fraction of scientific names retrieved from all the scientific names in a document. &#x0201c;cspan&#x0201d; in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref> indicates the number of contextual features. When no contextual features were used, increasing the number of training examples did not yield any significant improvements in precision or recall as in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>A indicated by the red circles which all clustered together. Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>B illustrates this more clearly, where all the red circles are close to each other in the P-R space. The blue circles are the result of using classifiers with a single contextual feature on either side of the candidate name. We can see that all the classifiers corresponding to the blue circles perform better than any of the classifiers corresponding to the red circles that did not use any contextual information during the training phase. All the circles colored other than red in Figures&#x02009;
<xref ref-type="fig" rid="F1">1</xref>A and
<xref ref-type="fig" rid="F1">1</xref>B represent the precision and recall values of classifiers trained with one or more contextual features on either side of the candidate names.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>Precision and recall plots for various parameter values and settings on the American Seashells test data.</bold><bold>A</bold>, Plot of precision and recall (P-R) values with different training set sizes and different neighborhoods for contextual features indicated with cspan values corresponding to the number of contextual features from 0 to 7. <bold>B</bold>, This plot has precision and recall values with cspan&#x02009;=&#x02009;0 (no contextual information), cspan =1 and cspan&#x02009;&#x0003e;&#x02009;1. <bold>C</bold>, P-R plots with increasing training set of positive examples and different context spans (cspan&#x02009;=&#x02009;1 to 7). <bold>D</bold>, Summarization of the results in C for cspan&#x02009;=&#x02009;5 corresponding to 5 contextual features on either side of the candidate name. The stars in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref><bold>C</bold>corresponding to cspan&#x02009;=&#x02009;5 were all summarized in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref><bold>D</bold>with different symbols. The star is Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref><bold>D</bold> is 4<sup>th</sup> star from the top in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref><bold>C</bold>.</p></caption><graphic xlink:href="1471-2105-13-211-1"/></fig><p>Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>C illustrates the effect of increasing number of contextual features and increasing the number of positive examples in the training set. For example, the blue stars in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>C correspond to using five contextual features on either side of the candidate name with increasing positive example size during training. This was more clearly represented in Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>D, where we used five contextual features (cspan&#x02009;=&#x02009;5) on either side of the candidate name for each classifier with increasing sizes of positive example sets form 3,000 to 19,000 in increments of 2,000 for training. It can be seen from Figure&#x02009;
<xref ref-type="fig" rid="F1">1</xref>D that increasing the positive example set contributed to the better precision of the corresponding classifier with a slightly lower value for recall.</p><p>In our subsequent experiments we compared the precision and recall values of Na&#x000ef;ve Bayes and Maximum Entropy classification algorithms with various parameter estimation methods like GIS, IIS, and L-BFGS on the manually annotated <italic>American Seashell</italic> book. We also compared the Decision Tree Learning algorithm
[<xref ref-type="bibr" rid="B28">28</xref>,<xref ref-type="bibr" rid="B47">47</xref>] implemented in the NLTK toolkit. For the comparison of the algorithms, we used a context span of 1 corresponding to features derived from a word on either side of the candidate name for which the recall was higher than the other configurations with a good precision (&#x0003e; 0.8). Comparison of the algorithms was performed both with and without the use of a stop-list of English words used as part of the pre-filtering process as described in Methods. The results are summarized in Table&#x02009;
<xref ref-type="table" rid="T3">3</xref>. The Naive Bayes algorithm has the highest F-score (harmonic mean of precision and recall values) compared to other algorithms for this dataset when applied with and without a stop-list during pre-filtering. All the algorithms with the exception of the Decision Tree learning algorithm performed well with a better precision when a stop-list was used, although it did not have much impact on the recall values. Having a stop-list eliminates English words or other common words to generate a cleaner set of candidate names. However, the results from Decision Tree learning algorithm, which is an implementation of the C4.5 algorithm
[<xref ref-type="bibr" rid="B47">47</xref>], are not significantly improved through use of the stop-list. If we have more labelled datasets for scientific name recognition, it would be interesting to see how well the learned decision tree performs on them. The Maximum Entropy algorithm with the limited memory variant of the BFGS algorithm also performs well with a high precision of 0.97 with a stop-list and 0.88 without the stop-list, but the recall values are relatively lower. However, with the GIS estimation, the Maximum Entropy approach has the second best F-score of 0.7455 after the Na&#x000ef;ve Bayes algorithm as shown in Table&#x02009;
<xref ref-type="table" rid="T3">3</xref>.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Precision and recall values for na&#x000ef;ve bayes, maximum entropy (iis, gis, l-bfgs) and decision tree learning algorithms on the american seashells book</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/><col align="left"/><col align="left"/></colgroup><thead valign="top"><tr><th align="left"><bold>ALGORITHM</bold></th><th align="left"><bold>STOPLIST</bold></th><th align="left"><bold>PRECISION</bold></th><th align="left"><bold>RECALL</bold></th><th align="left"><bold>F-SCORE</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">Na&#x000ef;ve Bayes<hr/></td><td align="left" valign="bottom">Yes<hr/></td><td align="left" valign="bottom">0.9487<hr/></td><td align="left" valign="bottom">0.6897<hr/></td><td align="left" valign="bottom">0.7987<hr/></td></tr><tr><td align="left" valign="bottom">Na&#x000ef;ve Bayes<hr/></td><td align="left" valign="bottom">No<hr/></td><td align="left" valign="bottom">0.7901<hr/></td><td align="left" valign="bottom">0.6877<hr/></td><td align="left" valign="bottom">0.7353<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (IIS)<hr/></td><td align="left" valign="bottom">Yes<hr/></td><td align="left" valign="bottom">0.9563<hr/></td><td align="left" valign="bottom">0.5951<hr/></td><td align="left" valign="bottom">0.7336<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (IIS)<hr/></td><td align="left" valign="bottom">No<hr/></td><td align="left" valign="bottom">0.8175<hr/></td><td align="left" valign="bottom">0.5933<hr/></td><td align="left" valign="bottom">0.6875<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (GIS)<hr/></td><td align="left" valign="bottom">Yes<hr/></td><td align="left" valign="bottom">0.9541<hr/></td><td align="left" valign="bottom">0.6118<hr/></td><td align="left" valign="bottom">0.7455<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (GIS)<hr/></td><td align="left" valign="bottom">No<hr/></td><td align="left" valign="bottom">0.8151<hr/></td><td align="left" valign="bottom">0.6108<hr/></td><td align="left" valign="bottom">0.6983<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (L-BFGS)<hr/></td><td align="left" valign="bottom">Yes<hr/></td><td align="left" valign="bottom">0.9707<hr/></td><td align="left" valign="bottom">0.5481<hr/></td><td align="left" valign="bottom">0.7006<hr/></td></tr><tr><td align="left" valign="bottom">MaxEnt (L-BFGS)<hr/></td><td align="left" valign="bottom">No<hr/></td><td align="left" valign="bottom">0.8883<hr/></td><td align="left" valign="bottom">0.5410<hr/></td><td align="left" valign="bottom">0.6724<hr/></td></tr><tr><td align="left" valign="bottom">Decision Tree<hr/></td><td align="left" valign="bottom">Yes<hr/></td><td align="left" valign="bottom">0.9820<hr/></td><td align="left" valign="bottom">0.5969<hr/></td><td align="left" valign="bottom">0.7424<hr/></td></tr><tr><td align="left">Decision Tree</td><td align="left">No</td><td align="left">0.9793</td><td align="left">0.5882</td><td align="left">0.7349</td></tr></tbody></table></table-wrap></sec><sec><title>Results on biodiversity text with errors</title><p>Figure&#x02009;
<xref ref-type="fig" rid="F2">2</xref> summarizes the results of running the NetiNeti with Na&#x000ef;ve Bayes algorithm on the annotated corpus (&#x0201c;<italic>American Seashell</italic>&#x0201d; book). We also compare our results with those of TaxonFinder. It can be seen that NetiNeti performs better both in terms of precision and recall. We further analysed the 81 names that did not match the manual lookup from NetiNeti and 115 names from TaxonFinder and noticed that among the 81 names, about 22 names were true false positives like geographic locations, common names and author names. The remaining 59 names were either a part of a scientific name, a different variant of a string that the system found from the one that was annotated, etc. Among the 115 names missed by TaxonFinder, about 40 names were true false positives and the rest of the names again were only part of a name or a different variant of a scientific name. The 14 names that are present in NetiNeti and TaxonFinder but not in the manual list were mostly parts of scientific names identified by both approaches and some common true false positives.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Comparison between NetiNeti and TaxonFinder on </bold><bold><italic>American Seashells </italic></bold><bold>Book in BHL.</bold></p></caption><graphic xlink:href="1471-2105-13-211-2"/></fig><p>When calculating the precision and recall reported in Figure&#x02009;
<xref ref-type="fig" rid="F2">2</xref>, we have taken into account only the true false positives. We can see that the recall for TaxonFinder is significantly lower compared to NetiNeti, while the precisions are comparable. For a dictionary-based approach like TaxonFinder, it is less likely to have many false positives as it only retrieves what is already present in a known set of names in the dictionary and so can have higher precision, but the recall can be very low as we have seen in the results summarised in Figure&#x02009;
<xref ref-type="fig" rid="F2">2</xref>, the number of false negatives (the number of correct names missed) can be high as it cannot find anything that is not a genus and species combination from the dictionaries used. Such an approach also cannot handle misspelled names, names with OCR errors, new species names, or other names not present in the dictionary. NetiNeti on the other hand will handle these well and it is a name discovery tool. A comparison of NetiNeti, TaxonFinder and FAT tool for the BHL book is presented in Table&#x02009;
<xref ref-type="table" rid="T1">1</xref>. The FAT approach has lower precision and recall values compared to NetiNeti and TaxonFinder approaches for this corpus. The names marked up by the FAT tool were compared with the manual mark up. 869 of the names identified by FAT did not match with the manually marked up set of names. Most of these unmatched names are species epithets with authorship information. We further analyzed a random sample of 100 names out of these 869 names and examined genus information interpreted by the tool in the marked up tags. 32 of the 100 mismatched names have correctly interpreted genus names and the remaining are all true false positives with incorrect genus tags. We estimated that 278 of these 869 are correct identifications and the adjusted precision and recall values for the FAT approach were summarized in Table&#x02009;
<xref ref-type="table" rid="T1">1</xref>. For many of the true false positives, the FAT tool tags the species epithet, but does not seem to recognize the genus name immediately preceding the species name.</p></sec><sec><title>Results on new species web pages</title><p>We have also conducted several small experiments on web pages with information about newly discovered species along with their scientific names. NetiNeti successfully discovers almost all the new species from the descriptions while the dictionary based TaxonFinder finds in most cases either only the genus or does not recognize the new name at all. The results were summarized in Table&#x02009;
<xref ref-type="table" rid="T4">4</xref>. The double starred names are those that were detected by NetiNeti and not detected by TaxonFinder. A few uninominal names that were not detected by NetiNeti but identified by TaxonFinder are displayed with a single star in the table. In this set, it can be seen that NetiNeti has only one false positive (indicated by &#x02018;FP&#x02019;) and was able to discover almost all of the new species&#x02019; mentions in web pages with new species. The name &#x0201c;Stephania&#x0201d; in the first entry in Table&#x02009;
<xref ref-type="table" rid="T4">4</xref> corresponding to TaxonFinder is a false positive as the name in the context refers to a photographer not the genus &#x0201c;Stephania&#x0201d;.</p><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>Comparison of NetiNeti and TaxonFinder on web pages with new species descriptions</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="left"/><col align="left"/></colgroup><thead valign="top"><tr><th align="left"><bold>URL</bold></th><th align="left"><bold>NetiNeti</bold></th><th align="left"><bold>TaxonFinder</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://www.livescience.com/environment/top-10-new-species-1.html">http://www.livescience.com/environment/ top-10-new-species-1.html</ext-link><hr/></td><td align="left" valign="bottom">Desmoxytes purpurosea **<hr/></td><td align="left" valign="bottom">Desmoxytes<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Electrolux addisoni **<hr/></td><td align="left" valign="bottom">Gryposaurus<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Gryposaurus monumentensis **<hr/></td><td align="left" valign="bottom">Megaceras<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Malo kingi **<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Megaceras briansaltini **<hr/></td><td align="left" valign="bottom">Narkidae<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Narkidae<hr/></td><td align="left" valign="bottom">Oxyuranus temporalis<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Oxyuranus temporalis<hr/></td><td align="left" valign="bottom">Philautus maia*<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Philautus<hr/></td><td align="left" valign="bottom">Stephania-FP<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Styloctenium mindorensis<hr/></td><td align="left" valign="bottom">Styloctenium mindorensis<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Tecticornia bibenda **<hr/></td><td align="left" valign="bottom">Tecticornia<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Xerocomus silwoodensis<hr/></td><td align="left" valign="bottom">Xerocomus silwoodensis<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://news.mongabay.com/2010/0419-hance_microbes.html">http://news.mongabay.com/2010/041 9-hance_ microbes.html</ext-link><hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Ceratium longipes<hr/></td><td align="left" valign="bottom">Ceratium longipes<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Culexiregiloricus trichiscalida **<hr/></td><td align="left" valign="bottom">Chlamydophrys*<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Lebbeus clarehanna **<hr/></td><td align="left" valign="bottom">Lebbeus<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Valdiviella insignis<hr/></td><td align="left" valign="bottom">Valdiviella insignis<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2009_species05">http://species.asu.edu/2009_species05</ext-link><hr/></td><td align="left" valign="bottom">S. ysbryda **<hr/></td><td align="left" valign="bottom">Selenochlamys<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Selenochlamys ysbryda **<hr/></td><td align="left" valign="bottom">Stylommatophora<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Trigonochlamydidae<hr/></td><td align="left" valign="bottom">Testacella *<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Trigonochlamydidae<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2010_species09">http://species.asu.edu/2010_species09</ext-link><hr/></td><td align="left" valign="bottom">G. carapo<hr/></td><td align="left" valign="bottom">G. carapo<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Gymnotus carapo<hr/></td><td align="left" valign="bottom">Gymnotidae<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Gymnotidae<hr/></td><td align="left" valign="bottom">Gymnotiformes*<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Gymnotus<hr/></td><td align="left" valign="bottom">Gymnotus<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Gymnotus omarorum **<hr/></td><td align="left" valign="bottom">Gymnotus carapo<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2010_species03">http://species.asu.edu/2010_species03</ext-link><hr/></td><td align="left" valign="bottom">Dioscorea orangeana<hr/></td><td align="left" valign="bottom">Dioscorea orangeana<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Dioscorea sambiranensis<hr/></td><td align="left" valign="bottom">Dioscorea sambiranensis<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Dioscoreaceae<hr/></td><td align="left" valign="bottom">Dioscoreaceae<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2010_species02">http://species.asu.edu/2010_species02</ext-link><hr/></td><td align="left" valign="bottom">Acrocirridae<hr/></td><td align="left" valign="bottom">Acrocirridae<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Swima bombiviridis **<hr/></td><td align="left" valign="bottom">Bombus<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Viridis-FP<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2010_species01">http://species.asu.edu/2010_species01</ext-link><hr/></td><td align="left" valign="bottom">Nepenthes attenboroughii **<hr/></td><td align="left" valign="bottom">Nepenthaceae<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Nepenthaceae<hr/></td><td align="left" valign="bottom">None-FP<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2009_species06">http://species.asu.edu/2009_species06</ext-link><hr/></td><td align="left" valign="bottom">Diplommatinidae<hr/></td><td align="left" valign="bottom">Diplommatinidae<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">O. vermiculum **<hr/></td><td align="left" valign="bottom">None-FP<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Opisthostoma vermiculum **<hr/></td><td align="left" valign="bottom">Opisthostoma<hr/></td></tr><tr><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="http://species.asu.edu/2010_species06">http://species.asu.edu/2010_species06</ext-link><hr/></td><td align="left" valign="bottom">Nephila<hr/></td><td align="left" valign="bottom">Nephila<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Nephila komaci **<hr/></td><td align="left" valign="bottom">Nephila turneri<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Nephila turneri<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td></tr><tr><td align="left" valign="bottom">&#x000a0;<hr/></td><td align="left" valign="bottom">Nephilidae **<hr/></td><td align="left" valign="bottom">&#x000a0;<hr/></td></tr><tr><td align="left">&#x000a0;</td><td align="left">Habitus-FP</td><td align="left">&#x000a0;</td></tr></tbody></table></table-wrap></sec><sec><title>Results on PMC full text and MEDLINE</title><p>The results of running NetiNeti with Na&#x000ef;ve Bayes algorithm for classification on 136 PMC full text articles are summarized in Figure&#x02009;
<xref ref-type="fig" rid="F3">3</xref>. Here we chose a subset of the articles that were specifically tagged with scientific names from the set of articles tagged with both common names and species names as an evaluation set in Linnaeus system. Among the 81 names that did not match with the manual annotation, 76 names are scientific names with misspellings mostly in one or two characters and names that were missed by the annotators. Only 5 names were true false positives that do not correspond to any scientific names. So the precision and recall for NetiNeti on this data set were 0.985 and 0.962 respectively. The Linnaeus system deals with species level names including common names, so we cannot make a direct comparison with our system.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p>Comparison of results of netiNeti with 136 PMC full Text open access articles.</p></caption><graphic xlink:href="1471-2105-13-211-3"/></fig><p>We also analysed the results of running NetiNeti on the whole of MEDLINE with Na&#x000ef;ve Bayes and Maximum Entropy (GIS) classifiers, which were the top two algorithms in terms of F-scores in Table&#x02009;
<xref ref-type="table" rid="T2">2</xref>. The results were summarized in Table&#x02009;
<xref ref-type="table" rid="T2">2</xref>. NetiNeti with the Na&#x000ef;ve Bayes algorithm found 193,596 unique binomial and trinomial names while the Maximum Entropy algorithm found 188,606 names. That is more than 3 times the number of species found by the dictionary-based Linnaeus system even though we focus only on scientific names. In the names extracted from MEDLINE, the errors include disease names like <italic>Enterohepatitis</italic>, terms like <italic>Amputatio interilio-abdominalis</italic> which was extracted from title of a PubMed article in Russian, chemical names like <italic>Aminoanthracene</italic>. Some of the errors in biodiversity text include terms like <italic>Operculum corneous</italic>, words associated with some geographic locations like <italic>Panaina</italic>. Biological terms and certain words associated with geographic locations can be the kind of errors common to both the corpora. Also, named entities with Latin-like endings can be incorrectly identified as scientific names of organisms by the system especially when there is little or no contextual information.</p><p>The system is highly scalable and we ran name finding on the recent update of MEDLINE with over 18 million abstracts in under 9&#x02009;hours on a 2.8 Ghz intel core i7 based machine running Mac OX 10.6 using 6 cores.</p><p>As NetiNeti also extracts names with errors and variations, a need to map the names to known identifiers in a master list of names or a database arises. We are working on highly efficient methods based on suffix-trees to do such a mapping.</p></sec><sec><title>Availability and requirements</title><p>The software system implementing NetiNeti can be accessed at
<ext-link ext-link-type="uri" xlink:href="http://namefinding.ubio.org">http://namefinding.ubio.org</ext-link>. Currently a Na&#x000ef;ve Bayes classifier is applied by default for name finding. The <italic>American Seashell</italic> book and a list of PubMed Central ids used for evaluation of NetiNeti can be found at
<ext-link ext-link-type="uri" xlink:href="http://ubio.org/netinetifiles">http://ubio.org/netinetifiles</ext-link></p></sec></sec><sec sec-type="conclusions"><title>Conclusions</title><p>In this article, we presented an approach for recognizing/discovering scientific names along with spelling errors and variations from various text sources in domains like biodiversity and biomedicine. We present NetiNeti as a solution to name discovery that uses machine learning techniques to classify candidate names generated by applying rules and pre-filtering methods on text. NetiNeti is highly scalable and configurable.</p><p>Whether to know the number of scientific names covered in a text, to extract all the sentences/paragraphs associated with scientific names or to tag mentions of genes, protein or other entities with scientific names or whether to incorporate species names as meta data elements for search, etc. or for taxonomic indexing, an identification and discovery tool like NetiNeti is very useful.</p></sec><sec><title>Authors&#x02019; contributions</title><p>LMA designed, developed the appraoch and implemented the system, performed the experiments described and is a major contributor in the preparation of this manuscript. HJM supervised the project and provided support for manuscript preparation and contributed to the manuscript. CNN reviewed the draft and provided support for the project. All authors read and approved the final manuscript.</p></sec></body><back><sec><title>Acknowledgements</title><p>This project was funded by the Ellison Medical Foundation and a grant from the National Library of Medicine (R01 LM009725). We thank Anna Shipunova for providing manual annotation and for helpful discussions on scientific names. Anna has more than 10&#x02009;years of experience in the Department of Biology at Moscow State University where biological text processing was her major focus. At the MBL she worked with the Encyclopedia of Life biodiversity informatics group before joining the Neti Neti project. We also would like to thank David Patterson and Nathan Wilson for helpful discussions and comments on the manuscript.</p></sec><ref-list><ref id="B1"><mixed-citation publication-type="other"><collab>MEDLINE</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.nlm.nih.gov/databases/databases_medline.html">http://www.nlm.nih.gov/databases/databases_medline.html</ext-link></comment></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="other"><collab>Biodiversity Heritage Library</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.biodiversitylibrary.org/">http://www.biodiversitylibrary.org/</ext-link></comment></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="book"><name><surname>Poon</surname><given-names>H</given-names></name><name><surname>Vanderwende</surname><given-names>L</given-names></name><source>Joint Inference for Knowledge Extraction from Biomedical Literature</source><year>2010</year><publisher-name>In: North American Chapter of the Association for Computational, Linguistics(NAACL-HLT). Los Angeles,CA</publisher-name></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><name><surname>Gerner</surname><given-names>M</given-names></name><name><surname>Nenadic</surname><given-names>G</given-names></name><name><surname>Bergman</surname><given-names>CM</given-names></name><article-title>LINNAEUS: a species name identification system for biomedical literature</article-title><source>BMC Bioinformatics</source><year>2010</year><volume>11</volume><fpage>85</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-85</pub-id><pub-id pub-id-type="pmid">20149233</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="book"><name><surname>Kappeler</surname><given-names>T</given-names></name><name><surname>Kaljurand</surname><given-names>K</given-names></name><name><surname>Rinaldi</surname><given-names>F</given-names></name><source>Automatic Detection of Focus Organisms in Biomedical Publications</source><year>2009</year><publisher-name>In: Association for Computational Linguistics (ACL)-Proceedings of the Workshop on BioNLP, Boulder, Colorado</publisher-name><fpage>80</fpage><lpage>88</lpage></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><name><surname>Hakenberg</surname><given-names>J</given-names></name><name><surname>Plake</surname><given-names>C</given-names></name><name><surname>Leaman</surname><given-names>R</given-names></name><name><surname>Schroeder</surname><given-names>M</given-names></name><name><surname>Gonzalez</surname><given-names>G</given-names></name><article-title>Inter-species normalization of gene mentions with GNAT</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><issue>16</issue><fpage>i126</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btn299</pub-id><pub-id pub-id-type="pmid">18689813</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Hanisch</surname><given-names>D</given-names></name><name><surname>Fundel</surname><given-names>K</given-names></name><name><surname>Mevissen</surname><given-names>HT</given-names></name><name><surname>Zimmer</surname><given-names>R</given-names></name><name><surname>Fluck</surname><given-names>J</given-names></name><article-title>ProMiner: rule-based protein and gene entity recognition</article-title><source>BMC Bioinformatics</source><year>2005</year><volume>6</volume><issue>1</issue><fpage>14</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-6-14</pub-id><pub-id pub-id-type="pmid">15663789</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Matthews</surname><given-names>M</given-names></name><article-title>Distinguishing the species of biomedical named entities for term identification</article-title><source>BMC Bioinformatics</source><year>2008</year><volume>11</volume><issue>9</issue><fpage>6</fpage><pub-id pub-id-type="pmid">20047656</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="other"><name><surname>Borthwick</surname><given-names>A</given-names></name><article-title>A Maximum Entropy Approach to Named Entity Recognition</article-title><source>New York University</source><year>1999</year></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="book"><name><surname>Chieu</surname><given-names>HL</given-names></name><name><surname>Ng</surname><given-names>HT</given-names></name><source>Named entity recognition: a maximum entropy approach using global information</source><year>2002</year><publisher-name>In: International Conference on Computational Linguistics (COLING), Taipei, Taiwan</publisher-name></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><name><surname>Patterson</surname><given-names>DJ</given-names></name><name><surname>Cooper</surname><given-names>J</given-names></name><name><surname>Kirk</surname><given-names>PM</given-names></name><name><surname>Pyle</surname><given-names>RL</given-names></name><name><surname>Remsen</surname><given-names>DP</given-names></name><article-title>Names are key to the big new biology</article-title><source>Trends Ecol Evol</source><year>2010</year><volume>25</volume><issue>12</issue><fpage>686</fpage><lpage>691</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2010.09.004</pub-id><pub-id pub-id-type="pmid">20961649</pub-id></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="other"><collab>Encyclopedia of Life</collab><comment><ext-link ext-link-type="uri" xlink:href="www.eol.org">www.eol.org</ext-link></comment></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><name><surname>Patterson</surname><given-names>DJ</given-names></name><name><surname>Remsen</surname><given-names>D</given-names></name><name><surname>Marino</surname><given-names>WA</given-names></name><name><surname>Norton</surname><given-names>C</given-names></name><article-title>Taxonomic indexing - Extending the role of taxonomy</article-title><source>Systematic Biology</source><year>2006</year><volume>55</volume><issue>3</issue><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1080/10635150500541680</pub-id><pub-id pub-id-type="pmid">16861205</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><name><surname>Leary</surname><given-names>PR</given-names></name><name><surname>Remsen</surname><given-names>DP</given-names></name><name><surname>Norton</surname><given-names>CN</given-names></name><name><surname>Patterson</surname><given-names>DJ</given-names></name><name><surname>Sarkar</surname><given-names>IN</given-names></name><article-title>uBioRSS: tracking taxonomic literature using RSS</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>11</issue><fpage>1434</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm109</pub-id><pub-id pub-id-type="pmid">17392332</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><name><surname>Page</surname><given-names>RD</given-names></name><article-title>TBMap: a taxonomic perspective on the phylogenetic database TreeBASE</article-title><source>BMC Bioinformatics</source><year>2007</year><volume>8</volume><fpage>158</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-8-158</pub-id><pub-id pub-id-type="pmid">17511869</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><name><surname>Sarkar</surname><given-names>IN</given-names></name><article-title>Biodiversity informatics: organizing and linking information across the spectrum of life</article-title><source>Brief Bioinform</source><year>2007</year><volume>8</volume><issue>5</issue><fpage>347</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1093/bib/bbm037</pub-id><pub-id pub-id-type="pmid">17704120</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><name><surname>Koning</surname><given-names>D</given-names></name><name><surname>Sarkar</surname><given-names>I</given-names></name><name><surname>Mortiz</surname><given-names>T</given-names></name><article-title>TaxonGrab: Extracting taxonomic names from text</article-title><source>Biodiversity Informatics</source><year>2005</year><volume>2</volume><fpage>2</fpage></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><name><surname>Sautter</surname><given-names>G</given-names></name><name><surname>B&#x000f6;hm</surname><given-names>K</given-names></name><name><surname>Agosti</surname><given-names>D</given-names></name><article-title>A combining approach to find all taxon names (FAT)</article-title><source>Biodiversity Informatics</source><year>2006</year><volume>3</volume></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="other"><name><surname>Hopcroft</surname><given-names>JE</given-names></name><name><surname>Motwani</surname><given-names>R</given-names></name><name><surname>Ullman</surname><given-names>JD</given-names></name><article-title>Introduction to automata theory languages and computation, 3 edn</article-title><source>Prentice Hall</source><year>2006</year></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><name><surname>Okazaki</surname><given-names>N</given-names></name><name><surname>Ananiadou</surname><given-names>S</given-names></name><article-title>Building an abbreviation dictionary using a term recognition approach</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><issue>24</issue><fpage>3089</fpage><lpage>3095</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl534</pub-id><pub-id pub-id-type="pmid">17050571</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><name><surname>Plake</surname><given-names>C</given-names></name><name><surname>Schiemann</surname><given-names>T</given-names></name><name><surname>Pankalla</surname><given-names>M</given-names></name><name><surname>Hakenberg</surname><given-names>J</given-names></name><name><surname>Leser</surname><given-names>U</given-names></name><article-title>AliBaba: PubMed as a graph</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><issue>19</issue><fpage>2444</fpage><lpage>2445</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl408</pub-id><pub-id pub-id-type="pmid">16870931</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><name><surname>Rebholz-Schuhmann</surname><given-names>D</given-names></name><name><surname>Arregui</surname><given-names>M</given-names></name><name><surname>Gaudan</surname><given-names>S</given-names></name><name><surname>Kirsch</surname><given-names>H</given-names></name><name><surname>Jimeno</surname><given-names>A</given-names></name><article-title>Text processing through Web services: calling Whatizit</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><issue>2</issue><fpage>296</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btm557</pub-id><pub-id pub-id-type="pmid">18006544</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Grover</surname><given-names>C</given-names></name><source>Learning the species of biomedical named entities from annotated corpora. In</source><year>2008</year><publisher-name>International Conference on Language Resources and Evaluation, Marrakech, Morocco</publisher-name></mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Tsujii</surname><given-names>J</given-names></name><name><surname>Ananiadou</surname><given-names>S</given-names></name><article-title>Disambiguating the species of biomedical named entities using natural language parsers</article-title><source>Bioinformatics</source><year>2010</year><volume>26</volume><issue>5</issue><fpage>661</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq002</pub-id><pub-id pub-id-type="pmid">20053840</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="other"><collab>Universal Protein Resource</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/">http://www.uniprot.org/</ext-link></comment></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="other"><collab>The Penn Tree Bank Project</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.cis.upenn.edu/~treebank/">http://www.cis.upenn.edu/~treebank/</ext-link></comment></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="book"><name><surname>Rish</surname><given-names>I</given-names></name><source>An empirical study of the naive bayes classifier</source><year>2001</year><publisher-name>In: International Joint Conference on Artificial Intelligence (IJCAI)- Workshop on Empirical Methods in Artificial Intelligence, Seattle, Washington</publisher-name></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="book"><name><surname>Mitchell</surname><given-names>TM</given-names></name><source>Machine Learning</source><year>1997</year><publisher-name>McGraw-Hill, New York</publisher-name></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal"><name><surname>Domingos</surname><given-names>P</given-names></name><name><surname>Pazzani</surname><given-names>M</given-names></name><article-title>On the optimality of the simple Bayesian classifier under zero&#x02013;one loss</article-title><source>Machine Learning</source><year>1997</year><volume>29</volume><issue>2&#x02013;3</issue><fpage>103</fpage><lpage>130</lpage></mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><name><surname>Beeferman</surname><given-names>D</given-names></name><name><surname>Berger</surname><given-names>A</given-names></name><name><surname>Lafferty</surname><given-names>J</given-names></name><article-title>Statistical models for text segmentation</article-title><source>Machine Learning</source><year>1999</year><volume>34</volume><issue>1&#x02013;3</issue><fpage>177</fpage><lpage>210</lpage></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><name><surname>Ratnaparkhi</surname><given-names>A</given-names></name><source>A maximum entropy model for part-of-speech tagging</source><year>1996</year><publisher-name>Empirical Methods in Natural Language Processing (EMNLP), In</publisher-name></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="book"><name><surname>Rosenfeld</surname><given-names>R</given-names></name><source>Adaptive Statistical Language Modeling</source><year>1994</year><publisher-name>A Maximum Entropy Approach, Carnegie Mellon University</publisher-name></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="book"><name><surname>Nigam</surname><given-names>K</given-names></name><name><surname>Lafferty</surname><given-names>J</given-names></name><name><surname>Mccallum</surname><given-names>A</given-names></name><source>Using Maximum Entropy for Text Classification</source><year>1999</year><publisher-name>International Joint Conferences on Artificial Intelligence (IJCAI) -Workshop on Machine, Learning for Information Filtering</publisher-name><fpage>61</fpage><lpage>67</lpage></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><name><surname>Berger</surname><given-names>AL</given-names></name><name><surname>DellaPietra</surname><given-names>SA</given-names></name><name><surname>DellaPietra</surname><given-names>VJ</given-names></name><article-title>A maximum entropy approach to natural language processing</article-title><source>Computational Linguistics</source><year>1996</year><volume>22</volume><issue>1</issue><fpage>39</fpage><lpage>71</lpage></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><name><surname>DellaPietra</surname><given-names>S</given-names></name><name><surname>DellaPietra</surname><given-names>V</given-names></name><name><surname>Lafferty</surname><given-names>J</given-names></name><article-title>Inducing features of random fields</article-title><source>Ieee Transactions on Pattern Analysis and Machine Intelligence</source><year>1997</year><volume>19</volume><issue>4</issue><fpage>380</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1109/34.588021</pub-id></mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><name><surname>Darroch</surname><given-names>JN</given-names></name><name><surname>Ratcliff</surname><given-names>D</given-names></name><article-title>Generalized Iterative Scaling for Log-Linear Models</article-title><source>Annals of Mathematical Statistics</source><year>1972</year><volume>43</volume><issue>5</issue><fpage>1470</fpage><pub-id pub-id-type="doi">10.1214/aoms/1177692379</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="book"><name><surname>Nocedal</surname><given-names>J</given-names></name><name><surname>Wright</surname><given-names>S</given-names></name><source>Numerical Optimization</source><year>1999</year><edition>2</edition><publisher-name>Springer, New York</publisher-name></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="book"><name><surname>Malouf</surname><given-names>R</given-names></name><source>A comparison of algorithms for maximum entropy parameter estimation</source><year>2002</year><publisher-name>Conference on Natural Language Learning (CoNLL), Taipei, Taiwan</publisher-name><fpage>49</fpage><lpage>55</lpage></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="other"><collab>Python Programming Language</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.python.org/">http://www.python.org/</ext-link></comment></mixed-citation></ref><ref id="B40"><mixed-citation publication-type="other"><collab>Natural Language Toolkit</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.nltk.org/">http://www.nltk.org/</ext-link></comment></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="other"><collab>MEGAM</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.umiacs.umd.edu/~hal/megam/">http://www.umiacs.umd.edu/~hal/megam/</ext-link></comment></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="book"><name><surname>Goodrich</surname><given-names>BSG</given-names></name><source>A Pictorial Geography of the World</source><year>1856</year><publisher-name>Charles D. Strong, Boston</publisher-name></mixed-citation></ref><ref id="B43"><mixed-citation publication-type="book"><name><surname>Abbott</surname><given-names>RT</given-names></name><source>American Seashells</source><year>1954</year><publisher-name>Van Nostrand, New York</publisher-name></mixed-citation></ref><ref id="B44"><mixed-citation publication-type="other"><collab>TaxonFinder</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.ubio.org/tools/recognize.php">http://www.ubio.org/tools/recognize.php</ext-link></comment></mixed-citation></ref><ref id="B45"><mixed-citation publication-type="other"><collab>GoldenGATE Editor</collab><comment><ext-link ext-link-type="uri" xlink:href="http://idaho.ipd.uni-karlsruhe.de/GoldenGATE/">http://idaho.ipd.uni-karlsruhe.de/GoldenGATE/</ext-link></comment></mixed-citation></ref><ref id="B46"><mixed-citation publication-type="other"><collab>PubMed Central</collab><comment><ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pmc/">http://www.ncbi.nlm.nih.gov/pmc/</ext-link></comment></mixed-citation></ref><ref id="B47"><mixed-citation publication-type="journal"><name><surname>Quinlan</surname><given-names>JR</given-names></name><article-title>Improved use of continuous attributes in C4.5</article-title><source>Journal of Artificial Intelligence Research</source><year>1996</year><volume>4</volume><fpage>77</fpage><lpage>90</lpage></mixed-citation></ref></ref-list></back></article>