<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Molecules</journal-id><journal-id journal-id-type="iso-abbrev">Molecules</journal-id><journal-id journal-id-type="publisher-id">molecules</journal-id><journal-title-group><journal-title>Molecules</journal-title></journal-title-group><issn pub-type="epub">1420-3049</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6982787</article-id><article-id pub-id-type="doi">10.3390/molecules25010044</article-id><article-id pub-id-type="publisher-id">molecules-25-00044</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Modeling Physico-Chemical ADMET Endpoints with Multitask Graph Convolutional Networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Montanari</surname><given-names>Floriane</given-names></name><xref rid="c1-molecules-25-00044" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><name><surname>Kuhnke</surname><given-names>Lara</given-names></name></contrib><contrib contrib-type="author"><name><surname>Ter Laak</surname><given-names>Antonius</given-names></name></contrib><contrib contrib-type="author"><name><surname>Clevert</surname><given-names>Djork-Arn&#x000e9;</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Costa</surname><given-names>Giosu&#x000e8;</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-molecules-25-00044">Digital Technologies, Bayer AG, 13353 Berlin, Germany; <email>lara.kuhnke@bayer.com</email> (L.K.); <email>antoniuster.laak@bayer.com</email> (A.T.L.); <email>djork-arne.clevert@bayer.com</email> (D.-A.C.)</aff><author-notes><corresp id="c1-molecules-25-00044"><label>*</label>Correspondence: <email>floriane.montanari@bayer.com</email></corresp></author-notes><pub-date pub-type="epub"><day>21</day><month>12</month><year>2019</year></pub-date><pub-date pub-type="collection"><month>1</month><year>2020</year></pub-date><volume>25</volume><issue>1</issue><elocation-id>44</elocation-id><history><date date-type="received"><day>01</day><month>12</month><year>2019</year></date><date date-type="accepted"><day>20</day><month>12</month><year>2019</year></date></history><permissions><copyright-statement>&#x000a9; 2019 by the authors.</copyright-statement><copyright-year>2019</copyright-year><license license-type="open-access"><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><p>Simple physico-chemical properties, like logD, solubility, or melting point, can reveal a great deal about how a compound under development might later behave. These data are typically measured for most compounds in drug discovery projects in a medium throughput fashion. Collecting and assembling all the Bayer in-house data related to these properties allowed us to apply powerful machine learning techniques to predict the outcome of those assays for new compounds. In this paper, we report our finding that, especially for predicting physicochemical ADMET endpoints, a multitask graph convolutional approach appears a highly competitive choice. For seven endpoints of interest, we compared the performance of that approach to fully connected neural networks and different single task models. The new model shows increased predictive performance compared to previous modeling methods and will allow early prioritization of compounds even before they are synthesized. In addition, our model follows the generalized solubility equation without being explicitly trained under this constraint.</p></abstract><kwd-group><kwd>ADMET prediction</kwd><kwd>multitask learning</kwd><kwd>graph convolutional networks</kwd><kwd>solubility</kwd><kwd>QSAR</kwd></kwd-group></article-meta></front><body><sec sec-type="intro" id="sec1-molecules-25-00044"><title>1. Introduction</title><p>Properties, such as solubility, logD, or serum albumin binding have a direct impact on the likelihood of a compound to be successful in clinical trials [<xref rid="B1-molecules-25-00044" ref-type="bibr">1</xref>,<xref rid="B2-molecules-25-00044" ref-type="bibr">2</xref>]. While measuring these endpoints can be done in a relatively high throughput fashion, it still requires the compounds to be synthesized. In silico prediction tools allow to rank and prioritize compounds before they are synthesized, limiting the amount of experiments performed and thereby saving time and money in drug discovery projects.</p><p>Machine learning approaches are typically used to map the structure of compounds to their properties, a method called quantitative structure-activity relationship (QSAR). Common algorithms include multiple linear regression, random forest or support vector machine in combination with circular fingerprints or molecular properties to describe the molecules [<xref rid="B3-molecules-25-00044" ref-type="bibr">3</xref>,<xref rid="B4-molecules-25-00044" ref-type="bibr">4</xref>,<xref rid="B5-molecules-25-00044" ref-type="bibr">5</xref>,<xref rid="B6-molecules-25-00044" ref-type="bibr">6</xref>,<xref rid="B7-molecules-25-00044" ref-type="bibr">7</xref>]. Water solubility and melting point are two endpoints for which a lot of previous modeling was published [<xref rid="B3-molecules-25-00044" ref-type="bibr">3</xref>,<xref rid="B4-molecules-25-00044" ref-type="bibr">4</xref>,<xref rid="B5-molecules-25-00044" ref-type="bibr">5</xref>,<xref rid="B8-molecules-25-00044" ref-type="bibr">8</xref>,<xref rid="B9-molecules-25-00044" ref-type="bibr">9</xref>,<xref rid="B10-molecules-25-00044" ref-type="bibr">10</xref>,<xref rid="B11-molecules-25-00044" ref-type="bibr">11</xref>,<xref rid="B12-molecules-25-00044" ref-type="bibr">12</xref>,<xref rid="B13-molecules-25-00044" ref-type="bibr">13</xref>].</p><p>Membrane affinity and human serum albumin binding, on the other hand, are rarely the subject of QSAR publications. In 2002, Kratochwil and colleagues collected a dataset of 138 compounds with known serum albumin binding and built a partial least squares (PLS) model on top of similarity matrices obtained by comparing pharmacophoric features of the training set. The maximum cross-validation R<sup>2</sup> was reported at 0.48 [<xref rid="B14-molecules-25-00044" ref-type="bibr">14</xref>]. Another study by Ghafourian and Amin [<xref rid="B7-molecules-25-00044" ref-type="bibr">7</xref>] used a public dataset of 792 compounds and tried different QSAR methods (linear models, regression trees, random forest, etc.) and reported a boosted tree model with validation R<sup>2</sup> of 0.65 as best performer.</p><p>Since the Merck molecular activity challenge in 2012 [<xref rid="B15-molecules-25-00044" ref-type="bibr">15</xref>], it became clear that Deep Learning can increase the performance of QSAR models [<xref rid="B16-molecules-25-00044" ref-type="bibr">16</xref>,<xref rid="B17-molecules-25-00044" ref-type="bibr">17</xref>], at least when sufficient data is available.</p><p>The idea to learn jointly over multiple endpoints, or <italic>multitask learning</italic>, is neither new nor specific to the cheminformatics field. Multitask neural networks share parameters in (some of) their hidden layers between all tasks, forcing the learning of a joint representation of the input that will be useful to all tasks. The main advantages of multitask learning are (i) a regularization effect, as the model has to use the same amount of parameters to learn more tasks; (ii) a transfer learning effect, whereby learning related tasks helps extracting features that are useful in a more general way; and (iii) a dataset augmentation effect, as smaller tasks can be combined together with large tasks avoiding overfitting on the small task [<xref rid="B18-molecules-25-00044" ref-type="bibr">18</xref>].</p><p>In 2014, Dahl et al. published the first deep multitask approach to classify the bioactivity of compounds in about 20 different assays. They observed an increased ROC AUC performance of 0.04 on average compared to random forest models, with improvements on individual tasks ranging from no improvement to up to +0.17 [<xref rid="B16-molecules-25-00044" ref-type="bibr">16</xref>].</p><p>In 2016, Kearnes and colleagues [<xref rid="B19-molecules-25-00044" ref-type="bibr">19</xref>] published a related work whereby 22 undisclosed ADMET endpoints from Vertex Pharmaceuticals were modeled together in a large multitask neural network. The endpoints had between a few hundred and a few ten thousand data points. The authors showed that single task neural networks would outperform Random Forest AUC performance by 0.05 on average, while combining all endpoints in one model increased the average baseline AUC performance by 0.1.</p><p>The previously mentioned studies used pre-established molecular descriptors to encode the chemical structures in a computer-readable way, such as circular fingerprints [<xref rid="B20-molecules-25-00044" ref-type="bibr">20</xref>]. Recent progress in handling graph data in neural networks [<xref rid="B21-molecules-25-00044" ref-type="bibr">21</xref>,<xref rid="B22-molecules-25-00044" ref-type="bibr">22</xref>,<xref rid="B23-molecules-25-00044" ref-type="bibr">23</xref>] has been exploited in the cheminformatics field. In 2015, Duvenaud and colleagues proposed an algorithm to learn molecular fingerprints using convolutional networks on a graph representation of the compounds. In this paradigm, atoms correspond to nodes in the graph and bonds are the edges connecting the nodes. The features are learned at the node level, using the adjacency matrix of the graph to communicate information between neighboring nodes [<xref rid="B24-molecules-25-00044" ref-type="bibr">24</xref>].</p><p>Concurrently to our work, Feinberg et al. published on their approach at Merck to model ADMET properties using a type of graph convolutional networks named PotentialNet [<xref rid="B25-molecules-25-00044" ref-type="bibr">25</xref>]. They found great improvement over classical approaches using fingerprints and Random Forest models for many endpoints, including protein plasma binding, solubility, and logD. The average absolute improvement in R<sup>2</sup> over 31 tasks was 0.14 in temporal splits. PotentialNet is a type of graph convolutional networks that have been designed to predict protein-ligand affinities based in gated graph neural networks. They distinguish different edge types and use a gated recurrent unit to learn information selectively [<xref rid="B26-molecules-25-00044" ref-type="bibr">26</xref>].</p><p>In this paper, we combine ten different physico-chemical ADMET endpoints into a single multitask graph convolutional regression model. Our graph convolutional networks are simple in that they do not distinguish bond types and do not contain a recurrent unit, but rather follow the Duvenaud algorithm. We show how changing the learning paradigm (from regular machine learning methods to deep learning), the way to describe compounds (from traditional circular fingerprints to end-to-end learned features) and combining endpoints into one model help improve the performance for most of the endpoints.</p><p>We also show that adding helper tasks (three endpoints for which no good prediction is required) can help boost the performance on more difficult, smaller endpoints like solubility. Similarly, we show that for very easy and large tasks, combining them into a multitask model does not bring them further predictivity. All validations are performed in varied settings beyond random splits, mimicking real world use cases such as time splits or cluster splits.</p></sec><sec sec-type="results" id="sec2-molecules-25-00044"><title>2. Results and Discussion</title><sec id="sec2dot1-molecules-25-00044"><title>2.1. Datasets Sizes and Overlaps</title><p><xref rid="molecules-25-00044-t001" ref-type="table">Table 1</xref> reports the different datasets used in this study. The smallest dataset is solubility measured from powder material, while the largest is logD at acidic pH. <xref ref-type="fig" rid="molecules-25-00044-f001">Figure 1</xref> reports the pairwise correlations between datasets using shared compounds between endpoints. As expected, all solubility endpoints are correlated between them. LogD in acid and neutral pH are also correlated. Human serum albumin binding (LOH) is correlated with solubility, while logD is anticorrelated with solubility. Melting point (LMP) is not very strongly correlated with any other endpoints in this study. Value distributions of the ten endpoints can be found in the <xref ref-type="app" rid="app1-molecules-25-00044">Supplementary Materials Figure S2</xref>. In total, the datasets together contain 537,443 unique compounds, of which about 79% occur only in one endpoint, 11% are shared between two endpoints, 9% are shared between three. and 1% between four or more. The pairs of endpoints with most overlapping compounds are membrane affinity (LOM) with human serum albumin binding (LOH), LOM with the solubility without assay annotation (LOQ), LOM with the nephelometric solubility assay (LON), logD (LOD) with DMSO solubility (LOO), LON with LOH, and LOQ with LOH.</p></sec><sec id="sec2dot2-molecules-25-00044"><title>2.2. Performance of Single Task Models</title><p>Three types of single task models were built: Random Forest (RF) and fully-connected, feed-forward neural networks (STNN), as well as graph convolutional networks (GCNN). Both RF and STNN are built upon circular fingerprints, while graph convolutional networks learn their feature representation in an end-to-end fashion, starting from the molecular graph and 75 simple atomic descriptors as initial node features.</p><p><xref rid="molecules-25-00044-t002" ref-type="table">Table 2</xref> shows the leave-cluster-out cross-validation performance for the different endpoints in different modeling situations. The first three columns correspond to the single task case, where a model is built for each endpoint independently from the others. For all tasks except melting point and solubility from powder, fully connected neural networks greatly outperform Random Forest. On average, R<sup>2</sup> is improved by 0.06 and Spearman&#x02019;s rho is improved by 0.05 in the cluster cross-validation setting. These improvements are in line with previous observations [<xref rid="B16-molecules-25-00044" ref-type="bibr">16</xref>,<xref rid="B19-molecules-25-00044" ref-type="bibr">19</xref>]. The endpoints that are best modeled by the STNN are the two logD, membrane affinity and solubility from nephelometry. These are large tasks (between 64,000 and 230,000 datapoints, <xref rid="molecules-25-00044-t001" ref-type="table">Table 1</xref>). Solubility from powder and from DMSO not fully dissolved and melting point are the less well predicted endpoints. The two solubility endpoints have the least data and the low performance in a cluster split setting can be explained by overfitting, but melting point is actually one of the largest tasks with 90,000 training examples, so the reason for the poorer performance might have to be found somewhere else. Melting point is notoriously difficult to predict [<xref rid="B27-molecules-25-00044" ref-type="bibr">27</xref>] even though the experimental data is very accurate.</p><p>Switching from a fixed compound representation (circular fingerprints) to learnt features (graph convolutional networks) allows us to further gain predictive performance in many cases. On average, R<sup>2</sup> is improved by 0.06 (over STNN) and by 0.12 (over RF) while Spearman&#x02019;s rho is improved by 0.05 (over STNN) and by 0.29 (over RF) in the cluster cross-validation setting. The only task for which graph convolutional features seem to be detrimental is solubility from powder (LOP) which shows a drop in R<sup>2</sup>. In random split cross-validation though, the performance of the graph convolutional network for LOP is on par with the ones of RF and STNN (see <xref ref-type="app" rid="app1-molecules-25-00044">Supplementary information Table S2</xref>). We, therefore, assume that the training of graph convolutional networks tends to overfit on smaller training sets. This would explain why the performance in random split appears high (compounds in the test splits are likely similar to compounds in the training set, so the learnt features work well also for the test data) while the performance in cluster splits drops significantly (cluster split cross-validation shows performance in chemical areas that are far away from the training set, where the learnt overfitted features generalize poorly).</p><p>It is worth mentioning that intensive hyperparameter selection was not necessary in our case. For Random Forest with ECFC6 fingerprints, we used the default settings from Pipeline Pilot, which is nowadays one of the go-to method for QSAR models in computational molecular design at Bayer. For STNN, a few pyramidal architectures were tested, dropout and input noise were included for controlling overfitting, and the details of batch size, learning rate, etc. were tuned on a cross-validation split for the task melting point only and applied to all other endpoints. As can be seen from <xref rid="molecules-25-00044-t002" ref-type="table">Table 2</xref>, those parameters seem to perform well on the other ADMET datasets tested, which is something already observed by Ma and colleagues [<xref rid="B17-molecules-25-00044" ref-type="bibr">17</xref>].</p><p>Note that our final settings follow the guidance provided by the authors: most of our endpoints are log-transformed, we use 4 hidden layers of decreasing sizes with decreasing amounts of dropout and ReLU as activation function. Two main differences are our usage of input noise followed by a tanh transformation to counteract the fact that dropout at input is not appropriate for our sparse fingerprint data and the choice of a bias of &#x02013;1 for the output layer which we found experimentally to improve performance. In another study, Zhou et al. evaluated different parameters and architectures for single task models for industrial ADME endpoints and found that a pyramidal architecture, dropout and weight decay were beneficial, that models built with ReLU were less sensitive to other hyperparameters, and that regression tasks require smaller learning rates than classification tasks [<xref rid="B28-molecules-25-00044" ref-type="bibr">28</xref>]. The graph convolutional STNN settings were also taken as recommended in DeepChem and not further optimized due to the lengthy training process, but the good performance of the trained models shows here again a practical robustness to adjustable parameters.</p></sec><sec id="sec2dot3-molecules-25-00044"><title>2.3. Performance in Multitask Setting</title><p>Since many of the endpoints of interest have some biological relations and actual correlations (<xref ref-type="fig" rid="molecules-25-00044-f001">Figure 1</xref>), we hypothesized that learning all the tasks together would bring further performance improvement. Indeed, by learning simultaneously several tasks, the model has to learn feature representations that are useful for all tasks (regularization aspect) and smaller tasks will benefit from the chemical space coverage of the larger tasks.</p><p>We built fully-connected multitask networks and graph convolutional multitask networks (<xref rid="molecules-25-00044-t002" ref-type="table">Table 2</xref>, last two columns). In the fully connected version (MTNN), the task that sees most improvement is the small solubility from powder endpoint (LOP, 0.29 increase in R<sup>2</sup> and 0.16 increase in Spearman&#x02019;s rho). Most of the larger tasks are either not affected or show poorer performance in multitask than in single task approach.</p><p>This confirms previous observations that larger tasks are negatively affected by joint training [<xref rid="B17-molecules-25-00044" ref-type="bibr">17</xref>]. On the other hand, all solubility endpoints get better predicted. This can be explained by the high correlation between the different solubility assays (<xref ref-type="fig" rid="molecules-25-00044-f001">Figure 1</xref>). The best MTNN model used balanced task weighting when calculating the loss, meaning that tasks with large amount of training data would see their loss down weighted with respect to less represented tasks. One consequence is that the model is allowed to make more errors in the larger tasks, a phenomenon that can be seen when looking at the performance of our largest endpoint, logD in acidic pH (LOA). This could explain the lower performance in MTNN for this particular task (0.08 decrease in R<sup>2</sup> and 0.03 decrease in Spearman&#x02019;s rho).</p><p>We saw in the single task approach that graph convolutional networks showed higher performance than fully-connected networks, and the same is true in the multitask learning approach: on average, R<sup>2</sup> increased by 0.17 with respect to the non-convolutional network and Spearman&#x02019;s rho by 0.09. Comparing the single task with the multitask convolutional networks leads to similar observations as when comparing STNN with MTNN in the non-convolutional setting. The average improvement in performance is 0.14 in R<sup>2</sup> and 0.06 in Spearman&#x02019;s rho, but the endpoint-by-endpoint picture is more nuanced. Endpoints like logD (LOD) or melting point (LMP) show no change in performance, while the acidic logD (LOA, our largest task) is negatively impacted in the multitask setting. The tasks benefitting the most from the joint training are the two smaller solubility endpoints (solubility from powder LOP and solubility from DMSO not fully dissolved, LOX). We also notice that the standard deviations of both reported metrics are the smallest for the multitask graph convolutional model, meaning that learning is very stable even across potentially very different cross-validation folds (we report in <xref ref-type="app" rid="app1-molecules-25-00044">supplementary Table S1</xref> the standard deviations of the cluster cross-validation results, which in practice contains folds of unequal sizes and difficulty).</p></sec><sec id="sec2dot4-molecules-25-00044"><title>2.4. Effect of Helper Tasks</title><p>Not all endpoints under consideration here are of interest to medicinal chemists, our end users. The nephelometric assay (LON) is not in use anymore and, therefore, the training set contains only historical data. The solubility from DMSO not fully dissolved (LOX) probably contains a lot of artefacts. And finally, the other solubility assay where no assay information was recorded (LOQ) also contains mostly historical data (see <xref rid="molecules-25-00044-t001" ref-type="table">Table 1</xref>). It means that the actual performance of the models on those three datasets is of little importance, but we included them for completeness and because, in joint training, they might help train the other solubility endpoints (LOO and LOP). We compared the effect of including or not those three helper tasks into the multitask graph convolutional model.</p><p>From <xref rid="molecules-25-00044-t003" ref-type="table">Table 3</xref>, one observes that the endpoints&#x02019; performances stay stable without the helper tasks. The two solubility endpoints, which we would assume to benefit most from the helper tasks (recall that these are different solubility assays), show indeed a slightly lower performance in the absence of helper tasks (&#x02212;0.02 R<sup>2</sup> for the DMSO solubility and &#x02212;0.02 Spearman&#x02019;s rho for the powder solubility). We deduce that adding the helper tasks is not detrimental to the proper learning of the model but will help reaching more accurate predictions in solubility. Beyond considerations on the performance level, one can also argue that adding more related endpoints will also enrich the chemical space covered by the training set, helping the graph convolutions to learn meaningful atom representations and increasing the generalization capability of the model.</p></sec><sec id="sec2dot5-molecules-25-00044"><title>2.5. General Solubility Equation</title><p>The aqueous solubility of a small molecule is linked to its melting point and octanol-water partition coefficient by the general solubility equation (GSE) [<xref rid="B29-molecules-25-00044" ref-type="bibr">29</xref>]:<disp-formula id="FD1-molecules-25-00044"><label>(1)</label><mml:math id="mm1"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>logS</mml:mi></mml:mrow><mml:mi mathvariant="normal">w</mml:mi></mml:msub><mml:mtext>&#x000a0;</mml:mtext><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mn>0.01</mml:mn><mml:mtext>&#x000a0;</mml:mtext><mml:mo>&#x000d7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>LMP</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>25</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">K</mml:mi><mml:mrow><mml:mi>ow</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></disp-formula>
where logS<sub>w</sub> is the logarithm of base 10 of the aqueous solubility in mol/L, LMP is the melting point in Celsius degrees, and K<sub>ow</sub> is the partition coefficient.</p><p>We applied this formula to our dataset. In total, 105 compounds had measurements for all three endpoints LOO, LMP, and LOD. The Pearson correlation between the predicted logS<sub>w</sub> using the GSE with the original 105 LOO data points is 0.75.</p><p>We compared this correlation with the one obtained on the cluster split test set by our multitask graph convolutional network. On almost 4000 LOO data points not seen by the model (and in a different chemical space than the training set), the Pearson correlation coefficient between predictions and measurements is 0.81. We also used the model to predict melting point and LogD for these 4000 test datapoints and see whether the model predictions also follow the GSE. For this, we used the predictions of the model for melting point and LogD, obtained the aqueous solubilities according to the Yalkowsky equation and compared these with the predicted LOO. The Pearson correlation is here 0.83, meaning that our model follows globally the GSE model of aqueous solubility without actually being trained on that constraint. Correlations plots can be found in <xref ref-type="app" rid="app1-molecules-25-00044">Supplementary Materials Figure S3</xref>. We saw from the endpoints correlation matrix (<xref ref-type="fig" rid="molecules-25-00044-f001">Figure 1</xref>) that LogD is clearly anti-correlated with solubility. To check that the GSE property of our network is not simply due to the correlation of logD and solubility in the training data, we also computed the Pearson correlation between the predicted &#x02212;logD and the predicted solubility for the 4000 test datapoints: this correlation is 0.71, a clear drop in magnitude with respect to the correlation when taking into account both the predicted logD and melting point and following the GSE formula.</p></sec><sec id="sec2dot6-molecules-25-00044"><title>2.6. Performance in Time Splits</title><p>All the results previously commented were obtained by clustering the compounds by structure, then validating the models on left-out clusters of compounds (leave-cluster-out cross-validation). This type of validation shows how well the model generalizes and performs on unseen chemical space. Another way to evaluate models in an industrial setting is to apply time splits. In this approach, all measured data up to a given date are used for training while all recent data are used as a separate test set. We retrained our MTNN graph convolutional model on such a historical subset of our assays, and used all data measured after June 2014 as test set. <xref rid="molecules-25-00044-t004" ref-type="table">Table 4</xref> reports the obtained performance on the test sets. Note that, for LOA, no test date could be retrieved so the split is random.</p><p>The number of data points for each endpoint vary, as some assays are not often used anymore (melting point, membrane affinity) while others are intensively requested in the course of ongoing drug discovery projects. In terms of performance, we observe slightly lower values in the time split than in the average of the leave-cluster-out cross-validations (R<sup>2</sup> dropped by 0.06 and Spearman&#x02019;s rho by 0.03). Still, the performance of our multitask model is solid also in this prospective type of validation. We added the root mean squared error (RMSE) in addition to the usually reported R<sup>2</sup> and Spearman coefficients for the reader to have a better idea of the typical errors the model is making in time splits. We note that even in such a difficult setting, the model manages an error below one log unit for the two solubility assays of interest, and below half a log unit for the logD predictions. A similar table in the <xref ref-type="app" rid="app1-molecules-25-00044">supplementary information</xref> shows the results for the <italic>strict time splits</italic> (see <xref ref-type="sec" rid="sec3-molecules-25-00044">Section 3</xref> for more details), where a compound measured in several endpoints can only occur either in training or in test for all endpoints (<xref ref-type="app" rid="app1-molecules-25-00044">Table S3</xref>). From the relatively robust performance in leave-cluster-out and prospective time split validation, we conclude that a weekly retraining of the model to aggregate newly measured data points is not mandatory in the production phase of the model.</p></sec></sec><sec id="sec3-molecules-25-00044"><title>3. Materials and Methods</title><sec id="sec3dot1-molecules-25-00044"><title>3.1. Dataset</title><p>In this work, we collected in-house data for the following ADMET endpoints: logD in neutral and acidic pH, solubility (various assay settings), melting point, membrane affinity, and human serum albumin binding (<xref rid="molecules-25-00044-t001" ref-type="table">Table 1</xref>).</p><p>Biological data corresponding to a given assay was preprocessed the following way: when the same compound is measured more than once for that assay, then the average of the measurements is taken as final experimental value. In case a measurement is preceded by an unequal sign (&#x0003c;10 &#x000b5;M) for example), we report either the double of the value (in case the qualifier is &#x02018;&#x0003e;&#x02019;) or half of the value (in case the qualifier is &#x02018;&#x0003c;&#x02019;). For human serum albumin binding and membrane affinity, the log<sub>10</sub> of the experimental value is taken. For melting point and logD, the values are taken as reported in the experiment. For solubility, the reported value in mg/L is first transformed to mol/L then a log<sub>10</sub> transformation is applied.</p><p>For the chemical data, we used the Standardize Molecule tool from Pipeline Pilot (Dassault Syst&#x000e8;mes BIOVIA, San Diego, CA, USA), selecting &#x0201c;Standardize Charges&#x0201d;, &#x0201c;Keep largest fragment&#x0201d; and &#x0201c;Clear stereo&#x0201d;. Canonical tautomers are generated, then molecules are standardized as neutral by deprotonating bases and protonating acids. <xref ref-type="fig" rid="molecules-25-00044-f002">Figure 2</xref> shows the distribution of molecular properties in the aggregated dataset containing 537,443 unique compounds.</p></sec><sec id="sec3dot2-molecules-25-00044"><title>3.2. Model Validation</title><sec id="sec3dot2dot1-molecules-25-00044"><title>3.2.1. Data Splits</title><p>Models were evaluated in both a cross-validation and a separate test set fashion. Different splitting strategies were applied. We clustered the compounds of the combined datasets using the k-means algorithm (K = 10) and different versions of the ECFC6 fingerprints. Clusters not containing compounds of every task were merged into larger clusters. One cluster was chosen as a test set while the others served as the different folds for the cross-validation set up. Random splits were also performed where compounds would be assigned to a fold randomly, but keeping the folds of the same sizes as those obtained by the clustering procedure and ensuring that each fold contains representatives from each task.</p><p>Time splits could not be performed in a cross-validation fashion because we would have needed to find up to 10 measurement dates for which each endpoint would have enough data measured. Instead, we used one temporal split to separate training from test sets. We distinguish two different types of time splits: one where a measurement date is fixed and for each task independently, later measurements are taken as test sets while earlier measurements belong to the training set. This is later referred to as <italic>taskwise time split</italic> as it ignores the fact that a compound measured earlier in one assay might be measured later in another. We also propose a <italic>strict time split</italic> where the training sets are further filtered to remove any compound that would occur in the test set of another task. For one task, logD at acidic pH (LOA), no test dates were available, and compounds were split randomly even in the time split settings.</p></sec><sec id="sec3dot2dot2-molecules-25-00044"><title>3.2.2. Performance Measures</title><p>The models predict continuous values. The performance of such regression models is evaluated by the coefficient of determination <italic>r<sup>2</sup></italic> (which measures the concordance between predicted and experimental values) and the Spearman correlation coefficient <italic>rho</italic> (which measures the ranking capabilities of the models). In the case of cross-validation, the individual fold performances are averaged and reported.</p></sec></sec><sec id="sec3dot3-molecules-25-00044"><title>3.3. Machine Learning Models</title><sec id="sec3dot3dot1-molecules-25-00044"><title>3.3.1. Random Forest</title><p>Single task models were built using Random Forest regression as implemented in Pipeline Pilot v.18.1 (Dassault Syst&#x000e8;mes BIOVIA, San Diego, CA, USA). The input features are extended connectivity fingerprint counts of diameter 6 (hereafter referred to as ECFC6) folded to 1024 or 2048 [<xref rid="B20-molecules-25-00044" ref-type="bibr">20</xref>].</p></sec><sec id="sec3dot3dot2-molecules-25-00044"><title>3.3.2. Fully-Connected Single Task Network</title><p>For the fully connected neural networks, we used a pyramidal architecture with 4 hidden layers (of dimensions 2000, 1000, 500 and 100 respectively) and as input features we used ECFC6 fingerprint counts folded to 1024 or 2048. The activation function used in the hidden units was ReLU, following the observation from Zhou et al. that ReLU seems superior to sigmoid for regression tasks [<xref rid="B28-molecules-25-00044" ref-type="bibr">28</xref>]. A decreasing amount of dropout was applied to each layer (50% in the first two hidden layers, 25% in the next, and none in the last hidden layer). Weights were initialized using He&#x02019;s method [<xref rid="B30-molecules-25-00044" ref-type="bibr">30</xref>]. Biases in the hidden layers were initialized to 0 and to &#x02212;1 for the output layer.</p><p>Because fingerprints are typically sparse, using dropout on the input features would not have a lot of effect during training. Input noise is used instead to effectively randomly &#x0201c;drop in&#x0201d; chemical features at training time. For this, we generate positive integers by rounding and taking the absolute value from samples of a normal distribution with zero mean and standard deviation of 3 to mimic fingerprint counts. Then, the real inputs are replaced by these noisy fingerprints with a probability p (in our experiments, <italic>p</italic> = 0.01 or <italic>p</italic> = 0.02 worked best). To smoothen out the inputs, we apply the hyperbolic tangent (<italic>tanh</italic>) function directly after the input noise step (<xref ref-type="fig" rid="molecules-25-00044-f003">Figure 3</xref>). The mean squared error is used as a loss function. The models were implemented in Tensorflow version 1.2.1. Hyperparameters, such as hidden layer dimensions, learning rate, weight decay, learning rate scheduling, etc., were optimized on the melting point endpoint and then applied to all other tasks.</p></sec><sec id="sec3dot3dot3-molecules-25-00044"><title>3.3.3. Fully-Connected Multitask Network</title><p>For the multitask version, the same architecture (four hidden layers of dimensions 2000, 1000, 500, and 100, respectively; see <xref ref-type="fig" rid="molecules-25-00044-f003">Figure 3</xref>) as for the single task networks was used. The input noise probability was reduced to <italic>p</italic> = 0.01 and the learning rate, batch size, learning rate scheduling, number of epochs and weight decay were adjusted using cross-validation in the training set.</p><p>To learn in a multitask fashion, the loss function corresponds to a weighted average of individual tasks&#x02019; mean squared errors. This means that endpoints with different output ranges (for example, melting points in Celsius degrees, and can reach over 200) could potentially participate differently in the global loss. To avoid this problem, we scale each endpoint values to zero mean and unit standard deviation using standard scaling. Due to the unequal sizes of the different tasks, we explore different ways of averaging the individual task losses: in the &#x0201c;simple&#x0201d; setting, each task receives a weight of 1/N with N being the number of tasks represented in the training minibatch. In the &#x0201c;balanced&#x0201d; setting, tasks with fewer examples are proportionally upweighted compared to tasks with more examples in the minibatch. Missing values (input examples without label for some of the tasks) are ignored and do not participate in the task&#x02019;s individual losses. If a task does not have any training example in a minibatch then it is ignored and does not participate in the overall loss.</p></sec><sec id="sec3dot3dot4-molecules-25-00044"><title>3.3.4. Graph Convolutions</title><p>Graph convolutional networks learn node features by propagating features from neighboring nodes and learning affine transformations that will help for the task at hand [<xref rid="B23-molecules-25-00044" ref-type="bibr">23</xref>]. In case of molecules, the nodes are the atoms and the edges are the bonds of the molecular graph. A training example is a whole graph and the task is a classification or regression at the graph level. Here, we use the implementation of the Duvenaud algorithm [<xref rid="B24-molecules-25-00044" ref-type="bibr">24</xref>] in DeepChem v.1.2.1 [<xref rid="B31-molecules-25-00044" ref-type="bibr">31</xref>]. We keep the architecture and hyperparameters suggested by the authors for ADMET predictions, namely:
<list list-type="simple"><list-item><label>-</label><p>75 input atomic features (see <xref ref-type="app" rid="app1-molecules-25-00044">Figure S1</xref> for details);</p></list-item><list-item><label>-</label><p>two graph convolution steps with a feature dimension of 128 each, with ReLU activation functions; and</p></list-item><list-item><label>-</label><p>a dense layer with 256 units and ReLU activation functions.</p></list-item></list></p><p>These operations lead to learned continuous atomic features of dimension 256. To make a prediction at the molecule level, the individual atom features have to be aggregated. For this, the feature values are averaged across atoms (mean feature) and the maximum value across atoms is also taken (max feature). These two representations are concatenated and a tanh activation function is applied to give rise to a final molecule representation of size 512.</p><p>The learning rate was set to 0.001, and a batch size of 128 was used. The models were trained for 40 epochs. Adam optimization was performed with learning rate decay every 1000 steps.</p><p>The same architecture and hyperparameters were used in the multitask setting. Endpoint values were standardized with zero mean and unit standard deviation like in the fully-connected multitask counterpart. The loss is a task-weighted MSE.</p></sec></sec></sec><sec sec-type="conclusions" id="sec4-molecules-25-00044"><title>4. Conclusions</title><p>In this work, we built a predictive model for seven ADMET assays corresponding to endpoints of high interest: logD, solubility, melting point, membrane affinity, and human serum albumin binding. Combining all the data available, we were able to apply deep learning methods to learn to predict these endpoints. We showed that, as previously observed, neural networks generally outperform Random Forest in the case of large physicochemical datasets, and that joint training approaches bring further performance improvements at least for the smaller endpoints. Moving away from classical compound representations, we showed that graph convolutional networks are a very powerful method that seems particularly suited for more &#x0201c;physico-chemical&#x0201d; endpoints. The best model, a multitask graph convolutional model with three additional helper tasks, showed very robust performance both in cluster splits and temporal splits. This does not mean that ADMET modeling is a solved problem, since in our experience graph convolutional approaches did not work as well for more complex endpoints like Caco2 permeation or in vitro metabolic stability (validations not shown). Also, multitask modeling is still pretty much a trial-and-error type of work, where it is not clear beforehand which tasks should be combined together nor which hyperparameters would work for a particular task combination. One interesting road to explore would be to extract the learned molecular representation from the last fully-connected layer of the multitask graph convolutional network and use it to try and predict other endpoints in a kind of transfer learning approach. Since our model is trained at predicting general physicochemical properties of small molecules, we can assume that this representation will be useful to predict more complicated endpoints linked to toxicity, environmental safety or target binding.</p></sec></body><back><ack><title>Acknowledgments</title><p>D.-A.C. and F.M. acknowledge funding from the Bayer AG Life Science Collaboration (&#x0201c;DeepMinDS&#x0201d;).</p></ack><app-group><app id="app1-molecules-25-00044"><title>Supplementary Materials</title><supplementary-material content-type="local-data" id="molecules-25-00044-s001"><media xlink:href="molecules-25-00044-s001.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><p>The following are available online at <uri xlink:href="https://www.mdpi.com/1420-3049/25/1/44/s1">https://www.mdpi.com/1420-3049/25/1/44/s1</uri>, Figure S1: Input atomic features for the graph convolutional models, Figure S2: Distribution of experimental values for the ADMET endpoints of interest, Table S1: Standard deviations of cluster split cross-validation folds not used for parameter tuning (complementary to <xref rid="molecules-25-00044-t002" ref-type="table">Table 2</xref>), Table S2: Performance of the different models in random split cross-validation, Table S3: Performance of the multitask graph convolutional model in the strict time split test set, Figure S3: Correlations between solubility in the data, solubility as deduced from the General Solubility Equation (GSE) and solubility predicted by the model. The code to train multitask graph convolutional networks is available on github: <uri xlink:href="https://github.com/fmonta/mtnngc_admet">https://github.com/fmonta/mtnngc_admet</uri>.</p></app></app-group><notes><title>Author Contributions</title><p>Conceptualization: F.M., A.T.L., and D.-A.C.; methodology: F.M., L.K., A.T.L., and D.-A.C.; software: F.M.; validation: F.M. and L.K.; formal analysis: F.M.; investigation: F.M.; resources: D.-A.C.; data curation: A.T.L.; writing&#x02014;original draft preparation: F.M.; writing&#x02014;review and editing: F.M., A.T.L., L.K., and D.-A.C.; visualization: F.M.; supervision: D.-A.C. and A.T.L.; project administration: L.K.; funding acquisition: D.-A.C. All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Funding</title><p>This research received no external funding.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results.</p></notes><ref-list><title>References</title><ref id="B1-molecules-25-00044"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waring</surname><given-names>M.J.</given-names></name><name><surname>Arrowsmith</surname><given-names>J.</given-names></name><name><surname>Leach</surname><given-names>A.R.</given-names></name><name><surname>Leeson</surname><given-names>P.D.</given-names></name><name><surname>Mandrell</surname><given-names>S.</given-names></name><name><surname>Owen</surname><given-names>R.M.</given-names></name><name><surname>Pairaudeau</surname><given-names>G.</given-names></name><name><surname>Pennie</surname><given-names>W.D.</given-names></name><name><surname>Pickett</surname><given-names>S.D.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><etal/></person-group><article-title>An analysis of the attrition of drug candidates from four major pharmaceutical companies</article-title><source>Nat. Rev. Drug Discov.</source><year>2015</year><volume>14</volume><fpage>475</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1038/nrd4609</pub-id><?supplied-pmid 26091267?><pub-id pub-id-type="pmid">26091267</pub-id></element-citation></ref><ref id="B2-molecules-25-00044"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gleeson</surname><given-names>M.P.</given-names></name><name><surname>Hersey</surname><given-names>A.</given-names></name><name><surname>Montanari</surname><given-names>D.</given-names></name><name><surname>Overington</surname><given-names>J.</given-names></name></person-group><article-title>Probing the links between in vitro potency, ADMET and physicochemical parameters</article-title><source>Nat. Rev. Drug Discov.</source><year>2011</year><volume>10</volume><fpage>197</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1038/nrd3367</pub-id><?supplied-pmid 21358739?><pub-id pub-id-type="pmid">21358739</pub-id></element-citation></ref><ref id="B3-molecules-25-00044"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zang</surname><given-names>Q.</given-names></name><name><surname>Mansouri</surname><given-names>K.</given-names></name><name><surname>Williams</surname><given-names>A.J.</given-names></name><name><surname>Judson</surname><given-names>R.S.</given-names></name><name><surname>Allen</surname><given-names>D.G.</given-names></name><name><surname>Casey</surname><given-names>W.M.</given-names></name><name><surname>Kleinstreuer</surname><given-names>N.C.</given-names></name></person-group><article-title>In Silico Prediction of Physicochemical Properties of Environmental Chemicals Using Molecular Fingerprints and Machine Learning</article-title><source>J. Chem. Inf. Model.</source><year>2017</year><volume>57</volume><fpage>36</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.6b00625</pub-id><?supplied-pmid 28006899?><pub-id pub-id-type="pmid">28006899</pub-id></element-citation></ref><ref id="B4-molecules-25-00044"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watkins</surname><given-names>M.</given-names></name><name><surname>Sizochenko</surname><given-names>N.</given-names></name><name><surname>Rasulev</surname><given-names>B.</given-names></name><name><surname>Leszczynski</surname><given-names>J.</given-names></name></person-group><article-title>Estimation of melting points of large set of persistent organic pollutants utilizing QSPR approach</article-title><source>J. Mol. Model.</source><year>2016</year><volume>22</volume><fpage>55</fpage><pub-id pub-id-type="doi">10.1007/s00894-016-2917-0</pub-id><?supplied-pmid 26874948?><pub-id pub-id-type="pmid">26874948</pub-id></element-citation></ref><ref id="B5-molecules-25-00044"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetko</surname><given-names>I.V.</given-names></name><name><surname>Lowe</surname><given-names>D.M.</given-names></name><name><surname>Williams</surname><given-names>A.J.</given-names></name></person-group><article-title>The development of models to predict melting and pyrolysis point data associated with several hundred thousand compounds mined from PATENTS</article-title><source>J. Cheminform.</source><year>2016</year><volume>8</volume><fpage>2</fpage><pub-id pub-id-type="doi">10.1186/s13321-016-0113-y</pub-id><pub-id pub-id-type="pmid">26807157</pub-id></element-citation></ref><ref id="B6-molecules-25-00044"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhhatarai</surname><given-names>B.</given-names></name><name><surname>Teetz</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>T.</given-names></name><name><surname>&#x000d6;berg</surname><given-names>T.</given-names></name><name><surname>Jeliazkova</surname><given-names>N.</given-names></name><name><surname>Kochev</surname><given-names>N.</given-names></name><name><surname>Pukalov</surname><given-names>O.</given-names></name><name><surname>Tetko</surname><given-names>I.V.</given-names></name><name><surname>Kovarich</surname><given-names>S.</given-names></name><name><surname>Papa</surname><given-names>E.</given-names></name><etal/></person-group><article-title>CADASTER QSPR Models for Predictions of Melting and Boiling Points of Perfluorinated Chemicals</article-title><source>Mol. Inform.</source><year>2011</year><volume>30</volume><fpage>189</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1002/minf.201000133</pub-id><pub-id pub-id-type="pmid">27466773</pub-id></element-citation></ref><ref id="B7-molecules-25-00044"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghafourian</surname><given-names>T.</given-names></name><name><surname>Amin</surname><given-names>Z.</given-names></name></person-group><article-title>QSAR models for the prediction of plasma protein binding</article-title><source>Bioimpacts</source><year>2013</year><volume>3</volume><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="pmid">23678466</pub-id></element-citation></ref><ref id="B8-molecules-25-00044"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>T.</given-names></name><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Bryant</surname><given-names>S.H.</given-names></name></person-group><article-title>Binary Classification of Aqueous Solubility Using Support Vector Machines with Reduction and Recombination Feature Selection</article-title><source>J. Chem. Inf. Model.</source><year>2011</year><volume>51</volume><fpage>229</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1021/ci100364a</pub-id><pub-id pub-id-type="pmid">21214224</pub-id></element-citation></ref><ref id="B9-molecules-25-00044"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fioressi</surname><given-names>S.E.</given-names></name><name><surname>Bacelo</surname><given-names>D.E.</given-names></name><name><surname>Rojas</surname><given-names>C.</given-names></name><name><surname>Aranda</surname><given-names>J.F.</given-names></name><name><surname>Duchowicz</surname><given-names>P.R.</given-names></name></person-group><article-title>Conformation-independent quantitative structure-property relationships study on water solubility of pesticides</article-title><source>Ecotoxicol. Environ. Saf.</source><year>2019</year><volume>171</volume><fpage>47</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.ecoenv.2018.12.056</pub-id><pub-id pub-id-type="pmid">30594756</pub-id></element-citation></ref><ref id="B10-molecules-25-00044"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>H.</given-names></name><name><surname>Shah</surname><given-names>P.</given-names></name><name><surname>Nguyen</surname><given-names>K.</given-names></name><name><surname>Yu</surname><given-names>K.R.</given-names></name><name><surname>Kerns</surname><given-names>E.</given-names></name><name><surname>Kabir</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name></person-group><article-title>Predictive models of aqueous solubility of organic compounds built on A large dataset of high integrity</article-title><source>Bioorg. Med. Chem.</source><year>2019</year><volume>27</volume><fpage>3110</fpage><lpage>3114</lpage><pub-id pub-id-type="doi">10.1016/j.bmc.2019.05.037</pub-id><pub-id pub-id-type="pmid">31176566</pub-id></element-citation></ref><ref id="B11-molecules-25-00044"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergstr&#x000f6;m</surname><given-names>C.A.S.</given-names></name><name><surname>Larsson</surname><given-names>P.</given-names></name></person-group><article-title>Computational prediction of drug solubility in water-based systems: Qualitative and quantitative approaches used in the current drug discovery and development setting</article-title><source>Int. J. Pharm.</source><year>2018</year><volume>540</volume><fpage>185</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.ijpharm.2018.01.044</pub-id><?supplied-pmid 29421301?><pub-id pub-id-type="pmid">29421301</pub-id></element-citation></ref><ref id="B12-molecules-25-00044"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nigsch</surname><given-names>F.</given-names></name><name><surname>Bender</surname><given-names>A.</given-names></name><name><surname>van Buuren</surname><given-names>B.</given-names></name><name><surname>Tissen</surname><given-names>J.</given-names></name><name><surname>Nigsch</surname><given-names>E.</given-names></name><name><surname>Mitchell</surname><given-names>J.B.O.</given-names></name></person-group><article-title>Melting Point Prediction Employing k-Nearest Neighbor Algorithms and Genetic Parameter Optimization</article-title><source>J. Chem. Inf. Model.</source><year>2006</year><volume>46</volume><fpage>2412</fpage><lpage>2422</lpage><pub-id pub-id-type="doi">10.1021/ci060149f</pub-id><?supplied-pmid 17125183?><pub-id pub-id-type="pmid">17125183</pub-id></element-citation></ref><ref id="B13-molecules-25-00044"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chinta</surname><given-names>S.</given-names></name><name><surname>Rengaswamy</surname><given-names>R.</given-names></name></person-group><article-title>Machine Learning Derived Quantitative Structure Property Relationship (QSPR) to Predict Drug Solubility in Binary Solvent Systems</article-title><source>Ind. Eng. Chem. Res.</source><year>2019</year><volume>58</volume><fpage>3082</fpage><lpage>3092</lpage><pub-id pub-id-type="doi">10.1021/acs.iecr.8b04584</pub-id></element-citation></ref><ref id="B14-molecules-25-00044"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kratochwil</surname><given-names>N.A.</given-names></name><name><surname>Huber</surname><given-names>W.</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>F.</given-names></name><name><surname>Kansy</surname><given-names>M.</given-names></name><name><surname>Gerber</surname><given-names>P.R.</given-names></name></person-group><article-title>Predicting plasma protein binding of drugs: A new approach</article-title><source>Biochem. Pharmacol.</source><year>2002</year><volume>64</volume><fpage>1355</fpage><lpage>1374</lpage><pub-id pub-id-type="doi">10.1016/S0006-2952(02)01074-2</pub-id><pub-id pub-id-type="pmid">12392818</pub-id></element-citation></ref><ref id="B15-molecules-25-00044"><label>15.</label><element-citation publication-type="web"><article-title>Merck Molecular Activity Challenge | Kaggle</article-title><comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/MerckActivity">https://www.kaggle.com/c/MerckActivity</ext-link></comment><date-in-citation content-type="access-date" iso-8601-date="2019-12-20">(accessed on 20 December 2019)</date-in-citation></element-citation></ref><ref id="B16-molecules-25-00044"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>G.E.</given-names></name><name><surname>Jaitly</surname><given-names>N.</given-names></name><name><surname>Salakhutdinov</surname><given-names>R.</given-names></name></person-group><article-title>Multi-task Neural Networks for QSAR Predictions</article-title><source>arXiv</source><year>2014</year><pub-id pub-id-type="arxiv">1406.1231</pub-id></element-citation></ref><ref id="B17-molecules-25-00044"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J.</given-names></name><name><surname>Sheridan</surname><given-names>R.P.</given-names></name><name><surname>Liaw</surname><given-names>A.</given-names></name><name><surname>Dahl</surname><given-names>G.E.</given-names></name><name><surname>Svetnik</surname><given-names>V.</given-names></name></person-group><article-title>Deep Neural Nets as a Method for Quantitative Structure&#x02013;Activity Relationships</article-title><source>J. Chem. Inf. Model.</source><year>2015</year><volume>55</volume><fpage>263</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1021/ci500747n</pub-id><?supplied-pmid 25635324?><pub-id pub-id-type="pmid">25635324</pub-id></element-citation></ref><ref id="B18-molecules-25-00044"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>R.</given-names></name></person-group><article-title>Multitask Learning</article-title><source>Mach. Learn.</source><year>1997</year><volume>28</volume><fpage>41</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1023/A:1007379606734</pub-id></element-citation></ref><ref id="B19-molecules-25-00044"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kearnes</surname><given-names>S.</given-names></name><name><surname>Goldman</surname><given-names>B.</given-names></name><name><surname>Pande</surname><given-names>V.</given-names></name></person-group><article-title>Modeling Industrial ADMET Data with Multitask Networks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1606.08793</pub-id></element-citation></ref><ref id="B20-molecules-25-00044"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>D.</given-names></name><name><surname>Hahn</surname><given-names>M.</given-names></name></person-group><article-title>Extended-Connectivity Fingerprints</article-title><source>J. Chem. Inf. Model.</source><year>2010</year><volume>50</volume><fpage>742</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1021/ci100050t</pub-id><pub-id pub-id-type="pmid">20426451</pub-id></element-citation></ref><ref id="B21-molecules-25-00044"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruna</surname><given-names>J.</given-names></name><name><surname>Zaremba</surname><given-names>W.</given-names></name><name><surname>Szlam</surname><given-names>A.</given-names></name><name><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><article-title>Spectral Networks and Locally Connected Networks on Graphs</article-title><source>arXiv</source><fpage>2013</fpage><pub-id pub-id-type="arxiv">1312.6203</pub-id></element-citation></ref><ref id="B22-molecules-25-00044"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henaff</surname><given-names>M.</given-names></name><name><surname>Bruna</surname><given-names>J.</given-names></name><name><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><article-title>Deep Convolutional Networks on Graph-Structured Data</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1506.05163</pub-id></element-citation></ref><ref id="B23-molecules-25-00044"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kipf</surname><given-names>T.N.</given-names></name><name><surname>Welling</surname><given-names>M.</given-names></name></person-group><article-title>Semi-Supervised Classification with Graph Convolutional Networks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1609.02907</pub-id></element-citation></ref><ref id="B24-molecules-25-00044"><label>24.</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Duvenaud</surname><given-names>D.</given-names></name><name><surname>Maclaurin</surname><given-names>D.</given-names></name><name><surname>Aguilera-Iparraguirre</surname><given-names>J.</given-names></name><name><surname>G&#x000f3;mez-Bombarelli</surname><given-names>R.</given-names></name><name><surname>Hirzel</surname><given-names>T.</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name><name><surname>Adams</surname><given-names>R.P.</given-names></name></person-group><article-title>Convolutional Networks on Graphs for Learning Molecular Fingerprints</article-title><source>Proceedings of the Advances in Neural Information Processing Systems 28 (NIPS 2015)</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>7&#x02013;12 December 2015</conf-date></element-citation></ref><ref id="B25-molecules-25-00044"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>E.N.</given-names></name><name><surname>Sheridan</surname><given-names>R.</given-names></name><name><surname>Joshi</surname><given-names>E.</given-names></name><name><surname>Pande</surname><given-names>V.S.</given-names></name><name><surname>Cheng</surname><given-names>A.C.</given-names></name></person-group><article-title>Step Change Improvement in ADMET Prediction with PotentialNet Deep Featurization</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1903.11789</pub-id></element-citation></ref><ref id="B26-molecules-25-00044"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinberg</surname><given-names>E.N.</given-names></name><name><surname>Sur</surname><given-names>D.</given-names></name><name><surname>Wu</surname><given-names>Z.</given-names></name><name><surname>Husic</surname><given-names>B.E.</given-names></name><name><surname>Mai</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Sun</surname><given-names>S.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Ramsundar</surname><given-names>B.</given-names></name><name><surname>Pande</surname><given-names>V.S.</given-names></name></person-group><article-title>PotentialNet for Molecular Property Prediction</article-title><source>ACS Cent. Sci.</source><year>2018</year><volume>4</volume><fpage>1520</fpage><lpage>1530</lpage><pub-id pub-id-type="doi">10.1021/acscentsci.8b00507</pub-id><?supplied-pmid 30555904?><pub-id pub-id-type="pmid">30555904</pub-id></element-citation></ref><ref id="B27-molecules-25-00044"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>L.D.</given-names></name><name><surname>Palmer</surname><given-names>D.S.</given-names></name><name><surname>Nigsch</surname><given-names>F.</given-names></name><name><surname>Mitchell</surname><given-names>J.B.O.</given-names></name></person-group><article-title>Why Are Some Properties More Difficult To Predict than Others? A Study of QSPR Models of Solubility, Melting Point, and Log P</article-title><source>J. Chem. Inf. Model.</source><year>2008</year><volume>48</volume><fpage>220</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1021/ci700307p</pub-id><?supplied-pmid 18186622?><pub-id pub-id-type="pmid">18186622</pub-id></element-citation></ref><ref id="B28-molecules-25-00044"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Cahya</surname><given-names>S.</given-names></name><name><surname>Combs</surname><given-names>S.A.</given-names></name><name><surname>Nicolaou</surname><given-names>C.A.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Desai</surname><given-names>P.V.</given-names></name><name><surname>Shen</surname><given-names>J.</given-names></name></person-group><article-title>Exploring Tunable Hyperparameters for Deep Neural Networks with Industrial ADME Data Sets</article-title><source>J. Chem. Inf. Model.</source><year>2019</year><volume>59</volume><fpage>1005</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.8b00671</pub-id><pub-id pub-id-type="pmid">30586300</pub-id></element-citation></ref><ref id="B29-molecules-25-00044"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>N.</given-names></name><name><surname>Yalkowsky</surname><given-names>S.H.</given-names></name></person-group><article-title>Estimation of the aqueous solubility I: Application to organic nonelectrolytes</article-title><source>J. Pharm. Sci.</source><year>2001</year><volume>90</volume><fpage>234</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1002/1520-6017(200102)90:2&#x0003c;234::AID-JPS14&#x0003e;3.0.CO;2-V</pub-id><pub-id pub-id-type="pmid">11169540</pub-id></element-citation></ref><ref id="B30-molecules-25-00044"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="arxiv">1502.01852</pub-id></element-citation></ref><ref id="B31-molecules-25-00044"><label>31.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ramsundar</surname><given-names>B.</given-names></name><name><surname>Eastman</surname><given-names>P.</given-names></name><name><surname>Walters</surname><given-names>P.</given-names></name><name><surname>Pande</surname><given-names>V.</given-names></name><name><surname>Leswing</surname><given-names>K.</given-names></name><name><surname>Wu</surname><given-names>Z.</given-names></name></person-group><source>Deep Learning for the Life Sciences</source><publisher-name>O&#x02019;Reilly Media Inc</publisher-name><publisher-loc>Sebastopol, CA, USA</publisher-loc><year>2019</year></element-citation></ref></ref-list></back><floats-group><fig id="molecules-25-00044-f001" orientation="portrait" position="float"><label>Figure 1</label><caption><p>Pearson&#x02019;s correlation coefficients between pairs of endpoints. When less than 25 compounds were measured in both members of the pairs, no correlation is reported. Endpoint codes are listed in <xref rid="molecules-25-00044-t001" ref-type="table">Table 1</xref>.</p></caption><graphic xlink:href="molecules-25-00044-g001"/></fig><fig id="molecules-25-00044-f002" orientation="portrait" position="float"><label>Figure 2</label><caption><p>Distribution of molecular properties (number of rotatable bonds, number of aromatic rings, molecular weight, number of H bond acceptors, and the number of H bond donors) in the aggregated dataset containing 537,443 unique molecules tested in at least one of the endpoints of interest.</p></caption><graphic xlink:href="molecules-25-00044-g002"/></fig><fig id="molecules-25-00044-f003" orientation="portrait" position="float"><label>Figure 3</label><caption><p>Input feature preprocessing and architecture of the fully connected neural networks. When only one output unit exists, then we talk about single task neural networks (STNN).</p></caption><graphic xlink:href="molecules-25-00044-g003"/></fig><table-wrap id="molecules-25-00044-t001" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-25-00044-t001_Table 1</object-id><label>Table 1</label><caption><p>ADMET datasets used to train the models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Endpoint</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Code</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"># Compounds</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Data Transformation</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Helper Task</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LogD (pH7.5)</td><td align="center" valign="middle" rowspan="1" colspan="1">LOD</td><td align="center" valign="middle" rowspan="1" colspan="1">76,548</td><td align="center" valign="middle" rowspan="1" colspan="1">none</td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LogD (pH2.3)</td><td align="center" valign="middle" rowspan="1" colspan="1">LOA</td><td align="center" valign="middle" rowspan="1" colspan="1">236,280</td><td align="center" valign="middle" rowspan="1" colspan="1">none</td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Membrane affinity</td><td align="center" valign="middle" rowspan="1" colspan="1">LOM</td><td align="center" valign="middle" rowspan="1" colspan="1">64,506</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Human serum albumin binding</td><td align="center" valign="middle" rowspan="1" colspan="1">LOH</td><td align="center" valign="middle" rowspan="1" colspan="1">61,398</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub></td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Melting point</td><td align="center" valign="middle" rowspan="1" colspan="1">LMP</td><td align="center" valign="middle" rowspan="1" colspan="1">90,589</td><td align="center" valign="middle" rowspan="1" colspan="1">none</td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solubility (DMSO)</td><td align="center" valign="middle" rowspan="1" colspan="1">LOO</td><td align="center" valign="middle" rowspan="1" colspan="1">38,841</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub>(mol/L)</td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solubility (powder)</td><td align="center" valign="middle" rowspan="1" colspan="1">LOP</td><td align="center" valign="middle" rowspan="1" colspan="1">2334</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub>(mol/L)</td><td align="center" valign="middle" rowspan="1" colspan="1">no</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solubility (nephelometry)</td><td align="center" valign="middle" rowspan="1" colspan="1">LON</td><td align="center" valign="middle" rowspan="1" colspan="1">88,301</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub>(mol/L)</td><td align="center" valign="middle" rowspan="1" colspan="1">yes</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Solubility (DMSO not fully dissolved)</td><td align="center" valign="middle" rowspan="1" colspan="1">LOX</td><td align="center" valign="middle" rowspan="1" colspan="1">7392</td><td align="center" valign="middle" rowspan="1" colspan="1">log<sub>10</sub>(mol/L)</td><td align="center" valign="middle" rowspan="1" colspan="1">yes</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Solubility (no assay annotation)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LOQ</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">50,016</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">log<sub>10</sub>(mol/L)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">yes</td></tr></tbody></table></table-wrap><table-wrap id="molecules-25-00044-t002" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-25-00044-t002_Table 2</object-id><label>Table 2</label><caption><p>Performance of different learning algorithm in the ten ADMET endpoints. We report the average of cluster split cross-validation folds (not used for parameter tuning). The best performing method is given in bold (as well as those for which standard deviations overlap, see <xref ref-type="app" rid="app1-molecules-25-00044">supplementary Table S1</xref> for standard deviations of the folds).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Random Forest</th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">STNN <sup>a</sup></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">STNN Graph Conv <sup>b</sup></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MTNN <sup>c</sup></th><th colspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">MTNN Graph Conv <sup>d</sup></th></tr><tr><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOD <sup>e</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.81</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.87</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">0.88</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.88</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.94</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOA <sup>f</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">0.89</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.94</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.97</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.96</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOM <sup>g</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">0.68</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.64</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.80</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.51</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.75</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.71</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.84</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOH <sup>h</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.39</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.65</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.82</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LMP <sup>i</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.39</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.63</td><td align="center" valign="middle" rowspan="1" colspan="1">0.31</td><td align="center" valign="middle" rowspan="1" colspan="1">0.66</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.51</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.71</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.51</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.73</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOO <sup>j</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.43</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.66</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.47</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.69</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.47</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.73</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.49</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.71</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.59</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.77</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOP <sup>k</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.09</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.03</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.48</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>&#x02212;0.17</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.59</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.32</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.64</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.56</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.76</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LON <sup>l</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.50</td><td align="center" valign="middle" rowspan="1" colspan="1">0.69</td><td align="center" valign="middle" rowspan="1" colspan="1">0.53</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.59</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" rowspan="1" colspan="1">0.54</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.68</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.83</bold>
</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOX <sup>m</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">0.37</td><td align="center" valign="middle" rowspan="1" colspan="1">0.64</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.48</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">0.72</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.58</bold>
</td><td align="center" valign="middle" rowspan="1" colspan="1">
<bold>0.78</bold>
</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LOQ <sup>n</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.46</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.70</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.51</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.58</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.77</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.53</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.69</bold>
</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
<bold>0.85</bold>
</td></tr></tbody></table><table-wrap-foot><fn><p><sup>a</sup> single task neural network, <sup>b</sup> single task graph convolutional network, <sup>c</sup> multitask neural network, <sup>d</sup> multitask graph convolutional network, <sup>e</sup> logD, <sup>f</sup> logD in acidic pH, <sup>g</sup> membrane affinity, <sup>h</sup> human serum albumin binding, <sup>i</sup> melting point, <sup>j</sup> solubility from DMSO, <sup>k</sup> solubility from powder, <sup>l</sup> solubility from nephelometry, <sup>m</sup> solubility from DMSO not fully dissolved, <sup>n</sup> solubility no assay information.</p></fn></table-wrap-foot></table-wrap><table-wrap id="molecules-25-00044-t003" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-25-00044-t003_Table 3</object-id><label>Table 3</label><caption><p>Performance of the multitask graph convolutional model without helper tasks. Average of cluster split cross-validation folds. In parenthesis, difference with the results from the multitask graph convolutional model in <xref rid="molecules-25-00044-t002" ref-type="table">Table 2</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOD <sup>a</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.87 (&#x02212;0.01)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.94</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOA <sup>b</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.92 (+0.01)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.96</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOM <sup>c</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.71</td><td align="center" valign="middle" rowspan="1" colspan="1">0.84</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOH <sup>d</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.65</td><td align="center" valign="middle" rowspan="1" colspan="1">0.83 (+0.01)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LMP <sup>e</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.52 (+0.01)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.73</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOO <sup>f</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.57 (&#x02212;0.02)</td><td align="center" valign="middle" rowspan="1" colspan="1">0.76 (&#x02212;0.01)</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LOP <sup>g</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.56</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.74 (&#x02212;0.02)</td></tr></tbody></table><table-wrap-foot><fn><p><sup>a</sup> logD, <sup>b</sup> logD in acidic pH, <sup>c</sup> membrane affinity, <sup>d</sup> human serum albumin binding, <sup>e</sup> melting point, <sup>f</sup> solubility from DMSO, <sup>g</sup> solubility from powder.</p></fn></table-wrap-foot></table-wrap><table-wrap id="molecules-25-00044-t004" orientation="portrait" position="float"><object-id pub-id-type="pii">molecules-25-00044-t004_Table 4</object-id><label>Table 4</label><caption><p>Performance of the multitask graph convolutional model in a time split dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">R<sup>2</sup></th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Spearman</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">RMSE</th><th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Test Set Size</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOD <sup>a</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.87</td><td align="center" valign="middle" rowspan="1" colspan="1">0.93</td><td align="center" valign="middle" rowspan="1" colspan="1">0.33</td><td align="center" valign="middle" rowspan="1" colspan="1">32,794</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOA <sup>b</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.91</td><td align="center" valign="middle" rowspan="1" colspan="1">0.96</td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">46,481</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOM <sup>c</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.69</td><td align="center" valign="middle" rowspan="1" colspan="1">0.86</td><td align="center" valign="middle" rowspan="1" colspan="1">0.49</td><td align="center" valign="middle" rowspan="1" colspan="1">197</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOH <sup>d</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.55</td><td align="center" valign="middle" rowspan="1" colspan="1">0.78</td><td align="center" valign="middle" rowspan="1" colspan="1">0.61</td><td align="center" valign="middle" rowspan="1" colspan="1">614</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LMP <sup>e</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.35</td><td align="center" valign="middle" rowspan="1" colspan="1">0.59</td><td align="center" valign="middle" rowspan="1" colspan="1">45 &#x000b0;C</td><td align="center" valign="middle" rowspan="1" colspan="1">55</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">LOO <sup>f</sup></td><td align="center" valign="middle" rowspan="1" colspan="1">0.48</td><td align="center" valign="middle" rowspan="1" colspan="1">0.74</td><td align="center" valign="middle" rowspan="1" colspan="1">0.90</td><td align="center" valign="middle" rowspan="1" colspan="1">22,803</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">LOP <sup>g</sup></td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.54</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.75</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.84</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">935</td></tr></tbody></table><table-wrap-foot><fn><p><sup>a</sup> logD, <sup>b</sup> logD in acidic pH (random split), <sup>c</sup> membrane affinity, <sup>d</sup> human serum albumin binding, <sup>e</sup> melting point, <sup>f</sup> solubility from DMSO, <sup>g</sup> solubility from powder.</p></fn></table-wrap-foot></table-wrap></floats-group></article>