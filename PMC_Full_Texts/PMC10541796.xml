<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">10541796</article-id><article-id pub-id-type="doi">10.1093/gigascience/giad071</article-id><article-id pub-id-type="publisher-id">giad071</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject><subject>AcademicSubjects/SCI02254</subject></subj-group></article-categories><title-group><article-title>Confound-leakage: confound removal in machine learning leads to leakage</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5072-542X</contrib-id><name><surname>Hamdan</surname><given-names>Sami</given-names></name><aff>
<institution>Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich</institution>, 52428 J&#x000fc;lich, <country country="DE">Germany</country></aff><aff>
<institution>Institute of Systems Neuroscience, Medical Faculty, Heinrich-Heine University D&#x000fc;sseldorf</institution>, 40225 D&#x000fc;sseldorf, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7883-7076</contrib-id><name><surname>Love</surname><given-names>Bradley C</given-names></name><aff>
<institution>Department of Experimental Psychology, University College London</institution>, WC1H 0AP London, <country country="GB">UK</country></aff><aff>
<institution>The Alan Turing Institute</institution>, London NW1 2DB, <country country="GB">UK</country></aff><aff>
<institution>European Lab for Learning &#x00026; Intelligent Systems (ELLIS)</institution>, WC1E 6BT, London, <country country="GB">UK</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2739-8528</contrib-id><name><surname>von&#x000a0;Polier</surname><given-names>Georg G</given-names></name><aff>
<institution>Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich</institution>, 52428 J&#x000fc;lich, <country country="DE">Germany</country></aff><aff>
<institution>Department of Child and Adolescent Psychiatry, Psychosomatics and Psychotherapy, University Hospital Frankfurt</institution>, 60528 Frankfurt, <country country="DE">Germany</country></aff><aff>
<institution>Department of Child and Adolescent Psychiatry, Psychosomatics and Psychotherapy, RWTH Aachen University</institution>, 52074 Aachen, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7726-6470</contrib-id><name><surname>Weis</surname><given-names>Susanne</given-names></name><aff>
<institution>Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich</institution>, 52428 J&#x000fc;lich, <country country="DE">Germany</country></aff><aff>
<institution>Institute of Systems Neuroscience, Medical Faculty, Heinrich-Heine University D&#x000fc;sseldorf</institution>, 40225 D&#x000fc;sseldorf, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2858-6715</contrib-id><name><surname>Schwender</surname><given-names>Holger</given-names></name><aff>
<institution>Institute of Mathematics, Heinrich-Heine University D&#x000fc;sseldorf</institution>, 40225 D&#x000fc;sseldorf, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6363-2759</contrib-id><name><surname>Eickhoff</surname><given-names>Simon B</given-names></name><aff>
<institution>Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich</institution>, 52428 J&#x000fc;lich, <country country="DE">Germany</country></aff><aff>
<institution>Institute of Systems Neuroscience, Medical Faculty, Heinrich-Heine University D&#x000fc;sseldorf</institution>, 40225 D&#x000fc;sseldorf, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0289-5480</contrib-id><name><surname>Patil</surname><given-names>Kaustubh R</given-names></name><!--k.patil@fz-juelich.de--><aff>
<institution>Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich</institution>, 52428 J&#x000fc;lich, <country country="DE">Germany</country></aff><aff>
<institution>Institute of Systems Neuroscience, Medical Faculty, Heinrich-Heine University D&#x000fc;sseldorf</institution>, 40225 D&#x000fc;sseldorf, <country country="DE">Germany</country></aff><xref rid="cor1" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">Correspondence address. Kaustubh R. Patil. Institute of Neuroscience and Medicine, Brain and Behaviour (INM-7), Forschungszentrum J&#x000fc;lich, J&#x000fc;lich, 52428 Germany. E-mail: <email>k.patil@fz-juelich.de</email></corresp></author-notes><pub-date pub-type="epub" iso-8601-date="2023-09-30"><day>30</day><month>9</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>30</day><month>9</month><year>2023</year></pub-date><volume>12</volume><elocation-id>giad071</elocation-id><history><date date-type="received"><day>06</day><month>1</month><year>2023</year></date><date date-type="rev-recd"><day>01</day><month>6</month><year>2023</year></date><date date-type="accepted"><day>17</day><month>8</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2023. Published by Oxford University Press GigaScience.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giad071.pdf"/><abstract><title>Abstract</title><sec id="abs1"><title>Background</title><p>Machine learning (ML) approaches are a crucial component of modern data analysis in many fields, including epidemiology and medicine. Nonlinear ML methods often achieve accurate predictions, for instance, in personalized medicine, as they are capable of modeling complex relationships between features and the target. Problematically, ML models and their predictions can be biased by confounding information present in the features. To remove this spurious signal, researchers often employ featurewise linear confound regression (CR). While this is considered a standard approach for dealing with confounding, possible pitfalls of using CR in ML pipelines are not fully understood.</p></sec><sec id="abs2"><title>Results</title><p>We provide new evidence that, contrary to general expectations, linear confound regression can increase the risk of confounding when combined with nonlinear ML approaches. Using a simple framework that uses the target as a confound, we show that information leaked via CR can increase null or moderate effects to near-perfect prediction. By shuffling the features, we provide evidence that this increase is indeed due to confound-leakage and not due to revealing of information. We then demonstrate the danger of confound-leakage in a real-world clinical application where the accuracy of predicting attention-deficit/hyperactivity disorder is overestimated using speech-derived features when using depression as a confound.</p></sec><sec id="abs3"><title>Conclusions</title><p>Mishandling or even amplifying confounding effects when building ML models due to confound-leakage, as shown, can lead to untrustworthy, biased, and unfair predictions. Our expose of the confound-leakage pitfall and provided guidelines for dealing with it can help create more robust and trustworthy ML models.</p></sec></abstract><kwd-group kwd-group-type="keywords"><kwd>confounding</kwd><kwd>data-leakage</kwd><kwd>machine-learning</kwd><kwd>clinical applications</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Deutsche Forschungsgemeinschaft</institution><institution-id institution-id-type="DOI">10.13039/501100001659</institution-id></institution-wrap>
</funding-source></award-group></funding-group><counts><page-count count="14"/></counts></article-meta></front><body><boxed-text id="box1" position="float"><p>
<bold>Key Points:</bold>
</p><list list-type="bullet"><list-item><p>Confound removal is essential for building insightful and trustworthy machine learning (ML) models.</p></list-item><list-item><p>Confound removal can increase performance when combined with nonlinear ML.</p></list-item><list-item><p>This can be due to confound information leaking into the features.</p></list-item><list-item><p>Possible reasons are skewed feature distributions and the feature of limited precision.</p></list-item><list-item><p>Confound removal should be applied with utmost care in combination with nonlinear ML.</p></list-item></list></boxed-text><sec sec-type="intro" id="sec1"><title>Introduction</title><p>Machine learning (ML) approaches have revolutionized biomedical data analysis by providing powerful tools, especially nonlinear models, that can model complex feature&#x02013;target relationships [<xref rid="bib1" ref-type="bibr">1</xref>, <xref rid="bib2" ref-type="bibr">2</xref>]. However, the very power these nonlinear models bring to data anar models, that can model complex feature&#x02013;target realysis also leads to new challenges. Specifically, as we will detail, when a standard confound removal approach is paired with nonlinear models, new and surprising issues arise as the unintended is discovered and misinterpreted as a true effect.</p><p>Imagine building a diagnostic classifier for attention-deficit/hyperactivity disorder (ADHD) based on speech patterns. This will be a useful clinical tool aiding objective diagnosis [<xref rid="bib3" ref-type="bibr">3</xref>]. However, like most disorders, ADHD has comorbidity, for instance, with depression. Ideally, an ADHD diagnostic classifier should only rely upon characteristics of ADHD and ignore that of depression. This is an example of confounding, where it is desirable that the confound depression is disregarded by the classifier. Another example of confounding is the effect of aging and neurodegenerative diseases on the brain. In a study to build a neuroimaging-based diagnostic classifier, the nonpathological aging signal is confounding [<xref rid="bib4" ref-type="bibr">4</xref>]. Confounding is ubiquitous, and further examples include batch effects in genomics [<xref rid="bib5" ref-type="bibr">5&#x02013;7</xref>], scanner effects in neuroimaging [<xref rid="bib8" ref-type="bibr">8</xref>], patient and process information in radiographs [<xref rid="bib9" ref-type="bibr">9</xref>], and group differences like naturally different brain sizes in investigation of brain size&#x02013;independent sex differences [<xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib11" ref-type="bibr">11</xref>]. Ignoring confounding effects in an ML application can render predictions untrustworthy and insights questionable [<xref rid="bib12" ref-type="bibr">12</xref>] as this information can be exploited by learning algorithms [<xref rid="bib13" ref-type="bibr">13</xref>], leading to spurious feature&#x02013;target relationships [<xref rid="bib14" ref-type="bibr">14</xref>] (e.g., classification based on depression instead of ADHD or age instead of neuronal pathology). The benefits of big data in ML applications are obvious, especially when modeling weak relationships, but big data also lead to an increased risk of inducing confounded models [<xref rid="bib4" ref-type="bibr">4</xref>, <xref rid="bib11" ref-type="bibr">11</xref>, <xref rid="bib15" ref-type="bibr">15</xref>, <xref rid="bib16" ref-type="bibr">16</xref>]. Confounding, thus, is a crucial concern and, if not properly, treated can threaten real-world applicability of ML.</p><p>When confounding masks the true feature&#x02013;target relationship, its removal can clean the signal of interest, leading to higher generalizability (e.g., removal of batch effects in genomics) [<xref rid="bib7" ref-type="bibr">7</xref>]. On the other hand, when confounding introduces artifactual relationships, the same procedure can reduce prediction accuracy [<xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref>]. In either case, removing or adjusting for confounding effects is crucial for obtaining unbiased results, as otherwise an ML model might mostly rely on confounds, rendering signals of interest redundant. Two methods for treating confounding are commonly employed in data analysis with the goal of building an accurate ML model that is not biased by the confounding information. Data can be stratified based on the confounding variables, but it may introduce confounding information [<xref rid="bib19" ref-type="bibr">19</xref>], falsely increase test-set performance by removing harder-to-classify data points [<xref rid="bib20" ref-type="bibr">20</xref>], and can result in excessive data loss. As confounds share variation&#x02014;usually presumed linear variance&#x02014;with both the target and the features, another common method is confound regression (CR), which removes the confounding variance, also called confounded signal, from each feature separately using a linear regression model [<xref rid="bib4" ref-type="bibr">4</xref>, <xref rid="bib20" ref-type="bibr">20</xref>]. The resulting residualized features are considered confound free and used for subsequent analysis. CR has become the default method to counter confounding in observational studies, including in ML applications [<xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib21" ref-type="bibr">21</xref>]. Typically, a 2-step CR&#x02013;ML workflow is constructed while avoiding risks associated with typical data leakage by applying CR in a cross-validation&#x02013;consistent manner [<xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib22" ref-type="bibr">22</xref>]. It is important to note that we use a practitioner-oriented operational definition of confounds as a set of variables suspected to share an unwanted effect with both the features and target, which does not imply causality as in more formal definitions [<xref rid="bib23" ref-type="bibr">23</xref>].</p><p>A CR&#x02013;ML workflow typically attenuates prediction performance as it removes variance from the features that is informative of the target. If an increase in performance is observed after CR, it can be explained by either (i) <italic toggle="yes">information-reveal</italic>: CR reveals information that was masked by confounding or (ii) <italic toggle="yes">confound-leakage</italic>: leakage of confounding information into the features. In the case of information-reveal, CR could suppress linear confounding or noise, in turn enhancing the underlying (non)linear signal and making learning easier for a suitable ML algorithm [<xref rid="bib13" ref-type="bibr">13</xref>]. This would be a positive effect similar to removing simple shortcuts in the data [<xref rid="bib24" ref-type="bibr">24</xref>, <xref rid="bib25" ref-type="bibr">25</xref>]. If this is the case, then the resulting CR&#x02013;ML workflow would be valuable for modeling nonlinear relationships. Alternatively, as CR is a univariate operation applied to each feature, multivariate confounding (across features) could be revealed, which could help prediction albeit undesirably. On the other hand, confound-leakage would be an even more worrisome outcome as it would leak confounding information into the features instead of removing it. Confound-leakage would be detrimental to the validity and interpretability of the ensuing CR&#x02013;ML workflow and in some cases could lead to dangerous outcomes. CR has been reported to induce biases into statistical workflows, albeit not incorporating ML, leading to incorrectly inflated group differences inference in combined batch effects removal and group difference analysis [<xref rid="bib26" ref-type="bibr">26</xref>]. It is important to note that CR is not without other pitfalls; for instance, it might fail to completely remove confounding information [<xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib27" ref-type="bibr">27</xref>]. Still, CR is considered the de facto method, and therefore analyzing the hitherto unknown pitfall of leaking confounding information through CR is helpful. Furthermore, there were speculations of confound-leakage in ML workflows [<xref rid="bib18" ref-type="bibr">18</xref>], but it has not yet been systematically shown, analyzed, or explained.</p><p>To disentangle the 2 possible explanations of performance increase after CR, we systematically analyzed the 2-step CR&#x02013;ML workflow. For analysis purposes and to gain detailed knowledge, we propose a framework that uses the target as a confound (TaCo), in which we use a single confound that is the target. As a confound needs to share variation with both the target and the feature, any possible confound must share all confounded signal with the target. Hence, the target can be seen as a &#x0201c;superconfound,&#x0201d; subsuming all possible confounding effects. Although it is unlikely to encounter a confound equal to the target in real applications, TaCo provides a framework for systematic evaluation. It should be noted that real confounds will fall on the continuum from weak (low confounded signal) to strong (TaCo) depending on their degree of similarity with the target. Indeed, as we show, the TaCo framework reveals strong effects where the prediction accuracy is boosted from moderate to perfect as well as weaker effects for confounds weakly correlated with the target. A previous work has used TaCo for evaluating the validity and reliability of confound adjustment methods [<xref rid="bib21" ref-type="bibr">21</xref>].</p><p>To this end, we performed extensive empirical analyses on several benchmark datasets, providing strong evidence for confound-leakage. First, we showcase confound-leakage in walk-through analyses. Then, using the TaCo framework, we systematically answer whether the improvement in prediction performance after CR is due to leakage. For this, we used benchmark datasets as well as several conceptually simple simulations covering both classification and regression problems. Finally, with a clinically relevant task of ADHD diagnosis using speech-related features with depression as a confound, we demonstrate the misleading impact of confound-leakage.</p></sec><sec sec-type="results" id="sec2"><title>Results</title><sec id="sec2-1"><title>Walk-through analysis</title><p>The goal of this section&#x000a0;is to introduce readers to our analysis approach with intuitive examples. We show 1 exemplary case of TaCo removal for a binary classification task and a CR scenario with a weaker confound in a regression task. In both cases, we randomly split the data into <inline-formula><tex-math id="TM0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$70\%$\end{document}</tex-math></inline-formula> train and <inline-formula><tex-math id="TM0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$30\%$\end{document}</tex-math></inline-formula> test parts. The CR and prediction models were learned on the training data, and the results are reported on the test split. We will show that confound-leakage can be concluded if performance increases after performing CR on shuffled features (<inline-formula><tex-math id="TM0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}_{CR}$\end{document}</tex-math></inline-formula>).</p><sec id="sec2-1-1"><title>TaCo removal for binary classification</title><p>We analyzed the &#x0201c;bank investment&#x0201d; data to predict whether a customer will subscribe to term deposit given their financial and socioeconomic information. We used a decision tree (DT) with limited maximum depth of 2 for visualization ease. This example is meant to demonstrate key aspects of our proposed analyses (Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>).</p><fig position="float" id="fig1"><label>Figure 1:</label><caption><p>A walk-through analysis demonstrating our analysis pipeline and confound-leakage using DT. The results shown here are on the <inline-formula><tex-math id="TM0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$30\%$\end{document}</tex-math></inline-formula> test split. For the binary classification walk-through using the bank investment dataset, a subset of the features used is shown before CR (A) and after CR (B). Induced DTs and their performance before (C) or after CR (D). The DT after CR (D) is based on minute differences in only 2 features and still performs nearly perfectly and better compared to the DT on raw data (C). The regression analysis walk-through using simulated data is depicted as feature&#x02013;target relationships with the dotted line showing the predicted values (E, F). The nonnormal distribution of the feature conditioned on the confound leaks information usable by the DT. Here, CR removes the linear relationship, as intended, but introduces a stronger nonlinear one by shifting the distribution of <italic toggle="yes">X<sub>CR</sub></italic> given <italic toggle="yes">confound</italic> = 0 in between the 2 peaks of <italic toggle="yes">X<sub>CR</sub></italic> given confound = 1 (F).</p></caption><graphic xlink:href="giad071fig1" position="float"/></fig><p>TaCo removal showed a much higher area under the curve for the receiver operating characteristic curve (AUCROC) of 0.98 compared to the baseline AUCROC of 0.75 without CR. Still, the TaCo-removed features were highly similar to the original features (median Pearson&#x02019;s correlation: 0.99, Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1A,B</xref>). The 2 ensuing DTs were, however, completely different and relied on different features. Notably, these drastic differences were induced by minute feature alterations after CR that are hardly detectable by humans but are effectively captured by DT (Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1C,D</xref>). Such performance increase can be due to revealed information or confound-leakage. Therefore, we sought to gain evidence to distinguish between these 2 scenarios using 2 complementary measurements: (i) destroying the relationship between features and target and (ii) use of confound-predicted features.</p><p>To destroy the feature&#x02013;target relation, we shuffled each feature before CR (<inline-formula><tex-math id="TM0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}$\end{document}</tex-math></inline-formula>) to create <inline-formula><tex-math id="TM0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}_{CR}$\end{document}</tex-math></inline-formula> and repeated the analysis. As there should be no predictive information in the shuffled features, the only explanation for above chance-level performance is CR leaking information into the confound-removed features <inline-formula><tex-math id="TM0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$X _{CR}$\end{document}</tex-math></inline-formula> (i.e., confound-leakage). We applied the shuffling procedure to a train-test split in this walk-through analysis. But it should be noted that when combined with a (nested) cross-validation and Bayesian Region of Practical Equivalence (ROPE) approach, this procedure can be used to compare models similarly as a permutation test (see section&#x000a0;&#x0201c;Feature shuffling approach&#x0201d;). We observed chance-level performance without CR (AUCROC = 0.48) for the shuffled features. However, a performance increase after TaCo removal was observed (AUCROC = 0.99). This analysis shows that performance increase after TaCo removal with shuffled features indicates the possibility of confound-leakage.</p></sec><sec id="sec2-1-2"><title>Confound removal for regression</title><p>As an example of a weaker confound on a regression task, we simulated a binary confound and then sampled a feature from different distributions for each confound value (confound equal to 0 or 1). Then we added the confound to a normally distributed target (<italic toggle="yes">M</italic> = 0 and <italic toggle="yes">SD</italic> = 0.50; Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1E,F</xref>). This creates a clear confounding situation, where the confound affects both the feature (point-biserial correlation = 0.71, <italic toggle="yes">P</italic> &#x0003c; 0.01) and the target (point-biserial correlation = 0.71, <italic toggle="yes">P</italic> &#x0003c; 0.01) and thus leads to a spurious relationship between the feature and the target (Pearson's correlation = 0.51, <italic toggle="yes">P</italic> &#x0003c; 0.01). Following the same procedure as in the previous example, we observed increased performance after CR using a DT with limited depth of 2 (<italic toggle="yes">R</italic><sup>2</sup> using <italic toggle="yes">X</italic> = 0.29, <italic toggle="yes">X<sub>CR</sub></italic> = 0.42). As in these simulated data, only a spurious relation (via confound) exists between the feature and target, it is safe to assume that an increased performance after CR is due to confound-leakage. Furthermore, we found a probable mechanism behind this confound-leakage to be the distribution of the features conditioned on the confound. More precisely, CR shifts the feature values for confound = 1 in between most feature values for the confound = 0 (Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1E</xref>). This leaks the confounding information into the feature instead of removing it (Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1F</xref>). The shuffled features, however, were not sensitive to confound-leakage (<italic toggle="yes">X</italic> = 0,<inline-formula><tex-math id="TM0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}=-0.01$\end{document}</tex-math></inline-formula>), which is expected considering the probable cause for such leakage depends on the joint distribution of the confound and the feature. When shuffling the features within each confound category to preserve the joint distribution, we observed an increase in performance after CR (<italic toggle="yes">M</italic> = 0.29 before to <italic toggle="yes">M</italic> = 0.42). This result indicates that shuffling the features might not be always sensitive to confound-leakage. We, nevertheless, use independently shuffled features in our analysis for practicality, particularly in the context of continuous or multiple confounding factors.</p></sec></sec><sec id="sec2-2"><title>Analyses of benchmark data</title><sec id="sec2-2-1"><title>TaCo removal increases performance of nonlinear methods</title><p>Our systematic and cross-validation (CV)&#x02013;consistent analysis comprised comparison between TaCo removal pipelines and no-CR pipelines on 10 UC Irvine (UCI) datasets. TaCo removal led to a meaningful increase in out-of-sample scoring using all tested nonlinear models, random forest (RF) (7/10 datasets), DT (8/10), support vector machine (SVM) with radial basis function (RBF) kernel (5/10), and multilayer perceptron (MLP) (7/10) (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2</xref>, <xref rid="sup11" ref-type="supplementary-material">Supplementary Fig.&#x000a0;S1</xref>). This suggests that confound-leakage is a risk associated with the usage of a CR&#x02013;ML pipeline with nonlinear ML models. Furthermore, this suggests that the DT-based algorithms (DT and RF) are most susceptible to showing increased performance.</p><fig position="float" id="fig2"><label>Figure 2:</label><caption><p>Performance on the UCI benchmark datasets when using raw vs. CR features (A) and raw vs. the predicted features given the confound/TaCo/<inline-formula><tex-math id="TM0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\hat{X}$\end{document}</tex-math></inline-formula> (B). The 2 columns correspond to (i) TaCo removal with 4 ML algorithms (logistic regression [LR], DT, RF, MLP) and (ii) CR with simulated confound with different correlations to the target (range 0.2&#x02013;0.8) with RF. (A, B) Performance using the original features. (C, D) Performance on shuffled features. To check whether a difference between the performance of 2 models is meaningful, we used the Bayesian ROPE approach to identify what is most probable: performance being higher before removal (&#x0003c;), being higher after removal (&#x0003e;), or equivalent (=) (see the Methods section&#x000a0;for details). When using a linear model (LR), TaCo removal leads to reduction in prediction performance, as expected. In contrast, nonlinear models lead to a higher performance for all datasets. This increase could be explained by confound removal revealing information already in the data (suppression) or confound removal leaking information into the features (confound-leakage). Shuffling the features destroys the association between features and the target; therefore, subsequent performance increase after TaCo removal indicates the possibility of confound-leakage (C, D). The simulated confounds show that an increase after CR is also possible for confounds weakly related to the target (B, D), and 1 dataset (Blood) shows strong evidence of confound-leakage.</p></caption><graphic xlink:href="giad071fig2" position="float"/></fig></sec><sec id="sec2-2-2"><title>CR using weaker confounds also increases performance</title><p>As the target is the strongest possible confound, TaCo represents an extreme case. To test whether the potential leakage we found with TaCo extends to CR in general, using the UCI datasets, we simulated confounds related to the target at different strengths measured by Pearson&#x02019;s correlation ranging from 0.2 to 0.8. Depending on the dataset, different amounts of correlated confounds led to leakage after CR. We observed potential confound-leakage for 5 of the 10 datasets with at least 1 of the confound-target strengths. As expected, a higher target-confound correlation led to more leakage (i.e., higher performance after CR) (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2C</xref>).</p></sec><sec id="sec2-2-3"><title>Increased performance after TaCo removal is due to confound-leakage</title><p>As described in the walk-through analysis (see &#x0201c;TaCo removal for binary classification&#x0201d;), we measure the performance after first shuffling the features to evaluate whether the increased performance after TaCo removal/CR is due to information reveal or confound-leakage. After shuffling the features, both pipelines, no CR and TaCo removal, should perform close to chance level if the improved performance is due to revealed information. Indeed, the no-CR pipeline performed close to the chance level, while the TaCo-removal pipeline increased the performance (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2</xref>, TaCo CR Shuffled). As there should be no predictive information in the shuffled features, above chance-level performance could only be obtained if the CR leaks information. Thus, this result provides strong evidence in favor of the confound-leakage.</p><p>For the simulated weaker confounds, these results were less strong, but we still found 5 of 10 datasets where <italic toggle="yes">X<sub>CR</sub></italic> and 9 of 10 where <inline-formula><tex-math id="TM0010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}_{CR}$\end{document}</tex-math></inline-formula> performed above chance level.</p></sec><sec id="sec2-2-4"><title>Possible mechanisms for confound-leakage</title><p>As a multitude of mechanisms could lead to confound-leakage, exhaustively identifying all possible mechanisms is out of the scope of this article. Rather, we want to highlight 2 possible mechanisms leading to confound-leakage inspired by the walk-through analyses: (i) confound-leakage due to continuous features deviating from normal distributions (see &#x0201c;Confound removal for regression&#x0201d;) and (ii) confound-leakage due to unbalanced features of limited precision (see &#x0201c;TaCo removal for binary classification&#x0201d;). Both mechanisms could be summarized under the umbrella of (small) differences of the conditional distributions of features given the confound inside of CV-folds.</p><p>As DT-based models are very popular ML algorithms [<xref rid="bib28" ref-type="bibr">28</xref>] and seem to be most susceptible to the described problems (see &#x0201c;TaCo removal increases performance of nonlinear methods&#x0201d;), we will focus on them in our simulations to decrease the complexity of our results. Furthermore, we will use a DT whenever there is only 1 features and RF when there are multiple features.</p></sec><sec id="sec2-2-5"><title>Confound-leakage due to deviation from normal distributions</title><p>Consider simulating a standard normal feature not informative of a binary target. Then consider adding a smaller distribution around opposing extreme values separately for each class of a binary target (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3A</xref>). The resulting feature only differs systematically w.r.t. the classes at the extreme values. As CR with a binary confound is equivalent to subtracting the mean for each confounding group from the respective feature, this operation is now biased toward the extreme parts of the feature distribution. Consequently, <italic toggle="yes">X<sub>CR</sub></italic> exposes confounding information in terms of decrease in the overlap of the feature distributions conditioned on the confound (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3A,B</xref>). In other words, confounding information leaked via CR in turn increases the prediction performance (AUROC from 0.51 before to 0.58 after TaCo removal). To show that the increased performance is not only due to better prediction of extreme values, we also tested the same model on a test set without the extreme values. The results were in line with previous observations, as the AUROC improved from 0.48 before to 0.57 after CR.</p><fig position="float" id="fig3"><label>Figure 3:</label><caption><p>Two mechanisms for confound-leakage. First mechanism where nonnormal distributions get shifted apart through CR. (A, B) An example using a simulation with extreme values on opposing sides for 1 feature conditioned on the TaCo. (C, D) A simplified version (binary target for visualization purposes) of the house price UCI benchmark dataset. Here, the distributions of the feature conditional on the TaCo are different (C): a narrow distribution (TaCo = 1) and a distribution with 2 peaks (TaCo = 0). TaCo removal shifts the narrow distribution in between the two peaks (D), leaking information usable by nonlinear ML algorithms. The second mechanism, leakage through minute differences in the feature after CR, is highlighted through the visualization of the DT trained on the heart dataset after CR (E). Distribution plots visualize the data at each decision node. The decision boundary is shown as a dotted line. For decision nodes before leaf nodes, the side of the decision node leading into a prediction is colored to represent the predicted label as diagnosed (green) or not (purple). The minute differences in the 2 used features that perfectly separate the data into the 2 classes can be seen.</p></caption><graphic xlink:href="giad071fig3" position="float"/></fig><p>We also observed higher performance after similar decreased overlap due to TaCo removal in a simplified version of the &#x0201c;house pricing&#x0201d; UCI benchmark dataset (Fig. <xref rid="fig3" ref-type="fig">3C,D</xref>), providing real-world evidence for this phenomenon.</p><p>Lastly, we investigated whether such effects could also occur when randomly sampling nonnormal distributed features instead of carefully constructing the features conditioned on the confound. To this end, we sampled an increasing number of features (1 to 100) either using a random normal or skewed (&#x003c7;<sup>2</sup>, <italic toggle="yes">df</italic> = 3) distribution independent of a normally distributed target.</p><p>Using RF, we observed increased performance after TaCo removal with skewed features but not with normally distributed features (e.g., <italic toggle="yes">R</italic><sup>2</sup> of <italic toggle="yes">M</italic> = 0.23 with <italic toggle="yes">SD</italic> = 0.06 compared to <italic toggle="yes">R</italic><sup>2</sup> of <italic toggle="yes">M</italic> = &#x02212;0.04 with <italic toggle="yes">SD</italic> = 0.04, respectively, with 100 features). Importantly, this effect increased with the number of features (Fig. <xref rid="fig4" ref-type="fig">4</xref>). To further illustrate this point, we performed another simulation depicting a typical confounding situation. Here, we sampled an increasing number of features (1 to 100) with different &#x003c7;<sup>2</sup> distribution given a binary confound (<italic toggle="yes">df</italic> = 3 (4) and scale = 0.5 (1) for confound = 0 (1)). The target was sampled from a normal distribution (<italic toggle="yes">M</italic> = 0, <italic toggle="yes">SD</italic> = 0.2), and the confound was added to it. Analysis of these data shows an increased performance after confound removal from <italic toggle="yes">M</italic> = &#x02212;0.52 (<italic toggle="yes">SD</italic> = 0.02) to <italic toggle="yes">M</italic> = &#x02212;0.50 (<italic toggle="yes">SD</italic> = 0.03) using 1 feature and from <italic toggle="yes">M</italic> = &#x02212;0.02 (<italic toggle="yes">SD</italic> = 0.01) to <italic toggle="yes">M</italic> = 0.18 (<italic toggle="yes">SD</italic> = 0.01) using 100 features. These results demonstrate that the effect of confound-leakage increases with increasing number of features. These simulations show that skewed features and, by extension, potentially other nonnormal distributed features can lead to confound-leakage. Interestingly, another consequence of nonnormal distributions is insufficient removal of confounding information [<xref rid="bib21" ref-type="bibr">21</xref>].</p><fig position="float" id="fig4"><label>Figure 4:</label><caption><p>Prediction performance of an RF trained with (blue) or without (red) confound removal on an increasing number of features. Each feature was sampled from a random standard normal distribution (<italic toggle="yes">M</italic> = 0, <italic toggle="yes">SD</italic> = 1), a random &#x003c7;<sup>2</sup> distribution with <italic toggle="yes">df</italic> = 3, or a &#x003c7;<sup>2</sup> distribution with a <italic toggle="yes">df</italic> = 3, scale = 0.5 or <italic toggle="yes">df</italic> = 4, scale = 1 for the confound being equal to 0 and 1, respectively. (A) The RF trained on the normally distributed features did not achieve performance above the chance level (<italic toggle="yes">R</italic><sup>2</sup> &#x0003c; 0) irrespective of confound removal. (B, C) When training the RF on either of the &#x003c7;<sup>2</sup> distributed features, confound removal resulted in above chance-level performance (<italic toggle="yes">R</italic><sup>2</sup> &#x0003e; 0). This effect increased with an increasing number of features and can only be explained by confound removal leaking information into the features.</p></caption><graphic xlink:href="giad071fig4" position="float"/></fig></sec><sec id="sec2-2-6"><title>Confound-leakage due to limited precision features</title><p>A similar effect was observed with binary features, where unbalanced feature distributions conditioned on the confound led to leakage. Using simulations, first we confirmed that a binary feature perfectly balanced in respect to the TaCo did not lead to confound-leakage (AUCROC of <italic toggle="yes">M</italic> = 0.50, <italic toggle="yes">SD</italic> = 0). Then, we repeated similar simulations but now we swapped 2 randomly selected distinct values of the feature within each CV-fold, preserving the marginal distribution of the feature but slightly changing its distribution conditional on the confound. This can be seen as adding a small amount of noise to the feature. Still, such a simple manipulation led to drastic leakage after TaCo removal with perfect AUCROC (<italic toggle="yes">M</italic> = 1.00, <italic toggle="yes">SD</italic> = 0.00), compared to AUCROC without CR (<italic toggle="yes">M</italic> = 0.52, <italic toggle="yes">SD</italic> = 0).</p><p>To further demonstrate this effect, we analyzed a simple demonstrative classification task using DT and 2 binary features derived from the UCI &#x0201c;heart dataset&#x0201d; representing the resting electrocardiographic (Restecg) results. Without CR, the DT had 117 nodes and achieved a moderate AUCROC (<italic toggle="yes">M</italic> = 0.74, <italic toggle="yes">SD</italic> = 0.06). In stark contrast, after TaCo removal, the DT was extremely simple with only 5 nodes and achieved near-perfect AUROC (<italic toggle="yes">M</italic> = 0.99, <italic toggle="yes">SD</italic> = 0.01) (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3E</xref>). Tellingly, this DT was able to make accurate predictions based on numerically minute differences in feature values. The reason for this becomes apparent when remembering that CR with a binary confound is equivalent to subtracting the mean of the corresponding confounding group from the respective feature. When applied to a binary feature, this results in 4 distinct values for a residual feature (Fig <xref rid="fig3" ref-type="fig">3E</xref>). When taken together with the results on the benchmark UCI data (see &#x0201c;Analyses of benchmark data&#x0201d;), we can see that such minute differences can be exploited by models such as DTs, RFs, and MLPs but likely not by linear models. It is important to note that leakage through minute differences was observed for not only binary features but also other features with a limited precision (values containing only integers or with limited fractional parts). To demonstrate this, we predicted a random continuous target using either a normally distributed feature or the same feature rounded to the first digit. The original nonrounded feature performed at chance level both before (<italic toggle="yes">R</italic><sup>2</sup>: <italic toggle="yes">M</italic> = &#x02212;1.10, <italic toggle="yes">SD</italic> = 0.06) and after TaCo removal (<italic toggle="yes">R</italic><sup>2</sup>: <italic toggle="yes">M</italic> = &#x02212;1.03, <italic toggle="yes">SD</italic> = 0.07), while after rounding, it led to an improvement from <italic toggle="yes">M</italic> = &#x02212;0.08 (<italic toggle="yes">SD</italic> = 0.01) to <italic toggle="yes">M</italic> = 0.70 (<italic toggle="yes">SD</italic> = 0.16) after TaCo removal. Features with limited precision (i.e., with no or rounded fractional part) are common, for instance, age in years, questionnaires in psychology and social sciences, and transcriptomic data.</p></sec></sec><sec id="sec2-3"><title>Confound-leakage poses danger in clinical applications</title><p>ADHD is a common psychiatric disorder that is currently diagnosed based on symptomatology, but objective computerized diagnosis is desirable [<xref rid="bib29" ref-type="bibr">29</xref>]. Ideally, a predictive model for diagnosing ADHD should not be biased by comorbid conditions (e.g., depression) [<xref rid="bib30" ref-type="bibr">30</xref>]. To this end, comorbidity can be treated as a confound. However, a confound-leakage affected model, albeit with appealing performance, could lead to misleading diagnosis and treatment. To highlight the danger of confound-leakage on this clinically relevant task, we analyzed a dataset with speech-derived features with the task to distinguish individuals with ADHD from controls. Our version of the dataset is a balanced subsample of the dataset described by von Polier et al. [<xref rid="bib3" ref-type="bibr">3</xref>].</p><p>The baseline RF model without CR provided mean AUROC (<italic toggle="yes">M</italic> = 0.71, <italic toggle="yes">SD</italic> = 0.02). We then removed 4 confounds commonly considered for this task&#x02014;age, sex, education level, and depression score (Beck&#x02019;s Depression Inventory, BDI)&#x02014;via featurewise CR in a CV-consistent manner. This resulted in a much higher AUCROC (<italic toggle="yes">M</italic> = 0.86, <italic toggle="yes">SD</italic> = 0.02). This model would be very attractive for real-world application if its performance is true (i.e., not impacted by leakage). However, as we have shown with our analyses, confound-leakage can lead to such performance improvement. If confound-leakage is indeed driving the performance, then this model could misclassify individuals as having ADHD because of confounding effects (e.g., their sex or depression), leading to misdiagnosis and wrong therapeutic interventions. To disentangle the effect of each confound, we looked at the performance after CR for each confound separately. Performing CR with BDI led to a high AUCROC with original features after CR (<italic toggle="yes">M</italic> = 0.91, <italic toggle="yes">SD</italic> = 0.01) and shuffled features (<italic toggle="yes">M</italic> = 0.84, <italic toggle="yes">SD</italic> = 0.01) (Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5A,B</xref>). This result revealed that BDI is driving the potential leakage, owing to its strong relation to the target (point-biserial correlation, <italic toggle="yes">r</italic> = 0.61, <italic toggle="yes">P</italic> &#x0003c; 0.01). Furthermore, a permutation test also led to the same conclusion (see <xref rid="sup11" ref-type="supplementary-material">Methods and Supplementary Fig.&#x000a0;S2</xref>). Training CR models only on healthy individuals can be helpful in clinical applications [<xref rid="bib4" ref-type="bibr">4</xref>]. We investigated this variant of CR, and again the AUCROC increased for original features after CR <italic toggle="yes">M</italic> = 0.83 (<italic toggle="yes">SD</italic> = 0.02) and an increase with shuffled features from <italic toggle="yes">M</italic> = 0.51 (<italic toggle="yes">SD</italic> = 0.05) to <italic toggle="yes">M</italic> = 0.79 (<italic toggle="yes">SD</italic> = 0.02), suggesting that confound leakage is also a concern for variants of CR. Lastly, we wanted to evaluate why we observe confound-leakage on this dataset. The limited precision of features cannot be the reason here as all features are continuous. Therefore, we hypothesized that the confound leaked due to some features deviating from normal distributions. To this end, we first compared the feature importance between the RF after CR and using the original features. Here, we observed the RFs&#x02019; 10 most important features were completely different (Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5C,D</xref>), indicating that the 2 RF models rely on different relationships in the data. Next we visualized the distributions of the 2 most important features of the RF after CR for both models. This visualization (Fig. <xref rid="fig5" ref-type="fig">5E,F</xref>) clearly shows that CR has shifted the distributions due to deviations from normal distributions leaking information in their joint distribution. Furthermore, we trained new DTs using only these 2 features before or after CR. This led to an increase of AUCROC from 0.61 to 0.70 after CR only using these features. These analyses clearly demonstrate that real-world applications could suffer from confound-leakage and users should exercise care when implementing and validating a CR&#x02013;ML workflow.</p><fig position="float" id="fig5"><label>Figure 5:</label><caption><p>The real-world ADHD speech dataset. The performance when using different confounds (A, B), most important features of RF when using Beck&#x02019;s Depression Inventory (BDI) as confound (C, D), and visualization of confound-leakage due to deviation from normal distributions (E, F). (A) The performance of an RF predicting ADHD vs. healthy controls using the original features. To check whether a difference is meaningful, we used the Bayesian ROPE approach to identify what is most probable: performance being higher before removal (&#x0003c;), being higher after removal (&#x0003e;), or equivalent (=) (see Methods section). An increased performance can be observed when using all confounds, BDI as a confound, or the TaCo. The same pattern appears when the features were shuffled (B). This shows that the increase in performance is due to confound-leakage, and BDI is a driving factor for this leakage as it leaks information when used as a confound. (C, D) The 10 most important features for using <italic toggle="yes">X</italic> and <italic toggle="yes">X<sub>CR</sub></italic> as features. The feature ranking is shown as a white label on top of each cell. The most important features are different for <italic toggle="yes">X</italic> and <italic toggle="yes">X<sub>CR</sub></italic>. Furthermore, the most important features of 1 model ranked as very unimportant in the other. (E, F) Decision boundaries of DT trained on the 2 most important features after CR. The background colors indicate the prediction of the model, and the points show the true target value and the x-axis the 2 most important features. The distribution of each feature conditioned on the target is shown as the density plots. One can see that CR leaks information by cleanly separating the blue and red points.</p></caption><graphic xlink:href="giad071fig5" position="float"/></fig></sec></sec><sec sec-type="discussion" id="sec3"><title>Discussion</title><p>Here, we exposed a hitherto unexplained pitfall in CR&#x02013;ML workflows that use featurewise linear confound removal&#x02014;a method popular in epidemiological and clinical applications. Specifically, we have shown this method can counterintuitively introduce confounding, which can be exploited by some nonlinear ML algorithms. Thus, in addition to the already known pitfalls of residual confounding [<xref rid="bib21" ref-type="bibr">21</xref>], our results show that CR may actually introduce confounding information. We provide evidence of confound-leakage using a range of systematic controlled experiments on real and simulated data comprising both classification and regression tasks. First, to establish confound-leakage as opposed to information-reveal (of possibly nonlinear information) as the reason behind increased performance after CR, we proposed the TaCo framework (i.e., using the target as &#x0201c;superconfound&#x0201d;). This extreme case of confounding allowed us to establish the existence, the extent, and possible mechanisms of confound-leakage. Specifically, by comparing the without CR baseline performance with CR after feature shuffling (<inline-formula><tex-math id="TM0011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\tilde{X}_{CR}$\end{document}</tex-math></inline-formula>), this framework can identify confound-leakage as the cause of increased predictive performance. We then extended the same framework to the more realistic scenario of weaker confounds showing that also there confound-leakage can occur.</p><p>To identify risk factors of confound-leakage, we performed several analyses. First, we demonstrated a mechanism by which confound-leakage can occur: differences of the conditional distributions of features given the confound. In the case of continuous features, nonnormal distributions (e.g., skewed distributions) and in the case of discrete features, frequency imbalances can cause leakage, although other mechanisms could exist. Additionally, we show that features of limited precision (e.g., age in years and counts) also showed susceptibility due to this mechanism. Lastly, our results showed that the risk of confound-leakage increases with the number of features, which is especially problematic in the era of &#x0201c;big data,&#x0201d; where tens of thousands of features are a norm.</p><p>Still, we would like to highlight that we do not claim to have found all possible ways confound-leakage can happen. For instance, it is possible that other modeling approaches, even linear ones, could be susceptible to confound-leakage, although we did not find evidence for it in our analyses. Nonetheless, confound-leakage can bias the data and may negatively impact subsequent statistical analysis [<xref rid="bib21" ref-type="bibr">21</xref>].</p><p>It is important to note that although similar, confound-leakage is not equal to collider bias. Colliders are variables causally influenced by both the features and target [<xref rid="bib19" ref-type="bibr">19</xref>]. Both collider bias and confound-leakage describe situations where variable adjustment can lead to spurious relationships between features and target. However, the collider bias assumes that the removed variable has to be caused by both the features and the target, which is not shared by confound-leakage. One cannot exclude the possibility of collider removal using CR for many of our experiments as our operational definition of confounds does not include any assumption of causality. Still, we observe confound-leakage through CR for at least 1 causally defined confound (see &#x0201c;Walk-through analysis&#x0201d;) and variables showing relationship only with the target. Such associations are not covered by the causal relationships described by a collider. In other words, the mechanisms of confound-leakage can lead to leaked information due to any variable related to the target and not only colliders or causal confounds.</p><p>Taken together, our extensive results show that the commonly used data types and settings of nonlinear ML pipelines are susceptible to confound-leakage when using featurewise linear CR. Therefore, this method should be applied with care, and the ensuing models should be closely inspected, especially in critical decision domains. We concretely demonstrated this using an application scenario from precision medicine by building models for diagnosis of ADHD. We found that the attempt to control for comorbidity with depression using CR led to confound-leakage. As many disorders often exhibit severe comorbidity (e.g., AHDH and depression, as we demonstrated here, but also neurodegenerative disorders are strongly confounded by aging-related factors [<xref rid="bib31" ref-type="bibr">31</xref>] as well as comorbidity in mental disorders [<xref rid="bib32" ref-type="bibr">32</xref>, <xref rid="bib33" ref-type="bibr">33</xref>]), the issue of confound-leakage should be carefully assessed in all such applications. We recommend the following best practices when applying CR together with nonlinear ML algorithms:</p><list list-type="order"><list-item><p>Assess confounding strength: Check the confounds&#x02019; relation to each feature and the target. In general, confounds strongly related to the target pose a greater danger of leaking predictive information. Here, we used a straightforward approach of measuring the correlations between the confound and target/feature. Other methods can be employed (e.g., proposed by Spisak [<xref rid="bib27" ref-type="bibr">27</xref>]). Furthermore, measuring how dependent the predictions of a model are on the confound by permutation testing [<xref rid="bib34" ref-type="bibr">34</xref>, <xref rid="bib35" ref-type="bibr">35</xref>] or the approach proposed by Dinga et&#x000a0;al. [<xref rid="bib21" ref-type="bibr">21</xref>] can be helpful. To gain additional information, the reader might be interested in methods to estimate the variance in the target explained by ML predictions that confounds cannot explain [<xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib27" ref-type="bibr">27</xref>].</p></list-item><list-item><p>Compare performance with and without CR: If the performance increases after CR, one should investigate the reason behind the increase.</p></list-item><list-item><p>Gain evidence against or in favor of the confound-leakage: The procedure of shuffling the features followed by CR as we defined in the TaCo framework can provide clues regarding confound-leakage. Our shuffling approach can be seen as a single iteration of permutation testing. As our experiments suggest this is sufficient to obtain an indication of confound-leakage. However, a permutation test-based null distribution can quantify the variability and provide additional information. It is important to note, however, that while this can provide evidence for confound-leakage, we are not aware of a procedure to definitively exclude confound-leakage as an explanation.</p></list-item><list-item><p>Carefully choose alternatives: If confound-leakage seems probable, then consider alternative confound adjustment methods. Stratification [<xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib36" ref-type="bibr">36</xref>] is commonly in conventional ML or unlearning of confounding effects [<xref rid="bib37" ref-type="bibr">37</xref>], which is common in deep learning and further general approaches that promote fairness [<xref rid="bib12" ref-type="bibr">12</xref>, <xref rid="bib38" ref-type="bibr">38</xref>]. Note, however, that these procedures may also entail pitfalls. Hence, we caution researchers to exercise care when applying any confound adjustment protocol and to carefully consider limitations of the modeling approach used.</p></list-item></list><sec id="sec3-1"><title>Conclusions and future directions</title><p>Important societal questions involving health and economic policy can be informed by applying powerful nonlinear ML models to large datasets. To draw appropriate conclusions, confounds must be removed without introducing new issues that cloud the results. In the present study, we performed extensive numerical experiments to gather evidence for confound-leakage. Using feature shuffling and predictions due to confound predicted features as proposed here, investigators can get an initial indication of whether their pipeline and data are susceptible to confound-leakage. We highlighted the conditions most likely to lead to leakage. Although we made progress on understanding these issues, there is no full-proof method for detecting and eliminating leakage. We hope our results prompt others to push further, perhaps expanding on the standard definition we adopted for confounds by introducing causal analyses. We hope our and allied efforts inform both researchers and practitioners who incorporate ML models into their data analyses. As a starting point, we suggest following the guidelines we provide to mitigate against confound-leakage.</p></sec></sec><sec sec-type="materials|methods" id="sec4"><title>Methods</title><sec id="sec4-1"><title>Data</title><p>We analyzed several ML benchmark datasets from diverse domains to draw generalizable conclusions. To ensure reproducibility, most datasets come from the openly accessible UCI repository [<xref rid="bib39" ref-type="bibr">39</xref>]. We included 5 classification tasks and 5 regression tasks with different sample sizes and numbers of features. All classification problems were binary or were binarized, and class labels were balanced to exclude biases due to class imbalance [<xref rid="bib40" ref-type="bibr">40</xref>].</p><p>We also used one clinical dataset, a balanced subsample of the ADHD speech dataset described by von Polier et&#x000a0;al. [3]. This data includes 126 individuals with 6,016 speech-related features, a binary target describing ADHD status (ADHD or control) and contains 4 confounds: gender, education level, age, and depression score measured using the BDI. For more information on the datasets, see <xref rid="sup11" ref-type="supplementary-material">Supplementary Table S1</xref>.</p></sec><sec id="sec4-2"><title>Confound removal</title><p>Confound removal was performed following the standard way of using linear regression models. Following the common practice, we applied CR to all the features. Specifically, for each feature, a linear regression model was fit with the feature as the dependent variable and the confounds as independent variables. The residuals of these models, that is, original feature minus the fitted values were used as confound-free features (<inline-formula><tex-math id="TM0012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$X_{CR}=X - \hat{X}$\end{document}</tex-math></inline-formula>). This procedure was performed in a CV-consistent fashion (i.e., the confound removal models were fitted on the training folds and applied to the training and test folds) [<xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib22" ref-type="bibr">22</xref>].</p></sec><sec id="sec4-3"><title>Target as a confound (TaCo)</title><p>The TaCo framework allows systematic analysis of confound removal effects. Confounding is a 3-way relationship between features, confounds, and the target. This means that a confound needs to share variance with both the feature and the target. Measuring or simulating such relationships can be hard, especially if linear univariate relationships cannot be assumed. Furthermore, effects of confound removal should increase with the actual strength of the confound. The target itself explains all the shared variance and thus is the strongest possible confound. Therefore, using the target as a confound (i.e., TaCo) measures the most possible extent of confounding. In addition, using the TaCo simplifies the analysis to a 2-way relationship. Lastly, the TaCo approach is applicable to any dataset and can help to measure the strongest possible extent of confound-leakage even without knowing the confounds.</p></sec><sec id="sec4-4"><title>Machine learning pipeline</title><p>To study the effect of CR on both linear and nonlinear ML algorithms, we employed a variety of algorithms: linear/LR, linear kernel SVM, RBF kernel SVM, DT, RF, and MLP with a single hidden layer (relu). Additionally, we used dummy models to evaluate chance-level performance.</p><p>In the preprocessing steps, we normalized the continuous features and continuous confounds to have a mean of zero and unit variance, again in a CV-consistent fashion. Any categorical features were one-hot encoded following standard practice.</p></sec><sec id="sec4-5"><title>Evaluation</title><p>We compared the performance of ML pipelines with and without CR. To this end, we computed the out-of-sample AUCROC for classification and predictive <italic toggle="yes">R</italic><sup>2</sup> from scikit-learn [<xref rid="bib41" ref-type="bibr">41</xref>] for regression problems in a 10&#x000a0;times repeated 5-fold nested CV. We employed the Bayesian ROPE approach [<xref rid="bib42" ref-type="bibr">42</xref>] to determine whether the results for a given dataset and algorithm with and without CR were meaningfully higher, lower, or not meaningfully different.</p></sec><sec id="sec4-6"><title>The Bayesian ROPE for model comparison</title><p>In this study, we used the Bayesian ROPE [<xref rid="bib42" ref-type="bibr">42</xref>] approach to qualify differences between K-fold cross-validation results coming from 2 models. This approach uses the Bayesian framework to compute probabilities of the metric falling into a defined region of practical equivalence or of 1 ML pipeline scoring higher than the other. This is achieved by defining a region of equivalence (here we used 0.05). Consequently, the Bayesian ROPE approach allows us to make probabilistic statements regarding whether and, if so, which of the ML pipelines score higher. We summarize these differences using the following symbols: = (highest probability of pipelines scoring practically equivalent), &#x0003c; (highest probability of right pipeline scoring higher), and &#x0003e; (highest probability of left pipeline scoring higher). Other possibilities, such as the significance test correcting for the dependency structure in K-fold CV [<xref rid="bib43" ref-type="bibr">43</xref>] or permutation testing by shuffling the target or features, can be employed when suitable.</p></sec><sec id="sec4-7"><title>Feature shuffling approach</title><p>Shuffling the features while keeping the confounds and target intact destroys the feature&#x02013;target and feature&#x02013;confound relationships while preserving the confound&#x02013;target relationship. Therefore, after feature shuffling, any confound adjustment method cannot reveal the feature&#x02013;target relationship, but it can still leak information. In other words, any performance above the chance level after CR on shuffled features is an indication of confound-leakage. Feature shuffling is also used in other approaches such as permutation testing (see section&#x000a0;&#x0201c;The Bayesian ROPE for model comparison&#x0201d;) to test effectiveness of confound adjustment methods [<xref rid="bib21" ref-type="bibr">21</xref>]. Permutation testing can be computationally expensive and, like other frequentist tests, it cannot accept the null hypothesis to establish equivalence. We, therefore, adopted a computationally feasible methodology. We shuffle the features, perform repeated nested cross-validation, and then apply the Bayesian ROPE. For completeness, we show that both permutation testing and the Bayesian ROPE detect confound leakage in the clinical dataset. In some cases, feature shuffling approaches might need further consideration, for instance, shuffling features within confound categories to preserve their joint distribution (see &#x0201c;Walk-through analysis&#x0201d;) and the possibility of suppression and leakage happening simultaneously. Nevertheless, they serve as a useful tool for detecting confound leakage, as shown in this work.</p></sec></sec><sec id="sec5"><title>Availability of Source Code and Requirements</title><list list-type="bullet"><list-item><p>Project name: Confound-leakage</p></list-item><list-item><p>Project homepage: <ext-link xlink:href="https://github.com/juaml/ConfoundLeakage" ext-link-type="uri">https://github.com/juaml/ConfoundLeakage</ext-link></p></list-item><list-item><p>Operating system(s): GNU/Linux</p></list-item><list-item><p>Programming language Python 3.10.8 [<xref rid="bib43" ref-type="bibr">43</xref>]</p></list-item><list-item><p>Other requirements: scikit-learn 0.24.2, baycomp 1.0.2, matplotlib 3.5.1, seaborn 0.11.2, dtreeviz 1.3.5, numpy 1.22.3, pandas 1.2.5</p></list-item><list-item><p>License: GNU Affero General Public License v3.0</p></list-item></list></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>giad071_GIGA-D-23-00004_Original_Submission</label><media xlink:href="giad071_giga-d-23-00004_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup2" position="float" content-type="local-data"><label>giad071_GIGA-D-23-00004_Revision_1</label><media xlink:href="giad071_giga-d-23-00004_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup3" position="float" content-type="local-data"><label>giad071_GIGA-D-23-00004_Revision_2</label><media xlink:href="giad071_giga-d-23-00004_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup4" position="float" content-type="local-data"><label>giad071_GIGA-D-23-00004_Revision_3</label><media xlink:href="giad071_giga-d-23-00004_revision_3.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup5" position="float" content-type="local-data"><label>giad071_Response_to_Reviewer_Comments_Original_Submission</label><media xlink:href="giad071_response_to_reviewer_comments_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup6" position="float" content-type="local-data"><label>giad071_Response_to_Reviewer_Comments_Revision_1</label><media xlink:href="giad071_response_to_reviewer_comments_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup7" position="float" content-type="local-data"><label>giad071_Response_to_Reviewer_Comments_Revision_2</label><media xlink:href="giad071_response_to_reviewer_comments_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup8" position="float" content-type="local-data"><label>giad071_Reviewer_1_Report_Original_Submission</label><caption><p>Richard Dinga -- 2/8/2023 Reviewed</p></caption><media xlink:href="giad071_reviewer_1_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup9" position="float" content-type="local-data"><label>giad071_Reviewer_2_Report_Original_Submission</label><caption><p>Qingyu Zhao -- 2/15/2023 Reviewed</p></caption><media xlink:href="giad071_reviewer_2_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup10" position="float" content-type="local-data"><label>giad071_Reviewer_2_Report_Revision_1</label><caption><p>Qingyu Zhao -- 6/12/2023 Reviewed</p></caption><media xlink:href="giad071_reviewer_2_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup11" position="float" content-type="local-data"><label>giad071_Supplemental_File</label><media xlink:href="giad071_supplemental_file.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgement</title><p>We thank the UCI machine learning repository [<xref rid="bib39" ref-type="bibr">39</xref>] and the original dataset contributors.</p></ack><sec sec-type="data-availability" id="sec6"><title>Data Availability</title><p>All 10 UCI benchmark datasets can be accessed freely at the UCI machine learning repository [<xref rid="bib39" ref-type="bibr">39</xref>]. Together with our simulated data (available under [<xref rid="bib44" ref-type="bibr">44</xref>]), the UCI benchmark datasets compose minimal data sets to reproduce our key findings. Additionally, we analyzed 1 real-world clinical dataset [<xref rid="bib3" ref-type="bibr">3</xref>]. These sensitive data are available from PeakProfiling GmbH with certain restrictions. Restrictions apply to the availability of the data, which were used under license for this study. Please contact J&#x000f6;rg Langner, the cofounder and CTO of PeakProfiling GmbH, with requests. An archival copy of the code and supporting data is also available via the <italic toggle="yes">GigaScience</italic> database, GigaDB [<xref rid="bib45" ref-type="bibr">45</xref>].</p></sec><sec id="sec7"><title>Additional Files</title><p>
<bold>Supplementary Fig.&#x000a0;S1</bold>. Performance on the UCI benchmark datasets when using raw vs. CR features (A) and raw vs. the predicted features given the confound/TaCo/&#x00058;&#x00302; (B). The 2 columns correspond to (i) TaCo removal with 6 ML algorithms (LR, DT, RF, MLP, Lin SVM, RBF SVM) and (ii) CR with simulated confound with different correlation to the target (range 0.2&#x02013;0.8) with RF. (A, B) Performance using the original features. (C, D) Performance on shuffled features. When using a linear model (LR), TaCo removal leads to reduction in prediction performance, as expected. In contrast, nonlinear models lead to a higher performance for all datasets. This increase could be explained by confound removal revealing information already in the data (suppression) or confound removal leaking information into the features (confound-leakage). Shuffling the features destroys the association between features and the target; therefore, subsequent performance increase after TaCo removal indicates the possibility of confound-leakage (C, D). The simulated confounds show that an increase after CR is also possible for confounds weakly related to the target (B, D), and 1 dataset (Blood) shows strong evidence of confound-leakage.</p><p>
<bold>Supplementary Fig.&#x000a0;S2</bold>. We performed permutation testing with 1,000 iterations. After shuffling the features, a significantly lower performance was observed compared to the original features <italic toggle="yes">X</italic>. No significant difference between raw and shuffled features was observed when using the <italic toggle="yes">X</italic><sub>CR</sub> features. This result is in line with the leakage hypothesis as the higher accuracy after shuffling and <italic toggle="yes">CR</italic> indicates leaking target-related confounding information into the features.</p><p>
<bold>Supplementary Table S1</bold>. Overview of all the datasets used. Shows each dataset with their associated problem type, sample size, feature number, and source. Our datasets cover a big range of features and sample sizes. All datasets with the exception of the speech ADHD one are freely accessible through the UCI machine learning repository.</p><p>
<bold>Supplementary Table S2</bold>. Overview of all the simulations used. Including pseudo-code to create the features (X), target (y), and confounds (c). Variables were sampled from normal distributions (<italic toggle="yes">N</italic>), with different means (<italic toggle="yes">M</italic>) and standard deviations (<italic toggle="yes">SD</italic>) or binary distributions (binary). <italic toggle="yes">repeat(list, number)</italic> indicates the repetition of a list of values ([value, value,...]) are repeated for a number of times. concat means the concatenation of multiple arrays, and <italic toggle="yes">where condition</italic> &#x02192; operation means that the operation is executed for where the condition is met.</p></sec><sec id="sec9"><title>Abbreviations</title><p>ADHD: attention-deficit/hyperactivity disorder; AUCROC: receiver operating characteristic curve; BDI: Beck&#x02019;s Depression Inventory; CR: confound regression; CV: cross-validation; DT: decision tree; LR: logistic regression; ML: machine learning; MLP: multilayer perceptron; RBF: radial basis function; RF: random forest; SVM: support vector machine; TaCo: target as a confound; UCI: UC Irvine.</p></sec><sec sec-type="ethical" id="sec10"><title>Ethical Approval</title><p>All procedures contributing to this work comply with the ethical standards of the relevant national and institutional committees on human experimentation and with the Declaration of Helsinki of 1975, as revised in 2008. The ADHD data collection and use involving human subjects/patients were approved by the ethics committee of the Charite Universitatsmedizin Berlin, Berlin, Germany; the approval number is EA4/014/10. All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived by the data collectors. The ethics protocols for analyses of these data were approved by the Heinrich Heine University D&#x000fc;sseldorf ethics committee (No. 4039, 4096).</p></sec><sec sec-type="COI-statement" id="sec11"><title>Competing Interests</title><p>The authors declare no competing financial or nonfinancial interests but the following personal financial interest: Georg G. von Polier participated and received payments in the national advisory board ADHD of Takeda.</p></sec><sec id="sec12"><title>Funding</title><p>This work was partly supported by the Helmholtz-AI project DeGen (ZT-I-PF-5-078), the Helmholtz Portfolio Theme &#x0201c;Supercomputing and Modeling for the Human Brain,&#x0201d; and Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project-ID 431549029&#x02013;SFB 1451 project B05.</p></sec><sec id="sec13"><title>Authors&#x02019; Contributions</title><p>Study concept and design: S.H., B.C.L, G.G.P., S.W., H.S., S.B.E., and K.R.P. Data collection and processing: G.G.P. for the ADHD data. Data analysis and interpretation: all authors. Drafting of the manuscript: S.H. Critical revision of the manuscript for important intellectual content and final approval: all authors. Supervision: B.C.L., S.B.E., and K.R.P.</p></sec><ref-list id="ref1"><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zeng</surname>
<given-names>LL</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>H</given-names></string-name>, <string-name><surname>Hu</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Multi-Site diagnostic classification of schizophrenia using discriminant deep learning with functional connectivity MRI</article-title>. <source>EBioMedicine</source>. <year>2018</year>;<volume>30</volume>:<fpage>74</fpage>&#x02013;<lpage>85</lpage>.. <pub-id pub-id-type="doi">10.1016/j.ebiom.2018.03.017</pub-id>.<pub-id pub-id-type="pmid">29622496</pub-id></mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Qin</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Lei</surname><given-names>D</given-names></string-name>, <string-name><surname>Pinaya</surname><given-names>WHL</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Using graph convolutional network to characterize individuals with major depressive disorder across multiple imaging sites</article-title>. <source>eBioMedicine</source>. <year>2022</year>;<volume>78</volume>:<fpage>103977</fpage>. <pub-id pub-id-type="doi">10.1016/j.ebiom.2022.103977</pub-id>.<pub-id pub-id-type="pmid">35367775</pub-id></mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>von&#x000a0;Polier</surname>
<given-names>GG</given-names>
</string-name>, <string-name><surname>Ahlers</surname><given-names>E</given-names></string-name>, <string-name><surname>Amunts</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Predicting adult attention deficit hyperactivity disorder (ADHD) using vocal acoustic features</article-title>. <comment>medRxiv.</comment><year>2021</year>. <ext-link xlink:href="http://medrxiv.org/lookup/doi/10.1101/2021.03.18.21253108" ext-link-type="uri">https://doi.org/10.1101/2021.03.18.21253108</ext-link>.</mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dukart</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Schroeter</surname><given-names>ML</given-names></string-name>, <string-name><surname>Mueller</surname><given-names>K</given-names></string-name></person-group>. <article-title>Age correction in dementia&#x02014;matching to a healthy brain</article-title>. <source>PLoS One</source>. <year>2011</year>;<volume>6</volume>:<fpage>e22193</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0022193</pub-id>.<pub-id pub-id-type="pmid">21829449</pub-id></mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Jo</surname>
<given-names>ES</given-names>
</string-name>, <string-name><surname>Gebru</surname><given-names>T</given-names></string-name></person-group>. <article-title>Lessons from archives: strategies for collecting sociocultural data in machine learning</article-title>. In: <source>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</source>. <publisher-name>Association for Computing Machinery</publisher-name>; <publisher-loc>New York, NY, USA</publisher-loc>. <year>2020:</year>; <fpage>306</fpage>&#x02013;<lpage>16</lpage>.. <pub-id pub-id-type="doi">10.1145/3351095.3372829</pub-id>.</mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Johnson</surname>
<given-names>WE</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>C</given-names></string-name>, <string-name><surname>Rabinovic</surname><given-names>A</given-names></string-name></person-group>. <article-title>Adjusting batch effects in microarray expression data using empirical Bayes methods</article-title>. <source>Biostatistics</source>. <year>2007</year>;<volume>8</volume>:<fpage>118</fpage>&#x02013;<lpage>27</lpage>.. <pub-id pub-id-type="doi">10.1093/biostatistics/kxj037</pub-id>.<pub-id pub-id-type="pmid">16632515</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Whalen</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Schreiber</surname><given-names>J</given-names></string-name>, <string-name><surname>Noble</surname><given-names>WS</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Navigating the pitfalls of applying machine learning in genomics</article-title>. <source>Nat Rev Genet</source>. <year>2022</year>;<volume>23</volume>:<fpage>169</fpage>&#x02013;<lpage>81</lpage>.. <pub-id pub-id-type="doi">10.1038/s41576-021-00434-9</pub-id>.<pub-id pub-id-type="pmid">34837041</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pomponio</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Erus</surname><given-names>G</given-names></string-name>, <string-name><surname>Habes</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Harmonization of large MRI datasets for the analysis of brain imaging patterns throughout the lifespan</article-title>. <source>Neuroimage</source>. <year>2020</year>;<volume>208</volume>:<fpage>116450</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116450</pub-id>.<pub-id pub-id-type="pmid">31821869</pub-id></mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Badgeley</surname>
<given-names>MA</given-names>
</string-name>, <string-name><surname>Zech</surname><given-names>JR</given-names></string-name>, <string-name><surname>Oakden-Rayner</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning predicts hip fracture using confounding patient and healthcare variables</article-title>. <source>NPJ Digit Med</source>. <year>2019</year>;<volume>2</volume>:<fpage>31</fpage>. <pub-id pub-id-type="doi">10.1038/s41746-019-0105-1</pub-id>.<pub-id pub-id-type="pmid">31304378</pub-id></mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Luders</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Toga</surname><given-names>AW</given-names></string-name>, <string-name><surname>Thompson</surname><given-names>PM</given-names></string-name></person-group>. <article-title>Why size matters: differences in brain volume account for apparent sex differences in callosal anatomy: the sexual dimorphism of the corpus callosum</article-title>. <source>Neuroimage</source>. <year>2014</year>;<volume>84</volume>:<fpage>820</fpage>&#x02013;<lpage>4</lpage>.. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.09.040</pub-id>.<pub-id pub-id-type="pmid">24064068</pub-id></mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Wiersch</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Hamdan</surname><given-names>S</given-names></string-name>, <string-name><surname>Hoffstaedter</surname><given-names>F</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Accurate sex prediction of cisgender and transgender individuals without brain size bias</article-title>. <comment>Sci Rep</comment><year>2023</year>;<fpage>24;13(1):1386824</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-023-37508-z</pub-id>.</mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mehrabi</surname>
<given-names>N</given-names>
</string-name>, <string-name><surname>Morstatter</surname><given-names>F</given-names></string-name>, <string-name><surname>Saxena</surname><given-names>N</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A survey on bias and fairness in machine learning</article-title>. <source>ACM Comput Surv</source>. <year>2021</year>;<volume>54</volume>:<fpage>1</fpage>&#x02013;<lpage>35</lpage>.. <pub-id pub-id-type="doi">10.1145/3457607</pub-id>.</mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>MacKinnon</surname>
<given-names>DP</given-names>
</string-name>, <string-name><surname>Krull</surname><given-names>JL</given-names></string-name>, <string-name><surname>Lockwood</surname><given-names>CM</given-names></string-name></person-group>. <article-title>Equivalence of the mediation, confounding and suppression effect</article-title>. <source>Prev Sci</source>. <year>2000</year>;<volume>1</volume>:<fpage>173</fpage>&#x02013;<lpage>181</lpage>.. <pub-id pub-id-type="doi">10.1023/A:1026595011371</pub-id>.<pub-id pub-id-type="pmid">11523746</pub-id></mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pourhoseingholi</surname>
<given-names>MA</given-names>
</string-name>, <string-name><surname>Baghestani</surname><given-names>AR</given-names></string-name>, <string-name><surname>Vahedi</surname><given-names>M</given-names></string-name></person-group>. <article-title>How to control confounding effects by statistical analysis</article-title>. <source>Gastroenterol Hepatol Bed Bench</source>. <year>2012</year>;<volume>5</volume>:<fpage>79</fpage>&#x02013;<lpage>83</lpage>.. <pub-id pub-id-type="doi">10.22037/ghfbb.v5i2.246</pub-id>.<pub-id pub-id-type="pmid">24834204</pub-id></mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Deng</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Dong</surname><given-names>W</given-names></string-name>, <string-name><surname>Socher</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>ImageNet: A large-scale hierarchical image database</article-title>. In: <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2009</year>;<fpage>248</fpage>&#x02013;<lpage>55</lpage>.. <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id>.</mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Alfaro-Almagro</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>McCarthy</surname><given-names>P</given-names></string-name>, <string-name><surname>Afyouni</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Confound modelling in UK Biobank brain imaging</article-title>. <source>NeuroImage</source>. <year>2021</year>;<volume>224</volume>:<fpage>248</fpage>&#x02013;<lpage>255</lpage>.. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117002</pub-id>.</mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rao</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Monteiro</surname><given-names>JM</given-names></string-name>, <string-name><surname>Mourao-Miranda</surname><given-names>J</given-names></string-name></person-group>. <article-title>Predictive modelling using neuroimaging data in the presence of confounds</article-title>. <source>NeuroImage</source>. <year>2017</year>;<volume>150</volume>:<fpage>23</fpage>&#x02013;<lpage>49</lpage>.. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.01.066</pub-id>.<pub-id pub-id-type="pmid">28143776</pub-id></mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chyzhyk</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Varoquaux</surname><given-names>G</given-names></string-name>, <string-name><surname>Milham</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>How to remove or control confounds in predictive models, with applications to brain biomarkers</article-title>. <source>GigaScience</source>. <year>2022</year>;<volume>11</volume>:<fpage>giac014</fpage>. <pub-id pub-id-type="doi">10.1093/gigascience/giac014</pub-id>.<pub-id pub-id-type="pmid">35277962</pub-id></mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Greenland</surname>
<given-names>S</given-names>
</string-name>
</person-group>. <article-title>Quantifying biases in causal models: classical confounding vs collider-stratification bias</article-title>. <source>Epidemiology</source>. <year>2003</year>;<volume>14</volume>:<fpage>300</fpage>&#x02013;<lpage>306</lpage>.. <pub-id pub-id-type="doi">10.1097/00001648-200305000-00009</pub-id>.<pub-id pub-id-type="pmid">12859030</pub-id></mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Snoek</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Mileti&#x00107;</surname><given-names>S</given-names></string-name>, <string-name><surname>Scholte</surname><given-names>HS</given-names></string-name></person-group>. <article-title>How to control for confounds in decoding analyses of neuroimaging data</article-title>. <source>NeuroImage</source>. <year>2019</year>;<volume>184</volume>:<fpage>741</fpage>&#x02013;<lpage>760</lpage>.. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.074</pub-id>.<pub-id pub-id-type="pmid">30268846</pub-id></mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Dinga</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Schmaal</surname><given-names>L</given-names></string-name>, <string-name><surname>Penninx</surname><given-names>BWJH</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Controlling for effects of confounding variables on machine learning predictions</article-title>. <comment>bioRxiv.</comment><year>2020</year>. <pub-id pub-id-type="doi">10.1101/2020.08.17.255034</pub-id>.</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>More</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Eickhoff</surname><given-names>SB</given-names></string-name>, <string-name><surname>Caspers</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Confound removal and normalization in practice: a neuroimaging based sex prediction case study</article-title>. <source>Lecture Notes Comput Sci</source>. <year>2021</year>;<volume>12461</volume>:<fpage>3</fpage>&#x02013;<lpage>18</lpage>.. <pub-id pub-id-type="doi">10.1007/978-3-030-67670-4_1</pub-id>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Weele</surname>
<given-names>TJV</given-names>
</string-name>, <string-name><surname>Shpitser</surname><given-names>I</given-names></string-name></person-group>. <article-title>On the definition of a confounder</article-title>. <source>Ann Stat</source>. <year>2013</year>;<volume>41</volume>:<fpage>196</fpage>&#x02013;<lpage>220</lpage>.. <pub-id pub-id-type="doi">10.1214/12-AOS1058</pub-id>.<pub-id pub-id-type="pmid">25544784</pub-id></mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dagaev</surname>
<given-names>N</given-names>
</string-name>, <string-name><surname>Roads</surname><given-names>BD</given-names></string-name>, <string-name><surname>Luo</surname><given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A too-good-to-be-true prior to reduce shortcut reliance</article-title>. <source>Pattern Recog Lett</source>. <year>2023</year>;<volume>166</volume>:<fpage>164</fpage>&#x02013;<lpage>71</lpage>.. <pub-id pub-id-type="doi">10.1016/j.patrec.2022.12.010</pub-id>.</mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Geirhos</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Jacobsen</surname><given-names>JH</given-names></string-name>, <string-name><surname>Michaelis</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Shortcut learning in deep neural networks</article-title>. <source>Nat Mach Intell</source>. <year>2020</year>;<volume>2</volume>:<fpage>665</fpage>&#x02013;<lpage>73</lpage>.. <pub-id pub-id-type="doi">10.1038/s42256-020-00257-z</pub-id>.</mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Nygaard</surname>
<given-names>V</given-names>
</string-name>, <string-name><surname>R&#x000f8;dland</surname><given-names>EA</given-names></string-name>, <string-name><surname>Hovig</surname><given-names>E</given-names></string-name></person-group>. <article-title>Methods that remove batch effects while retaining group differences may lead to exaggerated confidence in downstream analyses</article-title>. <source>Biostatistics</source>. <year>2016</year>;<volume>17</volume>:<fpage>29</fpage>&#x02013;<lpage>39</lpage>.. <pub-id pub-id-type="doi">10.1093/biostatistics/kxv027</pub-id>.<pub-id pub-id-type="pmid">26272994</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Spisak</surname>
<given-names>T</given-names>
</string-name>
</person-group>. <article-title>Statistical quantification of confounding bias in machine learning models</article-title>. <source>Gigascience</source>. <year>2022</year>;<volume>11</volume>:<fpage>giac082</fpage>. <pub-id pub-id-type="doi">10.1093/gigascience/giac082</pub-id>.<pub-id pub-id-type="pmid">36017878</pub-id></mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Grinsztajn</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Oyallon</surname><given-names>E</given-names></string-name>, <string-name><surname>Varoquaux</surname><given-names>G</given-names></string-name></person-group>. <article-title>Why do tree-based models still outperform deep learning on typical tabular data?</article-title>. <source>Adv Neural Inform Process Syst</source>. <year>2022</year>;<volume>35</volume>:<fpage>507</fpage>&#x02013;<lpage>20</lpage>.. <ext-link xlink:href="https://proceedings.neurips.cc/paper_files/paper/2022/file/0378c7692da36807bdec87ab043cdadc-Paper-Datasets_and_Benchmarks.pdf" ext-link-type="uri">https://doi.org/10.48550/arXiv.2207.08815</ext-link>.</mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Gualtieri</surname>
<given-names>CT</given-names>
</string-name>, <string-name><surname>Johnson</surname><given-names>LG</given-names></string-name></person-group>. <article-title>ADHD: is objective diagnosis possible?</article-title>. <source>Psychiatry</source>. <year>2005</year>;<volume>2</volume>:<fpage>44</fpage>&#x02013;<lpage>53</lpage>.. <ext-link xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2993524/" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2993524/</ext-link>.</mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Katzman</surname>
<given-names>MA</given-names>
</string-name>, <string-name><surname>Bilkey</surname><given-names>TS</given-names></string-name>, <string-name><surname>Chokka</surname><given-names>PR</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Adult ADHD and comorbid disorders: clinical implications of a dimensional approach</article-title>. <source>BMC Psychiatry</source>. <year>2017</year>;<volume>17</volume>:<fpage>302</fpage>. <pub-id pub-id-type="doi">10.1186/s12888-017-1463-3</pub-id>.<pub-id pub-id-type="pmid">28830387</pub-id></mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wyss-Coray</surname>
<given-names>T</given-names>
</string-name>
</person-group>. <article-title>Ageing, neurodegeneration and brain rejuvenation</article-title>. <source>Nature</source>. <year>2016</year>;<volume>539</volume>:<fpage>180</fpage>&#x02013;<lpage>6</lpage>.. <pub-id pub-id-type="doi">10.1038/nature20411</pub-id>.<pub-id pub-id-type="pmid">27830812</pub-id></mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Joshi</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Wozniak</surname><given-names>J</given-names></string-name>, <string-name><surname>Petty</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Psychiatric comorbidity and functioning in a clinically referred population of adults with autism spectrum disorders: a comparative study</article-title>. <source>J Autism Dev Disord</source>. <year>2013</year>;<volume>43</volume>:<fpage>1314</fpage>&#x02013;<lpage>25</lpage>.. <pub-id pub-id-type="doi">10.1007/s10803-012-1679-5</pub-id>.<pub-id pub-id-type="pmid">23076506</pub-id></mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Plana-Ripoll</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Pedersen</surname><given-names>CB</given-names></string-name>, <string-name><surname>Holtz</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Exploring comorbidity within mental disorders among a danish national population</article-title>. <source>JAMA Psychiatry</source>. <year>2019</year>;<volume>76</volume>:<fpage>259</fpage>&#x02013;<lpage>70</lpage>.. <pub-id pub-id-type="doi">10.1001/jamapsychiatry.2018.3658</pub-id>.<pub-id pub-id-type="pmid">30649197</pub-id></mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Epstein</surname>
<given-names>MP</given-names>
</string-name>, <string-name><surname>Duncan</surname><given-names>R</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A permutation procedure to correct for confounders in case-control studies, including tests of rare variation</article-title>. <source>Am J Hum Genet</source>. <year>2012</year>;<volume>91</volume>:<fpage>215</fpage>&#x02013;<lpage>23</lpage>.. <pub-id pub-id-type="doi">10.1016/j.ajhg.2012.06.004</pub-id>.<pub-id pub-id-type="pmid">22818855</pub-id></mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Chaibub&#x000a0;Neto</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Pratap</surname><given-names>A</given-names></string-name>, <string-name><surname>Perumal</surname><given-names>TM</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A permutation approach to assess confounding in machine learning applications for digital health</article-title>. In: <source>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining KDD &#x02019;19</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2019</year>:<fpage>54</fpage>&#x02013;<lpage>64</lpage>.. <pub-id pub-id-type="doi">10.1145/3292500.333090</pub-id>.</mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>McNamee</surname>
<given-names>R</given-names>
</string-name>
</person-group>. <article-title>Regression modelling and other methods to control confounding</article-title>. <source>Occup Environ Med</source>. <year>2005</year>;<volume>62</volume>:<fpage>500</fpage>&#x02013;<lpage>506</lpage>.. <pub-id pub-id-type="doi">10.1136/oem.2002.001115</pub-id>.<pub-id pub-id-type="pmid">15961628</pub-id></mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dinsdale</surname>
<given-names>NK</given-names>
</string-name>, <string-name><surname>Jenkinson</surname><given-names>M</given-names></string-name>, <string-name><surname>Namburete</surname><given-names>AIL</given-names></string-name></person-group>. <article-title>Deep learning-based unlearning of dataset bias for MRI harmonisation and confound removal</article-title>. <source>NeuroImage</source>. <year>2021</year>;<volume>228</volume>:<fpage>117689</fpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117689</pub-id>.<pub-id pub-id-type="pmid">33385551</pub-id></mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhao</surname>
<given-names>Q</given-names>
</string-name>, <string-name><surname>Adeli</surname><given-names>E</given-names></string-name>, <string-name><surname>Pohl</surname><given-names>KM</given-names></string-name></person-group>. <article-title>Training confounder-free deep learning models for medical applications</article-title>. <source>Nat Commun</source>. <year>2020</year>;<volume>11</volume>:<fpage>6010</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-020-19784-9</pub-id>.<pub-id pub-id-type="pmid">33243992</pub-id></mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Dua</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Graff</surname><given-names>C</given-names></string-name></person-group>. <source>UCI Machine Learning Repository</source>. <year>2017</year>. <ext-link xlink:href="https://archive.ics.uci.edu/citation" ext-link-type="uri">https://archive.ics.uci.edu/citation</ext-link>.</mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Collell</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Prelec</surname><given-names>D</given-names></string-name>, <string-name><surname>Patil</surname><given-names>KR</given-names></string-name></person-group>. <article-title>A simple plug-in bagging ensemble based on threshold-moving for classifying binary and multiclass imbalanced data</article-title>. <source>Neurocomputing</source>. <year>2018</year>;<volume>275</volume>:<fpage>330</fpage>&#x02013;<lpage>340</lpage>.. <pub-id pub-id-type="doi">10.1016/j.neucom.2017.08.035</pub-id>.<pub-id pub-id-type="pmid">29398782</pub-id></mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pedregosa</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Varoquaux</surname><given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Machine Learn Res</source>. <year>2011</year>:<volume>12</volume>:<issue>(85)</issue>: <fpage>2825</fpage>&#x02013;<lpage>30</lpage>.. <ext-link xlink:href="http://jmlr.org/papers/v12/pedregosa11a.html" ext-link-type="uri">http://jmlr.org/papers/v12/pedregosa11a.html</ext-link>.</mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Benavoli</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Corani</surname><given-names>G</given-names></string-name>, <string-name><surname>Dem&#x00161;ar</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis</article-title>. <source>J Machine Learn Res</source>. <year>2017</year>;<volume>18</volume>:<fpage>1</fpage>&#x02013;<lpage>36</lpage>.. <ext-link xlink:href="http://jmlr.org/papers/v18/16-305.html" ext-link-type="uri">http://jmlr.org/papers/v18/16-305.html</ext-link>.</mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Guido&#x000a0;Van</surname>&#x000a0;<given-names>R</given-names></string-name>, <collab>Python Development Team</collab></person-group>. <source>Python Tutorial: Release 3.6.4</source>. <year>2018</year>.1680921606. <ext-link xlink:href="https://dl.acm.org/doi/10.5555/3217518" ext-link-type="uri">https://dl.acm.org/doi/10.5555/3217518</ext-link>.</mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Hamdan</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Love</surname><given-names>B</given-names></string-name>, <string-name><surname>von&#x000a0;Polier</surname><given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Supporting code for &#x0201c;Confound-leakage: Confound Removal in Machine Learning Leads to Leakage.&#x0201d;</article-title>. <source>GitHub</source>. <year>2023</year>. <comment><ext-link xlink:href="https://github.com/juaml/ConfoundLeakage" ext-link-type="uri">https://github.com/juaml/ConfoundLeakage</ext-link></comment>.</mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Hamdan</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Love</surname><given-names>B</given-names></string-name>, <string-name><surname>von&#x000a0;Polier</surname><given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Supporting data for &#x0201c;Confound-leakage: Confound Removal in Machine Learning Leads to Leakage.&#x0201d;</article-title>. <comment>GigaScience Database. </comment><pub-id pub-id-type="doi">10.5524/102420</pub-id>.</mixed-citation></ref></ref-list></back></article>