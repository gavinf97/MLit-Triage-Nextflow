<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id><journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id><journal-id journal-id-type="publisher-id">nar</journal-id><journal-title-group><journal-title>Nucleic Acids Research</journal-title></journal-title-group><issn pub-type="ppub">0305-1048</issn><issn pub-type="epub">1362-4962</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6237755</article-id><article-id pub-id-type="pmid">30295871</article-id><article-id pub-id-type="doi">10.1093/nar/gky889</article-id><article-id pub-id-type="publisher-id">gky889</article-id><article-categories><subj-group subj-group-type="heading"><subject>Survey and Summary</subject></subj-group></article-categories><title-group><article-title>Multi-omic and multi-view clustering algorithms: review and cancer benchmark</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rappoport</surname><given-names>Nimrod</given-names></name><xref ref-type="aff" rid="AFF1"/></contrib><contrib contrib-type="author"><name><surname>Shamir</surname><given-names>Ron</given-names></name><!--<email>rshamir@tau.ac.il</email>--><xref ref-type="aff" rid="AFF1"/><xref ref-type="corresp" rid="COR1"/></contrib></contrib-group><aff id="AFF1">Blavatnik School of Computer Science, Tel Aviv University, Tel Aviv, Israel</aff><author-notes><corresp id="COR1">To whom correspondence should be addressed. Tel: +972 3 640 5383; Fax: +972 3 640 5384; Email: <email>rshamir@tau.ac.il</email></corresp></author-notes><pub-date pub-type="ppub"><day>16</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="epub" iso-8601-date="2018-10-08"><day>08</day><month>10</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>08</day><month>10</month><year>2018</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>46</volume><issue>20</issue><fpage>10546</fpage><lpage>10562</lpage><history><date date-type="accepted"><day>20</day><month>9</month><year>2018</year></date><date date-type="rev-recd"><day>17</day><month>9</month><year>2018</year></date><date date-type="received"><day>18</day><month>7</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018. Published by Oxford University Press on behalf of Nucleic Acids Research.</copyright-statement><copyright-year>2018</copyright-year><license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p></license></permissions><self-uri xlink:href="gky889.pdf"/><abstract><title>Abstract</title><p>Recent high throughput experimental methods have been used to collect large biomedical omics datasets. Clustering of single omic datasets has proven invaluable for biological and medical research. The decreasing cost and development of additional high throughput methods now enable measurement of multi-omic data. Clustering multi-omic data has the potential to reveal further systems-level insights, but raises computational and biological challenges. Here, we review algorithms for multi-omics clustering, and discuss key issues in applying these algorithms. Our review covers methods developed specifically for omic data as well as generic multi-view methods developed in the machine learning community for joint clustering of multiple data types. In addition, using cancer data from TCGA, we perform an extensive benchmark spanning ten different cancer types, providing the first systematic comparison of leading multi-omics and multi-view clustering algorithms. The results highlight key issues regarding the use of single- versus multi-omics, the choice of clustering strategy, the power of generic multi-view methods and the use of approximated p-values for gauging solution quality. Due to the growing use of multi-omics data, we expect these issues to be important for future progress in the field.</p></abstract><counts><page-count count="17"/></counts></article-meta></front><body><sec sec-type="intro" id="SEC1"><title>INTRODUCTION</title><p>Deep sequencing and other high throughput methods measure a large number of molecular parameters in a single experiment. The measured parameters include DNA genome sequence (<xref rid="B1" ref-type="bibr">1</xref>), RNA expression (<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B3" ref-type="bibr">3</xref>), DNA methylation (<xref rid="B4" ref-type="bibr">4</xref>) etc. Each such kind of data is termed &#x02018;omic&#x02019; (genomics, transcriptomics, methylomics, respectively). As costs decrease and technologies mature, larger and more diverse omic datasets are available.</p><p>Computational methods are imperative for analyzing such data. One fundamental analysis is clustering - finding coherent groups of samples in the data, such that samples within a group are similar, and samples in different groups are dissimilar (<xref rid="B5" ref-type="bibr">5</xref>). This analysis is often the first step done in data exploration. Clustering has many applications for biomedical research, such as discovering modules of co-regulated genes and finding subtypes of diseases in the context of precision medicine (<xref rid="B6" ref-type="bibr">6</xref>). Clustering is a highly researched computational problem, investigated by multiple scientific communities, and a myriad algorithms exist for this task.</p><p>While clustering each omic separately reveals patterns in the data, integrative clustering using several omics for the same set of samples has the potential to expose more fine-tuned structures that are not revealed by examining only a single data type. For example, cancer subtypes can be defined based on both gene expression and DNA methylation together. There are several reasons why a clustering based on multiple omics is desirable. First, Multi-omics clustering can reduce the effect of experimental and biological noise in the data. Second, different omics can reveal different cellular aspects, such as effects manifest at the genomic and epigenomic levels. Third, even within the same molecular aspect, each omic can contain data that are not present in other omics (e.g. mutation and copy number). Fourth, omics can represent data from different organismal levels, such as gene expression together with microbiome composition.</p><p>A problem akin to multi-omics clustering was investigated independently by the machine learning community, and is termed &#x02018;multi-view clustering&#x02019; (see (<xref rid="B7" ref-type="bibr">7</xref>) and &#x02018;A Survey on Multi-View Clustering&#x02019;). Multi-view clustering algorithms can be used to perform clustering of multi-omic data. In the past, methods developed within the machine learning community have proven useful in the analysis of biomedical datasets. However, by and large, multi-view clustering have not penetrated bioinformatics yet.</p><p>In this paper, we review methods for multi-omics clustering, and benchmark them on real cancer data. The data source is TCGA (The Cancer Genome Atlas) (<xref rid="B8" ref-type="bibr">8</xref>)&#x02014;a large multi-omic repository of data on thousands of cancer patients. We survey both multi-omics and multi-view methods, with the goal of exposing computational biologists to these algorithms. Throughout this review, we use the terms <italic>view</italic> and <italic>multi-view</italic> instead of omic and multi-omics in the context of Machine Learning algorithms.</p><p>Several recent reviews discussed multi-omics integration. (<xref rid="B9" ref-type="bibr">9&#x02013;11</xref>) review methods for multi-omics integration, and (<xref rid="B12" ref-type="bibr">12</xref>) review multi-omics clustering for cancer application. These reviews do not include a benchmark, and do not focus on multi-view clustering. (<xref rid="B13" ref-type="bibr">13</xref>) reviews only dimension reduction multi-omics methods. To the best of our knowledge, (<xref rid="B14" ref-type="bibr">14</xref>) is the only benchmark performed for multi-omics clustering, but it does not include machine learning methods. Furthermore, we believe the methods tested in the benchmark do not represent the current state of the art for multi omics clustering. Finally, (<xref rid="B7" ref-type="bibr">7</xref>) is a thorough review of multi-view methods, directed to the Machine Learning community. It does not discuss algorithms developed by the bioinformatics community, and does not cover biological applications.</p></sec><sec id="SEC2"><title>REVIEW OF MULTI-OMICS CLUSTERING METHODS</title><p>We divide the methods into several categories based on their algorithmic approach. <italic>Early integration</italic> is the most simple approach. It concatenates omic matrices to form a single matrix with features from multiple omics, and applies single-omic clustering algorithms on that matrix. In <italic>late integration</italic>, each omic is clustered separately and the clustering solutions are integrated to obtain a single clustering solution. Other approaches try to build a model that incorporates all omics, and are collectively termed <italic>intermediate integration</italic>. Those include: (i) methods that integrate sample similarities, (ii) methods that use joint dimension reduction for the different omics datasets and (iii) methods that use statistical modeling of the data.</p><p>The categories we present here are not clear-cut, and some of the algorithms presented fit into more than one category. For example, iCluster (<xref rid="B15" ref-type="bibr">15</xref>) is an early integration approach that also uses probabilistic modeling to project the data to a lower dimension. The algorithms are described in the categories where we consider them to fit most.</p><p>Multi-omics clustering algorithms can also be distinguishable by the set of omics that they support. <italic>General</italic> algorithms support any kind of omics data, and are therefore easily extendible to novel future omics. <italic>Omic specific</italic> algorithms are tailored to a specific combination of data types, and can therefore utilize known biological relationships (e.g. the correlation between copy number and expression). A mixture of these two approaches is to perform feature learning in an omic specific way, but then cluster those features using general algorithms. For example, one can replace a gene expression omic with an omic that scores expression in cellular pathways, and thus take advantage of existing biological knowledge.</p><p>Throughout this review, we use the following notation: a multi-omic dataset contains <italic>M</italic> omics. <italic>n</italic> is the number of samples (or patients for medical datasets), <italic>p</italic><sub><italic>m</italic></sub> is the number of features in the <italic>m</italic>&#x02019;th omics, and <italic>X</italic><sup><italic>m</italic></sup> is the <italic>n</italic> x <italic>p</italic><sub><italic>m</italic></sub> matrix with measurements from the <italic>m</italic>&#x02019;th omic. <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X_{ij}^{m}$\end{document}</tex-math></inline-formula> is therefore the value of the <italic>j</italic>&#x02019;th feature for the <italic>i</italic>&#x02019;th patient in the <italic>m</italic>&#x02019;th omic. <inline-formula><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$p = \Sigma _{m=1}^M{p_m}$\end{document}</tex-math></inline-formula> is the total number of features, and <italic>X</italic> is the <italic>n</italic> &#x000d7;&#x000a0;<italic>p</italic> matrix formed by the concatenation of all <italic>X</italic><sup><italic>m</italic></sup> matrices. Throughout the paper, for a matrix <italic>A</italic>, we use <italic>A</italic><sup><italic>t</italic></sup> to designate its transpose, and consistently with the <italic>X</italic><sup><italic>m</italic></sup> notation, we use <italic>A</italic><sup><italic>m</italic></sup> for matrix indexing (and not for matrix powering). Additional notation is chosen to follow the original publications and common conventions.</p><p>Figure&#x000a0;<xref ref-type="fig" rid="F1">1</xref> summarizes pictorially the different approaches to multi-omics clustering. A summary table of the methods reviewed here is given in Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref>.</p><fig id="F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Overview of multi-omics clustering approaches.</p></caption><graphic xlink:href="gky889fig1"/></fig><table-wrap id="tbl1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Multi-omic clustering methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Description</th><th rowspan="1" colspan="1">Refs.</th><th rowspan="1" colspan="1">Implementation</th></tr></thead><tbody><tr><td colspan="3" align="left" rowspan="1">
<bold>Early integration</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">LRAcluster<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">Data originate from low rank matrix, omic data distributions modeled based on it</td><td rowspan="1" colspan="1">(<xref rid="B16" ref-type="bibr">16</xref>)</td><td rowspan="1" colspan="1">R</td></tr><tr><td rowspan="1" colspan="1">Structured sparsity</td><td rowspan="1" colspan="1">Linear transformation projects data into a cluster membership orthogonal matrix</td><td rowspan="1" colspan="1">(<xref rid="B17" ref-type="bibr">17</xref>)</td><td rowspan="1" colspan="1">Matlab</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Alternate optimization</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">MV k-means, MV EM</td><td rowspan="1" colspan="1">Alternating <italic>k</italic>-means and EM. Each iteration is done w.r.t. a different view</td><td rowspan="1" colspan="1">(<xref rid="B18" ref-type="bibr">18</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Late integration</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">COCA</td><td rowspan="1" colspan="1">Per omic clustering solutions integrated with hierarchical clustering</td><td rowspan="1" colspan="1">(<xref rid="B19" ref-type="bibr">19</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td rowspan="1" colspan="1">Late fusion using latent models</td><td rowspan="1" colspan="1">Per omic clustering solutions integrated with PLSA</td><td rowspan="1" colspan="1">(<xref rid="B20" ref-type="bibr">20</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td rowspan="1" colspan="1">PINS<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">Integration of co-clustering patterns in different omics. The clusterings are based on perturbations to the data</td><td rowspan="1" colspan="1">(<xref rid="B21" ref-type="bibr">21</xref>)</td><td rowspan="1" colspan="1">R</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Similarity-based methods</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Spectral clustering generalizations</td><td rowspan="1" colspan="1">Generalizations of similarity based spectral clustering to multi-omics data</td><td rowspan="1" colspan="1">(<xref rid="B22" ref-type="bibr">22&#x02013;25</xref>)</td><td rowspan="1" colspan="1">Matlab</td></tr><tr><td rowspan="1" colspan="1">Spectral clustering with random walks</td><td rowspan="1" colspan="1">Generalizations of spectral clustering by random walks across similarity graphs</td><td rowspan="1" colspan="1">(<xref rid="B26" ref-type="bibr">26</xref>,<xref rid="B27" ref-type="bibr">27</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td rowspan="1" colspan="1">SNF<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">Integration of similarity networks by message passing</td><td rowspan="1" colspan="1">(<xref rid="B28" ref-type="bibr">28</xref>,<xref rid="B29" ref-type="bibr">29</xref>)</td><td rowspan="1" colspan="1">R, Matlab</td></tr><tr><td rowspan="1" colspan="1">rMKL-LPP<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">DR using multiple kernel learning; similarities maintained in lower dimension</td><td rowspan="1" colspan="1">(<xref rid="B30" ref-type="bibr">30</xref>)</td><td rowspan="1" colspan="1">**</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Dimension reduction</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">General DR framework</td><td rowspan="1" colspan="1">General framework for integration with DR</td><td rowspan="1" colspan="1">(<xref rid="B31" ref-type="bibr">31</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td rowspan="1" colspan="1">JIVE</td><td rowspan="1" colspan="1">Variation in data partitioned into joint and omic-specific</td><td rowspan="1" colspan="1">(<xref rid="B32" ref-type="bibr">32</xref>)</td><td rowspan="1" colspan="1">Matlab,R (<xref rid="B33" ref-type="bibr">33</xref>)</td></tr><tr><td rowspan="1" colspan="1">CCA<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">DR to axes of max correlation between datasets. Generalizations: Bayesian, kernels, &#x0003e;2 omics, sparse solutions, deep learning, count data</td><td rowspan="1" colspan="1">(<xref rid="B34" ref-type="bibr">34&#x02013;43</xref>), CCA for count data</td><td rowspan="1" colspan="1">R, two omics (<xref rid="B44" ref-type="bibr">44</xref>), R, multiple omics</td></tr><tr><td rowspan="1" colspan="1">PLS</td><td rowspan="1" colspan="1">DR to axes of max covariance between datasets. Generalizations: kernels, &#x0003e;2 omics, sparse solutions, partition into omic-specific and joint variation</td><td rowspan="1" colspan="1">(<xref rid="B45" ref-type="bibr">45&#x02013;52</xref>)</td><td rowspan="1" colspan="1">R, two omics, Matlab, multiple omics</td></tr><tr><td rowspan="1" colspan="1">MCIA</td><td rowspan="1" colspan="1">DR to axes of max covariance between multi-omic datasets</td><td rowspan="1" colspan="1">(<xref rid="B53" ref-type="bibr">53</xref>)</td><td rowspan="1" colspan="1">R</td></tr><tr><td rowspan="1" colspan="1">NMF generalizations<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">DR using generalizations of NMF to multi-omic data</td><td rowspan="1" colspan="1">(<xref rid="B54" ref-type="bibr">54&#x02013;57</xref>), EquiNMF, (<xref rid="B58" ref-type="bibr">58</xref>,<xref rid="B59" ref-type="bibr">59</xref>)</td><td rowspan="1" colspan="1">MultiNMF (Matlab)</td></tr><tr><td rowspan="1" colspan="1">Matrix tri- factorization</td><td rowspan="1" colspan="1">DR. Each omic describes the relationship between two entities</td><td rowspan="1" colspan="1">(<xref rid="B60" ref-type="bibr">60</xref>)</td><td rowspan="1" colspan="1">NA</td></tr><tr><td rowspan="1" colspan="1">Convex methods</td><td rowspan="1" colspan="1">DR with convex objective functions, allowing unique optimum and efficient computation</td><td rowspan="1" colspan="1">(<xref rid="B16" ref-type="bibr">16</xref>,<xref rid="B61" ref-type="bibr">61</xref>,<xref rid="B62" ref-type="bibr">62</xref>)</td><td rowspan="1" colspan="1">Matlab</td></tr><tr><td rowspan="1" colspan="1">Low-rank tensor MV clustering</td><td rowspan="1" colspan="1">Factorization based on low-rank tensors</td><td rowspan="1" colspan="1">(<xref rid="B63" ref-type="bibr">63</xref>)</td><td rowspan="1" colspan="1">Matlab</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Statistical methods</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">iCluster/Plus/Bayes<sup>&#x02022;</sup></td><td rowspan="1" colspan="1">Data originate from low dimensional representation, which determines the distribution of the observed data</td><td rowspan="1" colspan="1">(<xref rid="B15" ref-type="bibr">15</xref>,<xref rid="B64" ref-type="bibr">64</xref>,<xref rid="B65" ref-type="bibr">65</xref>)</td><td rowspan="1" colspan="1">R</td></tr><tr><td rowspan="1" colspan="1">PARADIGM</td><td rowspan="1" colspan="1">Probabilistic model of cellular pathways using factor graphs</td><td rowspan="1" colspan="1">(<xref rid="B66" ref-type="bibr">66</xref>)</td><td rowspan="1" colspan="1">REST API</td></tr><tr><td rowspan="1" colspan="1">Disagreement between clusters</td><td rowspan="1" colspan="1">Methods based mainly on hierarchical Dirichlet processes; clustering in different omics need not agree</td><td rowspan="1" colspan="1">(<xref rid="B67" ref-type="bibr">67&#x02013;71</xref>)</td><td rowspan="1" colspan="1">BCC (R)</td></tr><tr><td rowspan="1" colspan="1">Survival-based</td><td rowspan="1" colspan="1">Probabilistic model; patient survival data used in the clustering process</td><td rowspan="1" colspan="1">(<xref rid="B72" ref-type="bibr">72</xref>,<xref rid="B73" ref-type="bibr">73</xref>)</td><td rowspan="1" colspan="1">SBC (R)</td></tr><tr><td colspan="3" align="left" rowspan="1">
<bold>Deep learning</bold>
</td><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Deep learning methods</td><td rowspan="1" colspan="1">Neural networks used for integration. A variant of CCA, early integration and middle integration approaches</td><td rowspan="1" colspan="1">(<xref rid="B37" ref-type="bibr">37</xref>,<xref rid="B74" ref-type="bibr">74</xref>,<xref rid="B75" ref-type="bibr">75</xref>)</td><td rowspan="1" colspan="1">DeepCCA (Python)</td></tr></tbody></table><table-wrap-foot><fn id="T1TFN1"><p>DR: dimension reduction; EM: expectation maximization; MV: multi-view; PLSA: Probabilistic Latent Semantic Analysis; CCA: Canonical Correlation Analysis; PLS: Partial Least Squares; NMF: non-negative matrix factorization. <sup>&#x02022;</sup>Methods included in the benchmark. Single-omic <italic>k</italic>-means and spectral clustering were also included in the benchmark. ** Available from the authors upon request.</p></fn></table-wrap-foot></table-wrap><sec id="SEC2-1"><title>Early integration</title><p>Early integration is an approach that first concatenates all omic matrices, and then applies single-omic clustering algorithms on that concatenated matrix. It therefore enables the use of existing clustering algorithms. However, this approach has several drawbacks. First, without proper normalization, it may give more weight to omics with more features. Second, it does not consider the different distribution of data in the different omics. Finally, it increases the data dimension (the number of features), which is a challenge even in some single-omic datasets. When applying early integration algorithms designed specifically for multi-omics data, or when running single-omic methods on a concatenated matrix, these drawbacks must be addressed. Normalization of the features in different omics can assist in handling the different distributions, and feature selection can be used to decrease the dimension and to give different omics an equal prior opportunity to affect the results.</p><p>An additional way to handle the high dimension of the data is by using regularization, i.e.&#x000a0;adding additional constraints to a problem to avoid overfitting (<xref rid="B76" ref-type="bibr">76</xref>). Specifically, LASSO (Least Absolute Shrinkage and Selection Operator) regularization creates models where the number of features with non-zero effect on the model is low (<xref rid="B77" ref-type="bibr">77</xref>), and regularization of the nuclear norm is often used to induce data sparsity. Indeed, LASSO regularization is used by iCluster (<xref rid="B15" ref-type="bibr">15</xref>) (reviewed in a later section), and LRACluster uses nuclear norm regularization (reviewed in this section). While any clustering algorithm can be applied using early integration, we highlight here algorithms that were specifically developed for this task.</p><p>LRACluster (<xref rid="B16" ref-type="bibr">16</xref>) uses a probabilistic model, where numeric, count and binary features have distributions determined by a latent representation of the samples &#x00398;. For example, <inline-formula><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X_{ij}^{m}$\end{document}</tex-math></inline-formula> is distributed <inline-formula><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\propto exp(-\frac{1}{2}(X_{ij}^{m} - \Theta _{ij}^m)^2)$\end{document}</tex-math></inline-formula>, where &#x00398;<sup><italic>m</italic></sup> is of the same dimensions as <italic>X</italic><sup><italic>m</italic></sup>. The latent representation matrix is encouraged to be of low rank, by adding a regularization on its nuclear norm. The objective function for the algorithm is &#x02212;log&#x02009;(model'slikelihood) + &#x003bc; &#x000b7; |&#x00398;|<sub>*</sub> where &#x00398; is the concatenation of all &#x00398;<sup><italic>m</italic></sup> matrices, and | &#x000b7; |<sub>*</sub> is the nuclear norm. This objective is convex and provides a global optimal solution, which is found using a fast gradient-ascent algorithm. &#x00398; is subsequently clustered using <italic>k</italic>-means. This method was used to analyze pan-cancer TCGA data from eleven cancer types using four different omics, and to further find subtypes within these cancer types.</p><p>In (<xref rid="B17" ref-type="bibr">17</xref>), all omics are concatenated to a matrix <italic>X</italic> and the algorithm minimizes the following objective: <inline-formula><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$||XW + 1_nb^t -F||_2^2 + \gamma ||W||_{G_1}$\end{document}</tex-math></inline-formula>. <italic>W</italic> is a <italic>p</italic> x <italic>k</italic> projection matrix, <italic>F</italic> is an <italic>n</italic> x <italic>k</italic> cluster indicator matrix such that <italic>F</italic><sup><italic>t</italic></sup><italic>F</italic> = <italic>I</italic><sub><italic>k</italic></sub>, 1<sub><italic>n</italic></sub> is a column vector of length <italic>n</italic> of 1&#x02019;s, <italic>b</italic> is an intercept column vector of dimension <italic>k</italic> and &#x003b3; is a scalar. The algorithm therefore seeks a linear transformation such the projected data are as close to a cluster indicator matrix as possible. That indicator matrix is subsequently used for clustering. The regularization term uses the <italic>G</italic><sub>1</sub> norm, which is the <italic>l</italic><sub>2</sub> norm for <italic>W</italic> entries associated with a specific cluster and view, summed over all views and clusters. Therefore, features that do not contribute to the structure of a cluster will be assigned with low coefficients in <italic>W</italic>.</p></sec><sec id="SEC2-2"><title>Alternate optimization</title><p>Early research for integration of two views was performed in (<xref rid="B78" ref-type="bibr">78</xref>). This work improved classification accuracy for semi-supervised data with two views using an approach termed co-training, and inspired others to analyze multi-view data. One of the first attempts to perform multi-view clustering was (<xref rid="B18" ref-type="bibr">18</xref>). In this work, EM (expectation maximization) and <italic>k</italic>-means, which are widely used single-omic clustering algorithms, were adapted for multi-view clustering. Both EM and <italic>k</italic>-means are iterative algorithms, where each iteration improves the objective function value. The suggested multi-view versions perform optimization in each iteration with respect to a different omic in an alternating manner. This approach loses theoretical guarantees for convergence, but was found to outperform algorithms that use each view separately, and also naive early integration methods that cluster the concatenated matrix of the two views. Interestingly, (<xref rid="B18" ref-type="bibr">18</xref>) report improved results using the multi-view clustering algorithms on single-view datasets that were randomly split to simulate multi-view data. This was the first evidence for improved clustering using multiple views, and for the utility of a multi-view algorithm in clustering single-view data. While this work was very influential, other preliminary multi-view clustering methods (e.g. (<xref rid="B22" ref-type="bibr">22</xref>,<xref rid="B31" ref-type="bibr">31</xref>)) were since shown to achieve better results on datasets where the gold standard is known.</p></sec><sec id="SEC2-3"><title>Late integration</title><p>Late integration is an approach that allows to use existing single-omic clustering algorithms on single-omic data. First, each omic is clustered separately using a single-omic algorithm. Different algorithms can be used for each omic. Then, the different clusterings are integrated. The strength of late integration lies in that any clustering algorithm can be used for each omic. Algorithms that are known to work well on a particular omic can therefore be used, without having to create a model that unifies all of these algorithms. However, by utilizing only clustering solutions in the integration phase we can lose signals that are weak in each omic separately.</p><p>COCA (<xref rid="B19" ref-type="bibr">19</xref>) was applied to pan-cancer TCGA data, to investigate how tumors from different tissues cluster, and whether the obtained clusters match the tissue of origin. The algorithm first clusters each omic separately, such that the <italic>m</italic>&#x02019;th omic has <italic>c</italic><sub><italic>m</italic></sub> clusters. The clustering of sample <italic>i</italic> for omic <italic>m</italic> is encoded in a binary vector <italic>v</italic><sub><italic>im</italic></sub> of length <italic>c</italic><sub><italic>m</italic></sub>, where <italic>v</italic><sub><italic>im</italic></sub>(<italic>j</italic>) = 1 if <italic>i</italic> belongs to cluster <italic>j</italic> and 0 otherwise. The concatenation of the <italic>v</italic><sub><italic>im</italic></sub> vectors across all omics results in a binary cluster indicator vector for sample <italic>i</italic>. The <italic>n</italic> &#x000d7;&#x000a0;<italic>c</italic> binary matrix <italic>B</italic> of these indicator vectors, where <inline-formula><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$c = \Sigma _{i=1}^M{c_m}$\end{document}</tex-math></inline-formula>, is used as input to consensus clustering (<xref rid="B79" ref-type="bibr">79</xref>) to obtain the final clustering of the samples. Alternatively, in (<xref rid="B20" ref-type="bibr">20</xref>) a model based on Probabilistic Latent Semantic Analysis (<xref rid="B80" ref-type="bibr">80</xref>) was proposed for clustering <italic>B</italic>. These two methods allow any clustering algorithm to be used on each single omic, and therefore have an advantage when a method is known to perform well for a particular omic. Additionally, they can be used given the clustering solution only when the raw omic data are unavailable.</p><p>PINS (<xref rid="B21" ref-type="bibr">21</xref>) integrates clusters by examining their connectivity matrices for the different omics. Each such matrix <italic>S</italic><sup><italic>m</italic></sup> is a binary <italic>n</italic> x <italic>n</italic> matrix, where <inline-formula><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$S^m_{ij} = 1$\end{document}</tex-math></inline-formula> if patients <italic>i</italic> and <italic>j</italic> are clustered together in omic <italic>m</italic>, and 0 otherwise. These <italic>S</italic><sup><italic>m</italic></sup> matrices are averaged to obtain a single connectivity matrix, which is then clustered using different methods based on whether the different <italic>S</italic><sup><italic>m</italic></sup> matrices highly agree with each other or not. The obtained clusters are tested if they can be further split into smaller clusters. To determine the number of clusters for each omic and for the integrated clustering, perturbations are performed on the data by adding Gaussian noise to it, and the number of clusters is chosen such that the resulting clustering is robust to the perturbations. Unlike the previously presented late integration methods, PINS requires the original data and not only the clustering of each omic, since it performs perturbations to the data.</p><p>Several methods for ensemble clustering were developed over the years, and are reviewed in (<xref rid="B81" ref-type="bibr">81</xref>). While these were not originally developed for this purpose, they can be used for late multi-omics clustering as well.</p></sec><sec id="SEC2-4"><title>Similarity-based methods</title><p>Similarity-based methods use similarities or distances between samples in order to cluster data. These methods compute the similarities between samples in each omic separately, and vary in the way these similarities are integrated. The integration step uses only similarity values. Since in current multi-omic datasets, the number of samples is much smaller than the number of features, these algorithms are usually faster than methods that consider all features while performing integration. However, in such methods it may be more difficult to interpret the output in terms of the original features. An additional advantage of similarity-based methods is that they can easily support diverse omic types, including categorical and ordinal data. Each omic only requires a definition of a similarity measure.</p><sec id="SEC2-4-1"><title>Spectral clustering generalizations</title><p>Spectral clustering (<xref rid="B82" ref-type="bibr">82</xref>) is a widely used similarity-based method for clustering single-view data. The objective function for single-view spectral clustering is <italic>max</italic><sub><italic>U</italic></sub><italic>trace</italic>(<italic>U</italic><sup><italic>t</italic></sup><italic>LU</italic>) s.t. <italic>U</italic><sup><italic>t</italic></sup><italic>U</italic> = <italic>I</italic>, where <italic>L</italic> is the Laplacian (<xref rid="B83" ref-type="bibr">83</xref>) of the similarity matrix of dimension <italic>n</italic> &#x000d7; <italic>n</italic>, and <italic>U</italic> is of dimension <italic>n</italic> &#x000d7; <italic>k</italic>, where <italic>k</italic> is the number of clusters in the data. Intuitively, it means that samples that are similar to one another have similar row vectors in <italic>U</italic>. This problem is solved by taking the <italic>k</italic> first eigenvectors of <italic>L</italic> (details vary between versions that use the normalized and the unnormalized graph Laplacian), and clustering them with a simple algorithm such as <italic>k</italic>-means. The spectral clustering objective was shown to be a relaxation of the discrete normalized cut in a graph, providing an intuitive explanation for the clustering. Several multi-view clustering algorithms are generalizations of spectral clustering.</p><p>An early extension to two views performs clustering by computing a new similarity matrix, using the two views&#x02019; similarities (<xref rid="B22" ref-type="bibr">22</xref>). Denote by <italic>W</italic><sub>1</sub> and <italic>W</italic><sub>2</sub> the similarity matrices for the two views. Then the integrated similarity, <italic>W</italic>, is defined as <italic>W</italic><sub>1</sub><italic>W</italic><sub>2</sub>. Spectral clustering is performed on the block matrix
<disp-formula><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} {\left[\begin{array}{cc}0 &#x00026; W \\ W^t &#x00026; 0 \end{array}\right]} \end{equation*}\end{document}</tex-math></disp-formula>Note that each eigenvector for this matrix is of length 2<italic>n</italic>. Either half of the vector or an average of the two halves are used instead of the whole eigenvectors for clustering using k-means. Note that this method is limited in that it only supports two views.</p><p>(<xref rid="B23" ref-type="bibr">23</xref>) generalizes spectral clustering for more than two views. Instead of finding a global <italic>U</italic> matrix, a matrix <italic>U</italic><sup><italic>m</italic></sup> is defined for each omic. The optimization problem is:
<disp-formula><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{eqnarray*} &#x00026;&#x00026; max_{U^1, \ldots , U^M}{\Sigma _m{trace({U^m}^t L^m {U^m})}} \nonumber \\ &#x00026;&#x00026; + \lambda \cdot \text{Reg} \; \; \; \text{ s.t. } \forall m \; {U^m}^t U^m = I. \end{eqnarray*}\end{document}</tex-math></disp-formula><italic>L</italic><sup><italic>m</italic></sup> is the graph Laplacian for omic <italic>m</italic> and Reg is a regularization term equal to either <inline-formula><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Sigma _{m_1 \ne m_2}{U^{m_1} {U^{m_1}}^t U^{m_2} {U^{m_2}}^t}$\end{document}</tex-math></inline-formula> or &#x003a3;<sub><italic>m</italic></sub><italic>U</italic><sup><italic>m</italic></sup><italic>U</italic><sup><italic>mt</italic></sup><italic>U</italic>*<italic>U</italic>*<sup><italic>t</italic></sup> with the additional constraint that <italic>U</italic>* is an <italic>n</italic> x <italic>k</italic> matrix such that <italic>U</italic>*<sup><italic>t</italic></sup><italic>U</italic>* = <italic>I</italic>.</p><p>Chikhi (<xref rid="B24" ref-type="bibr">24</xref>) uses a different formulation, which does not require a different <italic>U</italic><sup><italic>m</italic></sup> for each omic, but instead uses the same <italic>U</italic> for all matrices. The following objective function is used:
<disp-formula><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} max_{U}{\Sigma _m{trace({U}^t L^m {U})}} \text{ s.t. } U^t U = I \end{equation*}\end{document}</tex-math></disp-formula>This is equivalent to performing spectral clustering on the Laplacian &#x003a3;<sub><italic>m</italic></sub><italic>L</italic><sup><italic>m</italic></sup>. The obtained clusters are then further improved in a greedy manner, by changing the assignment of samples to clusters, while looking directly at the discrete normalized cut objective, rather than the continuous spectral clustering objective.</p><p>Li (<xref rid="B25" ref-type="bibr">25</xref>) suggests a runtime improvement over (<xref rid="B23" ref-type="bibr">23</xref>). Instead of looking at the similarity matrix for all the samples, a small set of &#x02018;representative&#x02019; vectors, termed salient points, are calculated by running k-means on the concatenation of all omics and selecting the cluster centers. A similarity matrix is then computed between all these samples in the data and their <italic>s</italic> nearest salient points. Denote this similarity matrix for the <italic>m</italic>&#x02019;th omic by <italic>W</italic><sup><italic>m</italic></sup>, and let <italic>Z</italic><sup><italic>m</italic></sup> be its normalization such that rows sum to 1. These matrices are of dimension <italic>n</italic> &#x000d7; the number of salient points. Next, the matrices
<disp-formula><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} {\left[\begin{array}{cc}0 &#x00026; Z^m \\ {Z^m}^t &#x00026; 0 \end{array}\right]} \end{equation*}\end{document}</tex-math></disp-formula>are given as input to an algorithm with the same objective as (<xref rid="B24" ref-type="bibr">24</xref>). This way, similarities are not computed between all pairs of samples.</p><p>The methods above differ in several ways. (<xref rid="B23" ref-type="bibr">23</xref>) allows each omic to have a different low dimensional representation, and has a parameter that controls the trade-off between how similar these representations are, and how similarities in the original data are maintained in <italic>U</italic><sup><italic>m</italic></sup>. Therefore, it allows to express cases where the omics are not assumed to have the same similarity structure (e.g., two samples can be similar in one omic but different in another). On the other hand, Chikhi (<xref rid="B24" ref-type="bibr">24</xref>) assumes the same similarity structure, and its greedy optimization step can result in an improved solution in such cases. (<xref rid="B25" ref-type="bibr">25</xref>) can be used when the number of samples is exceptionally large.</p><p>Zhou and Burges (<xref rid="B26" ref-type="bibr">26</xref>) views similarity matrices as networks, and examines random walks on these networks. Random walks define a stationary distribution on each network, which captures its similarity patterns (<xref rid="B84" ref-type="bibr">84</xref>). Since that stationary distribution is less noisy than the original similarity measures, Zhou and Burges&#x000a0;(<xref rid="B26" ref-type="bibr">26</xref>) uses them instead to integrate the networks. Xia&#x000a0;(<xref rid="B27" ref-type="bibr">27</xref>) also examines random walks on the networks, but argues that the stationary distribution in each network can still be noisy. Instead, the authors compute a consensus transition matrix, that has minimum total distance to the per-omic transition matrices and is of minimal rank. Random walks are highly related to spectral clustering; using a normalized variant of the graph&#x02019;s Laplacian in spectral clustering results in a solution in which random walks seldom cross between clusters (<xref rid="B82" ref-type="bibr">82</xref>). These random walk-based methods are currently competitive with other spectral clustering methods.</p></sec><sec id="SEC2-4-2"><title>Similarity Network Fusion</title><p>SNF (Similarity Network Fusion) first constructs a similarity network for every omic separately (<xref rid="B28" ref-type="bibr">28</xref>,<xref rid="B29" ref-type="bibr">29</xref>). In each such network, the nodes are samples, and the edge weights measure the sample similarity. The networks are then fused together using an iterative procedure based on message passing (<xref rid="B85" ref-type="bibr">85</xref>). The similarity between samples is propagated between each node and its k nearest neighbors.</p><p>More formally, denote by <italic>W</italic><sup>(<italic>m</italic>)</sup> the similarity matrix for the <italic>m</italic>&#x02019;th omic. Initially a transition probability matrix between all samples is defined by:
<disp-formula><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} P_1^{(m)}(i,j) = \left\lbrace \begin{array}{@{}l@{\quad }l@{}}\frac{W^{(m)}(i, j)}{{ 2}\Sigma _{k \ne i}{W^{(m)}(i, k)}}, &#x00026; j \ne i \\ \frac{1}{2}, &#x00026; j = i \end{array}\right. \end{equation*}\end{document}</tex-math></disp-formula>and a transition porbability matrix between nearest neighbors is defined by:
<disp-formula><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} S^{(m)}(i,j) = \left\lbrace \begin{array}{@{}l@{\quad }l@{}}\frac{W^{(m)}(i, j)}{\Sigma _{k \ne i}{W^{(m)}(i, k)}}, &#x00026; j \in N_i\\ 0, &#x00026; \text{otherwise} \end{array}\right. \end{equation*}\end{document}</tex-math></disp-formula>
where <italic>N</italic><sub><italic>i</italic></sub> are <italic>i</italic>&#x02019;s k nearest neighbors in the input <italic>X</italic><sup><italic>m</italic></sup> matrices. The <italic>P</italic> matrices are updated iteratively using message passing between the nearest neighbors: <inline-formula><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$P_{q+1}^{(m)} = S^{(m)} \frac{\Sigma _{k \ne m}{P_q^{(k)}}}{M-1} {S^{(m)}}^q$\end{document}</tex-math></inline-formula> where <inline-formula><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$P_{q}^{(m)}$\end{document}</tex-math></inline-formula> is the matrix for omic <italic>m</italic> at iteration <italic>q</italic>. This process converges to a single similarity network, summarizing the similarity between samples across all omics. This network is partitioned using spectral clustering.</p><p>In (<xref rid="B29" ref-type="bibr">29</xref>), SNF is used on gene expression, methylation and miRNA expression data for several cancer subtypes from TCGA. In addition to partitioning the graph to obtain cancer sutbypes, the authors show that the fused network can be used for other computational tasks. For example, they show how to fit Cox proportional hazards (<xref rid="B86" ref-type="bibr">86</xref>), a model that predicts prognosis of patients, with a constraint such that similar patients in the integrated network will have similar predicted prognosis.</p></sec><sec id="SEC2-4-3"><title>rMKL-LPP</title><p>Kernel functions implicitly map samples to a high (possibly infinite) dimension, and can efficiently measure similarity between the samples in that dimension. Multiple kernel learning uses several kernels (similarity measures), usually by linearly combining them, and is often used in supervised analysis. (<xref rid="B30" ref-type="bibr">30</xref>) developed rMKL-LPP (regularized Multiple Kernel Learning with Locality Preserving Projections), which uses multiple kernel learning in unsupervised settings. The algorithm performs dimension reduction on the input omics such that similarities (defined using multiple kernels) between each sample and its nearest neighbors are maintained in low dimension. This representation is subsequently clustered with k-means. rMKL-LPP allows the use of diverse kernel functions, and even multiple kernels per omic. A regularization term is added to the optimization problem to avoid overfitting. The authors run the algorithm on five cancer types from TCGA, and show that using multiple kernels per omic improves the prognostic value of the clustering, and that regularization improves robustness.</p></sec></sec><sec id="SEC2-5"><title>Dimension reduction-based methods</title><p>Dimension reduction-based methods assume the data have an intrinsic low dimensional representation, with that low dimension often corresponding to the number of clusters. The views that we observe are all transformations of that low dimensional data to a higher dimension, and the parameters for the transformation differ between views. This general formulation was proposed by (<xref rid="B31" ref-type="bibr">31</xref>), which suggest to minimize <inline-formula><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Sigma _{m=1}^M{w_ml(X^{m}, f_m(B))}$\end{document}</tex-math></inline-formula>, where <italic>B</italic> is a matrix of dimension <italic>n</italic> &#x000d7;&#x000a0;<italic>p, f</italic><sub><italic>m</italic></sub> are the parametrized transformations, and <italic>w</italic><sub><italic>m</italic></sub> are weights for the different views, and <italic>l</italic> is a loss function. The work further provides an optimization algorithm when the <italic>f</italic><sub><italic>m</italic></sub> transformations are given by matrix multiplication. That is, <italic>f</italic><sub><italic>m</italic></sub>(<italic>B</italic>) = <italic>BP</italic><sup><italic>m</italic></sup>, and <italic>l</italic> is the squared Frobenius norm applied to <italic>X</italic><sup><italic>m</italic></sup> &#x02212; <italic>BP</italic><sup><italic>m</italic></sup>. Once <italic>B</italic> is calculated, single-omic clustering algorithm can be applied to it. This general framework is widely used. Since the transformation is often assumed to be linear, many of the dimension reduction methods are based on matrix factorization. Dimension reduction methods work with real-valued data. Applying these methods to discrete binary or count data is technically possible but often inappropriate.</p><p>An advantage of linear dimension reduction methods is that they provide some interpretation for the dominant features in each cluster. For example, in the general framework just presented, each entry in the <italic>P</italic><sup><italic>m</italic></sup> matrix can be considered as the weight of a feature in a cluster. Such interpretation is missing from similarity-based methods, which ignore the original features once the similarities between samples were calculated. Therefore, dimension reduction methods may be useful when an association between clusters and features is needed.</p><sec id="SEC2-5-1"><title>JIVE</title><p>(<xref rid="B32" ref-type="bibr">32</xref>) assumes that the variation in each omic can be partitioned to a variation that is joint between all omics, and an omic-specific variation: <italic>X</italic><sup><italic>m</italic><sup><italic>t</italic></sup></sup> = <italic>J</italic><sup><italic>m</italic></sup> + <italic>A</italic><sup><italic>m</italic></sup> + <italic>E</italic><sup><italic>m</italic></sup> where <italic>E</italic><sup><italic>m</italic></sup> are error terms. Let <italic>J</italic> and <italic>A</italic> be the concatenated <italic>J</italic><sup><italic>m</italic></sup> and <italic>A</italic><sup><italic>m</italic></sup> matrices, respectively. The model assumes that <italic>JA</italic><sup><italic>t</italic></sup> = 0, that is, the joint and omic specific variations are uncorrelated, and that <italic>rank</italic>(<italic>J</italic>) = <italic>r</italic> and <italic>rank</italic>(<italic>A</italic><sub><italic>i</italic></sub>) = <italic>r</italic><sub><italic>i</italic></sub> for each omic, so that the structure of each omic and the total joint variation are of low rank. In order for the weight of the different omics to be equal, the input omic matrices are normalized to have equal Frobenius norm. A penalty term is added to encourage variable sparsity. This method was applied to gene expression and miRNA data of Glioblastoma Multiforme brain tumors, and identified the joint variation between these omics.</p></sec><sec id="SEC2-5-2"><title>Correlation and covariance-based</title><p>Two of the most widely used dimension reduction methods are Canonical Correlation Analysis (CCA) (<xref rid="B34" ref-type="bibr">34</xref>) and Partial Least Squares (PLS) (<xref rid="B45" ref-type="bibr">45</xref>). Given two omics <italic>X</italic><sup>1</sup> and <italic>X</italic><sup>2</sup>, in CCA the goal is to find two projection vectors <italic>u</italic><sup>1</sup> and <italic>u</italic><sup>2</sup> of dimensions <italic>p</italic><sub>1</sub> and <italic>p</italic><sub>2</sub>, such that the projected data has maximum correlation:
<disp-formula><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} argmax_{u^1, u^2}{corr(X^1u^1, X^2u^2)} \end{equation*}\end{document}</tex-math></disp-formula>These projections are called the first canonical variates, and are the axis with maximal correlation between the omics. The k&#x02019;th pair of canonical variates, <inline-formula><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u^1_k$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u^2_k$\end{document}</tex-math></inline-formula> are found such that correlation between <inline-formula><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X^1u^1_k$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$X^2u^2_k$\end{document}</tex-math></inline-formula> is maximal, given that the new pair is uncorrelated (that is, orthogonal) to the previous canonical variates. Chaudhuri <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B87" ref-type="bibr">87</xref>) proved and showed empirically that if the data originate from normal or log concave distributions, the canonical variates can be used to cluster the data. CCA was formulated in a probabilistic framework such that the optimization solutions are maximum likelihood estimates (<xref rid="B88" ref-type="bibr">88</xref>), and further extended to a Bayesian framework (<xref rid="B35" ref-type="bibr">35</xref>). An additional expansion to perform CCA in high dimension is Kernel CCA (<xref rid="B36" ref-type="bibr">36</xref>). A deep-learning based CCA method, DeepCCA, was recently developed (<xref rid="B37" ref-type="bibr">37</xref>). Rather than maximize the correlation between linear projections of the data, the projections are taken to be functions of the data calculated using neural networks, and the optimization process optimizes the parameters for these networks.</p><p>Solving CCA requires inversion of the covariance matrix for the two omics. Omics data usually have a higher number of features than samples, and these matrices are therefore not invertible. To apply CCA to omics data, and to increase the interpretability of CCA&#x02019;s results, sparsity regularization was added (<xref rid="B38" ref-type="bibr">38</xref>,<xref rid="B39" ref-type="bibr">39</xref>).</p><p>CCA supports only two views. Several works extend it to more than two views, including MCCA (<xref rid="B39" ref-type="bibr">39</xref>) which maximizes the sum of pairwise correlations between projections and CCA-RLS (<xref rid="B40" ref-type="bibr">40</xref>). Luo <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B41" ref-type="bibr">41</xref>) generalize CCA to tensors in order to support more than two views.</p><p>Another line of work on CCA, with high relevance for omics data, investigated relationships between the features while performing the dimension reduction. ssCCA (structure constrained sparse CCA) allows to incorporate into the model known relationships between features in one of the input omics, and force entries in the <italic>u</italic><sup><italic>i</italic></sup> vector for that view to be close for similar features. This model has been developed by (<xref rid="B42" ref-type="bibr">42</xref>) and utilized microbiome&#x02019;s phylogenies as the feature structure. Another model that considers relationship between features was developed in (<xref rid="B43" ref-type="bibr">43</xref>). In this work, rather than defining similarities between features, they are partitioned into groups. Regularization is performed such that both irrelevant groups and irrelevant features within relevant groups are removed from the model. Finally, Podosinnikova <italic>et&#x000a0;al</italic>., in &#x02018;Beyond CCA: Moment matching for multi-view models&#x02019;, extended CCA to support count data, which are common in biological datasets.</p><p>PLS also follows a linear dimension reduction model, but maximizes the covariance between the projections, rather than the correlation. More formally, given two omics <italic>X</italic><sup>1</sup> and <italic>X</italic><sup>2</sup>, PLS computes a sequence of vectors <inline-formula><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u_k^1$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u_k^2$\end{document}</tex-math></inline-formula> for <italic>k</italic> = 1, 2, &#x02026; such that <inline-formula><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$cov(X^1 u_k^1, X^2 u_k^2)$\end{document}</tex-math></inline-formula> is maximal, given that <inline-formula><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${u_k^1}^tu_k^1=1$\end{document}</tex-math></inline-formula>, <inline-formula><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${u_k^2}^tu_k^2=1$\end{document}</tex-math></inline-formula>, and <inline-formula><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$cor(X^1 u_k^1, X^1 u_l^1) = 0$\end{document}</tex-math></inline-formula> for <italic>l</italic> &#x0003c; <italic>k</italic>. That is, new projections are not correlated with previous ones. PLS can be applied to data with more features than samples even without sparsity constraints. A sparse solution is nonetheless desirable, and one was developed (<xref rid="B46" ref-type="bibr">46</xref>,<xref rid="B47" ref-type="bibr">47</xref>). O2-PLS increases the interpretability of PLS by partitioning the variation in the datasets into joint variation between them, and variations that are specific for each dataset and that are not correlated with one another (<xref rid="B48" ref-type="bibr">48</xref>). While PLS and O2-PLS were originally developed for chemometrics, they were recently used for omics data as well (<xref rid="B89" ref-type="bibr">89</xref>,<xref rid="B90" ref-type="bibr">90</xref>). PLS was also extended to use the kernel framework (<xref rid="B49" ref-type="bibr">49</xref>), and a combined version of kernel PLS and O2 PLS was developed (<xref rid="B50" ref-type="bibr">50</xref>).</p><p>Like CCA, PLS was developed for two omics. MBPLS (Multi Block PLS) extends the model to more than two omics (<xref rid="B91" ref-type="bibr">91</xref>), and sMBPLS adds sparsity constraints. sMBPLS was developed specifically for omics data (<xref rid="B51" ref-type="bibr">51</xref>). It looks for a linear combination of projections of non-gene-expression omics that has maximal correlation with a projection of gene expression omic. An extension of O2PLS also exists for multi-view datasets (<xref rid="B52" ref-type="bibr">52</xref>).</p><p>Both CCA and PLS can be used in cases where high interpretability is wanted. The different <inline-formula><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u^1_k$\end{document}</tex-math></inline-formula> and <inline-formula><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$u^2_k$\end{document}</tex-math></inline-formula> vector pairs are those along which the correlation (or covariance) between patients is maximal. They can therefore be used to associate between features from the different views.</p><p>An additional method that is based on maximizing covariance in low dimension is MCIA (<xref rid="B53" ref-type="bibr">53</xref>), an extension of co-inertia analysis to more than two omics (<xref rid="B92" ref-type="bibr">92</xref>). It aims to find projections for all the omics such that the sum of squared covariances with a global variation axis is maximal: <inline-formula><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$max_{u^m, v}{\Sigma _{m=1}^M{cov^2(X^m u^m, v)}}$\end{document}</tex-math></inline-formula>. The projections of different omics can be used to evaluate the agreement between the different omics (the distance between projections reflects the level of disagreement between omics). Each of the projections can be used as a representation for clustering.</p></sec><sec id="SEC2-5-3"><title>Non-negative Matrix Factorization</title><p>Non-negative Matrix Factorization (NMF) assumes that the data have an intrinsic low dimensional non-negative representation, and that a nonnegative matrix projects it to the observed omic (<xref rid="B93" ref-type="bibr">93</xref>). It is therefore only suitable for non-negative data. For a single omic, denote by <italic>k</italic> the low dimension. The formulation is <italic>X</italic> &#x02248; <italic>WH</italic>, where <italic>X</italic> is the <italic>n</italic> &#x000d7;&#x000a0;<italic>p</italic> observed omic matrix, <italic>W</italic> is <italic>n</italic> &#x000d7;&#x000a0;<italic>k</italic> and <italic>H</italic> is <italic>k</italic> &#x000d7; <italic>p</italic>. The objective function is <inline-formula><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$||X - WH||_2^2$\end{document}</tex-math></inline-formula>, and it is minimized by updating <italic>W</italic> and <italic>H</italic> in an alternating manner, using multiplicative update rules, such that solutions remain non negative after each update (<xref rid="B94" ref-type="bibr">94</xref>). The low dimension representation <italic>W</italic> can be clustered using a simple single-omic algorithm. Like other dimension reduction methods, the <italic>W</italic> and <italic>H</italic> matrices can be used to better understand the weight of each feature in each cluster. The non-negativity constraint makes this weight more interpretable.</p><p>Several methods generalize this model to multi-omic data. MultiNMF (<xref rid="B54" ref-type="bibr">54</xref>) uses the following generalization: Each omic <italic>X</italic><sup><italic>m</italic></sup> is factorized into <italic>W</italic><sup><italic>m</italic></sup><italic>H</italic><sup><italic>m</italic></sup>. This model is equivalent to performing NMF on each omic separately. Integration between the omics is done by adding a constraint that the <italic>W</italic><sup><italic>m</italic></sup> matrices are close to a &#x02018;consensus&#x02019; matrix <italic>W</italic>*. The objective function is therefore: <inline-formula><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Sigma _{m=1}^M{||X^{m} - W^m H^m||_2^2} + \lambda \Sigma _{m=1}^M{||W^m - W^*||_2^2}$\end{document}</tex-math></inline-formula>. Kalayeh <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B55" ref-type="bibr">55</xref>) generalizes this method to support weights for features&#x02019; and samples&#x02019; similarity. (<xref rid="B56" ref-type="bibr">56</xref>) extend MultiNMF by further requiring that the low dimensional representation <italic>W</italic>* maintains similarities between samples (samples that are close in the original dimension must be close in <italic>W</italic>*). This approach combines factorization and similarity-based methods.</p><p>Joint NMF (<xref rid="B57" ref-type="bibr">57</xref>) uses a different formulation, where a sample has the same low dimensional representation for all omics: <italic>X</italic><sup><italic>m</italic></sup> &#x02248; <italic>WH</italic><sup><italic>m</italic></sup>. Note that by writing <italic>X</italic> = <italic>WH</italic> where <italic>X</italic> and <italic>H</italic> are obtained by matrix concatenation, this model is equivalent to early integration. Joint NMF is not directly used for clustering. Rather, the data are reduced to a large dimension (<italic>k</italic> = 200) and high values in <italic>W</italic> and <italic>H</italic><sup><italic>m</italic></sup> are used to associate samples and features with modules that are termed &#x02018;md-modules&#x02019;. The authors applied Joint NMF on miRNA, gene expression and methylation data from ovarian cancer patients, and showed that functional enrichment among features that are associated with md-modules that is more significant than the enrichment obtained in single-omic modules. In addition, patients in certain modules have significantly different prognosis compared to the rest of the patients. Much like (<xref rid="B56" ref-type="bibr">56</xref>) extends multiNMF, EquiNMF extends Joint NMF such that similarities in the original omics are maintained in lower dimension. (<xref rid="B58" ref-type="bibr">58</xref>) extends NMF to the case where different views can contain different samples, but constrains certain samples from different views to belong to the same cluster based on prior knowledge. Finally, PVC (<xref rid="B59" ref-type="bibr">59</xref>) performs partial multi-view clustering. In this setting, not all samples necessarily have measurements for all views.</p><p>The difference between MultiNMf and Joint NMF resembles the difference described previously between similarity-based methods. MultiNMF allows for different omics to have different representations, where the similarity between them is controlled by a parameter. It can therefore be used in cases where the different omics are not expected to have the same low dimensional representation.</p></sec><sec id="SEC2-5-4"><title>Matrix tri-factorization</title><p>An alternative factorization approach presented in (<xref rid="B60" ref-type="bibr">60</xref>) is tri-matrix factorization. In this framework, each input omic is viewed as describing a relationship between two entities, which are its rows and columns. For example, in a dataset with two omics, gene expression and DNA methylation of patients, there are three entities which are the patients, the genes and the CpG loci. The gene expression matrix describes a relationship between patients and genes, while the methylation matrix describes a relationship between patients and CpG loci.</p><p>Each omic matrix <italic>R</italic><sub><italic>ij</italic></sub> of dimension <italic>n</italic><sub><italic>i</italic></sub> &#x000d7; <italic>n</italic><sub><italic>j</italic></sub> that describes the relationship between entities <italic>i</italic> and <italic>j</italic> is factorized as <inline-formula><tex-math id="M34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$R_{ij} = G_iS_{ij}G^t_j$\end{document}</tex-math></inline-formula>, where <italic>G</italic><sub><italic>i</italic></sub> and <italic>G</italic><sub><italic>j</italic></sub> provide a low dimensional representation for entities <italic>i</italic> and <italic>j</italic> respectively and are of dimensions <italic>n</italic><sub><italic>i</italic></sub> &#x000d7; <italic>k</italic><sub><italic>i</italic></sub> and <italic>n</italic><sub><italic>j</italic></sub> &#x000d7; <italic>k</italic><sub><italic>j</italic></sub>, and <italic>S</italic><sub><italic>ij</italic></sub> is an omic-specific matrix of dimension <italic>k</italic><sub><italic>i</italic></sub> &#x000d7; <italic>k</italic><sub><italic>j</italic></sub>. As in NMF, the <italic>G</italic><sub><italic>i</italic></sub> matrices are non-negative. The same <italic>G</italic><sub><italic>i</italic></sub> matrix is used in all omics with entity <italic>i</italic>, and in this way data integration is achieved. In the above example, both the gene expression and DNA methylation omics will use the same <italic>G</italic> matrix to represent patients, but different matrices to represent genes and CpG loci. In this model, an additional matrix describing the relationship between genes and CpGs could optionally be used. This is a major advantage of matrix tri-factorization, as it allows to incorporate prior known relations between different entities, without changing the input omic matrices. (<xref rid="B60" ref-type="bibr">60</xref>) adds constraints to the formulation that can encourage entities to have similar representations. This framework was applied to diverse problems in bioinformatics, including in supervised settings: It was used to perform gene function prediction (<xref rid="B60" ref-type="bibr">60</xref>), and for patient survival regression (<xref rid="B95" ref-type="bibr">95</xref>).</p></sec><sec id="SEC2-5-5"><title>Convex formulations</title><p>A drawback of most factorization-based methods is that their objective functions are not convex, and therefore optimization procedures do not necessarily reach a global optimum, and highly depend on initialization. One solution to this issue is by formulating dimension reduction as a convex problem. White <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B61" ref-type="bibr">61</xref>) relaxes CCA&#x02019;s conditions and defines a convex variant of it. Performance was assessed on reducing noise in images, but the method can also be used for clustering. However, like CCA, the method only supports two views. Guo (<xref rid="B62" ref-type="bibr">62</xref>) presents a different convex formulation for dimension reduction, for the general factorization framework presented earlier, which minimizes <inline-formula><tex-math id="M35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\Sigma _{m=1}^M{||X^m - B P^m||_F^2} + \gamma ||B||_{2,1}$\end{document}</tex-math></inline-formula>. || &#x000b7; ||<sub>2, 1</sub> is the <italic>l</italic><sub>2, 1</sub> norm, namely the sum of the Euclidean norms of the matrix rows. This relaxation therefore supports multiple views. LRAcluster (<xref rid="B16" ref-type="bibr">16</xref>) also uses matrix factorization and has a convex objective function.</p></sec><sec id="SEC2-5-6"><title>Tensor-based methods</title><p>A natural extension of factorization methods for multi-omic data is to use tensors, which are higher order matrices. One such method is developed in (<xref rid="B63" ref-type="bibr">63</xref>). This method writes each omic matrix as <italic>X</italic><sup><italic>m</italic></sup> = <italic>Z</italic><sup><italic>m</italic></sup><italic>X</italic><sup><italic>m</italic></sup> + <italic>E</italic><sup><italic>m</italic></sup>, <italic>diag</italic>(<italic>Z</italic><sup><italic>m</italic></sup>) = 0, where <italic>Z</italic><sup><italic>m</italic></sup> is an <italic>n</italic> x <italic>n</italic> matrix and <italic>E</italic><sup><italic>m</italic></sup> are error matrices. The idea is that each sample in each omic can be represented as a linear combination of other samples (hence the <italic>diag</italic>(<italic>Z</italic><sup><italic>m</italic></sup>) = 0 constraint), and that its representation in that base (<italic>Z</italic><sup><italic>m</italic></sup>) can then be used for clustering. To integrate the different views, the different <italic>Z</italic><sup><italic>m</italic></sup> matrices are merged to a third-order tensor, <italic>Z</italic>. The objective function encourages <italic>Z</italic> to be sparse, and the <italic>E</italic><sup><italic>m</italic></sup> error matrices to have a small norm.</p></sec></sec><sec id="SEC2-6"><title>Statistical methods</title><p>Statistical methods model the probabilistic distribution of the data. Some of these methods view samples as originating from different clusters, where each cluster defines a distribution for the data, while other methods do not explicitly use the cluster structure in the model. An advantage of the statistical approach is that it allows to include biological knowledge as part of the model when determining the distribution functions. This can be done either using Bayesian priors or by choosing probabilistic functions, e.g. using normal distribution for gene expression data. An additional advantage of statistical frameworks is their ability to make &#x02018;soft&#x02019;, probabilistic decisions. For example, a statistical method can not only assign a sample to a cluster, but can also determine the probability that the sample belongs to the cluster. For most formulations, parameter estimation is computationally hard, and different heuristics are used. Several models under the Bayesian framework allow for samples to belong to different clusters in different omics.</p><sec id="SEC2-6-1"><title>iCluster and iCluster+</title><p>iCluster (<xref rid="B15" ref-type="bibr">15</xref>) assumes that the data originate from a low dimension representation, which determines the cluster membership for each sample: <italic>X</italic><sup><italic>m</italic><sup><italic>t</italic></sup></sup> = <italic>W</italic><sup><italic>m</italic></sup><italic>Z</italic> + &#x003b5;<sup><italic>m</italic></sup>, where <italic>Z</italic> is a <italic>k</italic> x <italic>n</italic> matrix, <italic>W</italic><sup><italic>m</italic></sup> is an omic specific <italic>p</italic><sub><italic>m</italic></sub> x <italic>k</italic> matrix, <italic>k</italic> is the number of clusters and &#x003b5;<sup><italic>m</italic></sup> is a normally distributed noise matrix. This model resembles other dimension reduction models, but here the distribution of noise is made explicit. Under this model iCluster maximizes the likelihood of the observed data with an additional regularization for sparse <italic>W</italic><sup><italic>m</italic></sup> matrices. Optimization is performed using an EM-like algorithm, and subsequently k-means is run on the lower dimension representation of the data <italic>Z</italic> to get the final clustering assignments. iCluster was applied to breast and lung cancer, using gene expression and copy number variations. iCluster was also recently used to cluster more than ten thousand tumors from 33 cancers in a pan-cancer analysis (<xref rid="B96" ref-type="bibr">96</xref>). Note that by concatenating all <italic>W</italic><sup><italic>m</italic></sup> matrices to a single <italic>W</italic> matrix, and rewriting the model as <italic>X</italic><sup><italic>t</italic></sup> = <italic>WZ</italic> + &#x003b5;, iCluster can be viewed as an early integration approach.</p><p>iCluster&#x02019;s runtime grows fast with the number of features, and therefore feature selection is essential before using it, as was shown in (<xref rid="B29" ref-type="bibr">29</xref>). Shen <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B15" ref-type="bibr">15</xref>) only use genes located on one or two chromosomes in their analysis.</p><p>Since iCluster&#x02019;s model uses matrix multiplication, it requires real-values features. An extension called iCluster+ (<xref rid="B64" ref-type="bibr">64</xref>) includes different models for numeric, categorical and count data, but maintains the idea that data originate from a low dimension matrix <italic>Z</italic>. For categorical data, iCluster+ assumes the following model:
<disp-formula><tex-math id="M36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} Pr(X_{ij}^m=c | z_i) = \frac{exp(\alpha _{jcm} + \beta _{jcm} \cdot z_i)}{\Sigma _l{exp(\alpha _{jlm} + \beta _{jlm} \cdot z_i)}} \end{equation*}\end{document}</tex-math></disp-formula>while for numeric data the model remains linear with normal error:
<disp-formula><tex-math id="M37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}\begin{equation*} x_{ijm} = \gamma _{jm} + \delta _{jm} \cdot z_i + \epsilon _{ijm}, \epsilon _{ijm} \sim N(0, \sigma _{jm}^2) \end{equation*}\end{document}</tex-math></disp-formula>A regularization term encouraging sparse solution is added to the likelihood, and a Monte-Carlo Newton&#x02013;Raphson algorithm is used to estimate parameters. The <italic>Z</italic> matrix is used as in iCluster for the clustering. The latest extension of iCluster, which builds on iCluster+, is iClusterBayes (<xref rid="B65" ref-type="bibr">65</xref>). This method replaces the regularization in iCluster+ with full Bayesian regularization. This replacement results in faster execution, since the algorithm no longer needs to fine tune parameters for iCluster+&#x02019;s regularization.</p></sec><sec id="SEC2-6-2"><title>PARADIGM</title><p>PARADIGM (<xref rid="B66" ref-type="bibr">66</xref>) is the most explicit approach to modeling cellular processes and the relations among different omics. For each sample and each cellular pathway, a factor graph that represents the state of different entities within that pathway is created. As a degenerate example, a pathway may include nodes representing the mRNA levels of each gene in that pathway, and nodes representing those genes&#x02019; copy number. Each node in the factor graph can be either activated, nominal or deactivated, and the factor graph structure defines a distribution over these activation levels. For example, if a gene has high copy number it is more likely that it will be highly expressed. However, if a repressor for that gene is highly expressed, that gene is more likely to be deactivated. PARADIGM infers the activity of non-measured cellular entities to maximize the likelihood of the factor graph, and outputs an activity score for each entity per patient. These scores are used to cluster cancer patients from several tissues.</p><p>PARADIGM&#x02019;s model can be used for more than clustering. For example, PARADIGM-shift (<xref rid="B97" ref-type="bibr">97</xref>) predicts loss-of-function and gain-of-function mutations, by finding genes whose expression value as predicted based on upstream entities in the factor graph is different from their predicted expression value using downstream entities. However, PARADIGM relies heavily on known interactions, and requires specific modeling for each omic. It is also quite limited to the cellular level; For example, it is not clear how to incorporate into the model an omic describing the microbiome composition of each patient.</p></sec><sec id="SEC2-6-3"><title>Combining omic-specific and global clustering</title><p>All the methods discussed so far assume that there exists a consistent clustering structure across the different omics, and that analyzing the clusters in an integrative way will reveal this structure more accurately than analyzing each omic separately. However, this is not necessarily the case for biomedical datasets. For example, it is not clear that the methylation and expression profiles of cancer tumors really represent the same underlying cluster structure. Rather, it is possible that each omic represents a somewhat different cluster structure. Several methods take this view point using Bayesian statistics.</p><p>Savage <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B67" ref-type="bibr">67</xref>) define a hierarchical Dirichlet process model, which supports clustering on two omics. Each sample can be either <italic>fused</italic> or <italic>unfused</italic>. Fused samples belong to the same cluster in both omics, while unfused samples can belong to different clusters in different omics. Patterns of fused and unfused samples reveal the concordance between the two datasets. This model is extended in PSDF (<xref rid="B68" ref-type="bibr">68</xref>) to include feature selection. Savage <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B67" ref-type="bibr">67</xref>) apply the model to cluster genes using gene expression and ChIP-chip data, while (<xref rid="B68" ref-type="bibr">68</xref>) clusters cancer patients using expression and copy number data.</p><p>In MDI (<xref rid="B69" ref-type="bibr">69</xref>) each sample can have different cluster assignments in different omics. However, a prior is given such that the stronger an association between two omics is, the more likely a sample will belong to the same cluster in these two omics. This association strength adjusts the prior clustering agreement between two omics. In addition to these priors, MDI&#x02019;s model uses Dirichlet mixture model, and explicitly represents the distribution of the data within each cluster and omic. Since samples can belong to different clusters in different omics, no global clustering solution is returned by the algorithm. Instead, the algorithm outputs sets of samples that tend to belong to the same cluster.</p><p>A different Bayesian formulation is given by BCC (<xref rid="B70" ref-type="bibr">70</xref>). Like MDI, BCC assumes a Dirichlet mixture model, where the data originate from a mixture of distributions. However, BCC does assume a global clustering solution, where each sample maps to a single cluster. Given that a sample belongs to a global cluster, its probability to belong to that cluster in each omic is high, but it can also belong to a different cluster in that omic. Parameters are estimated using Gibbs sampling (<xref rid="B98" ref-type="bibr">98</xref>). BCC was used on gene expression, DNA methylation, miRNA expression and RPPA data for breast cancer from TCGA.</p><p>Like MDI and BCC, Clusternomics (<xref rid="B71" ref-type="bibr">71</xref>) uses a Dirichlet mixture model. Clusternomics suggests two different formulations. In the first, each omic has a different clustering solution, and the global clusters are represented as the Cartesian product of clusters from each omic. This approach does not perform integration of the multi-omic datasets. In the second formulation, global clusters are explicitly mapped to omic-specific clusters. That way, not all possible combinations of clusters from different omics are considered as global clusters.</p></sec><sec id="SEC2-6-4"><title>Survival-based clustering</title><p>One of the areas multi-omics clustering is widely used for is discovering disease subtypes. In this context, we may expect different disease subtypes to have a different prognosis, and this criterion is often used to assess clustering solutions. Ahmad and Fr&#x000f6;hlich (<xref rid="B72" ref-type="bibr">72</xref>) develop a Bayesian model for multi-omics clustering that considers patient prognosis while clustering the data. Patients within a cluster have both similar feature distribution and similar prognosis. This approach is not entirely unsupervised, as it considers patient survival data, which are also used to assess the solutions. Coretto <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B73" ref-type="bibr">73</xref>) also develop a probabilistic clustering method that considers survival, and that supports a large number of features compared to (<xref rid="B72" ref-type="bibr">72</xref>), which only uses a few dozen features. As the survival data are used as input to the model, it is not surprising that this approach gives clusters with more significantly different survival than other approaches. This was demonstrated on Glioblastoma Multiforme data by (<xref rid="B72" ref-type="bibr">72</xref>) and for data from several cancer types by (<xref rid="B73" ref-type="bibr">73</xref>), both from TCGA.</p></sec></sec><sec id="SEC2-7"><title>Deep multi-view methods</title><p>A recent development in machine learning is the advent of deep learning algorithms (<xref rid="B99" ref-type="bibr">99</xref>). These algorithms use multi-layered neural networks to perform diverse computational tasks, and were found to improve performance in several fields such as image recognition (<xref rid="B100" ref-type="bibr">100</xref>) and text translation (<xref rid="B101" ref-type="bibr">101</xref>). Neural networks and deep learning have also proven useful for multi-view applications (<xref rid="B102" ref-type="bibr">102</xref>), including unsupervised feature learning (<xref rid="B37" ref-type="bibr">37</xref>), (<xref rid="B103" ref-type="bibr">103</xref>). Learned features can be used for clustering, as described earlier for DeepCCA. Deep learning is already used extensively for biomedical data analysis (<xref rid="B104" ref-type="bibr">104</xref>).</p><p>Recent deep learning uses for multi-omics data include (<xref rid="B74" ref-type="bibr">74</xref>) and (<xref rid="B75" ref-type="bibr">75</xref>). Chaudhary <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B74" ref-type="bibr">74</xref>) use an autoencoder, which is a deep learning method for dimension reduction. The authors ran it on RNA-seq, methylation and miRNA-seq data in order to cluster Hapatocellular Carcinoma patients. The architecture implements an early integration approach, concatenating the features from the different omics. The autoencoder outputs a representation for each patient. Features from this representation are tested for association with survival, and significantly associated features are used to cluster the patients. The clusters obtained have significantly different survival. This result is compared to a similar analysis using the original features, and features learned with PCA (Principal Component Analysis) rather than autoencoders. However, the analysis in this work is not unsupervised, since the feature selection is based on patient survival.</p><p>Liang <italic>et&#x000a0;al</italic>.&#x000a0;(<xref rid="B75" ref-type="bibr">75</xref>) use a different approach. They analyze expression, methylation and miRNA ovarian cancer data using Deep Belief Networks (<xref rid="B105" ref-type="bibr">105</xref>) which explicitly consider the multi-omic structure of the data. The architecture contains separate hidden layers, each having inputs from one omic, followed by layers that receive input from all the single-omic hidden layers, thus integrating the different omics. A 3D representation over {0, 1} is learned for each patient, partitioning the patients into 2<sup>3</sup> = 8 clusters. The clustering results are compared to k-means clustering on the concatenation of all omics, but not to other multi-omics clustering methods.</p><p>Deep learning algorithms usually require many samples and few features. They use a large number of parameters, which makes them prone to overfitting. Current multi-omic datasets have the opposite characteristics&#x02014;they have many features and at least one order of magnitude less samples. The works presented here use only a few layers in their architectures to overcome this limitation, in comparison to the dozens of layers used by state-of-the-art architectures for imaging datasets. As the number of biomedical samples increases, deep multi-view learning algorithms might prove more beneficial for biomedical datasets.</p></sec></sec><sec id="SEC3"><title>BENCHMARK</title><p>In order to test the performance of multi-omics clustering methods, we compared nine algorithms on ten cancer types available from TCGA. We also compared the performance of the algorithms on each one of the single-omic datasets that make up the multi-omic datasets, for algorithms that are applicable to single-omic data. The nine algorithms were chosen to represent diverse approaches to multi-omics clustering. Within each approach, we chose methods with available software and clear usage guidelines (e.g. we chose PINS over COCA as a late integration method since COCA does not explicitly state how each single omic should be clustered), and that are widely used, so that a comparison of these methods will be most informative to the community. Three algorithms are early integration methods: LRAcluster, and k-means and spectral clustering on the omics concatenated into a single matrix. For similarity-based algorithms we used SNF and rMKL-LPP. For dimension reduction we used MCCA (<xref rid="B39" ref-type="bibr">39</xref>) and MultiNMF. We chose iClusterBayes as a statistical method, and PINS as a late integration approach.</p><p>The ten datasets contain cancer tumor multi-omics data, where each dataset is a different cancer type. All datasets contain three omics: gene expression, DNA methylation and miRNA expression. The number of patients range from 170 for AML to 621 for BIC. Full details on the datasets and cancer type acronyms appear in <xref ref-type="supplementary-material" rid="sup1">Supplementary File 2</xref>.</p><p>To assess the performance of a clustering solution, we used three metrics. First, we measured differential survival between the obtained clusters using the logrank test (<xref rid="B106" ref-type="bibr">106</xref>). Using this test as a metric assumes that if clusters of patients have significantly different survival, they are different in a biologically meaningful way. Second, we tested for the enrichment of clinical labels in the clusters. We chose six clinical labels for which we tested enrichment: gender, age at diagnosis, pathologic T, pathologic M, pathologic N and pathologic stage. The four latter parameters are discrete pathological parameters, measuring the progression of the tumor (T), metastases (M) and cancer in lymph nodes (N), and the total progression (pathologic stage). Enrichment for discrete parameters was calculated using the &#x003c7;<sup>2</sup> test for independence, and for numeric parameters using Kruskal-Wallis test. Not all clinical parameters were available for all cancer types, so a total of 41 clinical parameters were available for testing. Finally, we recorded the runtime of each method. We did not consider in the assessment computational measures for clustering quality, such as heterogeneity, homogeneity or the silhouette score (<xref rid="B107" ref-type="bibr">107</xref>), since the different methods perform different normalization on the features (and some even perform feature selection). Full details about the survival and phenotype data appear in <xref ref-type="supplementary-material" rid="sup1">Supplementary File 2</xref>.</p><p>To derive a p-value for the logrank test, the &#x003c7;<sup>2</sup> test for independence, and the Kruskal-Wallis test, the statistic for these three tests is assumed to have &#x003c7;<sup>2</sup> distribution. However, for the logrank test and &#x003c7;<sup>2</sup> test this approximation is not accurate for small sample sizes and unbalanced cluster sizes, especially for large values of the test statistic (this was shown for example in (<xref rid="B108" ref-type="bibr">108</xref>) for the logrank test in the case of two clusters). The p-values we report here are therefore estimated using permutation tests (i.e., we permuted the cluster labels between samples and used the test statistic to obtain an empirical p-value). We indeed observed large differences between the p-values based on permutation testing and based on the approximation, for both the logrank test and enrichment of clinical parameters. More details on the permutation tests appear in <xref ref-type="supplementary-material" rid="sup1">Supplementary File 1</xref>. After permutation testing, the p-values for the clinical labels were corrected for multiple hypotheses (since several labels were tested) using Bonferroni correction for each cancer type and method at significance level 0.05. Results for the statistical analyses are in <xref ref-type="supplementary-material" rid="sup1">Supplementary File 3</xref>.</p><p>We applied all nine methods to the ten multi-omics datasets, and to the thirty single-omic matrices comprising them. The only exceptions were MCCA, which we could not apply to single-omic data, and PINS, which crashed consistently on all BIC datasets<xref ref-type="fn" rid="FN1">*</xref>. All methods were run on a Windows machine, except for iCluster which was run on a Linux cluster utilizing up to 15 nodes in parallel. In general, we chose parameters for the methods as suggested by the authors. In case the authors suggested a parameter search, such search was performed, and the best solution was chosen as suggested by the authors, without considering the survival and clinical parameters that are used for assessment. The runtime we report for the methods includes the parameter search. The rationale is that the benchmark aims to record how a user would run the methods in terms of both results quality and total runtime. Details on hardware, data preprocessing and application of the methods appear in Supplementary File 1. Full clustering results appear in Supplementary File 4. All the processed raw data are available at <ext-link ext-link-type="uri" xlink:href="http://acgt.cs.tau.ac.il/multi_omic_benchmark/download.html">http://acgt.cs.tau.ac.il/multi_omic_benchmark/download.html</ext-link>, and all software scripts used are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Shamir-Lab/Multi-Omics-Cancer-Benchmark/">https://github.com/Shamir-Lab/Multi-Omics-Cancer-Benchmark/</ext-link>.</p><p>Figure&#x000a0;<xref ref-type="fig" rid="F2">2</xref> depicts the performance of the benchmarked methods on the different cancer datasets, and Figures&#x000a0;<xref ref-type="fig" rid="F3">3</xref> and&#x000a0;<xref ref-type="fig" rid="F4">4</xref> summarize the performance for multi-omics data and for each single-omic separately across all cancer types. No algorithm consistently outperformed all others in either differential survival or enriched clinical parameters. With respect to survival, MCCA had the total best prognostic value (sum of -log10 p-values = 17.53), while MultiNMF was second (16.07) and LRACluster third (15.72). The sum of p-values can be biased due to outliers, so we also counted the number of datasets for which a method&#x02019;s solution obtains significantly different survival. These results are reported in Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref>. Here, with the exception of iClusterBayes, all methods that were developed for multi-omics or multi-view data had at least four cancer types with significantly different survival. MCCA and LRACluster had five. These cancer types are not identical for all the algorithms.</p><fig id="F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>Performance of the algorithms on ten multi-omics cancer datasets. For each plot, the x-axis measures the differential survival between clusters (&#x02013;log<sub>10</sub> of logrank&#x02019;s test <italic>P</italic>-value), and the y-axis is the number of clinical parameters enriched in the clusters. Red vertical lines indicate the threshold for significantly different survival (<italic>P</italic>-value &#x02264; 0.05)</p></caption><graphic xlink:href="gky889fig2"/></fig><fig id="F3" orientation="portrait" position="float"><label>Figure 3.</label><caption><p>Mean performance of the algorithms on ten multi-omics cancer datasets. The x-axis measures the differential survival between clusters (mean &#x02013;log<sub>10</sub> of logrank&#x02019;s test <italic>P</italic>-value), and the y-axis is the mean number of clinical parameters enriched in the clusters.</p></caption><graphic xlink:href="gky889fig3"/></fig><fig id="F4" orientation="portrait" position="float"><label>Figure 4.</label><caption><p>Summarized performance of the algorithms across ten cancer datasets. For each plot, the x-axis measures the total differential prognosis between clusters (sum across all datasets of &#x02013;log<sub>10</sub> of logrank&#x02019;s test <italic>P</italic>-value), and the y-axis is the total number of clinical parameters enriched in the clusters across all cancer types. (<bold>A</bold>&#x02013;<bold>C</bold>) Results for single-omic datasets. (<bold>D</bold>) Results when each method uses the single omic that achieves the highest significance in survival. (<bold>E</bold>)&#x000a0;Same with respect to enrichment of clinical labels.</p></caption><graphic xlink:href="gky889fig4"/></fig><table-wrap id="tbl2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Cancer types with significant results per algorithm</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">
<italic>k</italic>-means</th><th rowspan="1" colspan="1">Spectral</th><th rowspan="1" colspan="1">LRAcluster</th><th rowspan="1" colspan="1">PINS</th><th rowspan="1" colspan="1">SNF</th><th rowspan="1" colspan="1">rMKL-LPP</th><th rowspan="1" colspan="1">MCCA</th><th rowspan="1" colspan="1">MultiNMF</th><th rowspan="1" colspan="1">iClusterBayes</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Significantly different survival</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td></tr><tr><td rowspan="1" colspan="1">Significant clinical enrichment</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">5</td></tr></tbody></table><table-wrap-foot><fn id="T2TFN1"><p>For each benchmarked algorithm, the number of cancer subtypes for which its clustering had significantly different prognosis (first row) and had at least one enriched clinical label (second row) are shown.</p></fn></table-wrap-foot></table-wrap><p>rMKL-LPP achieved the highest total number of significant clinical parameters, with 16 parameters. Spectral clustering came second with 14 and LRAcluster had 13. MCCA and MultiNMF, which had good results with respect to survival, had only 12 and 10 enriched parameters, respectively. rMKL-LPP did not outperform all other methods for all cancer types. For example, it had one enriched parameter for SKCM, while several other methods had two or three. We also considered the number of cancer types for which an algorithm had at least one enriched clinical label (Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref>). rMKL-LPP, spectral clustering, LRACluster and MCCA had enrichment in 8 cancer types, despite MCCA having a total of only 12 enriched parameters. Overall, rMKL-LPP outperformed all methods except MCCA, LRACluster and multiNMF with respect to both survival and clinical enrichment. MCCA, LRACluster and multiNMF had better prognostic value, but found less enriched clinical labels.</p><p>Each method determines the number of clusters for each dataset. These numbers are presented in Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref>. The numbers vary drastically among methods, from 2 or 3 (iCluster and MultiNMF) to more than 10 on average (MCCA). MCCA, LRACluster and rMKL-LPP partitioned the data into a relatively high number of clusters (average of 10.6, 9.4 and 6.7 respectively), and had good performance, which may indicate that clustering cancer patients into more clusters improves prognostic value and clinical significance. The higher number of clusters is controlled in the logrank and clinical enrichment tests by having more degrees of freedom for its &#x003c7;<sup>2</sup> statistic.</p><table-wrap id="tbl3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Number of clusters chosen by the benchmarked algorithms on ten multi-omics cancer datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">AML</th><th rowspan="1" colspan="1">BIC</th><th rowspan="1" colspan="1">COAD</th><th rowspan="1" colspan="1">GBM</th><th rowspan="1" colspan="1">KIRC</th><th rowspan="1" colspan="1">LIHC</th><th rowspan="1" colspan="1">LUSC</th><th rowspan="1" colspan="1">SKCM</th><th rowspan="1" colspan="1">OV</th><th rowspan="1" colspan="1">SARC</th><th rowspan="1" colspan="1">Means</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">K-means</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2.6</td></tr><tr><td rowspan="1" colspan="1">Spectral</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3.3</td></tr><tr><td rowspan="1" colspan="1">LRAcluster</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">11</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">9.4</td></tr><tr><td rowspan="1" colspan="1">PINS</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">NA</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">4.6</td></tr><tr><td rowspan="1" colspan="1">SNF</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2.8</td></tr><tr><td rowspan="1" colspan="1">rMKL-LPP</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">11</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">6.7</td></tr><tr><td rowspan="1" colspan="1">MCCA</td><td rowspan="1" colspan="1">11</td><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">11</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">15</td><td rowspan="1" colspan="1">10.6</td></tr><tr><td rowspan="1" colspan="1">MultiNMF</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2.2</td></tr><tr><td rowspan="1" colspan="1">iClusterBayes</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2</td><td rowspan="1" colspan="1">2.2</td></tr></tbody></table><table-wrap-foot><fn id="T3TFN1"><p>The right column is the average number of clusters across all cancer types.</p></fn></table-wrap-foot></table-wrap><p>The runtime of the different methods is reported in Table&#x000a0;<xref rid="tbl4" ref-type="table">4</xref>. Note that as mentioned earlier, iClusterBayes was run on a cluster, while the other methods were run on a desktop computer. All methods except for LRAcluster and iCluster took less than ten minutes per dataset on average. LRAcluster and iClusterBayes took about 56 and 72 minutes per dataset, respectively.</p><table-wrap id="tbl4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Runtime in seconds of the algorithms on ten multi-omics cancer datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">AML</th><th rowspan="1" colspan="1">BIC</th><th rowspan="1" colspan="1">COAD</th><th rowspan="1" colspan="1">GBM</th><th rowspan="1" colspan="1">KIRC</th><th rowspan="1" colspan="1">LIHC</th><th rowspan="1" colspan="1">LUSC</th><th rowspan="1" colspan="1">SKCM</th><th rowspan="1" colspan="1">OV</th><th rowspan="1" colspan="1">SARC</th><th rowspan="1" colspan="1">Means</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">K-means</td><td rowspan="1" colspan="1">96</td><td rowspan="1" colspan="1">1306</td><td rowspan="1" colspan="1">153</td><td rowspan="1" colspan="1">212</td><td rowspan="1" colspan="1">102</td><td rowspan="1" colspan="1">407</td><td rowspan="1" colspan="1">444</td><td rowspan="1" colspan="1">723</td><td rowspan="1" colspan="1">303</td><td rowspan="1" colspan="1">191</td><td rowspan="1" colspan="1">394</td></tr><tr><td rowspan="1" colspan="1">Spectral</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">3</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">4</td></tr><tr><td rowspan="1" colspan="1">LRAcluster</td><td rowspan="1" colspan="1">957</td><td rowspan="1" colspan="1">11655</td><td rowspan="1" colspan="1">1405</td><td rowspan="1" colspan="1">1370</td><td rowspan="1" colspan="1">991</td><td rowspan="1" colspan="1">3959</td><td rowspan="1" colspan="1">3353</td><td rowspan="1" colspan="1">5892</td><td rowspan="1" colspan="1">2299</td><td rowspan="1" colspan="1">2004</td><td rowspan="1" colspan="1">3388</td></tr><tr><td rowspan="1" colspan="1">PINS</td><td rowspan="1" colspan="1">41</td><td rowspan="1" colspan="1">NA</td><td rowspan="1" colspan="1">112</td><td rowspan="1" colspan="1">115</td><td rowspan="1" colspan="1">59</td><td rowspan="1" colspan="1">125</td><td rowspan="1" colspan="1">228</td><td rowspan="1" colspan="1">317</td><td rowspan="1" colspan="1">214</td><td rowspan="1" colspan="1">113</td><td rowspan="1" colspan="1">147</td></tr><tr><td rowspan="1" colspan="1">SNF</td><td rowspan="1" colspan="1">5</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">7</td><td rowspan="1" colspan="1">6</td><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">21</td><td rowspan="1" colspan="1">9</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">13</td></tr><tr><td rowspan="1" colspan="1">rMKL-LPP</td><td rowspan="1" colspan="1">222</td><td rowspan="1" colspan="1">192</td><td rowspan="1" colspan="1">205</td><td rowspan="1" colspan="1">221</td><td rowspan="1" colspan="1">191</td><td rowspan="1" colspan="1">255</td><td rowspan="1" colspan="1">213</td><td rowspan="1" colspan="1">333</td><td rowspan="1" colspan="1">263</td><td rowspan="1" colspan="1">238</td><td rowspan="1" colspan="1">233</td></tr><tr><td rowspan="1" colspan="1">MCCA</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">43</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">13</td><td rowspan="1" colspan="1">26</td><td rowspan="1" colspan="1">25</td><td rowspan="1" colspan="1">25</td><td rowspan="1" colspan="1">19</td><td rowspan="1" colspan="1">16</td><td rowspan="1" colspan="1">20</td></tr><tr><td rowspan="1" colspan="1">MultiNMF</td><td rowspan="1" colspan="1">19</td><td rowspan="1" colspan="1">51</td><td rowspan="1" colspan="1">25</td><td rowspan="1" colspan="1">19</td><td rowspan="1" colspan="1">17</td><td rowspan="1" colspan="1">35</td><td rowspan="1" colspan="1">27</td><td rowspan="1" colspan="1">45</td><td rowspan="1" colspan="1">21</td><td rowspan="1" colspan="1">23</td><td rowspan="1" colspan="1">28</td></tr><tr><td rowspan="1" colspan="1">iClusterBayes*</td><td rowspan="1" colspan="1">2628</td><td rowspan="1" colspan="1">7832</td><td rowspan="1" colspan="1">3213</td><td rowspan="1" colspan="1">2569</td><td rowspan="1" colspan="1">2756</td><td rowspan="1" colspan="1">5195</td><td rowspan="1" colspan="1">4682</td><td rowspan="1" colspan="1">6077</td><td rowspan="1" colspan="1">4057</td><td rowspan="1" colspan="1">3969</td><td rowspan="1" colspan="1">4298</td></tr></tbody></table><table-wrap-foot><fn id="T4TFN1"><p>The right column is the average runtime across all cancer types. *For iClusterBayes numbers are elapsed time on a multi-core platform.</p></fn></table-wrap-foot></table-wrap><p>Figure&#x000a0;<xref ref-type="fig" rid="F4">4</xref> also shows the performance of the benchmarked methods for single-omic data. While several methods had worse performance on single-omic datasets, some achieved better performance. For example, the highest number of enriched clinical parameters for both single and multi-omic datasets (<xref rid="B18" ref-type="bibr">18</xref>) was achieved by rMKL-LPP on gene expression. The gene expression solution also had better prognostic value than the multi-omic solution.</p><p>To further test how analysis of single-omic datasets compares to multi-omic datasets, we chose for each dataset and method the single omic that gave the best results for survival and clinical enrichment. In this analysis, rMKL-LPP had both the highest total number of enriched clinical parameters (<xref rid="B21" ref-type="bibr">21</xref>), and the highest total survival significance (21.86). The runtime, number of clusters, and survival and clinical enrichment analysis for single-omic datasets appear in Supplementary Files 1 and 3. These results suggest that analysis of multi-omics data does not consistently provide better prognostic value and clinical significance compared to analysis of single-omic data alone, especially when different single-omics are used for each cancer types.</p></sec><sec sec-type="discussion" id="SEC4"><title>DISCUSSION</title><p>We have reviewed methods for multi-omics and multi-view clustering. In our tests on 10 cancer datasets, overall, rMKL-LPP performed best in terms of clinical enrichment, and outperformed all methods except MCCA and MultiNMF with respect to survival. The high performance of MCCA and MultiNMF is remarkable, as these are multi-view methods that were not specifically developed for omics data (though MCCA was applied to it).</p><p>Throughout this review we provided guidelines about the advantages and disadvantages of different approaches and algorithms. In the benchmark, no single method consistently outperformed all others on any of the assessment criteria. While some methods were shown to do well, we cannot conclude from this that they should be always preferred. We also could not identify one &#x02018;best&#x02019; integration approach, but it is interesting to note that the top two performers with respect to survival were dimension reduction methods.</p><p>Careful consideration should be given when applying multi-view clustering methods to multi-omic data, since these data have characteristics that multi-view methods do not necessarily consider. The most prominent of these characteristics is the large number of features relative to the number of samples. For example, CCA inverts the covariance matrix of each omic. This matrix is not invertible when there are more features than samples, and sparsity regularization is necessary. Another feature of multi-omic data is the dependencies between features in different omics, but several multi-view algorithms assume conditional independence of the omics given the clustering structure. This dependency is rarely considered, since it greatly increases the complexity of models. An additional characteristic of current omic data types is that due to cellular regulation, they have an intrinsic lower dimensional representation. The characteristic is utilized by many methods.</p><p>In our benchmark, single-omic data alone sometimes gave better results than multi-omics data. This was intensified when for each algorithm the &#x02018;best&#x02019; single-omic for each cancer type was chosen. These results question the current assumptions underlying multi-omics analysis in general and multi-omics clustering in particular.</p><p>Several approaches may lead to improved results for multi-omics analysis. First, methods that suggest different clusterings in different omics were developed and reviewed here, but were not included in the benchmark, since it is not clear how to compare algorithms that do not output a global clustering solution to those that do. These methods may be more sensitive to strong signals appearing in only some of the omics. Second, future algorithms can perform omic selection in the same manner that algorithms today perform feature selection. In the benchmark, we let each method choose a single-omic for each cancer type given the results of the analysis, which are usually not available for real data. Methods that filter omics with contradicting signals might obtain a clearer clustering. Finally, while some methods for multi-omics clustering incorporate prior biological knowledge, few of them incorporate knowledge regarding the relationship between omics, or between features in different omics. Several statistical methods include some form of biological modeling by describing the distribution of the omics, and MDI tunes the similarity of clustering solutions in different omics based on the omics similarity. However, these methods do not model the biological relationships between omics. A notable exception is PARADIGM, which formulates the relationships between different omics. However, it also requires accurate prior knowledge about biochemical consequences of interactions, which is often unavailable. Methods that model relations between omics might benefit from additional biological knowledge, even without modeling whole pathways. For example, one can incorporate in a model the fact that promoter methylation is anti-correlated with gene expression. As far as we know, such methods were only developed for copy-number variation and gene expression data (e.g. (<xref rid="B109" ref-type="bibr">109</xref>)), and not in the context of clustering.</p><p>We detected large differences between the p-values derived from the &#x003c7;<sup>2</sup> approximation compared to the <italic>P</italic>-values derived from the permutation tests in the statistical tests we used. The differences were especially large due to the small sample size, small cluster sizes (in solutions with a high number of clusters) and due to a low number of events (high survival) for the logrank test. These p-values are used by single and multi-omic methods to assess their performance, and the logrank p-value is often the main argument for an algorithm&#x02019;s merit. The large differences between the <italic>P</italic>-values question the validity of analyses that are based on the &#x003c7;<sup>2</sup> approximation, at least for TCGA data. Future work must use exact or permutation-based calculations of the <italic>P</italic>-value in datasets with similar characteristics to those used here for the benchmark.</p><p>The benchmark we performed is not without limitations. Gauging performance using patient survival is somewhat biased to known cancer subtypes, which may have been used in treatment decisions. Additionally, cancer subtypes that are biologically different may have similar survival. This is also true for enrichment of clinical parameters, although we attempted to choose parameters that would not lead to this bias. However, these measures are widely used for clustering assessment, including in the papers describing some of the benchmarked methods. Another limitation of the benchmark is that it only examines clustering, while some of the methods have additional goals and output. For example, in dimension reduction algorithms, the low dimensional data can be used to analyze features, and not only patients, e.g.&#x000a0;by calculating axes of variation common to several omics. With respect to feature analysis, multi-omic algorithms can have an advantage over single-omic algorithms that we did not test. Finally, though we selected the parameters of each benchmarked method according to the guidelines given by the authors, judicious fine-tuning of the parameters may improve results.</p></sec><sec id="SEC5"><title>DATA AVAILABILITY</title><p>All the processed raw data are available at <ext-link ext-link-type="uri" xlink:href="http://acgt.cs.tau.ac.il/multi_omic_benchmark/download.html">http://acgt.cs.tau.ac.il/multi_omic_benchmark/download.html</ext-link>.</p></sec><sec sec-type="supplementary-material" id="SEC6"><title>SUPPLEMENTARY DATA</title><p>
<ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gky889#supplementary-data">Supplementary Data</ext-link> are available at NAR Online.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="sup1"><label>Supplementary Data</label><media xlink:href="gky889_supplemental_files.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack><title>ACKNOWLEDGEMENTS</title><p>The results published here are based upon data generated by The Cancer Genome Atlas managed by the NCI and NHGRI. Information about TCGA can be found at <ext-link ext-link-type="uri" xlink:href="http://cancergenome.nih.gov">http://cancergenome.nih.gov</ext-link>. We thank Nora K. Speicher for providing the rMKL-LPP tool and Ron Zeira for helpful comments.</p></ack><fn-group><title>Footnotes</title><fn id="FN1"><p>Correction after publication: We performed all the benchmarks on a 64-bit computer, using the 32-bit version of R. In later tests we observed that PINS did not crash on 64-bit R, and it only crashed on 32-bit R due to insufficient memory. The clustering that PINS obtained on the breast cancer dataset had 4 enriched clinical parameters, and the p-value for the logrank test on that clustering was 0.05.).</p></fn></fn-group><sec id="SEC7"><title>FUNDING</title><p>United States&#x02013;Israel Binational Science Foundation (BSF), Jerusalem, Israel and the United States National Science Foundation (NSF); Bella Walter Memorial Fund of the Israel Cancer Association (in part);&#x000a0;Edmond J. Safra Center for Bioinformatics at Tel-Aviv University (to N.R.) (in part). Funding for open access charge:&#x000a0;BSF&#x02013;NSF and ICA grant listed under Funders (in part).</p><p>
<italic>Conflict of interest statement</italic>. None declared.</p></sec><ref-list><title>REFERENCES</title><ref id="B1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Goodwin</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>McPherson</surname><given-names>J.D.</given-names></name>, <name name-style="western"><surname>McCombie</surname><given-names>W.R.</given-names></name></person-group>
<article-title>Coming of age: ten years of next-generation sequencing technologies</article-title>. <source>Nat. Rev. Genet.</source><year>2016</year>; <volume>17</volume>:<fpage>333</fpage>&#x02013;<lpage>351</lpage>.<pub-id pub-id-type="pmid">27184599</pub-id></mixed-citation></ref><ref id="B2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ozsolak</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Milos</surname><given-names>P.M.</given-names></name></person-group>
<article-title>RNA sequencing: advances, challenges and opportunities</article-title>. <source>Nat. Rev. Genet.</source><year>2011</year>; <volume>12</volume>:<fpage>87</fpage>&#x02013;<lpage>98</lpage>.<pub-id pub-id-type="pmid">21191423</pub-id></mixed-citation></ref><ref id="B3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Allison</surname><given-names>D.B.</given-names></name>, <name name-style="western"><surname>Cui</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Page</surname><given-names>G.P.</given-names></name>, <name name-style="western"><surname>Sabripour</surname><given-names>M.</given-names></name></person-group>
<article-title>Microarray data analysis: From disarray to consolidation and consensus</article-title>. <source>Nat. Rev. Genet.</source><year>2006</year>; <volume>7</volume>:<fpage>55</fpage>&#x02013;<lpage>65</lpage>.<pub-id pub-id-type="pmid">16369572</pub-id></mixed-citation></ref><ref id="B4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yong</surname><given-names>W.-S.</given-names></name>, <name name-style="western"><surname>Hsu</surname><given-names>F.-M.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>P.-Y.</given-names></name></person-group>
<article-title>Profiling genome-wide DNA methylation</article-title>. <source>Epigenet. Chromatin</source>. <year>2016</year>; <volume>9</volume>:<fpage>26</fpage></mixed-citation></ref><ref id="B5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Jain</surname><given-names>A.K.</given-names></name>, <name name-style="western"><surname>Murty</surname><given-names>M.N.</given-names></name>, <name name-style="western"><surname>Flynn</surname><given-names>P.J.</given-names></name></person-group>
<article-title>Data clustering: a review</article-title>. <source>ACM Comput. Surv.</source><year>1999</year>; <volume>31</volume>:<fpage>264</fpage>&#x02013;<lpage>323</lpage>.</mixed-citation></ref><ref id="B6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Prasad</surname><given-names>V.</given-names></name>, <name name-style="western"><surname>Fojo</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Brada</surname><given-names>M.</given-names></name></person-group>
<article-title>Precision oncology: origins, optimism, and potential</article-title>. <source>Lancet Oncol.</source><year>2016</year>; <volume>17</volume>:<fpage>e81</fpage>&#x02013;<lpage>e86</lpage>.<pub-id pub-id-type="pmid">26868357</pub-id></mixed-citation></ref><ref id="B7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>S.</given-names></name></person-group>
<article-title>Multi-view learning overview: Recent progress and new challenges</article-title>. <source>Information Fusion</source>. <year>2017</year>; <volume>38</volume>:<fpage>43</fpage>&#x02013;<lpage>54</lpage>.</mixed-citation></ref><ref id="B8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Network</surname><given-names>T.C.G.A.</given-names></name></person-group>
<article-title>Comprehensive genomic characterization defines human glioblastoma genes and core pathways</article-title>. <source>Nature</source>. <year>2008</year>; <volume>455</volume>:<fpage>1061</fpage>&#x02013;<lpage>1068</lpage>.<pub-id pub-id-type="pmid">18772890</pub-id></mixed-citation></ref><ref id="B9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Chaudhary</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Garmire</surname><given-names>L.X.</given-names></name></person-group>
<article-title>More is better: recent progress in multi-omics data integration methods</article-title>. <source>Front. Genet.</source><year>2017</year>; <volume>8</volume>:<fpage>84</fpage><pub-id pub-id-type="pmid">28670325</pub-id></mixed-citation></ref><ref id="B10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bersanelli</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Mosca</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Remondini</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Giampieri</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Sala</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Castellani</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Milanesi</surname><given-names>L.</given-names></name></person-group>
<article-title>Methods for the integration of multi-omics data: mathematical aspects</article-title>. <source>BMC Bioinformatics</source>. <year>2016</year>; <volume>17</volume>:<fpage>S15</fpage></mixed-citation></ref><ref id="B11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>F.-X.</given-names></name>, <name name-style="western"><surname>Ngom</surname><given-names>A.</given-names></name></person-group>
<article-title>A review on machine learning principles for multi-view biological data integration</article-title>. <source>Brief. Bioinformatics</source>. <year>2016</year>; <fpage>325</fpage>&#x02013;<lpage>340</lpage>.</mixed-citation></ref><ref id="B12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name></person-group>
<article-title>Integrative clustering methods of multi-omics data for molecule-based cancer classifications</article-title>. <source>Quant. Biol.</source><year>2016</year>; <volume>4</volume>:<fpage>58</fpage>&#x02013;<lpage>67</lpage>.</mixed-citation></ref><ref id="B13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Zeleznik</surname><given-names>O.A.</given-names></name>, <name name-style="western"><surname>Thallinger</surname><given-names>G.G.</given-names></name>, <name name-style="western"><surname>Kuster</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Gholami</surname><given-names>A.M.</given-names></name>, <name name-style="western"><surname>Culhane</surname><given-names>A.C.</given-names></name></person-group>
<article-title>Dimension reduction techniques for the integrative analysis of multi-omics data</article-title>. <source>Brief. Bioinformatics</source>. <year>2016</year>; <volume>17</volume>:<fpage>628</fpage>&#x02013;<lpage>641</lpage>.<pub-id pub-id-type="pmid">26969681</pub-id></mixed-citation></ref><ref id="B14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Tini</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Marchetti</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Priami</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Scott-Boyer</surname><given-names>M.-P.</given-names></name></person-group>
<article-title>Multi-omics integration-a comparison of unsupervised clustering methodologies</article-title>. <source>Brief. Bioinformatics</source>. <year>2017</year>; <comment>doi:10.1093/bib/bbx167</comment></mixed-citation></ref><ref id="B15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Olshen</surname><given-names>A.B.</given-names></name>, <name name-style="western"><surname>Ladanyi</surname><given-names>M.</given-names></name></person-group>
<article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics</source>. <year>2009</year>; <volume>25</volume>:<fpage>2906</fpage>&#x02013;<lpage>2912</lpage>.<pub-id pub-id-type="pmid">19759197</pub-id></mixed-citation></ref><ref id="B16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wu</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>M.Q.</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>J.</given-names></name></person-group>
<article-title>Fast dimension reduction and integrative clustering of multi-omics data using low-rank approximation: Application to cancer molecular classification</article-title>. <source>BMC Genomics</source>. <year>2015</year>; <volume>16</volume>:<fpage>1022</fpage><pub-id pub-id-type="pmid">26626453</pub-id></mixed-citation></ref><ref id="B17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Nie</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name></person-group>
<article-title>Multi-view clustering and feature learning via structured sparsity</article-title>. <source>Proc. ICML &#x02019;13</source>. <year>2013</year>; <volume>28</volume>:<fpage>352</fpage>&#x02013;<lpage>360</lpage>.</mixed-citation></ref><ref id="B18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bickel</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Scheffer</surname><given-names>T.</given-names></name></person-group>
<article-title>Multi-view clustering</article-title>. <source>Proc. ICDM 2004</source>. <year>2004</year>; <fpage>19</fpage>&#x02013;<lpage>26</lpage>.</mixed-citation></ref><ref id="B19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hoadley</surname><given-names>K.A.</given-names></name>, <name name-style="western"><surname>Yau</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Wolf</surname><given-names>D.M.</given-names></name>, <name name-style="western"><surname>Cherniack</surname><given-names>A.D.</given-names></name>, <name name-style="western"><surname>Tamborero</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Ng</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Leiserson</surname><given-names>M.D.</given-names></name>, <name name-style="western"><surname>Niu</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>McLellan</surname><given-names>M.D.</given-names></name>, <name name-style="western"><surname>Uzunangelov</surname><given-names>V.</given-names></name><etal/></person-group>
<article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>. <source>Cell</source>. <year>2014</year>; <volume>158</volume>:<fpage>929</fpage>&#x02013;<lpage>944</lpage>.<pub-id pub-id-type="pmid">25109877</pub-id></mixed-citation></ref><ref id="B20"><label>20.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Bruno</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Marchand-Maillet</surname><given-names>S.</given-names></name></person-group>
<article-title>Multiview clustering: A late fusion approach using latent models categories and subject descriptors</article-title>. <source>Proc. ACM SIGIR &#x02019;09</source>. <year>2009</year>; <publisher-loc>NY</publisher-loc><publisher-name>ACM Press</publisher-name><fpage>736</fpage>&#x02013;<lpage>737</lpage>.</mixed-citation></ref><ref id="B21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Tagett</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Diaz</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Draghici</surname><given-names>S.</given-names></name></person-group>
<article-title>A novel approach for data integration and disease subtyping</article-title>. <source>Genome Res.</source><year>2017</year>; <volume>27</volume>:<fpage>2025</fpage>&#x02013;<lpage>2039</lpage>.<pub-id pub-id-type="pmid">29066617</pub-id></mixed-citation></ref><ref id="B22"><label>22.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>de&#x000a0;Sa</surname><given-names>V.R.</given-names></name></person-group>
<article-title>Spectral Clustering with Two Views</article-title>. <source>Proceedings of the Workshop on Learning with Multiple Views, 22nd ICML</source>. <year>2005</year>; <fpage>20</fpage>&#x02013;<lpage>27</lpage>.</mixed-citation></ref><ref id="B23"><label>23.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Rai</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Daum&#x000e9;</surname><given-names>H.</given-names><suffix>III</suffix></name></person-group>
<article-title>Co-regularized multi-view spectral clustering</article-title>. <source>Proc. NIPS &#x02019;11</source>. <year>2011</year>; <publisher-loc>USA</publisher-loc><fpage>1413</fpage>&#x02013;<lpage>1421</lpage>.</mixed-citation></ref><ref id="B24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chikhi</surname><given-names>N.F.</given-names></name></person-group>
<article-title>Multi-view clustering via spectral partitioning and local refinement</article-title>. <source>Inform. Process. Manage.</source><year>2016</year>; <volume>52</volume>:<fpage>618</fpage>&#x02013;<lpage>627</lpage>.</mixed-citation></ref><ref id="B25"><label>25.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Nie</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name></person-group>
<article-title>Large-scale multi-view spectral clustering with bipartite graph</article-title>. <source>Proc. AAAI 15</source>. <year>2015</year>; <fpage>2750</fpage>&#x02013;<lpage>2756</lpage>.</mixed-citation></ref><ref id="B26"><label>26.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Burges</surname><given-names>C.J.C.</given-names></name></person-group>
<article-title>Spectral clustering and transductive learning with multiple views</article-title>. <source>Proc. ICML &#x02019;07</source>. <year>2007</year>; <fpage>1159</fpage>&#x02013;<lpage>1166</lpage>.</mixed-citation></ref><ref id="B27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Xia</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Pan</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Du</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Yin</surname><given-names>J.</given-names></name></person-group>
<article-title>Robust multi-view spectral clustering via low-rank and sparse decomposition</article-title>. <source>AAAI Conf. Artif. Intell.</source><year>2014</year>; <fpage>2149</fpage>&#x02013;<lpage>2155</lpage>.</mixed-citation></ref><ref id="B28"><label>28.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Bo</surname><given-names>Wang</given-names></name>, <name name-style="western"><surname>Jiayan</surname><given-names>Jiang</given-names></name>, <name name-style="western"><surname>Wei</surname><given-names>Wang</given-names></name>, <name name-style="western"><surname>Zhi-Hua</surname><given-names>Zhou</given-names></name>, <name name-style="western"><surname>Zhuowen</surname><given-names>Tu</given-names></name></person-group>
<article-title>Unsupervised metric fusion by cross diffusion</article-title>. <source>2012 IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2012</year>; <publisher-name>IEEE</publisher-name><fpage>2997</fpage>&#x02013;<lpage>3004</lpage>.</mixed-citation></ref><ref id="B29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Mezlini</surname><given-names>A.M.</given-names></name>, <name name-style="western"><surname>Demir</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Fiume</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Tu</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Brudno</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Haibe-Kains</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Goldenberg</surname><given-names>A.</given-names></name></person-group>
<article-title>Similarity network fusion for aggregating data types on a genomic scale</article-title>. <source>Nat. Methods</source>. <year>2014</year>; <volume>11</volume>:<fpage>333</fpage>&#x02013;<lpage>337</lpage>.<pub-id pub-id-type="pmid">24464287</pub-id></mixed-citation></ref><ref id="B30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Speicher</surname><given-names>N.K.</given-names></name>, <name name-style="western"><surname>Pfeifer</surname><given-names>N.</given-names></name></person-group>
<article-title>Integrating different data types by regularized unsupervised multiple kernel learning with application to cancer subtype discovery</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>i268</fpage>&#x02013;<lpage>i275</lpage>.<pub-id pub-id-type="pmid">26072491</pub-id></mixed-citation></ref><ref id="B31"><label>31.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>P.S.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>Z.M.</given-names></name></person-group>
<article-title>A General Model for Multiple View Unsupervised Learning</article-title>. <source>Proceedings of the 2008 SIAM International Conference on Data Mining</source>. <year>2008</year>; <publisher-loc>Philadelphia, PA</publisher-loc><publisher-name>Society for Industrial and Applied Mathematics</publisher-name><fpage>822</fpage>&#x02013;<lpage>833</lpage>.</mixed-citation></ref><ref id="B32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lock</surname><given-names>E.F.</given-names></name>, <name name-style="western"><surname>Hoadley</surname><given-names>K.A.</given-names></name>, <name name-style="western"><surname>Marron</surname><given-names>J.S.</given-names></name>, <name name-style="western"><surname>Nobel</surname><given-names>A.B.</given-names></name></person-group>
<article-title>Joint and individual variation explained (JIVE) for integrated analysis of multiple data types</article-title>. <source>Ann. Appl. Stat.</source><year>2013</year>; <volume>7</volume>:<fpage>523</fpage>&#x02013;<lpage>542</lpage>.<pub-id pub-id-type="pmid">23745156</pub-id></mixed-citation></ref><ref id="B33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>O&#x02019;Connell</surname><given-names>M.J.</given-names></name>, <name name-style="western"><surname>Lock</surname><given-names>E.F.</given-names></name></person-group>
<article-title>R. JIVE for exploration of multi-source molecular data</article-title>. <source>Bioinformatics</source>. <year>2016</year>; <volume>32</volume>:<fpage>2877</fpage>&#x02013;<lpage>2879</lpage>.<pub-id pub-id-type="pmid">27273669</pub-id></mixed-citation></ref><ref id="B34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hotelling</surname><given-names>H.</given-names></name></person-group>
<article-title>Relations between two sets of variates</article-title>. <source>Biometrika</source>. <year>1936</year>; <volume>28</volume>:<fpage>321</fpage></mixed-citation></ref><ref id="B35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Klami</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Virtanen</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Kaski</surname><given-names>S.</given-names></name></person-group>
<article-title>Bayesian canonical correlation analysis</article-title>. <source>J. Mach. Learn.</source><year>2013</year>; <volume>13</volume>:<fpage>723</fpage>&#x02013;<lpage>773</lpage>.</mixed-citation></ref><ref id="B36"><label>36.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lai</surname><given-names>P.L.</given-names></name>, <name name-style="western"><surname>Fyfe</surname><given-names>C.</given-names></name></person-group>
<article-title>Kernel and Nonlinear Canonical Correlation Analysis</article-title>. <source>Int. J. Neural Syst.</source><year>2000</year>; <volume>10</volume>:<fpage>365</fpage>&#x02013;<lpage>377</lpage>.<pub-id pub-id-type="pmid">11195936</pub-id></mixed-citation></ref><ref id="B37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Andrew</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Arora</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Bilmes</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Livescu</surname><given-names>K.</given-names></name></person-group>
<article-title>Deep canonical correlation analysis</article-title>. <source>Proc. ICML &#x02019;13</source>. <year>2013</year>; <volume>28</volume>:<fpage>1247</fpage>&#x02013;<lpage>1255</lpage>.</mixed-citation></ref><ref id="B38"><label>38.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Parkhomenko</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Tritchler</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Beyene</surname><given-names>J.</given-names></name></person-group>
<article-title>Sparse canonical correlation analysis with application to genomic data integration</article-title>. <source>Stat. Applic. Genet. Mol. Biol.</source><year>2009</year>; <volume>8</volume>:<fpage>1</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="B39"><label>39.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Witten</surname><given-names>D.M.</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R.J.</given-names></name></person-group>
<article-title>Extensions of sparse canonical correlation analysis with applications to genomic data</article-title>. <source>Stat. Applic. Genet. Mol. Biol.</source><year>2009</year>; <volume>8</volume>:<fpage>Article28</fpage></mixed-citation></ref><ref id="B40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>V&#x000ed;a</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Santamar&#x000ed;a</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>P&#x000e9;rez</surname><given-names>J.</given-names></name></person-group>
<article-title>A learning algorithm for adaptive canonical correlation analysis of several data sets</article-title>. <source>Neural Netw.</source><year>2007</year>; <volume>20</volume>:<fpage>139</fpage>&#x02013;<lpage>152</lpage>.<pub-id pub-id-type="pmid">17113263</pub-id></mixed-citation></ref><ref id="B41"><label>41.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Ramamohanarao</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Wen</surname><given-names>Y.</given-names></name></person-group>
<article-title>Tensor canonical correlation analysis for multi-view dimension reduction</article-title>. <source>Proc. ICDE 2016</source>. <year>2016</year>; <fpage>1460</fpage>&#x02013;<lpage>1461</lpage>.</mixed-citation></ref><ref id="B42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Bushman</surname><given-names>F.D.</given-names></name>, <name name-style="western"><surname>Lewis</surname><given-names>J.D.</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>G.D.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group>
<article-title>Structure-constrained sparse canonical correlation analysis with an application to microbiome data analysis</article-title>. <source>Biostatistics</source>. <year>2013</year>; <volume>14</volume>:<fpage>244</fpage>&#x02013;<lpage>58</lpage>.<pub-id pub-id-type="pmid">23074263</pub-id></mixed-citation></ref><ref id="B43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Calhoun</surname><given-names>V.D.</given-names></name>, <name name-style="western"><surname>Deng</surname><given-names>H.W.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y.P.</given-names></name></person-group>
<article-title>Group sparse canonical correlation analysis for genomic data integration</article-title>. <source>BMC Bioinformatics</source>. <year>2013</year>; <volume>14</volume>:<fpage>245</fpage><pub-id pub-id-type="pmid">23937249</pub-id></mixed-citation></ref><ref id="B44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rohart</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Gautier</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>L&#x000ea;&#x000a0;Cao</surname><given-names>K.-A.</given-names></name></person-group>
<article-title>mixOmics: An R package for &#x02018;omics feature selection and multiple data integration</article-title>. <source>PLoS Computat. Biol.</source><year>2017</year>; <volume>13</volume>:<fpage>e1005752</fpage></mixed-citation></ref><ref id="B45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wold</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Sj&#x000f6;str&#x000f6;m</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>L.</given-names></name></person-group>
<article-title>PLS-regression: A basic tool of chemometrics</article-title>. <source>Chemom. Intell. Lab. Syst.</source><year>2001</year>; <volume>58</volume>:<fpage>109</fpage>&#x02013;<lpage>130</lpage>.</mixed-citation></ref><ref id="B46"><label>46.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>L&#x000ea;&#x000a0;Cao</surname><given-names>K.-A.</given-names></name>, <name name-style="western"><surname>Rossouw</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Robert-Grani&#x000e9;</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Besse</surname><given-names>P.</given-names></name></person-group>
<article-title>A sparse PLS for variable selection when integrating omics data</article-title>. <source>Stat.Applic. Genet.Mol. Biol.</source><year>2008</year>; <volume>7</volume>:<comment>doi:10.2202/1544-6115.1390</comment></mixed-citation></ref><ref id="B47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>L&#x000ea;&#x000a0;Cao</surname><given-names>K.-A.</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>P.G.</given-names></name>, <name name-style="western"><surname>Robert-Grani&#x000e9;</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Besse</surname><given-names>P.</given-names></name></person-group>
<article-title>Sparse canonical methods for biological data integration: application to a cross-platform study</article-title>. <source>BMC Bioinformatics</source>. <year>2009</year>; <volume>10</volume>:<fpage>34</fpage><pub-id pub-id-type="pmid">19171069</pub-id></mixed-citation></ref><ref id="B48"><label>48.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Trygg</surname><given-names>J.</given-names></name></person-group>
<article-title>O2-PLS for qualitative and quantitative analysis in multivariate calibration</article-title>. <source>J. Chemometrics</source>. <year>2002</year>; <volume>16</volume>:<fpage>283</fpage>&#x02013;<lpage>293</lpage>.</mixed-citation></ref><ref id="B49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rosipal</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Trejo</surname><given-names>L.J.</given-names></name>, <name name-style="western"><surname>Cristianini</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Shawe-Taylor</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Williamson</surname><given-names>B.</given-names></name></person-group>
<article-title>Kernel partial least squares regression in reproducing kernel Hilbert space</article-title>. <source>J. Mach. Learn. Res.</source><year>2001</year>; <volume>2</volume>:<fpage>97</fpage>&#x02013;<lpage>123</lpage>.</mixed-citation></ref><ref id="B50"><label>50.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rantalainen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Bylesj&#x000f6;</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Cloarec</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Nicholson</surname><given-names>J.K.</given-names></name>, <name name-style="western"><surname>Holmes</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Trygg</surname><given-names>J.</given-names></name></person-group>
<article-title>Kernel-based orthogonal projections to latent structures (K-OPLS)</article-title>. <source>J. Chemometrics</source>. <year>2007</year>; <volume>21</volume>:<fpage>376</fpage>&#x02013;<lpage>385</lpage>.</mixed-citation></ref><ref id="B51"><label>51.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>C.-C.</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>X.J.</given-names></name></person-group>
<article-title>Identifying multi-layer gene regulatory modules from multi-dimensional genomic data</article-title>. <source>Bioinformatics</source>. <year>2012</year>; <volume>28</volume>:<fpage>2458</fpage>&#x02013;<lpage>2466</lpage>.<pub-id pub-id-type="pmid">22863767</pub-id></mixed-citation></ref><ref id="B52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>L&#x000f6;fstedt</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Trygg</surname><given-names>J.</given-names></name></person-group>
<article-title>OnPLS-a novel multiblock method for the modelling of predictive and orthogonal variation</article-title>. <source>J. Chemometrics</source>. <year>2011</year>; <volume>25</volume>:<fpage>441</fpage>&#x02013;<lpage>455</lpage>.</mixed-citation></ref><ref id="B53"><label>53.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Kuster</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Culhane</surname><given-names>A.C.</given-names></name>, <name name-style="western"><surname>Gholami</surname><given-names>A.</given-names></name></person-group>
<article-title>A multivariate approach to the integration of multi-omics datasets</article-title>. <source>BMC Bioinformatics</source>. <year>2014</year>; <volume>15</volume>:<fpage>162</fpage><pub-id pub-id-type="pmid">24884486</pub-id></mixed-citation></ref><ref id="B54"><label>54.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Han</surname><given-names>J.</given-names></name></person-group>
<article-title>Multi-View Clustering via Joint Nonnegative Matrix Factorization</article-title>. <source>Proc. ICDM &#x02019;13</source>. <year>2013</year>; <publisher-loc>Philadelphia, PA</publisher-loc><publisher-name>Society for Industrial and Applied Mathematics</publisher-name><fpage>252</fpage>&#x02013;<lpage>260</lpage>.</mixed-citation></ref><ref id="B55"><label>55.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Kalayeh</surname><given-names>M.M.</given-names></name>, <name name-style="western"><surname>Idrees</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Shah</surname><given-names>M.</given-names></name></person-group>
<article-title>NMF-KNN: Image annotation using weighted multi-view non-negative matrix factorization</article-title>. <source>2014 IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2014</year>; <fpage>184</fpage>&#x02013;<lpage>191</lpage>.</mixed-citation></ref><ref id="B56"><label>56.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Nie</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>C.</given-names></name></person-group>
<article-title>Robust Manifold Nonnegative Matrix Factorization</article-title>. <source>ACM Trans. Knowledge Discov. Data</source>. <year>2014</year>; <volume>8</volume>:<fpage>1</fpage>&#x02013;<lpage>21</lpage>.</mixed-citation></ref><ref id="B57"><label>57.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>C.-C.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>P.W.</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>X.J.</given-names></name></person-group>
<article-title>Discovery of multi-dimensional modules by integrative analysis of cancer genomic data</article-title>. <source>Nucleic Acids Res.</source><year>2012</year>; <volume>40</volume>:<fpage>9379</fpage>&#x02013;<lpage>9391</lpage>.<pub-id pub-id-type="pmid">22879375</pub-id></mixed-citation></ref><ref id="B58"><label>58.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Zong</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>H.</given-names></name></person-group>
<article-title>Constrained NMF-based multi-view clustering on unmapped data</article-title>. <source>Proc. AAAI &#x02019;15</source>. <year>2015</year>; <volume>4</volume>:<fpage>3174</fpage>&#x02013;<lpage>3180</lpage>.</mixed-citation></ref><ref id="B59"><label>59.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>S.-Y.</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>Z.-H.</given-names></name></person-group>
<article-title>Partial multi-view clustering</article-title>. <source>Proc. AAAI &#x02019;14</source>. <year>2014</year>; <publisher-name>AAAI Press</publisher-name><fpage>1968</fpage>&#x02013;<lpage>1974</lpage>.</mixed-citation></ref><ref id="B60"><label>60.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>&#x0017d;itnik</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Zupan</surname><given-names>B.</given-names></name></person-group>
<article-title>Data fusion by matrix factorization</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2015</year>; <volume>37</volume>:<fpage>41</fpage>&#x02013;<lpage>53</lpage>.<pub-id pub-id-type="pmid">26353207</pub-id></mixed-citation></ref><ref id="B61"><label>61.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>White</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Schuurmans</surname><given-names>D.</given-names></name></person-group>
<article-title>Convex multi-view subspace learning</article-title>. <source>Proc. NIPS &#x02019;12</source>. <year>2012</year>; <publisher-loc>USA</publisher-loc><fpage>1673</fpage>&#x02013;<lpage>1681</lpage>.</mixed-citation></ref><ref id="B62"><label>62.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name></person-group>
<article-title>Convex subspace representation learning from multi-view data</article-title>. <source>AAAI 2013</source>. <year>2013</year>; <fpage>387</fpage>&#x02013;<lpage>393</lpage>.</mixed-citation></ref><ref id="B63"><label>63.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Fu</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Cao</surname><given-names>X.</given-names></name></person-group>
<article-title>Low-rank tensor constrained multiview subspace clustering</article-title>. <source>Proc. ICCV &#x02019;15</source>. <year>2015</year>; <publisher-name>IEEE</publisher-name><fpage>1582</fpage>&#x02013;<lpage>1590</lpage>.</mixed-citation></ref><ref id="B64"><label>64.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mo</surname><given-names>Q.</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Seshan</surname><given-names>V.E.</given-names></name>, <name name-style="western"><surname>Olshen</surname><given-names>A.B.</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Sander</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Powers</surname><given-names>R.S.</given-names></name>, <name name-style="western"><surname>Ladanyi</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>R.</given-names></name></person-group>
<article-title>Pattern discovery and cancer gene identification in integrated cancer genomic data</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2013</year>; <volume>110</volume>:<fpage>4245</fpage>&#x02013;<lpage>4250</lpage>.<pub-id pub-id-type="pmid">23431203</pub-id></mixed-citation></ref><ref id="B65"><label>65.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mo</surname><given-names>Q.</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Guo</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Vannucci</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Chan</surname><given-names>K.S.</given-names></name>, <name name-style="western"><surname>Hilsenbeck</surname><given-names>S.G.</given-names></name></person-group>
<article-title>A fully Bayesian latent variable model for integrative clustering analysis of multi-type omics data</article-title>. <source>Biostatistics</source>. <year>2018</year>; <volume>19</volume>:<fpage>71</fpage>&#x02013;<lpage>86</lpage>.<pub-id pub-id-type="pmid">28541380</pub-id></mixed-citation></ref><ref id="B66"><label>66.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Vaske</surname><given-names>C.J.</given-names></name>, <name name-style="western"><surname>Benz</surname><given-names>S.C.</given-names></name>, <name name-style="western"><surname>Sanborn</surname><given-names>J.Z.</given-names></name>, <name name-style="western"><surname>Earl</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Szeto</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Haussler</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Stuart</surname><given-names>J.M.</given-names></name></person-group>
<article-title>Inference of patient-specific pathway activities from multi-dimensional cancer genomics data using PARADIGM</article-title>. <source>Bioinformatics</source>. <year>2010</year>; <volume>26</volume>:<fpage>i237</fpage>&#x02013;<lpage>i245</lpage>.<pub-id pub-id-type="pmid">20529912</pub-id></mixed-citation></ref><ref id="B67"><label>67.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Savage</surname><given-names>R.S.</given-names></name>, <name name-style="western"><surname>Ghahramani</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Griffin</surname><given-names>J.E.</given-names></name>, <name name-style="western"><surname>de&#x000a0;la&#x000a0;Cruz</surname><given-names>B.J.</given-names></name>, <name name-style="western"><surname>Wild</surname><given-names>D.L.</given-names></name></person-group>
<article-title>Discovering transcriptional modules by Bayesian data integration</article-title>. <source>Bioinformatics</source>. <year>2010</year>; <volume>26</volume>:<fpage>i158</fpage>&#x02013;<lpage>i167</lpage>.<pub-id pub-id-type="pmid">20529901</pub-id></mixed-citation></ref><ref id="B68"><label>68.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Savage</surname><given-names>R.S.</given-names></name>, <name name-style="western"><surname>Markowetz</surname><given-names>F.</given-names></name></person-group>
<article-title>Patient-specific data fusion defines prognostic cancer subtypes</article-title>. <source>PLoS Comput. Biol.</source><year>2011</year>; <volume>7</volume>:<fpage>e1002227</fpage><pub-id pub-id-type="pmid">22028636</pub-id></mixed-citation></ref><ref id="B69"><label>69.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Kirk</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Griffin</surname><given-names>J.E.</given-names></name>, <name name-style="western"><surname>Savage</surname><given-names>R.S.</given-names></name>, <name name-style="western"><surname>Ghahramani</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Wild</surname><given-names>D.L.</given-names></name></person-group>
<article-title>Bayesian correlated clustering to integrate multiple datasets</article-title>. <source>Bioinformatics</source>. <year>2012</year>; <volume>28</volume>:<fpage>3290</fpage>&#x02013;<lpage>3297</lpage>.<pub-id pub-id-type="pmid">23047558</pub-id></mixed-citation></ref><ref id="B70"><label>70.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lock</surname><given-names>E.F.</given-names></name>, <name name-style="western"><surname>Dunson</surname><given-names>D.B.</given-names></name></person-group>
<article-title>Bayesian consensus clustering</article-title>. <source>Bioinformatics</source>. <year>2013</year>; <volume>29</volume>:<fpage>2610</fpage>&#x02013;<lpage>2616</lpage>.<pub-id pub-id-type="pmid">23990412</pub-id></mixed-citation></ref><ref id="B71"><label>71.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Gabasova</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Reid</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Wernisch</surname><given-names>L.</given-names></name></person-group>
<article-title>Clusternomics: Integrative context-dependent clustering for heterogeneous datasets</article-title>. <source>PLOS Comput. Biol.</source><year>2017</year>; <volume>13</volume>:<fpage>e1005781</fpage><pub-id pub-id-type="pmid">29036190</pub-id></mixed-citation></ref><ref id="B72"><label>72.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ahmad</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fr&#x000f6;hlich</surname><given-names>H.</given-names></name></person-group>
<article-title>Towards clinically more relevant dissection of patient heterogeneity via survival-based Bayesian clustering</article-title>. <source>Bioinformatics</source>. <year>2017</year>; <volume>33</volume>:<fpage>3558</fpage>&#x02013;<lpage>3566</lpage>.<pub-id pub-id-type="pmid">28961917</pub-id></mixed-citation></ref><ref id="B73"><label>73.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Coretto</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Serra</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Tagliaferri</surname><given-names>R.</given-names></name></person-group>
<article-title>Robust clustering of noisy high-dimensional gene expression data for patients subtyping</article-title>. <source>Bioinformatics</source>. <year>2018</year>; <comment>doi:10.1093/bioinformatics/bty502</comment></mixed-citation></ref><ref id="B74"><label>74.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Chaudhary</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Poirion</surname><given-names>O.B.</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Garmire</surname><given-names>L.X.</given-names></name></person-group>
<article-title>Deep learning-based multi-omics integration robustly predicts survival in liver cancer</article-title>. <source>Clin. Cancer Res.</source><year>2018</year>; <volume>24</volume>:<fpage>1248</fpage>&#x02013;<lpage>1259</lpage>.<pub-id pub-id-type="pmid">28982688</pub-id></mixed-citation></ref><ref id="B75"><label>75.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Liang</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Zeng</surname><given-names>J.</given-names></name></person-group>
<article-title>Integrative data analysis of multi-platform cancer data with a multimodal deep learning approach</article-title>. <source>IEEE/ACM Trans. Comput. Biol. Bioinformatics</source>. <year>2015</year>; <volume>12</volume>:<fpage>928</fpage>&#x02013;<lpage>937</lpage>.</mixed-citation></ref><ref id="B76"><label>76.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bickel</surname><given-names>P.J.</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Tsybakov</surname><given-names>A.B.</given-names></name>, <name name-style="western"><surname>van&#x000a0;de&#x000a0;Geer</surname><given-names>S.A.</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Vald&#x000e9;s</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Rivero</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Fan</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>van&#x000a0;der&#x000a0;Vaart</surname><given-names>A.</given-names></name></person-group>
<article-title>Regularization in statistics</article-title>. <source>Test</source>. <year>2006</year>; <volume>15</volume>:<fpage>271</fpage>&#x02013;<lpage>344</lpage>.</mixed-citation></ref><ref id="B77"><label>77.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group>
<article-title>Regression Selection and Shrinkage via the Lasso</article-title>. <source>J. R. Stat. Soc. B</source>. <year>1996</year>; <volume>58</volume>:<fpage>267</fpage>&#x02013;<lpage>288</lpage>.</mixed-citation></ref><ref id="B78"><label>78.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Blum</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Mitchell</surname><given-names>T.</given-names></name></person-group>
<article-title>Combining labeled and unlabeled data with co-training</article-title>. <source>Proc. COLT &#x02019;98</source>. <year>1998</year>; <publisher-loc>NY</publisher-loc><publisher-name>ACM Press</publisher-name><fpage>92</fpage>&#x02013;<lpage>100</lpage>.</mixed-citation></ref><ref id="B79"><label>79.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Monti</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Tamayo</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Mesirov</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Golub</surname><given-names>T.</given-names></name></person-group>
<article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</article-title>. <source>Mach. Learn.</source><year>2003</year>; <volume>52</volume>:<fpage>91</fpage>&#x02013;<lpage>118</lpage>.</mixed-citation></ref><ref id="B80"><label>80.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Hofmann</surname><given-names>T.</given-names></name></person-group>
<article-title>Probabilistic latent semantic analysis</article-title>. <source>Proc. UAI &#x02019;99</source>. <year>1999</year>; <publisher-loc>San Francisco</publisher-loc><publisher-name>Morgan Kaufmann Publishers Inc</publisher-name><fpage>289</fpage>&#x02013;<lpage>296</lpage>.</mixed-citation></ref><ref id="B81"><label>81.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Vega-Pons</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Ruiz-Shulcloper</surname><given-names>J.</given-names></name></person-group>
<article-title>A Survey of clustering ensemble algorithms</article-title>. <source>Int. J. Pattern Recognit. Artif. Intell.</source><year>2011</year>; <volume>25</volume>:<fpage>337</fpage>&#x02013;<lpage>372</lpage>.</mixed-citation></ref><ref id="B82"><label>82.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>von&#x000a0;Luxburg</surname><given-names>U.</given-names></name></person-group>
<article-title>A tutorial on spectral clustering</article-title>. <source>Stat. Comput.</source><year>2007</year>; <volume>17</volume>:<fpage>395</fpage>&#x02013;<lpage>416</lpage>.</mixed-citation></ref><ref id="B83"><label>83.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Mohar</surname><given-names>B.</given-names></name></person-group>
<article-title>The Laplacian spectrum of graphs</article-title>. <source>Graph Theory Combinatorics Applic.</source><year>1991</year>; <volume>2</volume>:<fpage>871</fpage>&#x02013;<lpage>898</lpage>.</mixed-citation></ref><ref id="B84"><label>84.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lo&#x000a0;Asz</surname><given-names>L.</given-names></name></person-group>
<article-title>Random walks on graphs: a survey</article-title>. <source>Combinatorics</source>. <year>1993</year>; <fpage>1</fpage>&#x02013;<lpage>46</lpage>.</mixed-citation></ref><ref id="B85"><label>85.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Pearl</surname><given-names>J.</given-names></name></person-group>
<source>Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference</source>. <year>1988</year>; <publisher-name>Morgan Kaufmann Publishers</publisher-name></mixed-citation></ref><ref id="B86"><label>86.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>D.R.</given-names></name>, <name name-style="western"><surname>Oakes</surname><given-names>D.</given-names></name></person-group>
<source>Analysis of Survival Data</source>. <year>1984</year>; <publisher-name>Chapman and Hall</publisher-name></mixed-citation></ref><ref id="B87"><label>87.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Chaudhuri</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Kakade</surname><given-names>S.M.</given-names></name>, <name name-style="western"><surname>Livescu</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Sridharan</surname><given-names>K.</given-names></name></person-group>
<article-title>Multi-view clustering via canonical correlation analysis</article-title>. <source>Proc. ICML &#x02019;09</source>. <year>2009</year>; <fpage>1</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="B88"><label>88.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bach</surname><given-names>F.R.</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>M.I.</given-names></name></person-group>
<article-title>A probabilistic interpretation of canonical correlation analysis</article-title>. <source>Dept. Statist. Univ. California Berkeley CA Tech. Rep.</source><year>2006</year>; <volume>688</volume>:<fpage>1</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="B89"><label>89.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Bylesj&#x000f6;</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Kusano</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Moritz</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Trygg</surname><given-names>J.</given-names></name></person-group>
<article-title>Data integration in plant biology: The O2PLS method for combined modeling of transcript and metabolite data</article-title>. <source>Plant J.</source><year>2007</year>; <volume>52</volume>:<fpage>1181</fpage>&#x02013;<lpage>1191</lpage>.<pub-id pub-id-type="pmid">17931352</pub-id></mixed-citation></ref><ref id="B90"><label>90.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>el&#x000a0;Bouhaddani</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Houwing-Duistermaat</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Salo</surname><given-names>P.</given-names></name>, <name name-style="western"><surname>Perola</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Jongbloed</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Uh</surname><given-names>H.-W.</given-names></name></person-group>
<article-title>Evaluation of O2PLS in omics data integration</article-title>. <source>BMC Bioinformatics</source>. <year>2016</year>; <volume>17</volume>:<fpage>S11</fpage></mixed-citation></ref><ref id="B91"><label>91.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hwang</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Stephanopoulos</surname><given-names>G.</given-names></name>, <name name-style="western"><surname>Chan</surname><given-names>C.</given-names></name></person-group>
<article-title>Inverse modeling using multi-block PLS to determine the environmental conditions that provide optimal cellular function</article-title>. <source>Bioinformatics</source>. <year>2004</year>; <volume>20</volume>:<fpage>487</fpage>&#x02013;<lpage>499</lpage>.<pub-id pub-id-type="pmid">14990444</pub-id></mixed-citation></ref><ref id="B92"><label>92.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Dray</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Chessel</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Thioulouse</surname><given-names>J.</given-names></name></person-group>
<article-title>Co-inertia analysis and the linking of ecological data tables</article-title>. <source>Ecology</source>. <year>2003</year>; <volume>84</volume>:<fpage>3078</fpage>&#x02013;<lpage>3089</lpage>.</mixed-citation></ref><ref id="B93"><label>93.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Seung</surname><given-names>H.S.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D.D.</given-names></name></person-group>
<article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>. <year>1999</year>; <volume>401</volume>:<fpage>788</fpage>&#x02013;<lpage>791</lpage>.<pub-id pub-id-type="pmid">10548103</pub-id></mixed-citation></ref><ref id="B94"><label>94.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>D.D.</given-names></name>, <name name-style="western"><surname>Seung</surname><given-names>H.S.</given-names></name></person-group>
<article-title>Algorithms for non-negative matrix factorization</article-title>. <source>Adv. Neural Inf. Proc. Syst.</source><year>2001</year>; <fpage>535</fpage>&#x02013;<lpage>541</lpage>.</mixed-citation></ref><ref id="B95"><label>95.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>&#x0017d;itnik</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Zupan</surname><given-names>B.</given-names></name></person-group>
<article-title>Survival regression by data fusion</article-title>. <source>Syst. Biomed.</source><year>2015</year>; <volume>2</volume>:<fpage>47</fpage>&#x02013;<lpage>53</lpage>.</mixed-citation></ref><ref id="B96"><label>96.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hoadley</surname><given-names>A.</given-names></name><etal/></person-group>
<article-title>Cell-of-origin patterns dominate the molecular classification of 10,000 tumors from 33 types of cancer</article-title>. <source>Cell</source>. <year>2018</year>; <volume>173</volume>:<fpage>291</fpage>&#x02013;<lpage>304</lpage>.<pub-id pub-id-type="pmid">29625048</pub-id></mixed-citation></ref><ref id="B97"><label>97.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ng</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Collisson</surname><given-names>E.A.</given-names></name>, <name name-style="western"><surname>Sokolov</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Goldstein</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Gonzalez-Perez</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Lopez-Bigas</surname><given-names>N.</given-names></name>, <name name-style="western"><surname>Benz</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Haussler</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Stuart</surname><given-names>J.M.</given-names></name></person-group>
<article-title>PARADIGM-SHIFT predicts the function of mutations in multiple cancers using pathway impact analysis</article-title>. <source>Bioinformatics</source>. <year>2012</year>; <volume>28</volume>:<fpage>i640</fpage>&#x02013;<lpage>i646</lpage>.<pub-id pub-id-type="pmid">22962493</pub-id></mixed-citation></ref><ref id="B98"><label>98.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Geman</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Geman</surname><given-names>D.</given-names></name></person-group>
<article-title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell., PAMI-6</source>. <year>1984</year>; <fpage>721</fpage>&#x02013;<lpage>741</lpage>.</mixed-citation></ref><ref id="B99"><label>99.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G.</given-names></name></person-group>
<article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>; <volume>521</volume>:<fpage>436</fpage>&#x02013;<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="B100"><label>100.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Geoffrey</surname><given-names>E. H.</given-names></name></person-group>
<article-title>ImageNet classification with deep Convolutional neural Networks</article-title>. <source>Proc. NIPS &#x02019;12</source>. <year>2012</year>; <volume>1</volume>:<fpage>1097</fpage>&#x02013;<lpage>1105</lpage>.</mixed-citation></ref><ref id="B101"><label>101.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Sutskever</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Vinyals</surname><given-names>O.</given-names></name>, <name name-style="western"><surname>Le</surname><given-names>Q.V.</given-names></name></person-group>
<article-title>Sequence to sequence learning with neural networks</article-title>. <source>Proc. NIPS&#x02019;14</source>. <year>2014</year>; <publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>3104</fpage>&#x02013;<lpage>3112</lpage>.</mixed-citation></ref><ref id="B102"><label>102.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ngiam</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Khosla</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Nam</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>H.</given-names></name>, <name name-style="western"><surname>Ng</surname><given-names>A.Y.</given-names></name></person-group>
<article-title>Multimodal deep learning</article-title>. <source>Proc. ICML &#x02019;11</source>. <year>2011</year>; <fpage>689</fpage>&#x02013;<lpage>696</lpage>.</mixed-citation></ref><ref id="B103"><label>103.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Arora</surname><given-names>R.</given-names></name>, <name name-style="western"><surname>Livescu</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Bilmes</surname><given-names>J.</given-names></name></person-group>
<article-title>On deep multi-view representation learning: objectives and optimization</article-title>. <source>Proc. ICML &#x02019;16</source>. <year>2016</year>; <fpage>1083</fpage>&#x02013;<lpage>1092</lpage>.</mixed-citation></ref><ref id="B104"><label>104.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Ching</surname><given-names>T.</given-names></name>, <name name-style="western"><surname>Himmelstein</surname><given-names>D.S.</given-names></name>, <name name-style="western"><surname>Beaulieu-Jones</surname><given-names>B.K.</given-names></name>, <name name-style="western"><surname>Kalinin</surname><given-names>A.A.</given-names></name>, <name name-style="western"><surname>Do</surname><given-names>B.T.</given-names></name>, <name name-style="western"><surname>Way</surname><given-names>G.P.</given-names></name>, <name name-style="western"><surname>Ferrero</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Agapow</surname><given-names>P.-M.</given-names></name>, <name name-style="western"><surname>Zietz</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Hoffman</surname><given-names>M.M.</given-names></name><etal/></person-group>
<article-title>Opportunities and obstacles for deep learning in biology and medicine</article-title>. <source>J. R. Soc. Interface</source>. <year>2018</year>; <volume>15</volume>:<fpage>20170387</fpage><pub-id pub-id-type="pmid">29618526</pub-id></mixed-citation></ref><ref id="B105"><label>105.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Hinton</surname><given-names>G.E.</given-names></name>, <name name-style="western"><surname>Osindero</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>Y.-W.</given-names></name></person-group>
<article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural Comput.</source><year>2006</year>; <volume>18</volume>:<fpage>1527</fpage>&#x02013;<lpage>1554</lpage>.<pub-id pub-id-type="pmid">16764513</pub-id></mixed-citation></ref><ref id="B106"><label>106.</label><mixed-citation publication-type="book">
<person-group person-group-type="author"><name name-style="western"><surname>Hosmer</surname><given-names>D.W.</given-names></name>, <name name-style="western"><surname>Lemeshow</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>May</surname><given-names>S.</given-names></name></person-group>
<source>Applied Survival Analysis: Regression Modeling of Time-to-Event Data</source>. <year>2008</year>; <publisher-name>Wiley-Interscience</publisher-name></mixed-citation></ref><ref id="B107"><label>107.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Rousseeuw</surname><given-names>P.J.</given-names></name>, <name name-style="western"><surname>Peter</surname></name></person-group>
<article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>J. Comput. Appl. Math.</source><year>1987</year>; <volume>20</volume>:<fpage>53</fpage>&#x02013;<lpage>65</lpage>.</mixed-citation></ref><ref id="B108"><label>108.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Vandin</surname><given-names>F.</given-names></name>, <name name-style="western"><surname>Papoutsaki</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Raphael</surname><given-names>B.J.</given-names></name>, <name name-style="western"><surname>Upfal</surname><given-names>E.</given-names></name></person-group>
<article-title>Accurate Computation of Survival Statistics in Genome-Wide Studies</article-title>. <source>PLOS Comput. Biol.</source><year>2015</year>; <volume>11</volume>:<fpage>1</fpage>&#x02013;<lpage>18</lpage>.</mixed-citation></ref><ref id="B109"><label>109.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author"><name name-style="western"><surname>Aure</surname><given-names>M.R.</given-names></name>, <name name-style="western"><surname>Steinfeld</surname><given-names>I.</given-names></name>, <name name-style="western"><surname>Baumbusch</surname><given-names>L.O.</given-names></name>, <name name-style="western"><surname>Liest&#x000f8;l</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Lipson</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Nyberg</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Naume</surname><given-names>B.</given-names></name>, <name name-style="western"><surname>Sahlberg</surname><given-names>K.K.</given-names></name>, <name name-style="western"><surname>Kristensen</surname><given-names>V.N.</given-names></name>, <name name-style="western"><surname>B&#x000f8;rresen-Dale</surname><given-names>A.-L.</given-names></name><etal/></person-group>
<article-title>Identifying in-trans process associated genes in breast cancer by integrated analysis of copy number and expression data</article-title>. <source>PLoS ONE</source>. <year>2013</year>; <volume>8</volume>:<fpage>1</fpage>&#x02013;<lpage>15</lpage>.</mixed-citation></ref></ref-list></back></article>