<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5738356</article-id><article-id pub-id-type="publisher-id">1827</article-id><article-id pub-id-type="doi">10.1038/s41467-017-01827-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Supervised learning in spiking neural networks with FORCE training</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Nicola</surname><given-names>Wilten</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Clopath</surname><given-names>Claudia</given-names></name><address><email>c.clopath@imperial.ac.uk</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2113 8111</institution-id><institution-id institution-id-type="GRID">grid.7445.2</institution-id><institution>Department of Bioengineering, </institution><institution>Imperial College London, Royal School of Mines, </institution></institution-wrap>London, SW7 2AZ UK </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>12</month><year>2017</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>12</month><year>2017</year></pub-date><pub-date pub-type="collection"><year>2017</year></pub-date><volume>8</volume><elocation-id>2208</elocation-id><history><date date-type="received"><day>19</day><month>12</month><year>2016</year></date><date date-type="accepted"><day>19</day><month>10</month><year>2017</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2017</copyright-statement><license license-type="OpenAccess"><license-p>
<bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Populations of neurons display an extraordinary diversity in the behaviors they affect and display. Machine learning techniques have recently emerged that allow us to create networks of model neurons that display behaviors of similar complexity. Here we demonstrate the direct applicability of one such technique, the FORCE method, to spiking neural networks. We train these networks to mimic dynamical systems, classify inputs, and store discrete sequences that correspond to the notes of a song. Finally, we use FORCE training to create two biologically motivated model circuits. One is inspired by the zebra finch and successfully reproduces songbird singing. The second network is motivated by the hippocampus and is trained to store and replay a movie scene. FORCE trained networks reproduce behaviors comparable in complexity to their inspired circuits and yield information not easily obtainable with other techniques, such as behavioral responses to pharmacological manipulations and spike timing statistics.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">FORCE training is a . Here the authors implement FORCE training in models of spiking neuronal networks and demonstrate that these networks can be trained to exhibit different dynamic behaviours.</p></abstract><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2017</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Human beings can naturally learn to perform a wide variety of tasks quickly and efficiently. Examples include learning the complicated sequence of motions in order to take a slap-shot in Hockey or learning to replay the notes of a song after music class. While there are approaches to solving these different problems in fields such as machine learning, control theory, and so on, humans use a very distinct tool to solve these problems: the spiking neural network.</p><p id="Par4">Recently, a broad class of techniques have been derived that allow us to enforce a certain behavior or dynamics onto a neural network<sup><xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR10">10</xref></sup>. These top-down techniques start with an intended task that a recurrent spiking neural network should perform, and determine what the connection strengths between neurons should be to achieve this. The dominant approaches are currently the FORCE method<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, spike-based or predictive coding networks<sup><xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref></sup>, and the neural engineering framework (NEF)<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. Both the NEF and spike-based coding approaches have yielded substantial insights into network functioning. Examples include how networks can be organized to solve behavioral problems<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> or process information in robust but metabolically efficient ways<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>.</p><p id="Par5">While the NEF and spike-based coding approaches create functional spiking networks, they are not agnostic toward the underlying network of neurons (although see refs. <sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup> for recent advances). Second, in order to apply either approach, the task has to be specified in terms of closed-form differential equations. This is a constraint on the potential problems these networks can solve (although see refs. <sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup> for recent advances). Despite these constraints, both the NEF and spike-based coding techniques have led to a resurgence in the top-down analysis of network function<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par6">Fortunately, FORCE training is agnostic toward both the network under consideration, or the tasks that the network has to solve. Originating in the field of reservoir computing, FORCE training takes a high dimensional dynamical system and utilizes this systems complex dynamics to perform computations<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR14">14</xref>&#x02013;<xref ref-type="bibr" rid="CR25">25</xref></sup>. Unlike other techniques, the target behavior does not have to be specified as a closed form differential equation for training. All that is required for training is a supervisor to provide an error signal. The use of any high dimensional dynamical system as a reservoir makes the FORCE method applicable to many more network types while the use of an error signal expands the potential applications for these networks. Unfortunately, FORCE training has only been directly implemented in networks of rate equations that phenomenologically represent how a firing rate varies smoothly in time, with little work done in spiking network implementations in the literature (although see refs. <sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup> for recent advances).</p><p id="Par7">We show that FORCE training can be directly applied to spiking networks and is robust against different implementations, neuron models, and potential supervisors. To demonstrate the potential of these networks, we FORCE train two spiking networks that are biologically motivated. The first of these circuits corresponds to the zebra finch HVC-RA circuit and reproduces the singing behavior while the second circuit is inspired by the hippocampus and encodes and replays a movie scene. Both circuits can be perturbed to determine how robust circuit functioning is post-training.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>FORCE training weight matrices</title><p id="Par8">We explored the potential of the FORCE method in training spiking neural networks to perform an arbitrary task. In these networks, the synaptic weight matrix is the sum of a set of static weights and a set of learned weights <italic>&#x003c9;</italic>&#x02009;=&#x02009;<italic>G&#x003c9;</italic>
<sup>0</sup>&#x02009;+&#x02009;<italic>Q&#x003b7;&#x003d5;</italic>
<sup><italic>T</italic></sup>. The static weights, <italic>G&#x003c9;</italic>
<sup>0</sup> are set to initialize the network into chaotic spiking<sup><xref ref-type="bibr" rid="CR26">26</xref>&#x02013;<xref ref-type="bibr" rid="CR28">28</xref></sup>. The learned component of the weights, <italic>&#x003d5;</italic>, are determined online using a supervised learning method called Recursive Least Squares (RLS). The quantity <italic>&#x003d5;</italic> also serves as a linear decoder for the network dynamics. The parameter <italic>&#x003b7;</italic> defines the tuning preferences of the neurons to the learned feedback term. The components of <italic>&#x003b7;</italic> are static and always randomly drawn. The parameters <italic>G</italic> and <italic>Q</italic> control the relative balance between the chaos inducing static weight matrix and the learned feedback term, respectively. The parameters are discussed in greater detail in&#x000a0;the Methods section. The goal of RLS is to minimize the squared error between the network dynamics <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {\hat x(t)} \right)$$\end{document}</tex-math><mml:math id="M2"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq1.gif"/></alternatives></inline-formula> and the target dynamics (i.e., the task, <italic>x</italic>(<italic>t</italic>)) (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>)<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. This method is successful if the network dynamics can mimic the target dynamics when RLS is turned off. We considered three types of spiking integrate-and-fire model: the theta model, the leaky integrate-and-fire (LIF), and the Izhikevich model (see Methods section for a more detailed explanation). The parameters for the models can be found in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. All networks considered were constrained to be intermediate in size (1000&#x02013;5000 neurons) and have low post-training average firing rates (&#x0003c;60&#x02009;Hz). The synaptic time constants were typically <italic>&#x003c4;</italic>
<sub>R</sub>&#x02009;=&#x02009;2&#x02009;ms and <italic>&#x003c4;</italic>
<sub>D</sub>&#x02009;=&#x02009;20&#x02009;ms with other values considered in Supplementary Material. The code used for this paper can be found on modelDB (<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>), under accession number 190565.<fig id="Fig1"><label>Fig. 1</label><caption><p>The FORCE method explained. <bold>a</bold> In the FORCE method, a spiking neural network contains a backbone of static and strong synaptic weights that scale like <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1{\mathrm{/}}\sqrt N$$\end{document}</tex-math><mml:math id="M4"><mml:mn>1</mml:mn><mml:mi mathvariant="normal">&#x02215;</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq2.gif"/></alternatives></inline-formula> to induce network level chaos (blue). A secondary set of weights are added to the weight matrix with the decoders determined through a time optimization procedure (red). The Recursive Least Squares technique (RLS) is used in all subsequent simulations. FORCE training requires a supervisor <italic>x</italic>(<italic>t</italic>) to estimate with <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat x(t)$$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq3.gif"/></alternatives></inline-formula>. <bold>b</bold> Prior to learning, the firing rates for 5 random neurons from a network of 1000 rate equations are in the chaotic regime. The chaos is controlled and converted to steady-state oscillations. <bold>c</bold> This allows the network to represent a 5&#x02009;Hz sinusoidal input (black). After learning, the network (blue) still displays the 5&#x02009;Hz sinusoidal oscillation as its macroscopic dynamics and the training is successful. The total training time is 5&#x02009;s. <bold>d</bold> The decoders for 20 randomly selected neurons in the network, before (<italic>t</italic>&#x02009;&#x0003c;&#x02009;5), during (5&#x02009;&#x02264;&#x02009;<italic>t</italic>&#x02009;&#x0003c;&#x02009;10) and after (<italic>t</italic>&#x02009;&#x02265;&#x02009;10) FORCE training. <bold>e</bold> The eigenvalue spectrum for the effective weight matrix before (red) and after (black) FORCE training. Note the lack of dominant eigenvalues in the weight matrix</p></caption><graphic xlink:href="41467_2017_1827_Fig1_HTML" id="d29e469"/></fig>
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Model parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Neuron model</th><th>Parameter</th><th>Value</th></tr></thead><tbody><tr><td>Izhikevich</td><td>
<italic>C</italic>
</td><td>250&#x02009;&#x003bc;F</td></tr><tr><td/><td>
<italic>v</italic>
<sub>r</sub>
</td><td>&#x02212;60&#x02009;mV</td></tr><tr><td/><td>
<italic>v</italic>
<sub><italic>t</italic></sub>
</td><td>&#x02212;20&#x02009;mV (&#x02212;40&#x02009;mV songbird example)</td></tr><tr><td/><td>
<italic>b</italic>
</td><td>0&#x000a0;nS</td></tr><tr><td/><td>
<italic>v</italic>
<sub>peak</sub>
</td><td>30&#x02009;mV</td></tr><tr><td/><td>
<italic>v</italic>
<sub>reset</sub>
</td><td>&#x02212;65&#x02009;mV</td></tr><tr><td/><td>
<italic>a</italic>
</td><td>0.01 ms<sup>&#x02212;1</sup>&#x000a0;(0.002&#x000a0;ms<sup>&#x02212;1</sup>, songbird example)</td></tr><tr><td/><td>
<italic>d</italic>
</td><td>200&#x000a0;pA (100&#x000a0;pA, songbird example)</td></tr><tr><td/><td>
<italic>I</italic>
<sub>Bias</sub>
</td><td>1000&#x02009;pA</td></tr><tr><td/><td>
<italic>k</italic>
</td><td>2.5&#x000a0;ns/mV</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>R</sub>
</td><td>2&#x02009;ms</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>D</sub>
</td><td>20&#x02009;ms</td></tr><tr><td>Theta model</td><td>
<italic>I</italic>
<sub>Bias</sub>
</td><td>0</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>R</sub>
</td><td>2&#x000a0;ms</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>D</sub>
</td><td>20&#x000a0;ms</td></tr><tr><td>LIF model</td><td>
<italic>&#x003c4;</italic>
<sub>m</sub>
</td><td>10&#x02009;ms</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>ref</sub>
</td><td>2&#x02009;ms</td></tr><tr><td/><td>
<italic>v</italic>
<sub>reset</sub>
</td><td>&#x02212;65&#x02009;mV</td></tr><tr><td/><td>
<italic>v</italic>
<sub>t</sub>
</td><td>&#x02212;40&#x02009;mV</td></tr><tr><td/><td>
<italic>I</italic>
<sub>Bias</sub>
</td><td>&#x02212;40&#x02009;pA (&#x02212;39&#x02009;pA, Lorenz example)</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>R</sub>
</td><td>2&#x02009;ms</td></tr><tr><td/><td>
<italic>&#x003c4;</italic>
<sub>D</sub>
</td><td>20&#x02009;ms</td></tr></tbody></table><table-wrap-foot><p>The parameters used for the Izhikevich, theta, and LIF neuron models, unless otherwise stated</p></table-wrap-foot></table-wrap>
</p></sec><sec id="Sec4"><title>FORCE trained rate networks learn using chaos</title><p id="Par9">To demonstrate the basic principle of this method and to compare with our spiking network simulations, we applied FORCE training to a network of rate equations (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>) as in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. We trained a network to learn a simple 5&#x02009;Hz sinusoidal oscillator. The static weight matrix initializes high-dimensional chaotic dynamics onto the network of rate equations (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>). These dynamics form a suitable reservoir to allow the network to learn from a target signal quickly. As in the original formulation of the FORCE method, the rates are heterogeneous across the network and varied strongly in time (see Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref> in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>). RLS is activated after a short initialization period for the network. After learning the appropriate weights <italic>&#x003d5;</italic>
<sub><italic>j</italic></sub> (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1d</xref>), the network reproduces the oscillation without any guidance from the teacher signal, albeit with a slight frequency and amplitude error (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1c</xref>). To ascertain how these networks can learn to perform the target dynamics, we computed the eigenvalues of the resulting weight matrix before and after learning (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1e</xref>). Unlike other top-down techniques, FORCE trained weight matrices are always high rank<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, and as we will demonstrate, can have dominant eigenvalues for large <italic>Q</italic>.<fig id="Fig2"><label>Fig. 2</label><caption><p>Using spiking neural networks to mimic dynamics with FORCE training. <bold>a</bold> The voltage trace for 5 randomly selected neurons in networks of 2000 integrate-and-fire spiking neurons. The models under consideration are the theta neuron (left), the leaky integrate-and-fire neuron (middle), and the Izhikevich model with spike frequency adaptation (right). For all networks under consideration, a spike was deleted (black arrow) from one of the neurons. This caused the spike train to diverge post-deletion, a clear indication of chaotic behavior. <bold>b</bold> A network of 2000 theta neurons (blue) was initialized in the chaotic regime and trained to mimic different oscillators (black) with FORCE training. The oscillators included the sinusoid, Van der Pol in harmonic and relaxation regimes, a non-smooth sawtooth oscillator, the oscillator formed from taking the product of a pair of sinusoids with 4&#x02009;Hz and 6&#x02009;Hz frequencies, and the same product of sinusoids with a Gaussian additive white noise distortion with a standard deviation of 0.05. <bold>c</bold> Three networks of different integrate-and-fire neurons were initialized in the chaotic regime (left), and trained using the FORCE method (center) to mimic a 5&#x02009;Hz sinusoidal oscillator (right). <bold>d</bold> A network of 5000 theta neurons was trained with the FORCE method to mimic the Lorenz system. RLS was used to learn the decoders using a 45&#x02009;s long trajectory of the Lorenz system as a supervisor. RLS was turned off after 50&#x02009;s with the resulting trajectory and chaotic attractor bearing a strong resemblance to the Lorenz system. The network attractor is shown in three different viewing planes for 50&#x02009;s post training</p></caption><graphic xlink:href="41467_2017_1827_Fig2_HTML" id="d29e872"/></fig>
</p></sec><sec id="Sec5"><title>FORCE trained spiking networks learn using chaos</title><p id="Par10">We sought to determine whether the FORCE method could train spiking neural networks given the impressive capabilities of this technique in training smooth systems of rate equations (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>) previously demonstrated in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. We implemented the FORCE method in different spiking neural networks of integrate-and-fire neurons in order to compare the robustness of the method across neuronal models and potential supervisors.</p><p id="Par11">First, to demonstrate the chaotic nature of these networks, we deleted a single spike in the network<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. After deletion, the resulting spike trains immediately diverged, indicating chaotic behavior (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>). To determine where the onset of chaos occurred as a function of <italic>G</italic>, we simulated networks of these neurons over a range of <italic>G</italic> parameters and computed the coefficients of variation and the interspike-interval (ISI) distributions (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). For the Izhikevich and Theta neurons, there was an immediate onset to chaotic spiking from quiescence (<italic>G</italic>&#x02009;&#x02248;&#x02009;10<sup>3</sup>, <italic>G</italic>&#x02009;&#x02248;&#x02009;0.02, respectively) as the bias currents for these models were placed at rheobase or threshold value. For the LIF model, we considered networks with both superthreshold and threshold bias currents (see Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> for parameters). In the superthreshold case, the LIF transitioned from tonic spiking to chaotic spiking (<italic>G</italic>&#x02009;&#x02248;&#x02009;0.04). The subthreshold case was qualitatively similar to the theta neuron model, with a transition to chaotic spiking at a small <italic>G</italic>-value (0&#x02009;&#x0003c;&#x02009;<italic>G</italic>&#x02009;&#x0003c;&#x02009;0.01). All neuron models exhibited bimodal interspike-interval distributions indicative of a possible transition to rate chaos for sufficiently high <italic>G</italic>-values<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Finally, a perturbation analysis revealed that all three networks of neurons contained flux-tubes of stability<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>).</p><p id="Par12">To FORCE train chaotic spiking networks, we used the original work in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup> as a guide to determine the parameter regime for (<italic>G</italic>, <italic>Q</italic>). In ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, the contributions of the static and learned synaptic inputs are of the same magnitude. Similarly, we scale <italic>Q</italic> to ensure the appropriate balance (see Methods secion for a derivation on how Q should be scaled). Finally, FORCE training in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup> works by quickly stabilizing the rates. Subsequent weight changes are devoted to stabilizing the weight matrix. For fast learning, RLS had to be applied on a faster time scale in the spiking networks vs. the rate networks (&#x0003c;&#x000a0;<italic>O</italic>(1)&#x02009;ms vs. <italic>O</italic>(10)&#x02009;ms, respectively). The learning rate <italic>&#x003bb;</italic> was taken to be fast enough to stabilize the spiking basis during the first presentation of the supervisor.</p><p id="Par13">With these modifications to the FORCE method, we successfully trained networks of spiking theta neurons to mimic various types of oscillators. Examples include sinusoids at different frequencies, sawtooth oscillations, Van der Pol oscillators, in addition to teaching signals with noise present (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). With a 20&#x02009;ms decay time constant, the product of sines oscillator presented a challenge to the theta neuron to learn. However, it could be resolved for network with larger decay time constants (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). All the oscillators in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref> were trained successfully for both the Izhikevich and LIF neuron models (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2,</xref> Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Furthermore, FORCE training was robust to the possible types of initial chaotic network states (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>)<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Finally, a three parameter sweep over the (<italic>G</italic>, <italic>Q</italic>, <italic>&#x003c4;</italic>
<sub>D</sub>) parameter space reveals that parameter regions for convergence are contiguous. This parameter sweep was conducted for sinusoidal supervisors at different frequencies (1, 5, 10, 20, and 40&#x02009;Hz). Oscillators with higher (lower) frequencies are learned over larger (<italic>G</italic>, <italic>Q</italic>) parameter regions in networks with faster (slower) synaptic decay time constants, <italic>&#x003c4;</italic>
<sub>D</sub> (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">4</xref>&#x02013;<xref rid="MOESM1" ref-type="media">6</xref>). Finally, we compared how the eigenvalues of the trained weight matrices varied (Supplementary Material, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">7</xref>). For spiking networks, we observe that in some cases, systems without dominant eigenvalues performed better than systems with dominant eigenvalues while in other cases the opposite was true.</p><p id="Par14">For the dynamics under consideration, the Izhikevich model had the greatest accuracy and fastest training times (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">4</xref>&#x02013;<xref rid="MOESM1" ref-type="media">6</xref>, Supplementary Table&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>). This is partially due to the fact that the Izhikevich neuron has spike frequency adaptation which operates on a longer time scale (i.e., 100&#x02009;ms). The long time scale affords the reservoir a greater capability for memory, allowing it to learn longer signals. Additionally, the dimensionality of the reservoir is increased by having an adaptation variable for each neuron.</p><p id="Par15">We wondered how the convergence rates of these networks would vary as a function of the network size, <italic>N</italic>, for both FORCE trained spiking and rate networks (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>). For a 5&#x02009;Hz sinusoidal supervisor, the spiking networks had a convergence rate of &#x02248;<italic>N</italic>
<sup>&#x02212;1/2</sup> in the <italic>L</italic>
<sub>2</sub> error. Rate networks had a higher order convergence rate, of &#x02248;<italic>N</italic>
<sup>&#x02212;1</sup> in the <italic>L</italic>
<sub>2</sub> error.</p><p id="Par16">As oscillators are simple dynamical systems, we wanted to assess if FORCE can train a spiking neural network to perform more complicated tasks. Thus, we considered two additional tasks: reproducing the dynamics for a low-dimensional chaotic system and statistically classifying inputs applied to a network of neurons. We trained a network of theta neurons using the Lorenz system as a supervisor (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2d</xref>). The network could reproduce the butterfly attractor and Lorenz-like trajectories after learning. As the supervisor was more complex, the training took longer and required more neurons (5000 neurons, 45&#x02009;s of training) yet was still successful. Furthermore, the Lorenz system could also be trained with a network of 5000 LIF neurons (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">9</xref>). To quantify the error and compare it with a rate network, we developed an attractor density based metric (Supplementary Materials) for the marginal density functions on the attractor. The spiking network had comparable performance to the rate network (0.27, 0.30, 0.24 for rate and 0.52, 0.38, 0.3 for spiking). Further, the spiking and rate networks were both able to regenerate the stereotypical Lorenz tent map, albeit with superior performance in the rate network (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">9</xref>). Finally, we showed that populations of neurons can be FORCE trained to statistically classify inputs, similar to a standard feedforward network (Supplementary Note&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>, Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">10</xref>&#x02013;<xref rid="MOESM1" ref-type="media">13</xref>).</p><p id="Par17">We wanted to determine if FORCE training could be adapted to generate weight matrices that respect Dales law, the constraint that a neuron can only be excitatory or inhibitory, not both. Unlike previous work<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, we opted to enforce Dales law dynamically as the network was trained (Supplementary Note&#x000a0;<xref rid="MOESM1" ref-type="media">2</xref>, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">13</xref>). The log(<italic>L</italic>
<sub>2</sub>) error over the test interval for this supervisor was &#x02212;1.34, which is comparable to the values in Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">6</xref>. While Dales law can be implemented, for simplicity, we train all remaining examples with unconstrained weight matrices.</p><p id="Par18">To summarize, for these three different neuron models, we have demonstrated that the FORCE method can be used to train a spiking network using a supervisor. The supervisor can be oscillatory, noisy, chaotic, and the training can occur in a manner that respects Dales law.</p></sec><sec id="Sec6"><title>FORCE training spiking networks to produce complex signals</title><p id="Par19">Neurons can encode complicated temporal sequences such as the mating songs that songbirds learn, store, and replay<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. We wondered whether FORCE could train spiking networks to encode similar types of naturally occurring spatio-temporal sequences. We formulated this as very long oscillations that are repeatedly presented to the network.</p><p id="Par20">The first pattern we considered was a sequence of pulses in a 5-dimensional supervisor. These pulses correspond to the notes in the first bar of Beethoven&#x02019;s Ode to Joy. A network of Izhikevich neurons was successfully FORCE trained to reproduce Ode to Joy (see Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a, c, d</xref> 82% accuracy during testing). The average firing rate of the network after training was 34&#x02009;Hz (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>), with variability in the neuronal responses from replay to replay, yet forming a stable peri-simulus time histogram (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">14</xref>&#x02013;<xref rid="MOESM1" ref-type="media">16</xref>). Furthermore, Ode to Joy could be learned by all neuron models at larger synaptic decay time constants (<italic>&#x003c4;</italic>
<sub>d</sub>&#x02009;=&#x02009;50, 100&#x02009;ms, see Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">4</xref>&#x02013;<xref rid="MOESM1" ref-type="media">6</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Using spiking neural networks for pattern storage and replay with FORCE training. <bold>a</bold> The notes in the song Ode to Joy by Beethoven are converted into a continuous 5-component teaching signal. The sheet music has been modified from Mutopia (<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>) by Wilten Nicola. The presence of a note is designated by an upward pulse in the signal. Quarter notes are the positive portion of a 2&#x02009;Hz sine wave form while half notes are represented with the positive part of a 1&#x02009;Hz sine wave. Each component in the teaching signal corresponds to the musical notes <bold>c</bold>&#x02013;<bold>g</bold>. The teaching signal is presented to a network of 5000 Izhikevich neurons continuously from start to finish until the network learns to reproduce the sequence through FORCE training. The network output in <bold>a</bold> is after 900&#x02009;s of FORCE training, or 225 applications of the target signal. For 1000&#x02009;s of simulation time after training, the song was played correctly 205 times comprising 820&#x02009;s of the signal (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">15</xref>, <xref rid="MOESM1" ref-type="media">16</xref>). <bold>b</bold> Five randomly selected neurons in the network and their resulting dynamics after FORCE training. The voltage traces are taken at the same time as the approximant in <bold>a</bold>. <bold>c</bold> The decoders before (1&#x02009;&#x0003c;<italic>&#x02009;t</italic>&#x02009;&#x0003c;&#x02009;900) and after (<italic>t</italic>&#x02009;&#x0003e;&#x02009;900) FORCE training. <bold>d</bold> The resulting eigenvalues for the weight matrix <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\omega _{ij}^0 + Q\eta _i\phi _j$$\end{document}</tex-math><mml:math id="M8"><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq4.gif"/></alternatives></inline-formula> before (black) and after (red) learning. Note that the onset to chaos occurs at <italic>G</italic>&#x02009;&#x02248;&#x02009;10<sup>3</sup> for the Izhikevich network with the parameters we have considered</p></caption><graphic xlink:href="41467_2017_1827_Fig3_HTML" id="d29e1267"/></fig>
</p><p id="Par21">While the network displayed some error in replaying the song, the errors were not random but stereotypical. The errors are primarily located after the two non-unique E-note repetitions that occur in the first bar and the end of the third bar (Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">14</xref>, <xref rid="MOESM1" ref-type="media">15</xref>) in addition to the non-unique ED sequences that occur at the end of the second bar and beginning of the fourth bar.</p></sec><sec id="Sec7"><title>FORCE trained networks can reproduce songbird singing</title><p id="Par22">While the Ode to Joy example was non-trivial to train, it pales in complexity to the singing of the zebra finch. To that end, we constructed a circuit that could reproduce a birdsong (in the form of a spectrogram) recorded from an adult zebra finch. The learned singing behavior of these birds is owed to two primary nuclei: the HVC (proper name) and the Robust nucleus of the Arcopallium (RA). The RA projecting neurons in HVC form a chain of spiking activity and each RA projecting neuron fires only once at a specific time in the song<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. This chain of firing is transmitted to and activates the RA circuit. Each RA neuron bursts at multiple precisely defined times during song replay<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. RA neurons then project to the lower motor nuclei which stimulate vocal muscles to generate the birdsong.</p><p id="Par23">To simplify matters, we will focus on a single network of RA neurons receiving a chain of inputs from area HVC (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4a</xref>). The HVC chain of inputs are modeled directly as a series of successive pulses and are not FORCE trained for simplicity. These pulses feed into a network of Izhikevich neurons that was successfully FORCE trained to reproduce the spectrogram of a recorded song from an adult zebra finch (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4b</xref>, see Supplementary Movie&#x000a0;<xref rid="MOESM4" ref-type="media">1</xref>). Additionally, by varying the (<italic>G</italic>, <italic>Q</italic>) parameters and some Izhikevich model parameters, the spiking statistics of RA neurons are easily reproduced both qualitatively and quantitatively (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4d, e</xref> inset, see Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b, c</xref> in ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, see Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> in ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>). The RA neurons burst regularly multiple times during song replay (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4b, c</xref>). Thus, we have trained our model network to match the song generation behavior of RA with spiking behavior that is consistent with experimental data.<fig id="Fig4"><label>Fig. 4</label><caption><p>Using spiking neural networks for pattern storage and replay: songbird singing. <bold>a</bold> A series of pulses constructed from the positive portion of a sinusoidal oscillator with a period of 20&#x02009;ms are used to model the chain of firing output from RA projection neurons in HVC. These neurons connect to neurons in RA and trigger singing behavior. FORCE training was used to train a network of 1000 Izhikevich neurons to reproduce the spectrogram of a 5&#x02009;s audio recording from an adult zebra finch. <bold>b</bold> The syllables in the song output for the teaching signal (top) and the network (bottom). <bold>c</bold> Spike raster plot for 30 neurons over 5 repetitions of the song. The spike raster plot is aligned with the syllables in <bold>b</bold>. <bold>d</bold> The distribution of instantaneous firing rates for the RA network post training. The <italic>&#x003b1;</italic> parameter corresponds to the up/down regulation factor of the excitatory synapses and to the different colors in the graphs of <bold>d</bold>, <bold>e</bold>. For <italic>&#x003b1;</italic>&#x02009;&#x0003e;&#x02009;1, excitation dominates over inhibition while the reverse is true for <italic>&#x003b1;</italic>&#x02009;&#x0003c;&#x02009;1. The inset of <bold>d</bold>, <bold>e</bold> is reproduced from ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Note that the <italic>y</italic>-axis of the model and data differ, this is due to normalization of the density functions. <bold>e</bold> The log of the interspike interval histogram. <bold>f</bold> The correlation between the network output and the teaching signal as the excitatory synapses are upscaled (negative <italic>x</italic>-axis) or downscaled (positive <italic>x</italic>-axis). <bold>g</bold> The spectrogram for the teaching signal, and the network output under different scaling factors for the excitatory synaptic weights. The panels in <bold>g</bold> correspond to the plots in <bold>d</bold>, <bold>e</bold> and the performance measure in <bold>f</bold>
</p></caption><graphic xlink:href="41467_2017_1827_Fig4_HTML" id="d29e1413"/></fig>
</p><p id="Par24">We wondered how our network would respond to potential manipulations in the underlying weight matrices. In particular, we considered manipulations to the excitatory synapses which alter the balance between excitation and inhibition in the network (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4f</xref>). We found that the network is robust toward downscaling excitatory synaptic weights. The network could still produce the song, albeit at a lower intensity. This was possible even when the excitatory synapses were reduced by 90% of their amplitude (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4f, g</xref>). However, upscaling excitatory synapses by as little as 15% drastically reduced song performance. The resulting song output had a very different spectral structure than the supervisor as opposed to downscaling excitation. Finally, upscaling the excitatory weights by 20% was sufficient to destroy singing, replacing it with high intensity seizure like activity. Interestingly, a similar result was found experimentally through the injection of bicuculine in RA<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Large doses of bicuculine resulted in strong bursting activity in RA accompanied by involuntary vocalizations. Smaller doses resulted in song degradation with increased noisiness, duration, and the appearance of new syllables.</p></sec><sec id="Sec8"><title>High-dimensional temporal signals improve FORCE training</title><p id="Par25">We were surprised at how robust the performance of the songbird network was given the high dimensionality and complicated structure of the output. We hypothesized that the performance of this network was strongly associated to the precise, clock-like inputs provided by HVC and that similar inputs could aid in the encoding and replay of other types of information. To test this hypothesis, we removed the HVC input pattern and found that the replay of the learned song was destroyed (not shown), similar to experimental lesioning of this area in adult canaries<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Due to the temporal information that these high-dimensional signals provide, we will subsequently refer to them as high-dimensional temporal signals (HDTS, see Methods section for further details).</p><p id="Par26">To further explore the benefits that an HDTS might provide, we FORCE trained a network of Izhikevich neurons to internally generate its own HDTS, while simultaneously also training it to reproduce the first bar of Ode to Joy. The entire supervisor consisted of the 5 notes used in the first bar of the song, in addition to 16 other components. These correspond to a sequence of 16 pulses that partition time. The network learned both the HDTS and the song simultaneously, with less training time, and greater accuracy than without the HDTS. As the HDTS helped in learning and replaying the first bar of a song, we wondered if these signals could help in encoding longer sequences. To test this hypothesis, we FORCE trained a network to learn the first four bars of Ode to Joy, corresponding to a 16&#x02009;s song in addition to its own, 64-dimensional HDTS. The network successfully learned the song post-training, without any error in the sequence of notes in spite of the length and complexity of the sequence (see Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">17</xref>, Supplementary Movie&#x000a0;<xref rid="MOESM5" ref-type="media">2</xref>). Thus, an internally generated HDTS can make FORCE training faster, more accurate, and more robust to learning longer signals. We refer to these networks as having an internally generated HDTS.</p></sec><sec id="Sec9"><title>FORCE trained encoding and replay of an episodic memory</title><p id="Par27">Given the improvements that an HDTS confers over supervisor duration, training time, and accuracy, we wanted to know if these input signals would help populations of neurons to learn natural high dimensional signals. To test this, we trained a network of Izhikevich neurons to learn a 1920 dimensional supervisor that corresponds to the pixels of an 8&#x02009;s scene from a movie (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>). Each component of the supervisor corresponds to the time evolution of a single pixel. The HDTS&#x02019;s were either generated by a separate network or were fed directly as an input into an encoding and replay network, as in the songbird example (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a, b</xref>). We refer to these two cases as the internally generated and the externally generated HDTS, respectively. In the former case, we demonstrate that an HDTS can be easily learned by a network while in the latter case we can freely manipulate the HDTS. As in the long Ode to Joy example, the HDTS could also be learned simultaneously to the movie scene, constituting a 1984 dimensional supervisor (1920&#x02009;+&#x02009;64 dimensional HDTS, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">18c, d</xref>). Note that this can be thought of as spontaneous replay, as opposed to cued recall where the network reconstructs a partially presented stimulus.<fig id="Fig5"><label>Fig. 5</label><caption><p>Using spiking neural networks for pattern storage and replay: movie replay. <bold>a</bold> Two types of networks were trained to replay an 8&#x02009;s clip from a movie. In the internally generated HDTS case, both an HDTS and a replay network are simultaneously trained. The HDTS projects onto the replay network similar to how HVC neurons project onto RA neurons in the birdsong circuit. The HDTS confers an 8&#x02009;Hz oscillation in the mean population activity. In the externally generated HDTS case, the network receives an HDTS supervisor that has not been trained, but is simple to manipulate and confers a 4&#x02009;Hz oscillation in the mean population activity. <bold>b</bold> The HDTS consists of 64 pulses generated from the positive component of a sinusoidal oscillator with a period of 250&#x02009;ms. The color of the pulses denotes its order in the temporal chain or equivalently, its dimension in the HDTS. <bold>c</bold> The external HDTS is compressed in time which results in speeding up the replay of the movie clip. The time-averaged correlation coefficient between the teaching signal and network output is used to measure performance. <bold>d</bold> The HDTS amplitude for the network with an internal HDTS was reduced. The network was robust to decreasing the amplitude of the HDTS. <bold>e</bold> Neurons in the replay network were removed and replay performance was measured. The replay performance decreases in an approximately linear fashion with the proportion of the replay network that is removed. <bold>f</bold> The mean population activity for the replay networks under HDTS compression, removal, and replay network lesioning</p></caption><graphic xlink:href="41467_2017_1827_Fig5_HTML" id="d29e1485"/></fig>
</p><p id="Par28">We were able to successfully train the network to replay the movie scene in both cases (time averaged correlation coefficient of <italic>r</italic>&#x02009;=&#x02009;0.98, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref>). Furthermore, we found that the HDTS inputs (shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>) were necessary both for training (<italic>r</italic>&#x02009;&#x0003c;&#x02009;0.44, varies depending on parameters) and replay (<italic>r</italic>&#x02009;=&#x02009;0.25, Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">18b</xref>). The network could still replay the individual frames from the movie scene without the HDTS; however, the order of scenes was incorrect and appeared to be chaotic. Thus, the HDTS input facilitates both learning and spontaneous replay of high dimensional signals.</p><p id="Par29">We were surprised to see that despite the complexity and high dimensionality of the encoding signal, the histogram of spike times across the replay network displayed a strong 4&#x02009;Hz modulation conferred by the HDTS. This histogram can be interpreted as the mean network activity. Unsurprisingly, reducing the HDTS amplitude yields a steady decrease in replay performance (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5d</xref>). Initially, this is mirrored through a decrease in the amplitude of the 4&#x02009;Hz oscillations in the mean population activity. Surprisingly however if we remove the HDTS, the mean activity displays a strong slower oscillation (&#x02248;2Hz) (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5f</xref>). The emergence of these slower oscillations corresponds to a sharp decline in replay performance as the scenes are no longer replayed in chronological order (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5f</xref>). The spikes were also non-uniformly distributed with respect to the mean-activity (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">19</xref>).</p><p id="Par30">We wanted to determine how removing neurons in the replay network would affect both the mean population activity, and replay performance. We found that the replay performance decreased approximately linearly with the proportion of neurons that were lesioned, with the amplitude of the mean activity also decreasing (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5e, f</xref>). The HDTS network however was much more sensitive to neural lesioning. We found lesioning 10% of a random selection of neurons in the HDTS network was sufficient to stop the HDTS output. Thus, the HDTS network is the critical element in this circuit that can drive accurate replay and is the most sensitive to damaging perturbations such as neural loss. Finally, we wondered how the frequency and amplitude of the HDTS (and thus the mean activity) would alter the learning accuracy of a network (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">18</xref>). There was an optimal input frequency located in the 8&#x02013;16&#x02009;Hz range for large regions of parameter space. This was robust to different neuronal parameters (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">18</xref>).</p><p id="Par31">It has been speculated that compressed or reversed replay of an event might be important for memory consolidation<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. Thus we wondered if networks trained with an HDTS could replay the scene in accelerated time by compressing the HDTS post-training. After training, we compressed the external HDTS in time (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5c</xref>). The network was able to replay the movie in compressed time (correlation of <italic>r</italic>&#x02009;&#x0003e;&#x02009;0.8) up to a compression factor of 16&#x000d7; with accuracy sharply dropping for further compression. The effect of time compression on the mean activity was to introduce higher frequency oscillations (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">20</xref>, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5f</xref>). The frequency of these oscillations scaled linearly with the amount of compression. However, with increasing compression frequency, large waves of synchronized activity also emerged in the mean population activity (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">20</xref>, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5f</xref>). Reverse replay was also was successfully achieved by reversing the order in which the HDTS components were presented to the network (accuracy of <italic>r</italic>&#x02009;=&#x02009;0.90). This loss in accuracy is due to the fact that temporal dynamics of the network is not reversed within a segment of the HDTS. Thus, the network can generalize to robustly compress or reverse replay, despite not being trained to do these tasks.</p><p id="Par32">Compression of a task dependent sequence of firing has been experimentally found in numerous sources<sup><xref ref-type="bibr" rid="CR40">40</xref>,<xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. For example, in ref. <sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, the authors found that a recorded sequence of neuronal cross correlations in rats elicited during a spatial sequence task reappeared in compressed time during sleep. The compression ratios between 5.4 and 8.1 and compressed sequences that were originally &#x02248;10&#x02009;s down to &#x02248;1.5&#x02009;s. This is similar to the compression ratios we found for our networks without incurring appreciable error in replay (up to a compression factor of 8&#x02013;16). Time compression and dilation has also been experimentally found in the striatum<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>. Here, the authors found that the neuronal firing sequences for striatal neurons were directly compressed in time<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Indeed, we also found that accelerated replay of the movie scene compressed the spiking behavior for neurons in the replay network (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">20</xref>).</p><p id="Par33">To summarize, an HDTS is necessary for encoding and replay of high dimensional natural stimuli. These movie clips can be thought of as proxies for episodic memories. Compression and reversal of the HDTS allows compressed and reversed replay of the memory proxy. At lower compression ratios, the mean population activity mirrors the HDTS while at higher compression ratios (&#x02265;8&#x000d7;), large synchronized events in the mean activity emerge that repeat with each movie replay. The optimal HDTS frequency mostly falls in the 8&#x02013;16&#x02009;Hz parameter range.</p></sec></sec><sec id="Sec10" sec-type="discussion"><title>Discussion</title><p id="Par34">We have shown that FORCE training can take initially chaotic networks of spiking neurons and use them to mimic the natural tasks and functions demonstrated by populations of neurons. For example, these networks were trained to learn low-dimensional dynamical systems, such as oscillators which are at the heart of generating both rhythmic and non rhythmic motion<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. We found FORCE training to be robust to the spiking model employed, initial network states, and synaptic connection types.</p><p id="Par35">Additionally, we showed that we could train spiking networks to display behaviors beyond low-dimensional dynamics by altering the supervisor used to train the network. For example, we trained a statistical classifier with a network of Izhikevich neurons that could discriminate its inputs. Extending the notion of an oscillator even further allowed us to store a complicated sequence in the form of the notes of a song, reproduce the singing behavior of songbirds, and encode and replay a movie scene. These tasks are aided by the inclusion of a high-dimensional temporal signal (HDTS) that discretizes time by segregating the neurons into assemblies.</p><p id="Par36">FORCE training is reminiscent of how songbirds learn their stereotypical learned songs<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR46">46</xref></sup>. Juvenile songbirds are typically presented with a species specific song or repertoire of songs from their parents or other members of their species. These birds internalize the original template song and subsequently use it as an error signal for their own vocalization<sup><xref ref-type="bibr" rid="CR35">35</xref>&#x02013;<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR46">46</xref>&#x02013;<xref ref-type="bibr" rid="CR49">49</xref></sup>. Our model reproduced the singing behavior of songbirds with FORCE training as the error correction mechanism. Both the spiking statistics of area RA and the song spectrogram were accurately reproduced after FORCE training. Furthermore, we demonstrated that altering the balance between excitation and inhibition post training degrades the singing behavior post-training. A shift to excess excitation alters the spectrogram in a highly non-linear way while a shift to excess inhibition reduces the amplitude of all frequencies.</p><p id="Par37">Inspired by the clock-like input pattern that songbirds use for learning and replay<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup> we used a similar HDTS to encode a longer and more complex sequence of notes in addition to a scene from a movie. We found that these signals made FORCE training faster and the subsequent replay more accurate. Furthermore, by manipulating the HDTS frequency we found that we could speed up or reverse movie replay in a robust fashion. We found that compressing replay resulted in higher frequency oscillations in the mean population activity. Attenuating the HDTS decreased replay performance while transitioning the mean activity from a 4&#x02013;8&#x02009;Hz oscillation to a slower (&#x02248;2&#x02009;Hz) oscillation. Finally, replay of the movie was robust to lesioning neurons in the replay network.</p><p id="Par38">While our episodic memory network was not associated with any particular hippocampal region, it is tempting to conjecture on how our results might be interpreted within the context of the hippocampal literature. In particular, we found that the HDTS conferred a slow oscillation in the mean population activity reminiscent of the slow theta oscillations observed in the hippocampus. The theta oscillation is strongly associated to memory; however, its computational role is not fully understood, with many theories proposed<sup><xref ref-type="bibr" rid="CR50">50</xref>&#x02013;<xref ref-type="bibr" rid="CR53">53</xref></sup>. For example, the theta oscillation has been proposed to serve as a clock for memory formation<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>.</p><p id="Par39">Here, we show a concrete example that natural stimuli that serve as proxies for memories can be bound to an underlying oscillation in a population of neurons. The oscillation forces the neurons to fire in discrete temporal assemblies. The oscillation (via the HDTS) can be sped up, or even reversed resulting in an identical manipulation of the memory. Additionally, we found that reducing the HDTS input severely disrupted replay and the underlying mean population oscillation. This mirrors experimental results that showed that theta power was predictive of correct replay<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. Furthermore, blocking the HDTS prevents learning and prevents accurate replay with networks trained with an HDTS present. Blocking the hippocampal theta oscillation pharmacologically<sup><xref ref-type="bibr" rid="CR56">56</xref></sup> or optogenetically<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> has also been found to disrupt learning.</p><p id="Par40">The role of the HDTS is reminiscent of the recent discovery of time cells, which also serve to partition themselves across a time interval in episodic memory tasks<sup><xref ref-type="bibr" rid="CR58">58</xref>&#x02013;<xref ref-type="bibr" rid="CR60">60</xref></sup>. How time cells are formed is ongoing research however they are dependent on the medial septum, and thus the hippocampal theta oscillation<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Time cells have been found in CA1<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, CA3<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> and temporally selective cells occur in the entorhinal cortex<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>.</p><p id="Par41">In a broader context, FORCE trained networks could be used in the future to elucidate hippocampal functions. For example, future FORCE trained networks can make use of biological constraints such as Dale&#x02019;s law in an effort to reproduce verified spike distributions for different neuron types with regards to the phase of the theta oscillation<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. These networks can also be explicitly constructed to represent the different components of the well studied hippocampal circuit.</p><p id="Par42">FORCE training is a powerful tool that allows one to use any sufficiently complicated dynamical system as a basis for universal computation. The primary difficulty in implementing the technique in spiking networks appears to be controlling the orders of magnitude between the chaos inducing weight matrix and the feedback weight matrix. If the chaotic weight matrix is too large in magnitude (via the <italic>G</italic> parameter), the chaos can no longer be controlled by the feedback weight matrix<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. However, if the chaos inducing matrix is too weak, the chaotic system no longer functions as a suitable reservoir. To resolve this, we derived a scaling argument for how <italic>Q</italic> should scale with <italic>G</italic> for successful training based on network behaviors observed in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Interestingly, the balance between these fluctuations could be related to the fading memory property, a necessary criterion for the convergence of FORCE trained rate networks<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>.</p><p id="Par43">Furthermore, while we succeeded in implementing the technique in other neuron types, the Izhikevich model was the most accurate in terms of learning arbitrary tasks or dynamics. This is due to the presence of spike frequency adaptation variables that operate on a much slower time scale than the neuronal equations. There may be other biologically relevant forces that can increase the capacity of the network to act as a reservoir through longer time scale dynamics, such as synaptic depression and NMDA mediated currents for example<sup><xref ref-type="bibr" rid="CR66">66</xref>&#x02013;<xref ref-type="bibr" rid="CR68">68</xref></sup>.</p><p id="Par44">Furthermore, we found that the inclusion of a high-dimensional temporal signal increased the accuracy and capability of a spiking network to reproduce long signals. In ref. <sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, another type of high-dimensional supervisor is used to train initially chaotic spiking networks. Here, the authors use a supervisor consisting of <italic>O</italic>(<italic>N</italic>
<sup>2</sup>) components (see ref. <sup><xref ref-type="bibr" rid="CR2">2</xref></sup> for more details). This is different from our approach involving the construction of an HDTS, which serves to partition the neurons into assemblies and is of lower dimensionality than <italic>O</italic>(<italic>N</italic>
<sup>2</sup>). However, from ref. <sup><xref ref-type="bibr" rid="CR2">2</xref></sup> and our work here, increasing the dimensionality of the supervisor does aid FORCE training accuracy and capability. Finally, it is possible that an HDTS would facilitate faster and more accurate learning in networks of rate equations and more general reservoir methods as well.</p><p id="Par45">Although FORCE trained networks have dynamics that are starting to resemble those of populations of neurons, at present all top-down procedures used to construct any functional spiking neural network need further work to become biologically plausible learning rules<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. For example, FORCE trained networks require non-local information in the form of the correlation matrix <bold><italic>P</italic></bold>(<italic>t</italic>). However, we should not dismiss the final weight matrices generated by these techniques as biologically implausible simply because the techniques are themselves biologically implausible.</p><p id="Par46">Aside from the original rate formulation in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, FORCE trained rate equations have been recently applied to analyzing and reproducing experimental data. For example, in ref. <sup><xref ref-type="bibr" rid="CR69">69</xref></sup>, the authors used a variant of FORCE training (referred to as Partial In-Network Training, PINning) to train a rate network to reproduce a temporal sequence of activity from mouse calcium imaging data. PINning uses minimal changes from a balanced weight matrix architecture to form neuronal sequences. In ref. <sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, the authors combine experimental manipulations with FORCE trained networks to demonstrate that preparatory activity prior to motor behavior is resistant to unilateral perturbations both experimentally, and in their FORCE trained rate models. In ref. <sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, the authors demonstrate the dynamics of reservoirs can explain the emergence of mixed selectivity in primate dorsal Anterior Cingulate Cortex (dACC). The authors use a modified version of FORCE training to implement an exploration/exploitation task that was also experimentally performed on primates. The authors found that the FORCE trained neurons had a similar dynamic form of mixed selective as experimentally recorded neurons in the dACC. Finally, in ref. <sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, the authors train a network of rate neurons to encode time on the scale of seconds. This network is subsequently used to learn different spatio-temporal tasks, such as a cursive writing task. These FORCE trained networks were able to account for psychophysical results such as Weber&#x02019;s law, where the variance of a response scales like the square of the time since the start of the response. In all cases, FORCE trained rate networks were able to account for and predict experimental findings. Thus, FORCE trained spiking networks can prove to be invaluable for generating novel predictions using voltage traces, spike times, and neuronal parameters.</p><p id="Par47">Top-down network training techniques have different strengths and uses. For example, the Neural Engineering Framework (NEF) and spike-based coding approaches solve for the underlying weight matrices immediately without training<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. The solutions can be analytical as in the spike based coding approach, or numerical, as in the NEF approach. Furthermore, the weight matrix solutions are valid over entire regions of the phase space, where as FORCE training uses individual trajectories as supervisors. Multiple trajectories have to be FORCE trained into a single network to yield a comparable level of global performance over a region. Both sets of solutions yield different insights into the structure, dynamics, and functions of spiking neural networks. For example, brain scale functional models can be constructed with NEF networks<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Spike-based coding networks demonstrate how higher order error scaling is possible by utilizing spiking sparsely and efficiently through balanced network solutions. While the NEF and spike based coding approaches provide immediate weight matrix solutions, both techniques are difficult to generalize to other types of networks or other types of tasks. Both the NEF and spike based coding approach require a system of closed form differential equations to determine the static weight matrix that yields the target dynamics.</p><p id="Par48">In summary, we showed that FORCE can be used to train spiking neural networks to reproduce complex spatio-temporal dynamics. This method could be used in the future to mechanically link neural activity to the complex behaviors of animals.</p></sec><sec id="Sec11" sec-type="materials|methods"><title>Methods</title><sec id="Sec12"><title>Rate equations</title><p id="Par49">The network of rate equations is given by the following:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tau _{\rm s}\dot s_{ i} = - s_{ i} + G\mathop {\sum}\limits_{j = 1}^N \omega _{ij}^0r_j + Q\eta _{ i}\hat x.$$\end{document}</tex-math><mml:math id="M10" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x01e61;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r_j = \left( {\begin{array}{*{20}{l}} {F\sqrt {s_j} } \hfill 	 {s_j \ge 0} \hfill \\ 0 \hfill 	 {s_j &#x0003c; 0} \hfill \end{array}} \right..$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>F</mml:mi><mml:msqrt><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat x(t) = \mathop {\sum}\limits_{j = 1}^N \phi _jr_j(t),$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq5"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega _{ij}^0$$\end{document}</tex-math><mml:math id="M16"><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq5.gif"/></alternatives></inline-formula> is the sparse and static weight matrix that induces chaos in the network by having <inline-formula id="IEq6"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega _{ij}^0$$\end{document}</tex-math><mml:math id="M18"><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq6.gif"/></alternatives></inline-formula> drawn from a normal distribution with mean 0 and variance (<italic>Np</italic>
<sup>2</sup>)<sup>&#x02212;1</sup>, where <italic>p</italic> is the degree of sparsity in the network. The variable<italic>s s</italic> can be thought of as neuronal currents with a postsynaptic filtering time constant of <italic>&#x003c4;</italic>
<sub><italic>s</italic></sub>. The encoding variables <italic>&#x003b7;</italic>
<sub><italic>i</italic></sub> are drawn from a uniform distribution over [&#x02212;1, 1] and set the neurons encoding preference over the phase space of <italic>x</italic>(<italic>t</italic>), the target dynamics. The quantities <italic>G</italic> and <italic>Q</italic> are constants that scale the static weight matrix and feedback term, respectively. The firing rates are given by <italic>r</italic>
<sub><italic>j</italic></sub> and correspond to the type-I normal form for firing. The variable <italic>F</italic> scales the firing rates. The decoders<italic>, &#x003d5;</italic>
<sub><italic>j</italic></sub> are determined by the recursive least mean squares (RLS) technique, iteratively<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. RLS is described in greater detail in the next section. For Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>,&#x000a0;<xref rid="MOESM1" ref-type="media">9</xref>, we also implemented the tanh(<italic>x</italic>) continuous variable equations from<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> for comparison purposes. These equations are described in greater detail in ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>.</p></sec><sec id="Sec13"><title>Integrate-and-fire networks</title><p id="Par50">Our networks consist of coupled integrate-and-fire neurons, that are one of the following three forms:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot \theta _{ i} = \left( {1 - {\mathrm{cos}}\left( {\theta _{ i}} \right)} \right) + \pi ^2\left( {1 + {\mathrm{cos}}\left( {\theta _{i}} \right)} \right)(I)\quad \left( {{\mathrm{Theta}}\,{\mathrm{neuron}}} \right).$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>&#x003c0;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="1em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="normal">Theta</mml:mi><mml:mspace width="0.3em"/><mml:mi mathvariant="normal">neuron</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\tau _{ m}\dot v_{ i} = - v_{ i} + I\quad \left( {{\mathrm{LIF}}\,{\mathrm{neuron}}} \right).$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mspace width="1em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="normal">LIF</mml:mi><mml:mspace width="0.3em"/><mml:mi mathvariant="normal">neuron</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C\dot v_{ i} = k\left( {v_{ i} - v_{\rm r}} \right)\left( {v_{i} - v_{\rm t}} \right) - u_{ i} + I\quad \left( {{\mathrm{Izhikevich}}\,{\mathrm{Neuron}}} \right).$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mi>C</mml:mi><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>I</mml:mi><mml:mspace width="1em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi mathvariant="normal">Izhikevich</mml:mi><mml:mspace width="0.3em"/><mml:mi mathvariant="normal">Neuron</mml:mi></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot u_{ i} = a\left( {b\left( {v_{ i} - v_{ r}} \right) - u_{ i}} \right).$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mo>&#x000b0;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>b</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>The currents, <italic>I</italic> are given by <italic>I</italic>&#x02009;=&#x02009;<italic>I</italic>
<sub>Bias</sub>&#x02009;+&#x02009;<italic>s</italic>
<sub>i</sub>, where <italic>I</italic>
<sub>Bias</sub> is a constant background current set near or at the rheobase (threshold to spiking) value. The currents are dimensionless in the case of the theta neuron while dimensional for the LIF and Izhikevich models. Note that we have absorbed a unit resistance into the current for the LIF model. The quantities <italic>&#x003b8;</italic>
<sub>i</sub>, or <italic>v</italic>
<sub>i</sub> are the voltage variables while <italic>u</italic>
<sub>i</sub> is the adaptation current for the Izhikevich model. These voltage variables are instantly set to a reset potential (<italic>v</italic>
<sub>reset</sub>) when a neurons membrane potential reaches a threshold (<italic>v</italic>
<sub>thr</sub>, LIF) or voltage peak (<italic>v</italic>
<sub>peak</sub>, theta model, Izhikevich model). The parameters for the neuron models can be found in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. The parameters from the Izhikevich model are from ref. <sup><xref ref-type="bibr" rid="CR73">73</xref></sup> with a slight modification to the <italic>k</italic> parameter. The LIF model has a refractory time period, <italic>&#x003c4;</italic>
<sub>ref</sub> where the neuron cannot spike. The adaptation current, <italic>u</italic>
<sub>i</sub> for the Izhikevich model increases by a discrete amount <italic>d</italic> every time neuron <italic>i</italic> fires a spike and serves to slow down the firing rate. The membrane time constant for the LIF model is given by <italic>&#x003c4;</italic>
<sub>m</sub>. The variables for the Izhikevich model include <italic>C</italic> (membrane capacitance), <italic>v</italic>
<sub>r</sub> (resting membrane potential), <italic>v</italic>
<sub>t</sub> (membrane threshold potential), <italic>a</italic> (reciprocal of the adaptation time constant), <italic>k</italic> (controls action potential half-width) and <italic>b</italic> (controls resonance properties of the model).</p><p id="Par51">The spikes are filtered by specific synapse types. For example, the single exponential synaptic filter is given by the following:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot r_j = - \frac{{r_j}}{{\tau _{s}}} + \frac{1}{{\tau _{ s}}}\mathop {\sum}\limits_{t_{jk} &#x0003c; t} \delta \left( {t - t_{jk}} \right),$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:msub><mml:mrow><mml:mi>&#x01e59;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003c4;</italic>
<sub>s</sub> is the synaptic time constant for the filter and <italic>t</italic>
<sub><italic>jk</italic></sub> is the <italic>k</italic>th spike fired by the <italic>j</italic>th neuron. The double exponential filter is given by<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot r_j = - \frac{{r_j}}{{\tau _{ d}}} + h_j.$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:msub><mml:mrow><mml:mi>&#x01e59;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot h_j = - \frac{{h_j}}{{\tau _{ r}}} + \frac{1}{{\tau _{ r}\tau _d}}\mathop {\sum}\limits_{t_{jk} &#x0003c; t} \delta \left( {t - t_{jk}} \right),$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:msub><mml:mrow><mml:mi>&#x01e23;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003c4;</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>&#x003b4;</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003c4;</italic>
<sub>r</sub> is the synaptic rise time, and <italic>&#x003c4;</italic>
<sub>d</sub> is the synaptic decay time. The filters can be of the simple exponential type, double exponential, or alpha synapse type (<italic>&#x003c4;</italic>
<sub>r</sub>&#x02009;=&#x02009;<italic>&#x003c4;</italic>
<sub>d</sub>). However, we will primarily consider the double exponential filter with a rise time of 2&#x02009;ms and a decay time of 20&#x02009;ms. Longer and shorter filters are considered in the Supplementary Materials.&#x000a0; Note that the the delta functions are carrying units of pA and thus the <italic>r</italic>(<italic>t</italic>) have units of current.&#x000a0;&#x000a0;</p><p id="Par52">The synaptic currents, <italic>s</italic>
<sub>i</sub>(<italic>t</italic>), are given by the equation:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_{ i} = \mathop {\sum}\limits_{j = 1}^N \omega _{ij}r_j$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>The matrix <italic>&#x003c9;</italic>
<sub><italic>ij</italic></sub> is the matrix of synaptic connections and controls the magnitude of the postsynaptic currents arriving at neuron <italic>i</italic> from neuron <italic>j</italic>.</p><p id="Par53">The primary goal of the network is to approximate the dynamics of an <italic>m</italic>-dimensional teaching signal, <bold><italic>x</italic></bold>(<italic>t</italic>), with the following approximant:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{\boldsymbol x}}(t) = \mathop {\sum}\limits_{j = 1}^N \phi _jr_j,$$\end{document}</tex-math><mml:math id="M36" display="block"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003d5;</italic>
<sub><italic>j</italic></sub> is a quantity that is commonly referred to as the linear decoder for the firing rate. The FORCE method decomposes the weight matrix into a static component, and a learned decoder <italic>&#x003d5;</italic>:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega _{ij} = G\omega _{ij}^0 + Q\eta _{ i} \cdot \phi _j^T$$\end{document}</tex-math><mml:math id="M38" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>The sparse matrix <inline-formula id="IEq7"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega _{ij}^0$$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq7.gif"/></alternatives></inline-formula> is the static weight matrix that induces chaos. It is drawn from a normal distribution with mean 0, and variance (<italic>Np</italic>)<sup>&#x02212;1</sup>, where <italic>p</italic> is the sparsity of the matrix. Additionally, the sample mean for these weight matrices was explicitly set to 0 for the LIF and theta neuron models to counterbalance the resulting firing rate heterogeneity. This was not necessary with the Izhikevich model. The variable <italic>G</italic> controls the chaotic behavior and its value varies from neuron model to neuron model. The encoding variables, <italic>&#x003b7;</italic>
<sub>i</sub> are drawn randomly and uniformly from [&#x02212;1, 1]<sup><italic>k</italic></sup>, where <italic>k</italic> is the dimensionality of the teaching signal. The encoders, <italic>&#x003b7;</italic>
<sub>i</sub>, contribute to the tuning preferences of the neurons in the network. The variable <italic>Q</italic> is increased to better allow the desired recurrent dynamics to tame the chaos present in the reservoir. The weights are determined dynamically through time by minimizing the squared error between the approximant and intended dynamics, <inline-formula id="IEq8"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{e}}(t) = \,{\hat{\boldsymbol x}}(t) - {\boldsymbol{x}}(t)$$\end{document}</tex-math><mml:math id="M42"><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mspace width="0.3em"/><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq8.gif"/></alternatives></inline-formula>. The RLS technique updates the decoders accordingly:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\phi (t) = \phi (t - \Delta t) - {\boldsymbol{e}}(t){\boldsymbol{P}}(t){\boldsymbol{r}}(t)$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{P}}(t) = {\boldsymbol{P}}(t - \Delta t) - \frac{{{\boldsymbol{P}}(t - \Delta t){\boldsymbol{r}}(t){\boldsymbol{r}}(t)^T{\boldsymbol{P}}(t - \Delta t)}}{{1 + {\boldsymbol{r}}(t)^T{\boldsymbol{P}}(t - \Delta t){\boldsymbol{r}}(t)}},$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>&#x00394;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <bold><italic>P</italic></bold>(<italic>t</italic>) is the network estimate for the inverse of the correlation matrix:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\boldsymbol{P}}(t)^{-1} = {\int}_0^t {\boldsymbol{r}}(t){\boldsymbol{r}}(t)^T{\kern 1pt} {\mathrm {d}t} + \lambda I_N.$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#x0222b;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">r</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>The parameter <italic>&#x003bb;</italic> acts both as a regularization parameter<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and controls the rate of the error reduction while <italic>I</italic>
<sub><italic>N</italic></sub> is an <italic>N</italic>&#x02009;&#x000d7;&#x02009;<italic>N</italic> identity matrix. The RLS technique can be seen to minimize the squared error between the target dynamics and the network approximant:<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C = {\int}_0^T \left( {\hat x(t) - x(t)} \right)^2{\mathrm {d}t} + \lambda \phi ^T\phi$$\end{document}</tex-math><mml:math id="M50" display="block"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#x0222b;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msup><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>&#x003d5;</mml:mi></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>and uses network generated correlations optimally by using the correlation matrix <italic>P</italic>(<italic>t</italic>). The network is initialized with <italic>&#x003d5;</italic>(0)&#x02009;=&#x02009;<bold>0</bold>, <bold><italic>P</italic></bold>(0)&#x02009;=&#x02009;<italic>I</italic>
<sub><italic>N</italic></sub>
<italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>, where <italic>I</italic>
<sub><italic>N</italic></sub> is an <italic>N</italic>-dimensional identity matrix. Note that in the original implementation of ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, Eq. (<xref rid="Equ14" ref-type="">14</xref>) contains the term 1&#x02009;+&#x02009;<bold><italic>r</italic></bold>(<italic>t</italic>)<sup><italic>T</italic></sup>
<bold><italic>P</italic></bold>(<italic>t</italic>&#x02009;&#x02212;&#x02009;&#x00394;<italic>t</italic>)<bold><italic>r</italic></bold>(<italic>t</italic>) in the denominator. This term adds a slight increase in accuracy and stability, but does not change the overall order of convergence of RLS. For comparison purposes, this modification was applied to Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">8</xref>. All other implementations used Eqs. <xref rid="Equ14" ref-type="">14</xref>, <xref rid="Equ15" ref-type="">15</xref>.</p></sec><sec id="Sec14"><title><italic>G</italic> and <italic>Q</italic> parameters</title><p id="Par54">In ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, the learned recurrent component and the static feedback term were both of the same order of magnitude, <italic>O</italic>(1). Furthermore, if the input stimulus had an amplitude that was too small (ref. <sup><xref ref-type="bibr" rid="CR1">1</xref></sup>), the chaotic dynamics could not be tamed. Operating on the hypothesis that the static term and the learned term require fluctuations of the same order of magnitude, one can derive the following equations using standard arguments about balanced networks:<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>
<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_{ i}(t) = \mathop {\sum}\limits_{j = 1}^N G\omega _{ij}^0r_j(t).$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\langle {s_{ i}(t)} \right\rangle \sim 0,N \to \infty.$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mfenced close="&#x027e9;" open="&#x027e8;" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>~</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>&#x0221e;</mml:mi><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\langle {s_{ i}(t)^2} \right\rangle \sim G^2E\left( {\left( {\bar \omega _{ij}^0} \right)^2} \right)\left\langle {r_{\rm i}(t)^2} \right\rangle,$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mfenced close="&#x027e9;" open="&#x027e8;" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>~</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>E</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close="&#x027e9;" open="&#x027e8;" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\bar \omega _{ij}^0 = \sqrt N \omega _{ij}^0$$\end{document}</tex-math><mml:math id="M58"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq9.gif"/></alternatives></inline-formula> the second term can be derived as follows:<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_{ i}(t)^2 = \frac{{G^2}}{N}\left( {\mathop {\sum}\limits_{j = 1}^N \left( {\bar \omega _{ij}^0} \right)^2r_j(t)^2 + \mathop {\sum}\limits_{j \ne k} b\bar \omega _{ij}^0\bar \omega _{ik}^0r_j(t)r_k(t)} \right)$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mi>b</mml:mi><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ22"><label>22</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim G^2E\left( {\left( {\bar \omega _{ij}^0} \right)^2} \right)\left\langle {r_j(t)^2} \right\rangle \,N \to \infty,$$\end{document}</tex-math><mml:math id="M62" display="block"><mml:mo>~</mml:mo><mml:msup><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>E</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mfenced close="&#x027e9;" open="&#x027e8;" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mspace width="0.3em"/><mml:mi>N</mml:mi><mml:mo>&#x02192;</mml:mo><mml:mi>&#x0221e;</mml:mi><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ22.gif" position="anchor"/></alternatives></disp-formula>where the last step is justified as we can ignore interneuronal correlations <italic>r</italic>
<sub>i</sub>(<italic>t</italic>)<italic>r</italic>
<sub><italic>j</italic></sub>(<italic>t</italic>) in addition to correlations between a weight matrix component and a firing rate, <italic>r</italic>
<sub><italic>j</italic></sub>(<italic>t</italic>) in the large network limit. If the term <italic>&#x003b7;</italic> &#x022c5; <italic>x</italic>(<italic>t</italic>) is <italic>O</italic>(1), this implies that <inline-formula id="IEq10"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Q = O\left( {G\sigma _\omega \sqrt {\left\langle {r_j(t)^2} \right\rangle } } \right),$$\end{document}</tex-math><mml:math id="M64"><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>G</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mfenced close="&#x027e9;" open="&#x027e8;" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:msqrt></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq10.gif"/></alternatives></inline-formula> where <italic>&#x003c3;</italic>
<sub><italic>&#x003c9;</italic></sub> is the standard deviation of the zero mean, static weight matrix distribution.</p></sec><sec id="Sec15"><title>High-dimensional temporal signals</title><p id="Par55">The HDTS serves to stabilize network dynamics by organizing the neurons into assemblies. These assemblies fire at precise intervals of time. To elaborate further, these signals are constructed as a discretization of time, <italic>T</italic> into <italic>m</italic> sub-intervals. Each subinterval generates an extra component (dimension) in the supervisor and within the subinterval, a pulse or upward deflection of the supervisor is generated. For example, if we consider the interval from [0, <italic>T</italic>] with <italic>m</italic> subintervals of width <inline-formula id="IEq11"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_n = \left[ {T\left( {\frac{{n - 1}}{m}} \right),T\left( {\frac{n}{m}} \right)} \right]$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mi>T</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq11.gif"/></alternatives></inline-formula>, <italic>n</italic>&#x02009;=&#x02009;1, 2, &#x02026; <italic>m</italic>. The supervisor is constructed as a series of pulses centered in the intervals <italic>I</italic>
<sub><italic>n</italic></sub>. This can be done in multiple ways. For example, Gaussian pulses can be used to construct the HDTS:<disp-formula id="Equ23"><label>23</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_n(t) = {\mathrm{exp}}\left( { - \frac{{\left( {t - T\left( {\frac{{2n - 1}}{{2m}}} \right)} \right)^2}}{{\sigma ^2}}} \right),$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>T</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ23.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x003c3;</italic> controls the width. Alternatively, one can also use piecewise defined functions such as<disp-formula id="Equ24"><label>24</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_n(t) = \left( {\begin{array}{*{20}{l}} {\left| {{\mathrm{sin}}(\frac{m\pi t}{T})} \right|} \hfill 	 {t \in I_n} \hfill \\ 0 \hfill 	 {{\mathrm{otherwise}}{\mathrm{.}}} \hfill \end{array}} \right.$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfenced close="&#x02223;" open="&#x02223;" separators=""><mml:mrow><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>m</mml:mi><mml:mi>&#x003c0;</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mi>t</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mi mathvariant="normal">otherwise</mml:mi><mml:mi mathvariant="normal">.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ24.gif" position="anchor"/></alternatives></disp-formula>The end effect of these signals is to divide the network into a series of assemblies. A neurons participation in the <italic>n</italic>th assembly is a function of the static weight matrix, in addition to <italic>&#x003b7;</italic>
<sub>in</sub>. Each assembly is activated by the preceding assemblies to propagate a long signal through the network. The assemblies are activated at a specific frequency in the network, dictated by the width of the pulses in the HDTS. Sinusoidal HDTS were used for the all figures with the exception of Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">18c</xref>, where a Gaussian HDTS was used.</p></sec><sec id="Sec16"><title>Figure 1 methods</title><p id="Par56">A network of rate equations is used to learn the dynamics of a teaching signal, <italic>x</italic>(<italic>t</italic>), a 5&#x02009;Hz sinusoidal oscillation. The network consists of 1000 rate neurons with <italic>F</italic>&#x02009;=&#x02009;10, <italic>&#x003c4;</italic>
<sub>s</sub>&#x02009;=&#x02009;10&#x02009;ms, <italic>G</italic>&#x02009;=&#x02009;1, and <italic>Q</italic>&#x02009;=&#x02009;1.5, <italic>p</italic>&#x02009;=&#x02009;0.1. The network is run for 1&#x02009;s initially, with RLS run for the next 4&#x02009;s, and shut off at the 5&#x02009;s mark. The network is subsequently run for another 5&#x02009;s to ensure learning occurs. The smooth firing rates of the network were less than 30&#x02009;Hz. The RLS parameters were taken to be &#x00394;<italic>t</italic>&#x02009;=&#x02009;2&#x02009;ms and <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;0.5&#x02009;ms.</p></sec><sec id="Sec17"><title>Figure 2 methods</title><p id="Par57">Networks of theta neurons were trained to reproduce different oscillators. The parameters were: <italic>N</italic>&#x02009;=&#x02009;2000, <italic>G</italic>&#x02009;=&#x02009;10 (sinusoid, sawtooth), <italic>G</italic>&#x02009;=&#x02009;15 (Van der Pol), <italic>G</italic>&#x02009;=&#x02009;25 (product of sinusoids), <italic>G</italic>&#x02009;=&#x02009;15 (product of sinusoids with noise), <italic>Q</italic>&#x02009;=&#x02009;10<sup>4</sup>, &#x00394;<italic>t</italic>&#x02009;=&#x02009;0.5&#x02009;ms, with an integration time step of 0.01&#x02009;ms. The resulting average firing rates for the network were 26.1&#x02009;Hz (sinusoid) 29.0&#x02009;Hz (sawtooth), 15.0&#x02009;Hz (Van der Pol relaxation), 18.8&#x02009;Hz (Van der Pol Harmonic), 23.0&#x02009;Hz (product of sinusoids), 21.1&#x02009;Hz (product of sines with noise). The Van der Pol oscillator is given by: <inline-formula id="IEq12"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\ddot x = \mu \left( {1 - x^2} \right)\dot x - x,$$\end{document}</tex-math><mml:math id="M72"><mml:mi>&#x01e8d;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mi>&#x01e8b;</mml:mi><mml:mo>-</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq12.gif"/></alternatives></inline-formula> for <italic>&#x003bc;</italic>&#x02009;=&#x02009;0.3 (harmonic) and <italic>&#x003bc;</italic>&#x02009;=&#x02009;5 (relaxation) and was rescaled in space to lie within [&#x02212;1, 1]<sup>2</sup> and sped up in time by a factor of 20. The networks were initialized with the static matrix with 10% connectivity. This induces chaotic spiking in the network. We allow the network to settle onto the chaotic attractor for a period of 5&#x02009;s. After this initial period, RLS is activated to turn on FORCE training for a duration of 5&#x02009;s. After this period of time, RLS was deactivated for the remainder 5&#x02009;s of simulation time for all teaching signals except signals that were the product of sinusoids. These signals required 50&#x02009;s of training time and were tested for a longer duration post training to determine convergence. The <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup> parameter used for RLS was taken to be the integration time step, 0.01&#x02009;ms for all implementations of the theta model in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. The integration time step used in all cases of the theta model was 0.01&#x02009;ms.</p><p id="Par58">The theta neuron parameters were as in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref> for the 5&#x02009;Hz sinusoidal oscillator. The LIF network consisted of 2000 neurons with <italic>G</italic>&#x02009;=&#x02009;0.04, <italic>Q</italic>&#x02009;=&#x02009;10, with 10% connectivity in the static weight matrix and &#x00394;<italic>t</italic>&#x02009;=&#x02009;2.5&#x02009;ms with an integration time step of 0.5&#x02009;ms. The average firing rate was 22.9&#x02009;Hz. The Izhikevich network consisted of 2000 neurons with <italic>G</italic>&#x02009;=&#x02009;5&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, <italic>Q</italic>&#x02009;=&#x02009;5&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, with 10% connectivity in the static weight matrix and &#x00394;<italic>t</italic>&#x02009;=&#x02009;0.8&#x02009;ms and an average firing rate of 36.7&#x02009;Hz. The <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup> parameter used for RLS was taken to be a small fraction of the integration time step, <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;0.0025&#x02009;ms for Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> while the Izhikevich model had <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;1&#x02009;ms and an integration time step of 0.04&#x02009;ms. The training time for all networks considered was 4&#x02009;s, followed by 5&#x02009;s of testing.</p><p id="Par59">The Lorenz system with the parameter <italic>&#x003c1;</italic>&#x02009;=&#x02009;28, <italic>&#x003c3;</italic>&#x02009;=&#x02009;10, and <italic>B</italic>&#x02009;=&#x02009;8/3 was used to train a network of 5000 theta neurons and is given by the equations:<disp-formula id="Equ25"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot x = \sigma (y - x)$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:mi>&#x01e8b;</mml:mi><mml:mo>=</mml:mo><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ25.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ26"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot y = x(\rho - z) - y$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mi>&#x01e8f;</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>&#x003c1;</mml:mi><mml:mo>-</mml:mo><mml:mi>z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>y</mml:mi></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ26.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ27"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot z = xy - Bz.$$\end{document}</tex-math><mml:math id="M78" display="block"><mml:mi>&#x0017c;</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>B</mml:mi><mml:mi>z</mml:mi><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ27.gif" position="anchor"/></alternatives></disp-formula>The connectivity parameters were <italic>G</italic>&#x02009;=&#x02009;14, and <italic>Q</italic>&#x02009;=&#x02009;10<sup>4</sup>. The average firing rate was 22.21&#x02009;Hz. The network was trained for 45&#x02009;s, and reproduced Lorenz-like trajectories and the attractor for the remaining 50&#x02009;s.</p></sec><sec id="Sec18"><title>Figure 3 methods</title><p id="Par60">The teaching signal was constructed using the positive component for a sinusoid with a frequency of 2&#x02009;Hz for quarter notes and 1&#x02009;Hz for half notes. Each pulse corresponds to the presence of a note and they can also be used as the amplitude/envelope of the harmonics corresponding to each note to generate an audio-signal from the network (Supplementary Audio &#x000a0;<xref rid="MOESM6" ref-type="media">1</xref>). The network consisted of 5000 Izhikevich neurons with identical parameters as before, <italic>G</italic>&#x02009;=&#x02009;1&#x02009;&#x000d7;&#x02009;10<sup>4</sup>, <italic>Q</italic>&#x02009;=&#x02009;4&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, and &#x00394;<italic>t</italic>&#x02009;=&#x02009;4&#x02009;ms and an integration time step of 0.04&#x02009;ms. The teaching signal was continually fed into the network for 900&#x02009;s, corresponding to 225 repetitions of the signal. After training, the network was simulated with RLS off for a period of 1000&#x02009;s. In this interval, correct replay of the song corresponded to 82% of the duration with 205 distinct, correct replays of the song within the signal. Correct replays were automatically classified by constructing a moving average error function:<disp-formula id="Equ28"><label>25</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E(t) = \mathop {\sum}\limits_{i = 1}^5 {\int}_t^{t + T} \left( {\hat x_i\left( {t^{\prime}} \right) - x_i\left( {t^{\prime}} \right)} \right)^2{ dt}^{\prime},$$\end{document}</tex-math><mml:math id="M80" display="block"><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo> &#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mo>&#x0222b;</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ28.gif" position="anchor"/></alternatives></disp-formula>where <italic>T</italic>&#x02009;=&#x02009;4 seconds is the length of the song. As we vary <italic>t</italic>, the teaching signal <italic>x</italic>(<italic>t</italic>&#x02032;) comes into alignment with correct playbacks in the network output, <inline-formula id="IEq13"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat x\left( {t^{\prime}} \right)$$\end{document}</tex-math><mml:math id="M82"><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msup><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x02032;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="41467_2017_1827_Article_IEq13.gif"/></alternatives></inline-formula>. This reduces the error creating local minima of <italic>E</italic>(<italic>t</italic>) that correspond to correct replays at times <italic>t</italic>&#x02009;=&#x02009;<italic>t</italic>
<sup>*</sup>. These are automatically classified as correct if <italic>E</italic>(<italic>t</italic>
<sup>*</sup>) is under a critical value. The average firing rate for the network was 34&#x02009;Hz. <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;2&#x02009;ms was used for RLS training.</p></sec><sec id="Sec19"><title>Figure 4 methods</title><p id="Par61">The teaching signal consisted of the spectrogram for a 5&#x02009;s long recording of the singing behavior of a male zebra finch that was generated with a 22.7&#x02009;ms window. This data were obtained from the CRCNS data repository<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>. The RA network consisted of 1000 neurons that was FORCE trained for 50&#x02009;s with <italic>G</italic>&#x02009;=&#x02009;1.3&#x02009;&#x000d7;&#x02009;10<sup>4</sup>, <italic>Q</italic>&#x02009;=&#x02009;1&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;2&#x02009;ms, &#x00394;<italic>t</italic>&#x02009;=&#x02009;4&#x02009;ms and an integration time step of 0.04&#x02009;ms. The average firing rate for the network was 52.93&#x02009;Hz. The HVC input was modeled as a sequence of 500 pulses that were constructed from the positive component of a sinusoidal oscillation with a period of 20&#x02009;ms. The pulses fired in a chain starting with the first component. This chain was multiplied by a static, <italic>N</italic>&#x02009;&#x000d7;&#x02009;500 feedforward weight matrix, <italic>W</italic>
<sup>in</sup> where each weight was drawn uniformly from the [&#x02212;8&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, 8&#x02009;&#x000d7;&#x02009;10<sup>3</sup>]. The network was tested for another 50&#x02009;s after RLS was turned off to test if learning was successful. We note that for this songbird example, the network could also learn the output with only the static, chaos inducing weight matrix and the feedforward HVC inputs (<italic>Q</italic>&#x02009;=&#x02009;0). However when <italic>Q</italic>&#x02009;=&#x02009;0, the network behavior is similar to the more classic liquid state regime.</p><p id="Par62">To manipulate the weight matrix post training, we introduced a parameter <italic>&#x003b1;</italic> that allowed us to control the balance between excitation and inhibition:<disp-formula id="Equ29"><label>26</label><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\omega _{ij} = \alpha \left( {G\omega _{ij}^0 + Q\eta _i \cdot \phi _j} \right)_ + + \left( {G\omega _{ij}^0 + Q\eta _i \cdot \phi _j} \right)_ -,$$\end{document}</tex-math><mml:math id="M84" display="block"><mml:msub><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>G</mml:mi><mml:msubsup><mml:mrow><mml:mi>&#x003c9;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>Q</mml:mi><mml:msub><mml:mrow><mml:mi>&#x003b7;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x022c5;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003d5;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>-</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ29.gif" position="anchor"/></alternatives></disp-formula>where (<italic>x</italic>)&#x000b1; denotes:<disp-formula id="Equ30"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(x)_\pm = \left( {\begin{array}{*{20}{l}} x 	 {x  &#x0003e;\left( &#x0003c; \right)0} \\ 0 	 {x \le \left( \ge \right)0} \end{array}} \right..$$\end{document}</tex-math><mml:math id="M86" display="block"><mml:msub><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>&#x000b1;</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>x</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>&#x0003c;</mml:mo></mml:mrow></mml:mfenced><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn><mml:mi>x</mml:mi><mml:mo>&#x02264;</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>&#x02265;</mml:mo></mml:mrow></mml:mfenced><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math><graphic xlink:href="41467_2017_1827_Article_Equ30.gif" position="anchor"/></alternatives></disp-formula>The values <italic>&#x003b1;</italic>&#x02009;=&#x02009;1 yield the original weight matrix, while <italic>&#x003b1;</italic>&#x02009;&#x0003e;&#x02009;1 amplifies the excitatory connections and <italic>&#x003b1;</italic>&#x02009;&#x0003c;&#x02009;1 diminishes the excitatory connections.</p></sec><sec id="Sec20"><title>Figure 5 methods</title><p id="Par63">The teaching signal consisted of an 8&#x02009;s movie clip&#x000a0;(see&#x000a0;[<xref ref-type="bibr" rid="CR77">77</xref>]). The frames were smoothed and interpolated so that RLS could be applied at any time point, instead of just the fixed times corresponding to the native frames per second of the original movie clip. The movie was downsampled to 1920 pixels (30&#x02009;&#x000d7;&#x02009;64) which formed the supervisor. For both implementations (external and internal HDTS), the replay network consisted of 1000 neurons with <italic>G</italic>&#x02009;=&#x02009;5&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, <italic>Q</italic>&#x02009;=&#x02009;4&#x02009;&#x000d7;&#x02009;10<sup>2</sup>, <italic>&#x003bb;</italic>
<sup>&#x02212;1</sup>&#x02009;=&#x02009;2&#x02009;ms, &#x00394;<italic>t</italic>&#x02009;=&#x02009;4&#x02009;ms and an integration time step of 0.04&#x02009;ms. For the external HDTS network, an HDTS was generated with 32 pulses that were 250&#x02009;ms in duration. The HDTS was multiplied by a static, <italic>N</italic>&#x02009;&#x000d7;&#x02009;32 feedforward weight matrix, <italic>W</italic>
<sup>in</sup> where each weight was drawn uniformly from [&#x02212;4&#x02009;&#x000d7;&#x02009;10<sup>3</sup>, 4&#x02009;&#x000d7;&#x02009;10<sup>3</sup>] and 74&#x02009;s of FORCE training was used. The network was simulated for another 370&#x02009;s after RLS was turned off to test if learning was successful. For the internal HDTS case, a separate HDTS network was trained with 2000 neurons. The supervisor for this network consisted of a 64-dimensional HDTS with pulses that were 125&#x02009;ms in duration. The HDTS was fed into an identical replay network as in the external HDTS case. The replay network was trained simultaneously to the HDTS network and had <italic>G</italic>&#x02009;=&#x02009;1&#x02009;&#x000d7;&#x02009;10<sup>4</sup>, <italic>Q</italic>&#x02009;=&#x02009;4&#x02009;&#x000d7;&#x02009;10<sup>3</sup>. The HDTS was fed into the replay network in an identical manner to the external HDTS case. RLS was applied to train the HDTS and replay network with identical parameters as in the external HDTS case, only with a longer training time (290&#x02009;s). We note that for this movie example, the network could also learn the output with only the static, chaos inducing weight matrix and the feedforward HDTS inputs (<italic>Q</italic>&#x02009;=&#x02009;0). However when <italic>Q</italic>&#x02009;=&#x02009;0, the network behavior is similar to the more classic liquid state regime.</p></sec><sec id="Sec21"><title>Data availability</title><p id="Par64">The code used for this paper can be found on modelDB<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, under accession number 190565.</p></sec></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec22"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2017_1827_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2017_1827_MOESM2_ESM.pdf"><caption><p>Peer Review File</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2017_1827_MOESM3_ESM.pdf"><caption><p>Description of Additional Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM4"><media xlink:href="41467_2017_1827_MOESM4_ESM.mp4"><caption><p>Supplementary Movie 1</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM5"><media xlink:href="41467_2017_1827_MOESM5_ESM.mp4"><caption><p>Supplementary Movie 2</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM6"><media xlink:href="41467_2017_1827_MOESM6_ESM.wav"><caption><p>Supplementary&#x000a0;Audio 1</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Electronic supplementary material</bold></p><p>
<bold>Supplementary Information</bold> accompanies this paper at 10.1038/s41467-017-01827-3.</p></fn><fn><p>
<bold>Publisher's note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was funded by the Canadian National Sciences and Engineering Research Council (NSERC) Post-doctoral Fellowship, by the Wellcome Trust (200790/Z/16/Z), the Leverhulme Trust (RPG-2015-171), and the BBSRC (BB/N013956/1 and BB/N019008/1). We would like to thank Frances Skinner, Chris Eliasmith, Larry Abbott, Raoul-Martin Memmesheimer, Brian DePasquale, and Dean Buonomano for their comments. Finally, we would like to especially thank the anonymous referees. Their comments and suggestions greatly improved this manuscript.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>W.N. performed wrote software and performed simulations. Investigation and analysis was performend by W.N. and C.C. W.N. and C.C. wrote the manuscript.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing interests</title><p id="Par65">The authors declare no competing financial interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><year>2009</year><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><?supplied-pmid 19709635?><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">DePasquale, B., Churchland, M. M. &#x00026; Abbott, L. Using firing-rate dynamics to train recurrent networks of spiking model neurons. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1601.07620">https://arxiv.org/abs/1601.07620</ext-link> (2016).</mixed-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>L</given-names></name><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Memmesheimer</surname><given-names>RM</given-names></name></person-group><article-title>Building functional networks of spiking model neurons</article-title><source>Nat. Neurosci.</source><year>2016</year><volume>19</volume><fpage>350</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nn.4241</pub-id><?supplied-pmid 26906501?><pub-id pub-id-type="pmid">26906501</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thalmeier</surname><given-names>D</given-names></name><name><surname>Uhlmann</surname><given-names>M</given-names></name><name><surname>Kappen</surname><given-names>HJ</given-names></name><name><surname>Memmesheimer</surname><given-names>R-M</given-names></name></person-group><article-title>Learning universal computations with spikes</article-title><source>PLoS. Comput. Biol</source><year>2016</year><volume>12</volume><fpage>e1004895</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004895</pub-id><?supplied-pmid 27309381?><pub-id pub-id-type="pmid">27309381</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname><given-names>M</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Den&#x000e9;ve</surname><given-names>S</given-names></name></person-group><article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title><source>PLoS. Comput. Biol.</source><year>2013</year><volume>9</volume><fpage>e1003258</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003258</pub-id><?supplied-pmid 24244113?><pub-id pub-id-type="pmid">24244113</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwemmer</surname><given-names>MA</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Den&#x000e9;ve</surname><given-names>S</given-names></name><name><surname>Shea-Brown</surname><given-names>ET</given-names></name></person-group><article-title>Constructing precisely computing networks with biophysical spiking neurons</article-title><source>J. Neurosci.</source><year>2015</year><volume>35</volume><fpage>10112</fpage><lpage>10134</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4951-14.2015</pub-id><?supplied-pmid 26180189?><pub-id pub-id-type="pmid">26180189</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Bourdoukan, R. &#x00026; Den&#x000e9;ve, S. Enforcing balance allows local supervised learning in spiking recurrent networks. In Advances in Neural Information Processing Systems, 982&#x02013;990 (2015).</mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><etal/></person-group><article-title>A large-scale model of the functioning brain</article-title><source>Science</source><year>2012</year><volume>338</volume><fpage>1202</fpage><lpage>1205</lpage><pub-id pub-id-type="doi">10.1126/science.1225266</pub-id><?supplied-pmid 23197532?><pub-id pub-id-type="pmid">23197532</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group><source>Neural engineering: Computation, representation, and dynamics in neurobiological systems</source><year>2002</year><publisher-loc>MIT Press</publisher-loc><publisher-name>Cambridge, MA</publisher-name></element-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Gilra, A. &#x00026; Gerstner, W. Predicting non-linear dynamics: a stable local learning scheme for recurrent spiking neural networks. <italic>arXiv:1702.06463</italic> (2017).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Alemi, A., Machens, C., Den&#x000e9;ve, S. &#x00026; Slotine, J.-J. Learning arbitrary dynamics in efficient, balanced spiking networks using local plasticity rules. Preprint at <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.08026">https://arxiv.org/abs/1705.08026</ext-link> (2017).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Brendel, W., Bourdoukan, R., Vertechi, P., Machens, C. K. &#x00026; Den&#x000e9;ve, S. Learning to represent signals spike by spike. <italic>arXiv:1703.03777</italic> (2017).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Huh, D. &#x00026; Sejnowski, T. J. Gradient descent for spiking neural networks. <italic>arXiv:1706.04698</italic> (2017).</mixed-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luko&#x00161;evi&#x0010d;ius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name><name><surname>Schrauwen</surname><given-names>B</given-names></name></person-group><article-title>Reservoir computing trends</article-title><source>KI-K&#x000fc;nstliche Intelligenz</source><year>2012</year><volume>26</volume><fpage>365</fpage><lpage>371</lpage><pub-id pub-id-type="doi">10.1007/s13218-012-0204-5</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luko&#x00161;evi&#x0010d;ius</surname><given-names>M</given-names></name><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Comput. Sci. Rev.</source><year>2009</year><volume>3</volume><fpage>127</fpage><lpage>149</lpage><pub-id pub-id-type="doi">10.1016/j.cosrev.2009.03.005</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Schrauwen, B., Verstraeten, D. &#x00026; Van Campenhout, J. An overview of reservoir computing: theory, applications and implementations. In Proceedings of the 15th European Symposium on Articial Neural Networks, 471&#x02013;482 (2007).</mixed-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dominey</surname><given-names>PF</given-names></name></person-group><article-title>Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning</article-title><source>Biol. Cybern.</source><year>1995</year><volume>73</volume><fpage>265</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1007/BF00201428</pub-id><?supplied-pmid 7548314?><pub-id pub-id-type="pmid">7548314</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschl&#x000e4;ger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural. Comput.</source><year>2002</year><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><?supplied-pmid 12433288?><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>H</given-names></name></person-group><article-title>The &#x0201c;echo state&#x0201d; approach to analysing and training recurrent neural networks-with an erratum note</article-title><source>Bonn, Ger.</source><year>2001</year><volume>148</volume><fpage>34</fpage></element-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Schliebs, S., Mohemmed, A. &#x00026; Kasabov, N. Are probabilistic spiking neural networks suitable for reservoir computing? In Neural Networks (IJCNN), The 2011 International Joint Conference on Neural Networks, 3156&#x02013;3163 (2011).</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozturk</surname><given-names>MC</given-names></name><name><surname>Principe</surname><given-names>JC</given-names></name></person-group><article-title>Computing with transiently stable states in Proceedings</article-title><source>IEEE International Joint Conference on Neural Networks</source><year>2005</year><volume>3</volume><fpage>1467</fpage><lpage>1472</lpage></element-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Maass, W. Liquid state machines: motivation, theory, and applications. In: B. Cooper, A. Sorbi (eds.) Computability in Context: Computation and Logic in the Real World, 275&#x02013;296 (Imperial College Press, 2011).</mixed-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Natschl&#x000e4;ger</surname><given-names>T</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Fading memory and kernel properties of generic cortical microcircuit models</article-title><source>J. Physiol.</source><year>2004</year><volume>98</volume><fpage>315</fpage><lpage>330</lpage></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wojcik</surname><given-names>GM</given-names></name><name><surname>Kaminski</surname><given-names>WA</given-names></name></person-group><article-title>Liquid state machine built of Hodgkin&#x02013;Huxley neurons and pattern recognition</article-title><source>Neurocomputing</source><year>2004</year><volume>58</volume><fpage>245</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1016/j.neucom.2004.01.051</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buonomano</surname><given-names>DV</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><article-title>Temporal information transformed into a spatial code by a neural network with realistic properties</article-title><source>Science</source><year>1995</year><volume>267</volume><fpage>1028</fpage><pub-id pub-id-type="doi">10.1126/science.7863330</pub-id><?supplied-pmid 7863330?><pub-id pub-id-type="pmid">7863330</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><year>1996</year><volume>274</volume><fpage>1724</fpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><?supplied-pmid 8939866?><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><article-title>Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons</article-title><source>Nat. Neurosci.</source><year>2014</year><volume>17</volume><fpage>594</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1038/nn.3658</pub-id><?supplied-pmid 24561997?><pub-id pub-id-type="pmid">24561997</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harish</surname><given-names>O</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name></person-group><article-title>Asynchronous rate chaos in spiking neuronal circuits</article-title><source>PLoS. Comput. Biol.</source><year>2015</year><volume>11</volume><fpage>e1004266</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004266</pub-id><?supplied-pmid 26230679?><pub-id pub-id-type="pmid">26230679</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Haykin, S. S. <italic>Neural networks and learning machines</italic> (Pearson Upper Saddle River, NJ, 2009).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Haykin, S. in Neural Networks and Learning Machines 3rd edn, 246-249 (Upper Saddle River, NJ, USA; Pearson, 2009).</mixed-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>ML</given-names></name><name><surname>Morse</surname><given-names>T</given-names></name><name><surname>Migliore</surname><given-names>M</given-names></name><name><surname>Carnevale</surname><given-names>NT</given-names></name><name><surname>Shepherd</surname><given-names>GM</given-names></name></person-group><article-title>ModelDB: a database to support computational neuroscience</article-title><source>J. Comput. Neurosci.</source><year>2004</year><volume>17</volume><fpage>7</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1023/B:JCNS.0000023869.22017.2e</pub-id><?supplied-pmid 15218350?><pub-id pub-id-type="pmid">15218350</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><article-title>Eigenvalue spectra of random matrices for neural networks</article-title><source>Phys. Rev. Lett.</source><year>2006</year><volume>97</volume><fpage>188104</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.97.188104</pub-id><?supplied-pmid 17155583?><pub-id pub-id-type="pmid">17155583</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>M</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>Beeren</surname><given-names>L</given-names></name><name><surname>H&#x000e4;usser</surname><given-names>M</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex</article-title><source>Nature</source><year>2010</year><volume>466</volume><fpage>123</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1038/nature09086</pub-id><?supplied-pmid 20596024?><pub-id pub-id-type="pmid">20596024</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monteforte</surname><given-names>M</given-names></name><name><surname>Wolf</surname><given-names>F</given-names></name></person-group><article-title>Dynamic flux tubes form reservoirs of stability in neuronal circuits</article-title><source>Phys. Rev. X</source><year>2012</year><volume>2</volume><fpage>041007</fpage></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahnloser</surname><given-names>RH</given-names></name><name><surname>Kozhevnikov</surname><given-names>AA</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird</article-title><source>Nature</source><year>2002</year><volume>419</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1038/nature00974</pub-id><?supplied-pmid 12214232?><pub-id pub-id-type="pmid">12214232</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>MA</given-names></name><name><surname>Jin</surname><given-names>DZ</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><article-title>Support for a synaptic chain model of neuronal sequence generation</article-title><source>Nature</source><year>2010</year><volume>468</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1038/nature09514</pub-id><?supplied-pmid 20972420?><pub-id pub-id-type="pmid">20972420</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardo</surname><given-names>A</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><article-title>Ensemble coding of vocal control in birdsong</article-title><source>J. Neurosci.</source><year>2005</year><volume>25</volume><fpage>652</fpage><lpage>661</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3036-04.2005</pub-id><?supplied-pmid 15659602?><pub-id pub-id-type="pmid">15659602</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vicario</surname><given-names>DS</given-names></name><name><surname>Raksin</surname><given-names>JN</given-names></name></person-group><article-title>Possible roles for GABAergic inhibition in the vocal control system of the zebra finch</article-title><source>Neuroreport</source><year>2000</year><volume>11</volume><fpage>3631</fpage><lpage>3635</lpage><pub-id pub-id-type="doi">10.1097/00001756-200011090-00046</pub-id><?supplied-pmid 11095533?><pub-id pub-id-type="pmid">11095533</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nottebohm</surname><given-names>F</given-names></name><name><surname>Stokes</surname><given-names>TM</given-names></name><name><surname>Leonard</surname><given-names>CM</given-names></name></person-group><article-title>Central control of song in the canary, Serinus canarius</article-title><source>J. Comp. Neurol.</source><year>1976</year><volume>165</volume><fpage>457</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1002/cne.901650405</pub-id><?supplied-pmid 1262540?><pub-id pub-id-type="pmid">1262540</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Euston</surname><given-names>DR</given-names></name><name><surname>Tatsuno</surname><given-names>M</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Fast-forward playback of recent memory sequences in prefrontal cortex during sleep</article-title><source>Science</source><year>2007</year><volume>318</volume><fpage>1147</fpage><lpage>1150</lpage><pub-id pub-id-type="doi">10.1126/science.1148979</pub-id><?supplied-pmid 18006749?><pub-id pub-id-type="pmid">18006749</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nat. Neurosci.</source><year>2007</year><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1038/nn1961</pub-id><?supplied-pmid 17828259?><pub-id pub-id-type="pmid">17828259</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname><given-names>GB</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><article-title>A scalable population code for time in the striatum</article-title><source>Curr. Biol.</source><year>2015</year><volume>25</volume><fpage>1113</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.02.036</pub-id><?supplied-pmid 25913405?><pub-id pub-id-type="pmid">25913405</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>N&#x000e1;dasdy</surname><given-names>Z</given-names></name><name><surname>Hirase</surname><given-names>H</given-names></name><name><surname>Czurk&#x000f3;</surname><given-names>A</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Replay and time compression of recurring spike sequences in the hippocampus</article-title><source>J. Neurosci.</source><year>1999</year><volume>19</volume><fpage>9497</fpage><lpage>9507</lpage><?supplied-pmid 10531452?><pub-id pub-id-type="pmid">10531452</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motanis</surname><given-names>H</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><article-title>Neural coding: time contraction and dilation in the striatum</article-title><source>Curr. Biol.</source><year>2015</year><volume>25</volume><fpage>R374</fpage><lpage>R376</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.02.057</pub-id><?supplied-pmid 25942552?><pub-id pub-id-type="pmid">25942552</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><etal/></person-group><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><year>2012</year><volume>487</volume><fpage>51</fpage><lpage>56</lpage><?supplied-pmid 22722855?><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Konishi</surname><given-names>M</given-names></name></person-group><article-title>Birdsong: from behavior to neuron</article-title><source>Ann. Rev. Neurosci.</source><year>1985</year><fpage>125</fpage><lpage>170</lpage></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mooney</surname><given-names>R</given-names></name></person-group><article-title>Different subthreshold mechanisms underlie song selectivity in identied HVc neurons of the zebra nch</article-title><source>J. Neurosci.</source><year>2000</year><volume>20</volume><fpage>5420</fpage><lpage>5436</lpage><?supplied-pmid 10884326?><pub-id pub-id-type="pmid">10884326</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouchard</surname><given-names>KE</given-names></name><name><surname>Brainard</surname><given-names>MS</given-names></name></person-group><article-title>Auditory-induced neural dynamics in sensory-motor circuitry predict learned temporal and sequential statistics of birdsong</article-title><source>Proc. Natl Acad. Sci.</source><year>2016</year><volume>113</volume><fpage>9641</fpage><lpage>9646</lpage><pub-id pub-id-type="doi">10.1073/pnas.1606725113</pub-id><?supplied-pmid 27506786?><pub-id pub-id-type="pmid">27506786</pub-id></element-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozhevnikov</surname><given-names>AA</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><article-title>Singing-related activity of identified HVC neurons in the zebra finch</article-title><source>J. Neurophysiol.</source><year>2007</year><volume>97</volume><fpage>4271</fpage><lpage>4283</lpage><pub-id pub-id-type="doi">10.1152/jn.00952.2006</pub-id><?supplied-pmid 17182906?><pub-id pub-id-type="pmid">17182906</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Theta oscillations in the hippocampus</article-title><source>Neuron</source><year>2002</year><volume>33</volume><fpage>325</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)00586-X</pub-id><?supplied-pmid 11832222?><pub-id pub-id-type="pmid">11832222</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W</given-names></name></person-group><article-title>EEG alpha and theta oscillations reflect cognitive and memory performance: a review and analysis</article-title><source>Brain Res. Rev.</source><year>1999</year><volume>29</volume><fpage>169</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/S0165-0173(98)00056-3</pub-id><?supplied-pmid 10209231?><pub-id pub-id-type="pmid">10209231</pub-id></element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Theta rhythm of navigation: link between path integration and landmark navigation, episodic and semantic memory</article-title><source>Hippocampus</source><year>2005</year><volume>15</volume><fpage>827</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1002/hipo.20113</pub-id><?supplied-pmid 16149082?><pub-id pub-id-type="pmid">16149082</pub-id></element-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Huhn</surname><given-names>Z</given-names></name><name><surname>&#x000c9;rdi</surname><given-names>P</given-names></name></person-group><article-title>Computational theories on the function of theta oscillations</article-title><source>Biol. Cybern.</source><year>2005</year><volume>92</volume><fpage>393</fpage><lpage>408</lpage><pub-id pub-id-type="doi">10.1007/s00422-005-0567-x</pub-id><?supplied-pmid 15900483?><pub-id pub-id-type="pmid">15900483</pub-id></element-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lubenov</surname><given-names>EV</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name></person-group><article-title>Hippocampal theta oscillations are travelling waves</article-title><source>Nature</source><year>2009</year><volume>459</volume><fpage>534</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1038/nature08010</pub-id><?supplied-pmid 19489117?><pub-id pub-id-type="pmid">19489117</pub-id></element-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname><given-names>JE</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>The theta-gamma neural code</article-title><source>Neuron</source><year>2013</year><volume>77</volume><fpage>1002</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.007</pub-id><?supplied-pmid 23522038?><pub-id pub-id-type="pmid">23522038</pub-id></element-citation></ref><ref id="CR56"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Givens</surname><given-names>B</given-names></name><name><surname>Olton</surname><given-names>DS</given-names></name></person-group><article-title>Bidirectional modulation of scopolamine-induced working memory impairments by muscarinic activation of the medial septal area</article-title><source>Neurobiol. Learn. Mem.</source><year>1995</year><volume>63</volume><fpage>269</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1006/nlme.1995.1031</pub-id><?supplied-pmid 7670840?><pub-id pub-id-type="pmid">7670840</pub-id></element-citation></ref><ref id="CR57"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyce</surname><given-names>R</given-names></name><name><surname>Glasgow</surname><given-names>SD</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name><name><surname>Adamantidis</surname><given-names>A</given-names></name></person-group><article-title>Causal evidence for the role of REM sleep theta rhythm in contextual memory consolidation</article-title><source>Science</source><year>2016</year><volume>352</volume><fpage>812</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1126/science.aad5252</pub-id><?supplied-pmid 27174984?><pub-id pub-id-type="pmid">27174984</pub-id></element-citation></ref><ref id="CR58"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Itskov</surname><given-names>V</given-names></name><name><surname>Amarasingham</surname><given-names>A</given-names></name><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Internally generated cell assembly sequences in the rat hippocampus</article-title><source>Science</source><year>2008</year><volume>321</volume><fpage>1322</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.1126/science.1159775</pub-id><?supplied-pmid 18772431?><pub-id pub-id-type="pmid">18772431</pub-id></element-citation></ref><ref id="CR59"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname><given-names>CJ</given-names></name><name><surname>Lepage</surname><given-names>KQ</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><article-title>Hippocampal &#x0201c;time cells&#x0201d; bridge the gap in memory for discontiguous events</article-title><source>Neuron</source><year>2011</year><volume>71</volume><fpage>737</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.012</pub-id><?supplied-pmid 21867888?><pub-id pub-id-type="pmid">21867888</pub-id></element-citation></ref><ref id="CR60"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><article-title>Time cells in the hippocampus: a new dimension for mapping memories</article-title><source>Nat. Rev. Neurosci.</source><year>2014</year><volume>15</volume><fpage>732</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1038/nrn3827</pub-id><?supplied-pmid 25269553?><pub-id pub-id-type="pmid">25269553</pub-id></element-citation></ref><ref id="CR61"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Lustig</surname><given-names>B</given-names></name><name><surname>Leonardo</surname><given-names>A</given-names></name><name><surname>Pastalkova</surname><given-names>E</given-names></name></person-group><article-title>Theta sequences are essential for internally generated hippocampal firing fields</article-title><source>Nat. Neurosci.</source><year>2015</year><volume>18</volume><fpage>282</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1038/nn.3904</pub-id><?supplied-pmid 25531571?><pub-id pub-id-type="pmid">25531571</pub-id></element-citation></ref><ref id="CR62"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salz</surname><given-names>DM</given-names></name><etal/></person-group><article-title>Time cells in hippocampal area CA3</article-title><source>J. Neurosci.</source><year>2016</year><volume>36</volume><fpage>7476</fpage><lpage>7484</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0087-16.2016</pub-id><?supplied-pmid 27413157?><pub-id pub-id-type="pmid">27413157</pub-id></element-citation></ref><ref id="CR63"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>NT</given-names></name><etal/></person-group><article-title>Medial entorhinal cortex selectively supports temporal coding by hippocampal</article-title><source>Neurons. Neuron</source><year>2017</year><volume>94</volume><fpage>677</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.003</pub-id><?supplied-pmid 28434800?><pub-id pub-id-type="pmid">28434800</pub-id></element-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizuseki</surname><given-names>K</given-names></name><name><surname>Sirota</surname><given-names>A</given-names></name><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Buzs&#x000e1;ki</surname><given-names>G</given-names></name></person-group><article-title>Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop</article-title><source>Neuron</source><year>2009</year><volume>64</volume><fpage>267</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.037</pub-id><?supplied-pmid 19874793?><pub-id pub-id-type="pmid">19874793</pub-id></element-citation></ref><ref id="CR65"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivkind</surname><given-names>A</given-names></name><name><surname>Barak</surname><given-names>O</given-names></name></person-group><article-title>Local dynamics in trained recurrent neural networks</article-title><source>Phys. Rev. Lett.</source><year>2017</year><volume>118</volume><fpage>258101</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.118.258101</pub-id><?supplied-pmid 28696758?><pub-id pub-id-type="pmid">28696758</pub-id></element-citation></ref><ref id="CR66"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Pawelzik</surname><given-names>K</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>Neural networks with dynamic synapses</article-title><source>Neural. Comput.</source><year>1998</year><volume>10</volume><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1162/089976698300017502</pub-id><?supplied-pmid 9573407?><pub-id pub-id-type="pmid">9573407</pub-id></element-citation></ref><ref id="CR67"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><article-title>On the computational power of circuits of spiking neurons</article-title><source>J. Comput. Syst. Sci.</source><year>2004</year><volume>69</volume><fpage>593</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/j.jcss.2004.04.001</pub-id></element-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Maass, W., Natschl&#x000e4;ger, T. &#x00026; Markram, H. A model for real-time computation in generic neural microcircuits. Adv. Neural Inform. Process. Syst. 15, NIPS 2002, MIT Press, Cambridge, MA 213&#x02013;220 (2002).</mixed-citation></ref><ref id="CR69"><label>69.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Harvey</surname><given-names>CD</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name></person-group><article-title>Recurrent network models of sequence generation and memory</article-title><source>Neuron</source><year>2016</year><volume>90</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.009</pub-id><?supplied-pmid 26971945?><pub-id pub-id-type="pmid">26971945</pub-id></element-citation></ref><ref id="CR70"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><year>2016</year><volume>532</volume><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature17643</pub-id><?supplied-pmid 27074502?><pub-id pub-id-type="pmid">27074502</pub-id></element-citation></ref><ref id="CR71"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enel</surname><given-names>P</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Quilodran</surname><given-names>R</given-names></name><name><surname>Dominey</surname><given-names>PF</given-names></name></person-group><article-title>Reservoir computing properties of neural dynamics in prefrontal cortex</article-title><source>PLoS Comput. Biol.</source><year>2016</year><volume>12</volume><fpage>e1004967</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004967</pub-id><?supplied-pmid 27286251?><pub-id pub-id-type="pmid">27286251</pub-id></element-citation></ref><ref id="CR72"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laje</surname><given-names>R</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><article-title>Robust timing and motor patterns by taming chaos in recurrent neural networks</article-title><source>Nat. Neurosci.</source><year>2013</year><volume>16</volume><fpage>925</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nn.3405</pub-id><?supplied-pmid 23708144?><pub-id pub-id-type="pmid">23708144</pub-id></element-citation></ref><ref id="CR73"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dur-e-Ahmad</surname><given-names>M</given-names></name><name><surname>Nicola</surname><given-names>W</given-names></name><name><surname>Campbell</surname><given-names>SA</given-names></name><name><surname>Skinner</surname><given-names>FK</given-names></name></person-group><article-title>Network bursting using experimentally constrained single compartment CA3 hippocampal neuron models with adaptation</article-title><source>J. Comput. Neurosci.</source><year>2012</year><volume>33</volume><fpage>21</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1007/s10827-011-0372-6</pub-id><?supplied-pmid 22131133?><pub-id pub-id-type="pmid">22131133</pub-id></element-citation></ref><ref id="CR74"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Chaotic balanced state in a model of cortical circuits</article-title><source>Neural. Comput.</source><year>1998</year><volume>10</volume><fpage>1321</fpage><lpage>1371</lpage><pub-id pub-id-type="doi">10.1162/089976698300017214</pub-id><?supplied-pmid 9698348?><pub-id pub-id-type="pmid">9698348</pub-id></element-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Crandall, S. R. &#x00026; Nick, T. A. Neural population spiking activity during singing: adult and longitudinal developmental recordings in the zebra finch. CRCNS.org 10.6080/K0NP22C8 (2014).</mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Sawer, C. &#x00026; Chan, D. <italic>Mutopia</italic> (Mutopia Project, 2008).</mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Nicola, W., &#x00026; Clopath, C. Supervised Learning in Spiking Neural Networks with FORCE Training.&#x000a0; arXiv:1609.02545 (2016).&#x000a0;</mixed-citation></ref></ref-list></back></article>