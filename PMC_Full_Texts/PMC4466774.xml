<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-archivearticle1.dtd?><?SourceDTD.Version 1.1?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS One</journal-id><journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, CA USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4466774</article-id><article-id pub-id-type="publisher-id">PONE-D-14-48184</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0128570</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A Ranking Approach to Genomic Selection</article-title><alt-title alt-title-type="running-head">A Ranking Approach to Genomic Selection</alt-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Blondel</surname><given-names>Mathieu</given-names></name><xref ref-type="aff" rid="aff001">
<sup>1</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Onogi</surname><given-names>Akio</given-names></name><xref ref-type="aff" rid="aff002">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Iwata</surname><given-names>Hiroyoshi</given-names></name><xref ref-type="aff" rid="aff002">
<sup>2</sup>
</xref><xref ref-type="corresp" rid="cor001">*</xref></contrib><contrib contrib-type="author"><name><surname>Ueda</surname><given-names>Naonori</given-names></name><xref ref-type="aff" rid="aff001">
<sup>1</sup>
</xref></contrib></contrib-group><aff id="aff001">
<label>1</label>
<addr-line>NTT Communication Science Laboratories, Kyoto, Japan</addr-line>
</aff><aff id="aff002">
<label>2</label>
<addr-line>The University of Tokyo, Tokyo, Japan</addr-line>
</aff><contrib-group><contrib contrib-type="editor"><name><surname>de la Fuente</surname><given-names>Alberto</given-names></name><role>Academic Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">
<addr-line>Leibniz-Institute for Farm Animal Biology (FBN), GERMANY</addr-line>
</aff><author-notes><fn fn-type="COI-statement" id="coi001"><p><bold>Competing Interests: </bold>This research was funded by the public program FIRST (<ext-link ext-link-type="uri" xlink:href="http://www.jst.go.jp/first/english/">http://www.jst.go.jp/first/english/</ext-link>). NTT will not register any patent or create any product out of this research. This does not alter the authors&#x02019; adherence to PLOS ONE policies on sharing data and materials.</p></fn><fn fn-type="con" id="contrib001"><p>Conceived and designed the experiments: MB AO. Performed the experiments: MB AO. Analyzed the data: MB AO HI NU. Wrote the paper: MB AO HI.</p></fn><corresp id="cor001">* E-mail: <email>aiwata@mail.ecc.u-tokyo.ac.jp</email></corresp></author-notes><pub-date pub-type="collection"><year>2015</year></pub-date><pub-date pub-type="epub"><day>12</day><month>6</month><year>2015</year></pub-date><volume>10</volume><issue>6</issue><elocation-id>e0128570</elocation-id><history><date date-type="received"><day>26</day><month>10</month><year>2014</year></date><date date-type="accepted"><day>28</day><month>4</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; 2015 Blondel et al</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Blondel et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are properly credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:type="simple" xlink:href="pone.0128570.pdf"/><abstract><sec id="sec001"><title>Background</title><p>Genomic selection (GS) is a recent selective breeding method which uses predictive models based on whole-genome molecular markers. Until now, existing studies formulated GS as the problem of modeling an individual&#x02019;s breeding value for a particular trait of interest, i.e., as a regression problem. To assess predictive accuracy of the model, the Pearson correlation between observed and predicted trait values was used.</p></sec><sec id="sec002"><title>Contributions</title><p>In this paper, we propose to formulate GS as the problem of ranking individuals according to their breeding value. Our proposed framework allows us to employ machine learning methods for ranking which had previously not been considered in the GS literature. To assess ranking accuracy of a model, we introduce a new measure originating from the information retrieval literature called normalized discounted cumulative gain (NDCG). NDCG rewards more strongly models which assign a high rank to individuals with high breeding value. Therefore, NDCG reflects a prerequisite objective in selective breeding: accurate selection of individuals with high breeding value.</p></sec><sec id="sec003"><title>Results</title><p>We conducted a comparison of 10 existing regression methods and 3 new ranking methods on 6 datasets, consisting of 4 plant species and 25 traits. Our experimental results suggest that tree-based ensemble methods including McRank, Random Forests and Gradient Boosting Regression Trees achieve excellent ranking accuracy. RKHS regression and RankSVM also achieve good accuracy when used with an RBF kernel. Traditional regression methods such as Bayesian lasso, wBSR and BayesC were found less suitable for ranking. Pearson correlation was found to correlate poorly with NDCG. Our study suggests two important messages. First, ranking methods are a promising research direction in GS. Second, NDCG can be a useful evaluation measure for GS.</p></sec></abstract><funding-group><funding-statement>MB was funded by the FIRST (funding program for world-leading R&#x00026;D on science and technology) program (<ext-link ext-link-type="uri" xlink:href="http://www.jst.go.jp/first/english/en-about-us/">http://www.jst.go.jp/first/english/en-about-us/</ext-link>). AO received grant-in-aid for Japan Society for the Promotion of Science (JSPS) (<ext-link ext-link-type="uri" xlink:href="http://www.jsps.go.jp/english/index.html">http://www.jsps.go.jp/english/index.html</ext-link>, grant 26.10661). HI and NU did not receive specific funding for this work. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. NTT Communication Science Laboratories provided support in the form of salaries for authors MB and NU, but did not have any additional role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the author contributions section.</funding-statement></funding-group><counts><fig-count count="3"/><table-count count="8"/><page-count count="23"/></counts><custom-meta-group><custom-meta id="data-availability"><meta-name>Data Availability</meta-name><meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value></custom-meta></custom-meta-group></article-meta><notes><title>Data Availability</title><p>All relevant data are within the paper and its Supporting Information files.</p></notes></front><body><sec sec-type="intro" id="sec004"><title>Introduction</title><p>Traditional selective breeding, based on phenotypic or pedigree information, has led to much genetic improvement. Genomic selection (GS) [<xref rid="pone.0128570.ref001" ref-type="bibr">1</xref>] is a recent selective breeding method which uses predictive models based on whole-genome molecular markers. Compared to traditional marker-assisted selection methods, the key benefit of GS is that it uses markers covering the whole genome, thus making it possible to predict polygenic traits. With the constantly decreasing cost of marker technology, genotyping is currently less costly than phenotyping in applied plant breeding programs [<xref rid="pone.0128570.ref002" ref-type="bibr">2</xref>]. GS has already been adopted by dairy industries worldwide and is expected to double genetic gains for milk production and other traits [<xref rid="pone.0128570.ref003" ref-type="bibr">3</xref>]. GS can also accelerate selection cycles, since markers can be genotyped at birth or even before [<xref rid="pone.0128570.ref004" ref-type="bibr">4</xref>]. The effectiveness of GS has been confirmed in numerous studies, both for plant and animal breeding [<xref rid="pone.0128570.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0128570.ref004" ref-type="bibr">4</xref>&#x02013;<xref rid="pone.0128570.ref010" ref-type="bibr">10</xref>].</p><p>Until now, GS has traditionally been formulated as the problem of predicting an individual&#x02019;s breeding value for a given trait of interest; for instance, grain yield or or milk production. Therefore, GS was fundamentally formulated as a regression problem. To estimate a regression model, many different parametric and non-parametric methods were proposed in the literature including BayesA, BayesB, best linear unbiased prediction (BLUP) in the original work of [<xref rid="pone.0128570.ref001" ref-type="bibr">1</xref>], Bayesian lasso [<xref rid="pone.0128570.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0128570.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0128570.ref012" ref-type="bibr">12</xref>], a fast EM algorithm for the BayesB model called wBSR [<xref rid="pone.0128570.ref013" ref-type="bibr">13</xref>] and reproducing kernel Hilbert space (RKHS) regression [<xref rid="pone.0128570.ref010" ref-type="bibr">10</xref>, <xref rid="pone.0128570.ref014" ref-type="bibr">14</xref>]. Recently, [<xref rid="pone.0128570.ref015" ref-type="bibr">15</xref>] compared popular methods from the GS literature with other machine learning methods including support vector regression, random forests and neural networks. Their results suggested that GS could be based on a reduced set of models such as Bayesian lasso, wBSR and random forests.</p><p>In this paper, we propose to formulate GS as a ranking problem. This is motivated by the fact that in order to select the most favorable individuals, we do not necessarily need to accurately predict breeding values. Instead, it is often sufficient to correctly rank individuals from most favorable to least favorable. As an example, consider the problem of selecting wheat lines according to their grain yield. In existing studies, this would be formulated as the problem of predicting grain yield from genotypes. In our approach, this is formulated instead as the problem of correctly ranking wheat lines in order of decreasing grain yield. Our proposed framework allows us to employ machine learning methods for ranking which had previously never been considered in the GS literature. Until now, the predictive accuracy of a model was typically assessed using the Pearson correlation between observed trait values and the predicted trait values (a.k.a. genomic estimated breeding values, GEBV). However, our experiments show that Pearson correlation may correlate poorly with ranking accuracy. In this paper, we introduce a new measure originating from the information retrieval literature called normalized discounted cumulative gain (NDCG) [<xref rid="pone.0128570.ref016" ref-type="bibr">16</xref>]. NDCG rewards more strongly models which assign high rank to individuals with high breeding value. In addition, NDCG focuses on the top individuals in the ranking, while Pearson correlation treats all individuals uniformly. Therefore, NDCG reflects a prerequisite objective in selective breeding: accurate selection of the top individuals with highest breeding value.</p></sec><sec id="sec005"><title>Regression-based genomic selection</title><sec id="sec006"><title>General approach</title><p>In this section, we first review the two-phase approach usually taken by traditional regression-based GS methods.</p><p>In the <bold>model estimation</bold> phase (also known as <bold>training phase</bold>), a reference population, which has been genotyped and whose trait values are known, is used to estimate a statistical model of the relationship between genotypes and the trait. For a reference population of size <italic>n</italic>, we denote the genotypes by <bold><italic>x</italic></bold>
<sub>1</sub>, &#x02026;, <bold><italic>x</italic></bold>
<sub><italic>n</italic></sub>, where <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> &#x02208; &#x1d4e7; &#x02286; &#x0211d;<sup><italic>p</italic></sup>, and the associated trait values by <italic>y</italic>
<sub>1</sub>, &#x02026;, <italic>y</italic>
<sub><italic>n</italic></sub>, where <italic>y</italic>
<sub><italic>i</italic></sub> &#x02208; &#x1d4e8; &#x02286; &#x0211d;. The number <italic>p</italic> indicates the number of molecular markers (e.g., DArT, SNP, &#x02026;) used for genotyping. For simplicity, in the remainder of this paper, we use <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> to refer to both individual <italic>i</italic> and its vector representation after genotyping. Likewise, phenotypic values or estimated breeding values will simply be referred to as &#x0201c;trait values&#x0201d;. Trait values of the reference population are typically obtained by field testings, which are expensive and time-consuming. Therefore, <italic>n</italic> is usually small. On the other hand, the number of markers <italic>p</italic> is usually large for genome-wide genotyping. Therefore, <italic>n</italic> is usually much smaller than <italic>p</italic>. This is known as the <italic>n</italic> &#x0226a; <italic>p</italic> problem. Existing GS approaches estimate a regression model <italic>h</italic>: &#x0211d;<sup><italic>p</italic></sup> &#x02192; &#x0211d; such that <italic><italic>h</italic></italic>(<bold>x</bold>
<sub><italic>i</italic></sub>) &#x02248; <italic>y</italic>
<sub><italic>i</italic></sub> for all <italic>i</italic> &#x02208; {1, &#x02026;, <italic>n</italic>}. We briefly review common approaches for estimating <italic>h</italic> further below. In the following, we denote by <italic>X</italic> the <italic>n</italic> &#x000d7; <italic>p</italic> matrix which gathers individuals <bold><italic>x</italic></bold>
<sub>1</sub>, &#x02026;, <bold><italic>x</italic></bold>
<sub><italic>n</italic></sub>, by <bold>y</bold> the <italic>n</italic>-dimensional vector which gathers their true trait values <italic>y</italic>
<sub>1</sub>, &#x02026;, <italic>y</italic>
<sub><italic>n</italic></sub> and by <inline-formula id="pone.0128570.e001"><alternatives><graphic xlink:href="pone.0128570.e001.jpg" id="pone.0128570.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> the <italic>n</italic>-dimensional vector which gathers the predicted values <inline-formula id="pone.0128570.e002"><alternatives><graphic xlink:href="pone.0128570.e002.jpg" id="pone.0128570.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0128570.e003"><alternatives><graphic xlink:href="pone.0128570.e003.jpg" id="pone.0128570.e003g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M3"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>In the <bold>candidate selection</bold> phase, predicted trait values are computed using the fitted model for candidate individuals to be selected. We denote the genotypes of <italic>m</italic> candidates by <inline-formula id="pone.0128570.e004"><alternatives><graphic xlink:href="pone.0128570.e004.jpg" id="pone.0128570.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Contrary to the reference population, the true trait values of the candidates are not known. Throughout this paper, we assume that the individuals from the reference population <bold><italic>x</italic></bold>
<sub>1</sub>, &#x02026;, <bold><italic>x</italic></bold>
<sub><italic>n</italic></sub> and candidate individuals <inline-formula id="pone.0128570.e005"><alternatives><graphic xlink:href="pone.0128570.e005.jpg" id="pone.0128570.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are sampled i.i.d. (independent and identically distributed) from the same (unknown) distribution.</p></sec><sec id="sec007"><title>Traditional evaluation measures</title><p>Model evaluation is the task of evaluating how good a model is and is crucial to choose the best model among several possible choices. In the GS literature, model evaluation has traditionally been carried out using mainly two measures: mean squared error (MSE) and Pearson correlation. MSE is defined by
<disp-formula id="pone.0128570.e006"><alternatives><graphic xlink:href="pone.0128570.e006.jpg" id="pone.0128570.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
The model is better when <inline-formula id="pone.0128570.e007"><alternatives><graphic xlink:href="pone.0128570.e007.jpg" id="pone.0128570.e007g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M7"><mml:mrow><mml:mtext mathvariant="normal">MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is lower and perfect fit is achieved when <inline-formula id="pone.0128570.e008"><alternatives><graphic xlink:href="pone.0128570.e008.jpg" id="pone.0128570.e008g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M8"><mml:mrow><mml:mtext mathvariant="normal">MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Obviously, <inline-formula id="pone.0128570.e009"><alternatives><graphic xlink:href="pone.0128570.e009.jpg" id="pone.0128570.e009g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M9"><mml:mrow><mml:mtext mathvariant="normal">MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> if <inline-formula id="pone.0128570.e010"><alternatives><graphic xlink:href="pone.0128570.e010.jpg" id="pone.0128570.e010g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>i</italic>. In other words, a model achieves zero error if it predicts perfectly all trait values. Note that this can only happen when heritability is one and all genetic variance is explained by the markers.</p><p>A more commonly used measure in the GS literature is the Pearson correlation between observed and predicted trait values. It is defined by
<disp-formula id="pone.0128570.e011"><alternatives><graphic xlink:href="pone.0128570.e011.jpg" id="pone.0128570.e011g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <inline-formula id="pone.0128570.e012"><alternatives><graphic xlink:href="pone.0128570.e012.jpg" id="pone.0128570.e012g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0128570.e013"><alternatives><graphic xlink:href="pone.0128570.e013.jpg" id="pone.0128570.e013g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. The model is better when <inline-formula id="pone.0128570.e014"><alternatives><graphic xlink:href="pone.0128570.e014.jpg" id="pone.0128570.e014g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M14"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is higher and perfect correlation is achieved when <inline-formula id="pone.0128570.e015"><alternatives><graphic xlink:href="pone.0128570.e015.jpg" id="pone.0128570.e015g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M15"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. Contrary to MSE, correlation does not require to accurately predict trait values. Indeed, it can be seen that <inline-formula id="pone.0128570.e016"><alternatives><graphic xlink:href="pone.0128570.e016.jpg" id="pone.0128570.e016g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M16"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> if there exists <italic>a</italic> &#x0003e; 0 and <italic>b</italic> such that <inline-formula id="pone.0128570.e017"><alternatives><graphic xlink:href="pone.0128570.e017.jpg" id="pone.0128570.e017g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M17"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>i</italic>. In other words, the set of points <inline-formula id="pone.0128570.e018"><alternatives><graphic xlink:href="pone.0128570.e018.jpg" id="pone.0128570.e018g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> must be collinear in order to achieve perfect correlation. Again, perfect correlation can only happen when heritability is one and all genetic variance is explained by the markers. When heritability is less than one, correlation has an upper limit which is equal to the square root of heritability. If the proportion of genetic variance that markers can explain is less than one, the upper limit decreases from the square root of heritability.</p></sec><sec id="sec008"><title>Overview of popular regression models</title><p>In this section, we briefly describe various regression models, focusing on the most popular ones in the GS literature.</p><sec id="sec009"><title>Ridge and RKHS regression</title><p>One of the first methods proposed for genomic selection was ridge regression, which is equivalent to best linear unbiased prediction (BLUP) in the context of mixed models [<xref rid="pone.0128570.ref001" ref-type="bibr">1</xref>]. Let <italic>I</italic>
<sub><italic>p</italic></sub> be the identity matrix of size <italic>p</italic> &#x000d7; <italic>p</italic>. The basic model is <bold><italic>y</italic></bold> = <italic>X</italic>
<bold><italic>&#x003b2;</italic></bold>+<italic>&#x003f5;</italic>, where <inline-formula id="pone.0128570.e019"><alternatives><graphic xlink:href="pone.0128570.e019.jpg" id="pone.0128570.e019g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M19"><mml:mrow><mml:mtext mathvariant="bold">&#x003b2;</mml:mtext><mml:mo>&#x0223c;</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0128570.e020"><alternatives><graphic xlink:href="pone.0128570.e020.jpg" id="pone.0128570.e020g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M20"><mml:mrow><mml:mi>&#x003b5;</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The solution for the marker effects can be obtained by <bold><italic>&#x003b2;</italic></bold> = (<italic>X</italic>
<sup>T</sup>
<italic>X</italic>+<italic>&#x003bb;I</italic>
<sub><italic>p</italic></sub>)<sup>&#x02212;1</sup>
<italic>X</italic>
<sup>T</sup>
<bold><italic>y</italic></bold>, where <inline-formula id="pone.0128570.e021"><alternatives><graphic xlink:href="pone.0128570.e021.jpg" id="pone.0128570.e021g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M21"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the ratio between the residual and marker variances. Predictions can be computed by <italic>h</italic>(<bold><italic>x</italic></bold>) = <bold><italic>&#x003b2;</italic></bold>
<sup>T</sup>
<bold><italic>x</italic></bold>. The representer theorem [<xref rid="pone.0128570.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0128570.ref018" ref-type="bibr">18</xref>] guarantees that <bold><italic>&#x003b2;</italic></bold> can be written as a linear combination of the data, i.e., <bold><italic>&#x003b2;</italic></bold> = <italic>X</italic>
<sup>T</sup>
<bold>&#x003b1;</bold> for some <bold>&#x003b1;</bold> &#x02208; &#x0211d;<sup><italic>n</italic></sup>. This has two important implications. The first is that we can equivalently compute <bold><italic>&#x003b2;</italic></bold> by <bold><italic>&#x003b2;</italic></bold> = <italic>X</italic>
<sup>T</sup>
<bold>&#x003b1;</bold> = <italic>X</italic>
<sup>T</sup>(<italic>XX</italic>
<sup>T</sup>+<italic>&#x003bb;I</italic>
<sub><italic>n</italic></sub>)<sup>&#x02212;1</sup>
<bold><italic>y</italic></bold>. The main difference is that we now need to invert a <italic>n</italic> &#x000d7; <italic>n</italic> matrix instead of a <italic>p</italic> &#x000d7; <italic>p</italic> one. This is advantageous in GS because we usually have <italic>n</italic> &#x0226a; <italic>p</italic>. The second implication is that ridge regression can now be &#x0201c;kernelized&#x0201d; by using the identity <inline-formula id="pone.0128570.e022"><alternatives><graphic xlink:href="pone.0128570.e022.jpg" id="pone.0128570.e022g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M22"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:mtext mathvariant="bold">&#x003b2;</mml:mtext><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>X</mml:mi><mml:mtext mathvariant="normal">T</mml:mtext></mml:msup><mml:mtext mathvariant="bold">&#x003b1;</mml:mtext><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mtext mathvariant="bold">&#x003b1;</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>K</italic> = <italic>XX</italic>
<sup><italic>T</italic></sup> and <bold>&#x003b1;</bold> = (<italic>K</italic>+<italic>&#x003bb;I</italic>
<sub><italic>n</italic></sub>)<sup>&#x02212;1</sup>
<bold><italic>y</italic></bold>. In practice, <italic>K</italic> can be replaced by any positive semidefinite kernel matrix with elements <italic>K</italic>
<sub><italic>ij</italic></sub> = &#x003ba;(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>,<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>), where &#x003ba; is the corresponding kernel function. Predictions can then be computed by <inline-formula id="pone.0128570.e023"><alternatives><graphic xlink:href="pone.0128570.e023.jpg" id="pone.0128570.e023g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M23"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003ba;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The result is known as kernel ridge regression in the machine learning literature and as RKHS regression in the GS literature. The elements <italic>K</italic>
<sub><italic>ij</italic></sub> correspond to inner products in a high-dimensional (possibly infinite) space called reproducing kernel Hilbert space (RKHS). This allows to model non-linear relationships between <italic>X</italic> and <bold><italic>y</italic></bold>. RKHS regression is equivalent to ridge regression when using a linear kernel.</p></sec><sec id="sec010"><title>Bayesian lasso (BL)</title><p>Bayesian lasso [<xref rid="pone.0128570.ref011" ref-type="bibr">11</xref>] is the Bayesian counterpart of the lasso [<xref rid="pone.0128570.ref019" ref-type="bibr">19</xref>]. Following the parameterization of [<xref rid="pone.0128570.ref020" ref-type="bibr">20</xref>], the effect of marker <italic>j</italic>, <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub>, was assumed to follow a hierarchical prior distribution,
<disp-formula id="pone.0128570.e024"><alternatives><graphic xlink:href="pone.0128570.e024.jpg" id="pone.0128570.e024g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>Normal</italic> and <italic>InvGamma</italic> indicate the normal and inverse gamma distributions, respectively, <inline-formula id="pone.0128570.e025"><alternatives><graphic xlink:href="pone.0128570.e025.jpg" id="pone.0128570.e025g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M25"><mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> determines the shrinkage magnitude for <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub>, <inline-formula id="pone.0128570.e026"><alternatives><graphic xlink:href="pone.0128570.e026.jpg" id="pone.0128570.e026g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M26"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the residual variance and <inline-formula id="pone.0128570.e027"><alternatives><graphic xlink:href="pone.0128570.e027.jpg" id="pone.0128570.e027g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M27"><mml:mrow><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is a hyper-parameter that defines the distribution of <inline-formula id="pone.0128570.e028"><alternatives><graphic xlink:href="pone.0128570.e028.jpg" id="pone.0128570.e028g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M28"><mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. We modified the method of [<xref rid="pone.0128570.ref020" ref-type="bibr">20</xref>] such that marker effects were conditional on the residual variance (precision), as in [<xref rid="pone.0128570.ref011" ref-type="bibr">11</xref>]. The prior distribution of <inline-formula id="pone.0128570.e029"><alternatives><graphic xlink:href="pone.0128570.e029.jpg" id="pone.0128570.e029g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M29"><mml:mrow><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the gamma distribution <italic>Gamma</italic>(<italic>&#x003d5;</italic>, <italic>&#x003c9;</italic>), where <italic>&#x003d5;</italic> and <italic>&#x003c9;</italic> are the shape and rate parameters, respectively.</p></sec><sec id="sec011"><title>Extended Bayesian lasso (EBL)</title><p>In the EBL [<xref rid="pone.0128570.ref021" ref-type="bibr">21</xref>], <inline-formula id="pone.0128570.e030"><alternatives><graphic xlink:href="pone.0128570.e030.jpg" id="pone.0128570.e030g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M30"><mml:mrow><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is replaced with <inline-formula id="pone.0128570.e031"><alternatives><graphic xlink:href="pone.0128570.e031.jpg" id="pone.0128570.e031g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M31"><mml:mrow><mml:msup><mml:mi>&#x003b4;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>&#x003b7;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>&#x003b4;</italic>
<sup>2</sup> is a global shrinkage factor and <inline-formula id="pone.0128570.e032"><alternatives><graphic xlink:href="pone.0128570.e032.jpg" id="pone.0128570.e032g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M32"><mml:mrow><mml:msubsup><mml:mi>&#x003b7;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is a shrinkage factor for marker <italic>j</italic>. The priors used are <italic>Gamma</italic>(<italic>&#x003d5;</italic>, <italic>&#x003c9;</italic>) for <italic>&#x003b4;</italic>
<sup>2</sup> and <italic>Gamma</italic>(<italic>&#x003c8;</italic>, <italic>&#x003b8;</italic>) for <inline-formula id="pone.0128570.e033"><alternatives><graphic xlink:href="pone.0128570.e033.jpg" id="pone.0128570.e033g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M33"><mml:mrow><mml:msubsup><mml:mi>&#x003b7;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p></sec><sec id="sec012"><title>Weighted Bayesian shrinkage regression (wBSR)</title><p>wBSR [<xref rid="pone.0128570.ref013" ref-type="bibr">13</xref>] uses the indicator variable <italic>&#x003b3;</italic>
<sub><italic>j</italic></sub> to determine whether the marker effect <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub> is included in the regression model (<italic>&#x003b3;</italic>
<sub><italic>j</italic></sub> = 1) or not (<italic>&#x003b3;</italic>
<sub><italic>j</italic></sub> = 0). A prior Bernoulli distribution, <italic>Bernoulli</italic>(<italic>&#x003b3;</italic>
<sub><italic>j</italic></sub>&#x02223;<italic>&#x003c0;</italic>), is assumed for <italic>&#x003b3;</italic>
<sub><italic>j</italic></sub>. The marker effect <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub> is assumed to follow a hierarchical prior distribution,
<disp-formula id="pone.0128570.e034"><alternatives><graphic xlink:href="pone.0128570.e034.jpg" id="pone.0128570.e034g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo>,</mml:mo></mml:mrow><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>InvChi2</italic> indicates a scaled inverse chi-squared distribution, <italic>&#x003bd;</italic> is the degree of freedom and <italic>S</italic>
<sup>2</sup> is the scaling parameter.</p></sec><sec id="sec013"><title>BayesC</title><p>In BayesC [<xref rid="pone.0128570.ref022" ref-type="bibr">22</xref>], <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub> is assumed to follow a spike and slab prior distribution,
<disp-formula id="pone.0128570.e035"><alternatives><graphic xlink:href="pone.0128570.e035.jpg" id="pone.0128570.e035g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>&#x003c1;</italic>
<sub><italic>j</italic></sub> is an indicator variable. The prior distributions used for <italic>&#x003c1;</italic>
<sub><italic>j</italic></sub> and <italic>&#x003c3;</italic>
<sup>2</sup> are <italic>Bernoulli</italic>(<italic>&#x003c1;</italic>
<sub><italic>j</italic></sub>&#x02223;<italic>&#x003c0;</italic>) and <italic>InvChi</italic>2(<italic>&#x003c3;</italic>
<sup>2</sup>&#x02223;<italic>&#x003bd;</italic>, <italic>S</italic>
<sup>2</sup>), respectively.</p></sec><sec id="sec014"><title>Stochastic search variable selection (SSVS)</title><p>SSVS [<xref rid="pone.0128570.ref023" ref-type="bibr">23</xref>] assumes the following prior distribution for <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub>,
<disp-formula id="pone.0128570.e036"><alternatives><graphic xlink:href="pone.0128570.e036.jpg" id="pone.0128570.e036g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>c</italic> &#x0003c; 1 determines the relative magnitude of the variances of the two normal distributions.</p></sec><sec id="sec015"><title>Bayesian mixture regression model (MIX)</title><p>For the prior distribution of <italic>&#x003b2;</italic>
<sub><italic>j</italic></sub>, MIX [<xref rid="pone.0128570.ref024" ref-type="bibr">24</xref>] assumes a mixture of two normal distributions with variances independent of one another:
<disp-formula id="pone.0128570.e037"><alternatives><graphic xlink:href="pone.0128570.e037.jpg" id="pone.0128570.e037g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0223c;</mml:mo><mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003c1;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo/><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
In [<xref rid="pone.0128570.ref024" ref-type="bibr">24</xref>], the prior distributions of <inline-formula id="pone.0128570.e038"><alternatives><graphic xlink:href="pone.0128570.e038.jpg" id="pone.0128570.e038g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M38"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0128570.e039"><alternatives><graphic xlink:href="pone.0128570.e039.jpg" id="pone.0128570.e039g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M39"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> were <inline-formula id="pone.0128570.e040"><alternatives><graphic xlink:href="pone.0128570.e040.jpg" id="pone.0128570.e040g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M40"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0128570.e041"><alternatives><graphic xlink:href="pone.0128570.e041.jpg" id="pone.0128570.e041g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M41"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. We modified the prior of <inline-formula id="pone.0128570.e042"><alternatives><graphic xlink:href="pone.0128570.e042.jpg" id="pone.0128570.e042g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M42"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to <inline-formula id="pone.0128570.e043"><alternatives><graphic xlink:href="pone.0128570.e043.jpg" id="pone.0128570.e043g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M43"><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>&#x003bd;</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:msup><mml:mi>S</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> so as to encourage clustering of markers according to the magnitude of their effects.</p></sec><sec id="sec016"><title>Random forests (RF)</title><p>RF [<xref rid="pone.0128570.ref025" ref-type="bibr">25</xref>] are an ensemble algorithm based on randomized regression trees. In RF, each tree is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. The final prediction is computed by averaging the predictions of all trees in the forest. This procedure is known to improve the bias-variance trade-off of regression trees and leads to highly accurate predictions. To further reduce overfitting, two heuristics are typically applied in RF. The first heuristic consists, when splitting a node during the construction of a tree, in selecting the best split from a random subset of the features (&#x0201c;<monospace>max_features</monospace>&#x0201d;). This both improves accuracy and reduces training time. The second heuristic consists in limiting the maximum depth of the regression trees (&#x0201c;<monospace>max_depth</monospace>&#x0201d;). This ensures that trees are not too complicated. Although careful tuning of these two parameters can improve accuracy, we find that RF are pretty robust to their choice.</p></sec><sec id="sec017"><title>Gradient boosted regression trees (GBRT)</title><p>In gradient boosting [<xref rid="pone.0128570.ref026" ref-type="bibr">26</xref>], an ensemble of regression models is built in a stage-wise fashion so as to minimize a differentiable loss function. GBRT refers to gradient boosting when the models are regression trees. GBRT starts with a base model <italic>h</italic>
<sub>0</sub>. For regression with squared loss, a common choice is the base model which always outputs the training set&#x02019;s target mean irrespective of <bold><italic>x</italic></bold>: <inline-formula id="pone.0128570.e044"><alternatives><graphic xlink:href="pone.0128570.e044.jpg" id="pone.0128570.e044g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M44"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Subsequently, GBRT incrementally adds new trees <italic>h</italic>
<sub>1</sub>, &#x02026;, <italic>h</italic>
<sub><italic>M</italic></sub> to obtain an ensemble <inline-formula id="pone.0128570.e045"><alternatives><graphic xlink:href="pone.0128570.e045.jpg" id="pone.0128570.e045g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M45"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. For the squared loss, the tree <italic>h</italic>
<sub><italic>s</italic></sub> at stage <italic>s</italic> is fitted against the residuals of the ensemble so far <italic>e</italic>
<sub>1</sub>, &#x02026;, <italic>e</italic>
<sub><italic>n</italic></sub>, where <inline-formula id="pone.0128570.e046"><alternatives><graphic xlink:href="pone.0128570.e046.jpg" id="pone.0128570.e046g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M46"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. For other differentiable loss functions, residuals are replaced with the negative gradient, which [<xref rid="pone.0128570.ref026" ref-type="bibr">26</xref>] calls &#x0201c;pseudo-responses&#x0201d;. At each stage <italic>s</italic>, GBRT finds <italic>&#x003b1;</italic>
<sub><italic>s</italic></sub> by line search and multiply the result by a small learning rate, typically between 10<sup>&#x02212;3</sup> and 1, to avoid overfitting. To further reduce overfitting, the same heuristics as RF can be applied (&#x0201c;<monospace>max_features</monospace>&#x0201d; and &#x0201c;<monospace>max_depth</monospace>&#x0201d;). Although past GS works did not consider GBRT, we include it in our comparison since it achieved among the leading results in the Yahoo! learning to rank challenge [<xref rid="pone.0128570.ref027" ref-type="bibr">27</xref>].</p></sec></sec></sec><sec id="sec018"><title>Ranking-based genomic selection</title><sec id="sec019"><title>General approach</title><p>Similarly to existing regression-based approaches, our approach is broken down into a model estimation phase and a candidate selection phase. The main difference is that, in our approach, we do not impose that the model <italic>h</italic> satisfy <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>) &#x02248; <italic>y</italic>
<sub><italic>i</italic></sub>. Instead, in our approach, <italic>h</italic> is a <bold>scoring</bold> function: it assigns a score to each candidate. The scores are then used to determine a <bold>ranking</bold> of the candidates. In this ranking framework, GS can be summarized by the following two phases:
<list list-type="order"><list-item><p>
<bold>Model estimation.</bold> Using the reference population, <bold>estimate</bold> a scoring function <italic>h</italic> which can be used for ranking. Intuitively, a perfect scoring function would satisfy <italic>h</italic>(<bold><italic>x</italic></bold>
<sub>(1)</sub>) &#x02265; <italic>h</italic>(<bold><italic>x</italic></bold>
<sub>(2)</sub>) &#x02265; &#x02026; &#x02265; <italic>h</italic>(<bold><italic>x</italic></bold>
<sub>(<italic>n</italic>)</sub>) if <italic>y</italic>
<sub>(1)</sub> &#x02265; <italic>y</italic>
<sub>(2)</sub> &#x02265; &#x02026; &#x02265; <italic>y</italic>
<sub>(<italic>n</italic>)</sub>.</p></list-item><list-item><p>
<bold>Candidate selection.</bold> Using <italic>h</italic>, <bold>rank</bold>
<italic>m</italic> candidates <inline-formula id="pone.0128570.e047"><alternatives><graphic xlink:href="pone.0128570.e047.jpg" id="pone.0128570.e047g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M47"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> by decreasing scores. We denote the ranked candidates by <inline-formula id="pone.0128570.e048"><alternatives><graphic xlink:href="pone.0128570.e048.jpg" id="pone.0128570.e048g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M48"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:mo>&#x02026;</mml:mo><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> &#x0227d;<sub><italic>h</italic></sub>
<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub> means <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>) &#x02265; <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>). Finally, <bold>select</bold> the top <italic>k</italic> candidates <inline-formula id="pone.0128570.e049"><alternatives><graphic xlink:href="pone.0128570.e049.jpg" id="pone.0128570.e049g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M49"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:mo>&#x02026;</mml:mo><mml:msub><mml:mo>&#x0227d;</mml:mo><mml:mi>h</mml:mi></mml:msub><mml:msub><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>&#x0203e;</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> for further field testing. Typically, the number of selected candidates <italic>k</italic> is chosen much smaller than the total number of candidates <italic>m</italic>, i.e., <italic>k</italic> &#x0226a; <italic>m</italic>.</p></list-item></list>
</p><p>Our ranking-based formulation naturally captures a prerequisite objective in selective breeding: accurate selection of individuals with high breeding value. In this section, since we do not impose that <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>) &#x02248; <italic>y</italic>
<sub><italic>i</italic></sub>, <inline-formula id="pone.0128570.e050"><alternatives><graphic xlink:href="pone.0128570.e050.jpg" id="pone.0128570.e050g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M50"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> does not necessarily represent predicted trait values. Instead, <inline-formula id="pone.0128570.e051"><alternatives><graphic xlink:href="pone.0128570.e051.jpg" id="pone.0128570.e051g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M51"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the <italic>n</italic>-dimensional vector which gathers predicted scores <inline-formula id="pone.0128570.e052"><alternatives><graphic xlink:href="pone.0128570.e052.jpg" id="pone.0128570.e052g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M52"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0128570.e053"><alternatives><graphic xlink:href="pone.0128570.e053.jpg" id="pone.0128570.e053g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M53"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, from which individuals can be sorted in decreasing order.</p></sec><sec id="sec020"><title>Evaluation measures for global ranking</title><p>As we explained previously, the Pearson correlation does not require to accurately predict trait values. It only requires the set of points <inline-formula id="pone.0128570.e054"><alternatives><graphic xlink:href="pone.0128570.e054.jpg" id="pone.0128570.e054g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M54"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to be collinear. In that sense, the Pearson correlation can be seen as a ranking measure. However, the collinearity requirement may sometimes be too strict. To illustrate the problem, consider the case when <bold><italic>y</italic></bold> = [3.5, 2.8, 1.2] and <inline-formula id="pone.0128570.e055"><alternatives><graphic xlink:href="pone.0128570.e055.jpg" id="pone.0128570.e055g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M55"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>10</mml:mn><mml:mo>.</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In this example, the model achieves perfect ranking since <italic>y</italic>
<sub>1</sub> &#x02265; <italic>y</italic>
<sub>2</sub> &#x02265; <italic>y</italic>
<sub>3</sub> and <inline-formula id="pone.0128570.e056"><alternatives><graphic xlink:href="pone.0128570.e056.jpg" id="pone.0128570.e056g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M56"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. However, because the true and predicted trait values are not perfectly collinear, the correlation is only equal to <inline-formula id="pone.0128570.e057"><alternatives><graphic xlink:href="pone.0128570.e057.jpg" id="pone.0128570.e057g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M57"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>92</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. This example shows that it is not necessary to achieve perfect correlation to achieve perfect ranking. In this section, we present two related measures for global ranking evaluation that do not assume collinearity: pairwise accuracy and Kendall&#x02019;s <italic>&#x003c4;</italic>.</p><p>Given the reference trait values <bold><italic>y</italic></bold>, we define the preference set as <italic>P</italic>(<bold><italic>y</italic></bold>) = {(<italic>i</italic>, <italic>j</italic>):<italic><italic>y</italic></italic>
<sub><italic>i</italic></sub> &#x0003e; <italic>y</italic>
<sub><italic>j</italic></sub>}. Intuitively, if (<italic>i</italic>, <italic>j</italic>) &#x02208; <italic>P</italic>(<bold><italic>y</italic></bold>), then <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> is preferred to <bold><italic>x</italic></bold>
<sub><italic>j</italic></sub> (e.g., <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> has higher grain yield than <bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>). Given the predicted scores <inline-formula id="pone.0128570.e058"><alternatives><graphic xlink:href="pone.0128570.e058.jpg" id="pone.0128570.e058g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M58"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, we define the set of concordant pairs as <inline-formula id="pone.0128570.e059"><alternatives><graphic xlink:href="pone.0128570.e059.jpg" id="pone.0128570.e059g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M59"><mml:mrow><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0003e;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and the set of discordant pairs as <inline-formula id="pone.0128570.e060"><alternatives><graphic xlink:href="pone.0128570.e060.jpg" id="pone.0128570.e060g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M60"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Pairs in the set <inline-formula id="pone.0128570.e061"><alternatives><graphic xlink:href="pone.0128570.e061.jpg" id="pone.0128570.e061g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M61"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> are neither concordant nor discordant.</p><p>Pairwise accuracy (c.f., e.g., [<xref rid="pone.0128570.ref028" ref-type="bibr">28</xref>]) is simply defined as the proportion of concordant pairs:
<disp-formula id="pone.0128570.e062"><alternatives><graphic xlink:href="pone.0128570.e062.jpg" id="pone.0128570.e062g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M62"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext mathvariant="normal">pairwise_accuracy</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where &#x02223;<italic>S</italic>&#x02223; is the cardinality of the set <italic>S</italic>. Pairwise accuracy is 0 when not a single pair was concordant and is 1 when all pairs were concordant. For binary trait values, pairwise accuracy is exactly equivalent to the area under the ROC curve (AUC) and is closely related to the Mann-Whitney-Wilcoxon statistic [<xref rid="pone.0128570.ref029" ref-type="bibr">29</xref>]. Pairwise ranking algorithms such as RankSVM [<xref rid="pone.0128570.ref030" ref-type="bibr">30</xref>], RankBoost [<xref rid="pone.0128570.ref031" ref-type="bibr">31</xref>] and RankNet [<xref rid="pone.0128570.ref032" ref-type="bibr">32</xref>] maximize an upper-bound on pairwise accuracy.</p><p>Another commonly used measure is Kendall&#x02019;s <italic>&#x003c4;</italic>[<xref rid="pone.0128570.ref033" ref-type="bibr">33</xref>]:
<disp-formula id="pone.0128570.e063"><alternatives><graphic xlink:href="pone.0128570.e063.jpg" id="pone.0128570.e063g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M63"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mo>-</mml:mo><mml:mo>|</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Intuitively, Kendall&#x02019;s <italic>&#x003c4;</italic> is the difference between the ratio of concordant pairs and the ratio of discordant pairs. Kendall&#x02019;s <italic>&#x003c4;</italic> is always between &#x02212;1 and 1.</p><p>Assuming <inline-formula id="pone.0128570.e064"><alternatives><graphic xlink:href="pone.0128570.e064.jpg" id="pone.0128570.e064g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M64"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is an empty set (which is likely to be the case, since <inline-formula id="pone.0128570.e065"><alternatives><graphic xlink:href="pone.0128570.e065.jpg" id="pone.0128570.e065g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M65"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is a continuous vector), Kendall&#x02019;s <italic>&#x003c4;</italic> and pairwise accuracy are directly related by the following formula:
<disp-formula id="pone.0128570.e066"><alternatives><graphic xlink:href="pone.0128570.e066.jpg" id="pone.0128570.e066g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M66"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo> &#x000d7; </mml:mo><mml:mtext mathvariant="normal">pairwise_accuracy</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Therefore, algorithms which are designed to maximize pairwise accuracy will also maximize Kendall&#x02019;s <italic>&#x003c4;</italic>.</p></sec><sec id="sec021"><title>Evaluation measures for top-<italic>k</italic> ranking</title><p>One issue with pairwise accuracy and Kendall&#x02019;s <italic>&#x003c4;</italic> is that they treat all pairs equally. Intuitively, a good ranking evaluation measure should fulfill two requirements. First, it should reward more strongly assigning a high rank to individuals with high breeding value. Second, it should focus on the top <italic>k</italic> individuals in the ranking. Oftentimes, it does not matter if a model cannot correctly order individuals with low breeding value. Instead, it is sufficient to rank as many individuals with high breeding value as possible at the top. In this section, we introduce two measures that fulfill the above two requirements: discounted cumulative gain (DCG) [<xref rid="pone.0128570.ref016" ref-type="bibr">16</xref>] and its normalized version (NDCG). In the information retrieval (IR) literature, NDCG has been popularly used to measure the ability of search engines to retrieve highly relevant documents in the top search results. In this paper, we use NDCG to measure the ability of GS models to select the top <italic>k</italic> individuals with highest breeding value.</p><p>To introduce discounted cumulative gain (DCG), we first note that any predicted score vector <inline-formula id="pone.0128570.e067"><alternatives><graphic xlink:href="pone.0128570.e067.jpg" id="pone.0128570.e067g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M67"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> induces a permutation <bold><italic>&#x003c0;</italic></bold> = [<italic>&#x003c0;</italic>
<sub>1</sub>, &#x02026;, <italic>&#x003c0;</italic>
<sub><italic>n</italic></sub>] of [1, &#x02026;, <italic>n</italic>] such that the scores are sorted in decreasing order: <inline-formula id="pone.0128570.e068"><alternatives><graphic xlink:href="pone.0128570.e068.jpg" id="pone.0128570.e068g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M68"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>&#x02265;</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Given the reference trait values <bold>y</bold> = [<italic>y</italic>
<sub>1</sub>, &#x02026;, <italic>y</italic>
<sub><italic>n</italic></sub>] and any such permutation <bold><italic>&#x003c0;</italic></bold>, the DCG at position <italic>k</italic> (we assume <italic>k</italic> &#x02264; <italic>n</italic>) is defined by
<disp-formula id="pone.0128570.e069"><alternatives><graphic xlink:href="pone.0128570.e069.jpg" id="pone.0128570.e069g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M69"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>DCG@</mml:mtext><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Here, <italic>g</italic>(<italic>y</italic>) is a monotonically increasing gain function and <italic>d</italic>(<italic>i</italic>) is a monotonically decreasing discount function. Common choices for the gain function are <italic>g</italic>(<italic>y</italic>) = <italic>y</italic> (linear gains) and <italic>g</italic>(<italic>y</italic>) = 2<sup><italic>y</italic></sup>&#x02212;1 (exponential gains). For the discount function, a common choice is <inline-formula id="pone.0128570.e070"><alternatives><graphic xlink:href="pone.0128570.e070.jpg" id="pone.0128570.e070g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M70"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. Intuitively, a model obtains the highest possible DCG@<italic>k</italic> when the order of the predicted scores <inline-formula id="pone.0128570.e071"><alternatives><graphic xlink:href="pone.0128570.e071.jpg" id="pone.0128570.e071g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M71"><mml:mrow><mml:mover accent="true"><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> agrees with the order of the true observed traits <bold><italic>y</italic></bold>.</p><p>DCG can be difficult to interpret because its values are unbounded. In practice, the normalized DCG (NDCG) is often used instead. If we define by <italic>&#x003c0;</italic>(<bold><italic>y</italic></bold>) = [<italic>&#x003c0;</italic>(<bold><italic>y</italic></bold>)<sub>1</sub>, &#x02026;, <italic>&#x003c0;</italic>(<bold><italic>y</italic></bold>)<sub><italic>n</italic></sub>] a permutation of [1, &#x02026;, <italic>n</italic>] for sorting <bold><italic>y</italic></bold> in decreasing order, i.e., <italic>y</italic>
<sub><italic>&#x003c0;</italic>(<bold><italic>y</italic></bold>)<sub>1</sub></sub> &#x02265; &#x02026; &#x02265; <italic>y</italic>
<sub><italic>&#x003c0;</italic>(<bold><italic>y</italic></bold>)<sub><italic>n</italic></sub></sub>, then NDCG at position <italic>k</italic> is defined by
<disp-formula id="pone.0128570.e072"><alternatives><graphic xlink:href="pone.0128570.e072.jpg" id="pone.0128570.e072g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M72"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>NDCG@</mml:mtext><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>DCG@</mml:mtext><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mtext>DCG@</mml:mtext><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Intuitively, NDCG is simply the ratio between the DCG score of the predicted ranking and the DCG score of the ideal ranking. NDCG is easier to interpret than DCG because its values are always between 0 and 1, assuming <inline-formula id="pone.0128570.e073"><alternatives><graphic xlink:href="pone.0128570.e073.jpg" id="pone.0128570.e073g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M73"><mml:mrow><mml:mtext mathvariant="bold-italic">y</mml:mtext><mml:mo>&#x02208;</mml:mo><mml:msubsup><mml:mo>&#x0211d;</mml:mo><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p><p>In our experiments, we also report results of Mean NDCG@<italic>K</italic>, which is simply the mean of NDCG scores from <italic>k</italic> = 1 to <italic>k</italic> = <italic>K</italic>:
<disp-formula id="pone.0128570.e074"><alternatives><graphic xlink:href="pone.0128570.e074.jpg" id="pone.0128570.e074g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M74"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mtext>Mean</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>NDCG@</mml:mtext><mml:mi>K</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mtext>NDCG@</mml:mtext><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
</p><p>We discuss the choices of the position <italic>k</italic>, gain function <italic>g</italic>(<italic>y</italic>) and discount function <italic>d</italic>(<italic>i</italic>) in the &#x0201c;Discussion&#x0201d; section.</p></sec><sec id="sec022"><title>Background on &#x0201c;learning to rank&#x0201d;</title><p>Estimating a ranking model, commonly known as &#x0201c;learning to rank&#x0201d;, has attracted a great deal of research in the machine learning community. Intuitively, when formulating GS as a ranking problem, we should estimate a model so as to directly maximize a ranking accuracy measure of interest, such as NDCG. Unfortunately, this turns out to be a non-convex problem that can be NP-hard [<xref rid="pone.0128570.ref034" ref-type="bibr">34</xref>]. To solve this problem, learning to rank approaches replace the true loss function by an easier to optimize one called surrogate loss function. Learning to rank approaches are typically divided into three categories depending on the type of surrogate loss function used.</p><p>
<bold>Pointwise</bold> approaches involve a surrogate loss function on individual samples <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>. They typically reduce ranking to either regression, classification or ordinal regression/classification. It was shown that DCG errors are bounded by regression [<xref rid="pone.0128570.ref034" ref-type="bibr">34</xref>] and classification [<xref rid="pone.0128570.ref035" ref-type="bibr">35</xref>] errors.</p><p>
<bold>Pairwise</bold> approaches involve a surrogate loss function on pairs of samples (<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>,<bold>x</bold>
<sub><italic>j</italic></sub>). Their main idea is that if <italic>y</italic>
<sub><italic>i</italic></sub> &#x0003e; <italic>y</italic>
<sub><italic>j</italic></sub>, then the model <italic>h</italic> does not need to predict <italic>y</italic>
<sub><italic>i</italic></sub> and <italic>y</italic>
<sub><italic>j</italic></sub> accurately: it only needs to respect the relative order <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>) &#x0003e; <italic>h</italic>(<bold>x</bold>
<sub><italic>j</italic></sub>). RankSVM [<xref rid="pone.0128570.ref030" ref-type="bibr">30</xref>], RankBoost [<xref rid="pone.0128570.ref031" ref-type="bibr">31</xref>] and RankNet [<xref rid="pone.0128570.ref032" ref-type="bibr">32</xref>] are based on pairwise versions of the hinge, exponential and logistic surrogate loss functions, respectively.</p><p>
<bold>Listwise</bold> approaches involve a surrogate loss function or algorithm based on a list of samples. Some listwise methods such as LambdaMART [<xref rid="pone.0128570.ref036" ref-type="bibr">36</xref>] can optimize top-<italic>k</italic> ranking accuracy directly.</p><p>For more details on &#x0201c;learning to rank&#x0201d;, we refer the reader to [<xref rid="pone.0128570.ref037" ref-type="bibr">37</xref>, <xref rid="pone.0128570.ref038" ref-type="bibr">38</xref>].</p></sec><sec id="sec023"><title>Overview of ranking models</title><p>Our ranking-based formulation allows us to employ machine learning methods for ranking which had never been considered in the GS literature before. For our experiments, we chose three representative methods of the pointwise, pairwise and listwise categories.</p><sec id="sec024"><title>McRank (pointwise)</title><p>McRank [<xref rid="pone.0128570.ref035" ref-type="bibr">35</xref>] is a method for indirectly optimizing NDCG through multiple classification. This is motivated by the fact that classification can be an easier task than regression. Suppose that the traits can only take on a finite number <italic>B</italic> of values, i.e., &#x1d4e8; = {<italic>b</italic>
<sub>1</sub>, <italic>b</italic>
<sub>2</sub>, &#x02026;, <italic>b</italic>
<sub><italic>B</italic></sub>}, where <italic>b</italic>
<sub><italic>r</italic></sub> &#x02208; &#x0211d;. If that is not the case, we can always discretize the trait values (of the training set only), as explained in our experiments. The main idea of McRank is to rank candidates according to their expected trait value, which can be computed by <inline-formula id="pone.0128570.e075"><alternatives><graphic xlink:href="pone.0128570.e075.jpg" id="pone.0128570.e075g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M75"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. McRank exists in two variants: multiclass and ordinal McRank. The variants differ in how they compute the <italic>Pr</italic>(<italic>y</italic> = <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) probabilities.</p><p>
<bold>Multiclass McRank</bold> models the <italic>Pr</italic>(<italic>y</italic> = <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) class probabilities using a probabilistic classifier. Any probabilistic classifier (e.g., logistic regression) can in theory be used. Although the original McRank paper used gradient boosting as classifier, in this paper, we used random forests, since they worked better in our experiments. Unfortunately, multiclass McRank completely ignores the natural ordering <italic>b</italic>
<sub>1</sub> &#x02264; <italic>b</italic>
<sub>2</sub> &#x02264; &#x02026; &#x02264; <italic>b</italic>
<sub><italic>B</italic></sub>.</p><p>
<bold>Ordinal McRank</bold> addresses this problem as follows. Notice that <italic>Pr</italic>(<italic>y</italic> = <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) = <italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>)&#x02212;<italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic>&#x02212;1</sub>&#x02223;<bold><italic>x</italic></bold>). In other words, we can model class probabilities <italic>Pr</italic>(<italic>y</italic> = <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) using cumulative probabilities <italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) and <italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic>&#x02212;1</sub>&#x02223;<bold><italic>x</italic></bold>). The advantage is that this takes into account the natural ordering <italic>b</italic>
<sub>1</sub> &#x02264; <italic>b</italic>
<sub>2</sub> &#x02264; &#x02026; &#x02264; <italic>b</italic>
<sub><italic>B</italic></sub>. Cumulative probabilities <italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>) can easily be modeled as follows. First, we partition the training data into two groups {<italic>y</italic>
<sub><italic>i</italic></sub> &#x02264; <italic>b</italic>
<sub><italic>r</italic></sub>} (positive class) and {<italic>y</italic>
<sub><italic>i</italic></sub> &#x02265; <italic>b</italic>
<sub><italic>r</italic>+1</sub>} (negative class). Using this partition, we can then train a two-class probabilistic classifier. The probability of the positive class gives <italic>Pr</italic>(<italic>y</italic> &#x02264; <italic>b</italic>
<sub><italic>r</italic></sub>&#x02223;<bold><italic>x</italic></bold>). Again, although any probabilistic classifier can be used, we used random forests since they performed best in our experiments.</p></sec><sec id="sec025"><title>RankSVM (pairwise)</title><p>Recall that we previously defined the preference set as <italic>P</italic>(<bold><italic>y</italic></bold>) = {(<italic>i</italic>, <italic>j</italic>):<italic>y</italic>
<sub><italic>i</italic></sub> &#x0003e; <italic>y</italic>
<sub><italic>j</italic></sub>}. The main idea of RankSVM [<xref rid="pone.0128570.ref030" ref-type="bibr">30</xref>] is to use the support vector machine (SVM) framework in order to find a model <italic>h</italic>(<bold><italic>x</italic></bold>) such that <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>) &#x0003e; <italic>h</italic>(<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>) for all (<italic>i</italic>, <italic>j</italic>) &#x02208; <italic>P</italic>(<bold><italic>y</italic></bold>). In this paper, we consider the kernelized version of RankSVM [<xref rid="pone.0128570.ref039" ref-type="bibr">39</xref>], which minimizes the objective
<disp-formula id="pone.0128570.e076"><alternatives><graphic xlink:href="pone.0128570.e076.jpg" id="pone.0128570.e076g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M76"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mtext>minimize</mml:mtext><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mo>&#x0211d;</mml:mo><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="1.em"/><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02261;</mml:mo><mml:mfrac><mml:mi>&#x003bb;</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mi>K</mml:mi><mml:mi>&#x003b1;</mml:mi><mml:mo>+</mml:mo><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>&#x003bb;</italic> is a regularization parameter, <italic>K</italic> &#x02208; &#x0211d;<sup><italic>n</italic> &#x000d7; <italic>n</italic></sup> is a kernel matrix with elements <italic>K</italic>
<sub><italic>ij</italic></sub> = <italic>&#x003ba;</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>,<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>), <italic>K</italic>
<sub><italic>i</italic></sub> &#x02208; &#x0211d;<sup><italic>n</italic></sup> is the <italic>i</italic>
<sup>th</sup> column of <italic>K</italic> and <italic>r</italic> &#x02208; {1, 2}. Once <bold>&#x003b1;</bold> has been obtained, the model <inline-formula id="pone.0128570.e077"><alternatives><graphic xlink:href="pone.0128570.e077.jpg" id="pone.0128570.e077g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M77"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>&#x003ba;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold-italic">x</mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be used to sort candidates in decreasing order. Using the kernelized version of RankSVM has the same advantages as RKHS regression. First, the model is non-linear if we use a non-linear kernel. This allows to model non-linear relationships. Second, the optimization problem is <italic>n</italic>-dimensional instead of <italic>p</italic>-dimensional, which is advantageous in GS. When <italic>r</italic> = 2, the RankSVM objective is differentiable and can thus be solved by gradient methods such as the conjugate gradient method or limited-memory BFGS (L-BFGS) [<xref rid="pone.0128570.ref040" ref-type="bibr">40</xref>]. The gradient expression necessary to run these methods is given by
<disp-formula id="pone.0128570.e078"><alternatives><graphic xlink:href="pone.0128570.e078.jpg" id="pone.0128570.e078g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M78"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x02207;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mi>K</mml:mi><mml:mi>&#x003b1;</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02208;</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>&#x003b1;</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:msub><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
The global solution, which is unique, is guaranteed to be found, since RankSVM has a strictly convex objective [<xref rid="pone.0128570.ref039" ref-type="bibr">39</xref>]. When <italic>r</italic> = 1, the conjugate gradient and L-BFGS methods cannot be used, since the RankSVM objective is not differentiable everywhere. Instead, it is possible to solve the objective using the subgradient method.</p></sec><sec id="sec026"><title>LambdaMART (listwise)</title><p>LambdaMART [<xref rid="pone.0128570.ref036" ref-type="bibr">36</xref>, <xref rid="pone.0128570.ref041" ref-type="bibr">41</xref>] is a method which builds upon GBRT (c.f. overview of regression models) to optimize NDCG@k. It achieved the leading results in the Yahoo! learning to rank challenge [<xref rid="pone.0128570.ref027" ref-type="bibr">27</xref>]. As explained previously, GBRT incrementally builds an ensemble of <italic>M</italic> regression trees <inline-formula id="pone.0128570.e079"><alternatives><graphic xlink:href="pone.0128570.e079.jpg" id="pone.0128570.e079g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M79"><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mtext mathvariant="bold">x</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> by adding on each stage a regression tree <italic>h</italic>
<sub><italic>s</italic></sub> fitted against &#x0201c;pseudo-responses&#x0201d;, the negative gradient of the objective function. To deal with the discontinuity of the NDCG objective, LambdaMART uses an approximation of the negative gradient called <italic>&#x003bb;</italic>-gradient. GBRT is also known as MART (multiple additive regression trees), hence the name LambdaMART. Let us define the following expressions on stage <italic>s</italic>:
<disp-formula id="pone.0128570.e080"><alternatives><graphic xlink:href="pone.0128570.e080.jpg" id="pone.0128570.e080g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M80"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>sign</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mtext>NDCG</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mspace width="52pt"/><mml:mi>&#x003bb;</mml:mi><mml:mtext>-gradient</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>for</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>the</mml:mtext><mml:mspace width="4.pt"/><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="4.pt"/><mml:mtext>pair</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>&#x02202;</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mo form="prefix">exp</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mspace width="52pt"/><mml:mtext>cross-entropy</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>derivative</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mspace width="52pt"/><mml:mtext>prediction</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>score</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>difference</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>&#x00394;</mml:mo><mml:msub><mml:mtext>NDCG</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>NDCG</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>gained</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>by</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>swapping</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="4.pt"/><mml:mtext>and</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>&#x003b1;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="52pt"/><mml:mtext>prediction</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>score</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>up</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:mtext>stage</mml:mtext><mml:mspace width="4.pt"/><mml:mi>s</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
Then, the <italic>&#x003bb;</italic>-gradient on stage <italic>s</italic> is an <italic>n</italic>-dimensional vector whose elements are defined by <inline-formula id="pone.0128570.e081"><alternatives><graphic xlink:href="pone.0128570.e081.jpg" id="pone.0128570.e081g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M81"><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. The <italic>&#x003bb;</italic>
<sub><italic>ij</italic></sub> can be interpreted as follows. If <bold><italic>x</italic></bold>
<sub><italic>i</italic></sub> has higher breeding value than <bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>, then <bold><italic>x</italic></bold>
<sub><italic>j</italic></sub> will get a push downwards of strength &#x02223;<italic>&#x003bb;</italic>
<sub><italic>ij</italic></sub>&#x02223;. Otherwise, <bold><italic>x</italic></bold>
<sub><italic>j</italic></sub> will get a push upwards of strength &#x02223;<italic>&#x003bb;</italic>
<sub><italic>ij</italic></sub>&#x02223;. However, if <bold>x</bold>
<sub><italic>i</italic></sub> and <bold>x</bold>
<sub><italic>j</italic></sub> are both not ranked in the top-<italic>k</italic> elements, then, by the definition of NDCG, there will be no gain from swapping them. Therefore, &#x00394;NDCG<sub><italic>ij</italic></sub> = <italic>&#x003bb;</italic>
<sub><italic>ij</italic></sub> = 0 in this case. For a detailed derivation and discussion of the <italic>&#x003bb;</italic>-gradient, see [<xref rid="pone.0128570.ref041" ref-type="bibr">41</xref>]. To reduce overfitting, the same techniques as RF and GBRT can be used.</p></sec><sec id="sec027"><title>Complexity comparison</title><p>We now briefly compare the computational complexity of training algorithms for ranking and regression. We assume that the number of samples <italic>n</italic> is much smaller than the number of markers <italic>p</italic>, as is usually the case in GS. For RKHS regression (a.k.a. kernel ridge regression), we pre-compute the kernel matrix <italic>K</italic>, which takes <italic>O</italic>(<italic>n</italic>
<sup>2</sup>
<italic>p</italic>). Once this is done, a closed form solution can be obtained by solving a system of linear equations, which takes <italic>O</italic>(<italic>n</italic>
<sup>3</sup>). Alternatively, an approximate solution can be obtained by any gradient solver, such as the conjugate gradient method or limited-memory BFGS (L-BFGS). In this case, the main cost per iteration comes from computing the gradient, which takes <italic>O</italic>(<italic>n</italic>
<sup>2</sup>). For kernel RankSVM, we also pre-compute the kernel matrix. Since a closed form solution is not available, we use a gradient method. The main cost per iteration stems from computing the gradient, which takes <italic>O</italic>(<italic>n</italic>
<sup>2</sup>), the same as for RKHS regression. For random forests, GBRT, McRank and LambdaMART, the computational cost is proportional to the number of trees in the ensemble. Tree induction takes <italic>O</italic>(<italic>p</italic>&#x02032;<italic>n</italic> log<sup>2</sup>
<italic>n</italic>) in average and <italic>O</italic>(<italic>p</italic>&#x02032;<italic>n</italic>
<sup>2</sup>log<italic>n</italic>) in worst case, where <italic>p</italic>&#x02032; &#x02264; <italic>p</italic> is the number of markers considered for node splitting [<xref rid="pone.0128570.ref042" ref-type="bibr">42</xref>]. For LambdaMART, each tree needs to be fitted against the <italic>&#x003bb;</italic>-gradient. Computing the <italic>&#x003bb;</italic>-gradient takes <italic>O</italic>(<italic>kn</italic>), where <italic>k</italic> is the parameter used for NDCG@k.</p></sec></sec></sec><sec id="sec028"><title>Experimental results</title><sec id="sec029"><title>Datasets</title><p>We evaluated the validity of our ranking approach using the following 6 datasets.</p><sec id="sec030"><title>Arabidopsis</title><p>This dataset comprises 422 lines of <italic>Arabidopsis thaliana</italic> developed by INRA [<xref rid="pone.0128570.ref043" ref-type="bibr">43</xref>]. We chose 3 traits, flowering time in short days (FLOSD), shoot dry matter in non-limiting nitrogen conditions (DM10) and shoot dry matter in limiting nitrogen conditions (DM3), which were also selected in [<xref rid="pone.0128570.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0128570.ref044" ref-type="bibr">44</xref>]. We excluded 5 lines which were not evaluated for these traits. Marker genotypes were available for 69 SSRs. We imputed the missing genotypes using the R package <italic>qtl</italic>[<xref rid="pone.0128570.ref045" ref-type="bibr">45</xref>]. The dataset is available at <ext-link ext-link-type="uri" xlink:href="http://publiclines.versailles.inra.fr/page/33">http://publiclines.versailles.inra.fr/page/33</ext-link>.</p></sec><sec id="sec031"><title>Barley</title><p>The Barley-CAP project evaluated the grain yield of 432 barley lines in Aberdeen, Idaho (trial name: NSGC_2012_NormN_Irr_Aberdeen). Among them, 381 lines were genotyped using 3945 SNPs. We imputed missing genotypes using BEAGLE [<xref rid="pone.0128570.ref046" ref-type="bibr">46</xref>]. The dataset is available at <ext-link ext-link-type="uri" xlink:href="http://triticeaetoolbox.org">http://triticeaetoolbox.org</ext-link>.</p></sec><sec id="sec032"><title>Maize</title><p>This dataset, used in the study of [<xref rid="pone.0128570.ref010" ref-type="bibr">10</xref>], comprises 264 maize lines genotyped using 1076 SNP markers. We imputed missing values using BEAGLE [<xref rid="pone.0128570.ref046" ref-type="bibr">46</xref>]. The dataset is provided as example data in SelectionTools [<xref rid="pone.0128570.ref047" ref-type="bibr">47</xref>], available at <ext-link ext-link-type="uri" xlink:href="http://www.uni-giessen.de/cms/fbz/fb09/institute/pflbz2/population-genetics/downloads">http://www.uni-giessen.de/cms/fbz/fb09/institute/pflbz2/population-genetics/downloads</ext-link>.</p></sec><sec id="sec033"><title>Rice</title><p>This dataset consists of 395 lines genotyped with 1311 SNPs [<xref rid="pone.0128570.ref048" ref-type="bibr">48</xref>]. We imputed missing genotypes using BEAGLE [<xref rid="pone.0128570.ref046" ref-type="bibr">46</xref>]. Among these lines, phenotypic values of a total of 34 traits were available for 383 lines, with some missing records [<xref rid="pone.0128570.ref049" ref-type="bibr">49</xref>]. We chose 335 lines without missing records in 14 traits. The traits were flowering time, flag leaf length, flag leaf width, number of panicles per plant, plant height, panicle length, primary panicle branch number, number of seeds per panicle, number of florets per panicle, seed length, seed width, seed volume, seed surface area and amylose content. The dataset is available at <ext-link ext-link-type="uri" xlink:href="http://www.ricediversity.org/data/index.cfm">http://www.ricediversity.org/data/index.cfm</ext-link>.</p></sec><sec id="sec034"><title>Wheat (CIMMYT)</title><p>This dataset comprises 599 wheat lines developed by the CIMMYT Global Wheat Breeding program [<xref rid="pone.0128570.ref010" ref-type="bibr">10</xref>]. Trait values correspond to grain yield evaluated in 4 different environments. Wheat lines were genotyped using 1447 DArT (Diversity Array Technology) markers. Markers may take on the values 1 or 0, indicating their presence or absence. Markers with allele frequency less than 0.05 or greater than 0.95 were removed. The total number of markers retained after this processing was 1279. The dataset is provided as part of the R package <italic>BLR</italic>.</p></sec><sec id="sec035"><title>Wheat (P&#x000e9;rez-Rodr&#x000ed;guez)</title><p>This dataset, used in the study of [<xref rid="pone.0128570.ref050" ref-type="bibr">50</xref>], consists of 306 lines genotyped with 1695 DArT markers. Phenotypic values for two traits, grain yield and days to heading, were available for these lines. The dataset is available at the same URL as for Maize data.</p><p>For SSR and SNP markers, genotypes were encoded by 0 (AA), 1 (AB), and 2 (BB). For DArT markers, the presence or absence of an allele was encoded by 0 and 1 for Wheat (CIMMYT) and by 0 and 2 for Wheat (P&#x000e9;rez-Rodr&#x000ed;guez).</p><p>For datasets comprised of several traits, we build one model per trait and report the averaged evaluation scores.</p><p>All traits described above are inherently non-negative quantities. In the case of the Wheat (CIMMYT) dataset, grain yield values were centered so as to have zero mean prior to public release of the dataset. As a result, the dataset contains negative yield values. In order to avoid negative NDCG scores, we converted the yield values back to positive values. To do so, since the original centering was not known to us, we simply shifted the yield values such that the smallest value in the entire dataset is zero.</p></sec></sec><sec id="sec036"><title>Experimental setup</title><sec id="sec037"><title>Cross-validation setup</title><p>To estimate the generalization performance of different models, i.e., the ranking accuracy on new candidates, we used a randomized cross-validation scheme. For each cross-validation iteration, the dataset was split into 80% for model estimation and 20% for evaluation. To ensure fair comparison, we made sure that all methods use the same splits. Evaluation scores were computed for 10 cross-validation iterations and averaged. We report results for 6 evaluation measures: Pearson correlation, Kendall&#x02019;s <italic>&#x003c4;</italic>, NDCG@1, NDCG@5, NDCG@10 and Mean NDCG@10. For models which need hyper-parameter tuning, we further used 5-fold cross-validation within the train split. To obtain the best possible results, we always selected the hyper-parameters which maximize the same measure as used for evaluation. For example, for NDCG@10 results, the hyper-parameters were selected to maximize NDCG@10.</p></sec><sec id="sec038"><title>Parameter inference for Bayesian regression methods</title><p>Parameters of the Bayesian regression methods were estimated using variational Bayesian approaches. The algorithms for BL and EBL were proposed by [<xref rid="pone.0128570.ref020" ref-type="bibr">20</xref>]. The wBSR algorithm was introduced by [<xref rid="pone.0128570.ref013" ref-type="bibr">13</xref>]. [<xref rid="pone.0128570.ref051" ref-type="bibr">51</xref>] introduced a variational Bayesian algorithm for linear regression with a spike and slab prior. A difference between the algorithm for BayesC in this study and that in [<xref rid="pone.0128570.ref051" ref-type="bibr">51</xref>] is that the authors used importance sampling to infer the posterior distribution of <italic>&#x003c3;</italic>
<sup>2</sup>, <inline-formula id="pone.0128570.e082"><alternatives><graphic xlink:href="pone.0128570.e082.jpg" id="pone.0128570.e082g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M82"><mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <italic>&#x003c0;</italic>, whereas we inferred the factorized posteriors of <italic>&#x003c3;</italic>
<sup>2</sup> and <inline-formula id="pone.0128570.e083"><alternatives><graphic xlink:href="pone.0128570.e083.jpg" id="pone.0128570.e083g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M83"><mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mn>0</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, and used a fixed value for <italic>&#x003c0;</italic> as described further below. The algorithms of SSVS and MIX were implemented by the second author and will be published elsewhere. Phenotypic values were standardized prior to training. All Bayesian regression methods were performed with a program written in C.</p></sec><sec id="sec039"><title>Model estimatation for other methods</title><p>For random forests and GBRT, we used implementations provided in the scikit-learn Python package [<xref rid="pone.0128570.ref052" ref-type="bibr">52</xref>, <xref rid="pone.0128570.ref053" ref-type="bibr">53</xref>]. For ridge and RKHS regression, we used the R package <italic>rrBLUP</italic>[<xref rid="pone.0128570.ref054" ref-type="bibr">54</xref>]. For McRank and LambdaMART, we used the source code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mblondel/ivalice">https://github.com/mblondel/ivalice</ext-link>. For RankSVM, we solved the objective function with <italic>r</italic> = 2 by L-BFGS [<xref rid="pone.0128570.ref040" ref-type="bibr">40</xref>]. We set the maximum number of iterations to 500.</p></sec><sec id="sec040"><title>Hyper-parameter tuning</title><p>For BL, we tested five values, 0.1, 1, 10, 30 and 100, for <italic>&#x003d5;</italic>. For <italic>&#x003c9;</italic>, we tested six log-spaced values from <italic>&#x003d5;</italic>/20<italic>p</italic> to 5<italic>&#x003d5;</italic>, where <italic>p</italic> is the number of markers. Because <inline-formula id="pone.0128570.e084"><alternatives><graphic xlink:href="pone.0128570.e084.jpg" id="pone.0128570.e084g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M84"><mml:mrow><mml:mtext mathvariant="normal">E</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> if <inline-formula id="pone.0128570.e085"><alternatives><graphic xlink:href="pone.0128570.e085.jpg" id="pone.0128570.e085g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M85"><mml:mrow><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x0223c;</mml:mo><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and the expectation of <inline-formula id="pone.0128570.e086"><alternatives><graphic xlink:href="pone.0128570.e086.jpg" id="pone.0128570.e086g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M86"><mml:mrow><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is <italic>&#x003d5;</italic>/<italic>&#x003c9;</italic>, these values of <italic>&#x003d5;</italic> and <italic>&#x003c9;</italic> correspond to the grids of <inline-formula id="pone.0128570.e087"><alternatives><graphic xlink:href="pone.0128570.e087.jpg" id="pone.0128570.e087g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M87"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>&#x003c4;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> from 1/10<italic>p</italic> to 10, which are obtained by replacing <inline-formula id="pone.0128570.e088"><alternatives><graphic xlink:href="pone.0128570.e088.jpg" id="pone.0128570.e088g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M88"><mml:mrow><mml:mn>2</mml:mn><mml:mo>/</mml:mo><mml:msubsup><mml:mi>&#x003bb;</mml:mi><mml:mi>B</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> with <italic>&#x003d5;</italic>/<italic>&#x003c9;</italic>. In total, this corresponds to 30 possible parameter combinations. For EBL, we used the same values for <italic>&#x003c8;</italic> and <italic>&#x003b8;</italic> and tested three values, 0.1, 1 and 10. For <italic>&#x003d5;</italic> and <italic>&#x003c9;</italic>, we tried the same values as for BL. Consequently, this corresponds to a total of 90 parameter combinations. For wBSR and BayesC, we fixed <italic>&#x003bd;</italic> to 4. For <italic>&#x003c0;</italic>, we tested 5 log-spaced values from 1/<italic>p</italic> to 1. For <italic>S</italic>
<sup>2</sup>, we tested 10 log-spaced values from 1/20<italic>p</italic> to 5. Because the expectation of <italic>InvChi</italic>2(<italic>&#x003bd;</italic>, <italic>S</italic>
<sup>2</sup>) is <italic>&#x003bd;S</italic>
<sup>2</sup>/(<italic>&#x003bd;</italic>&#x02212;2), these values of <italic>&#x003bd;</italic> and <italic>S</italic>
<sup>2</sup> correspond to the grids of <inline-formula id="pone.0128570.e089"><alternatives><graphic xlink:href="pone.0128570.e089.jpg" id="pone.0128570.e089g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M89"><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> from 1/10<italic>p</italic> to 10. This corresponds to a total of 50 possible parameter combinations for wBSR and BayesC. For SSVS and MIX, we fixed <italic>&#x003bd;</italic> and <italic>&#x003c0;</italic> to 4 and 0.01, respectively. For <italic>c</italic>, we tested 5 values from 10<sup>&#x02212;5</sup> to 10<sup>&#x02212;1</sup>. The values tested for <italic>S</italic>
<sup>2</sup> were the same as that of wBSR and BayesC. This corresponds to a total of 50 possible parameter combinations tested for SSVS and MIX.</p><p>For tree-based ensemble methods including random forests (RF), gradient boosting regression trees (GBRT), McRank and LambdaMART, we used 300 trees in the ensemble. The parameter <monospace>max_features</monospace> was set to 0.6, meaning that only 60% of the features are considered when searching for the best split during tree induction. This both speeds up tree induction and reduces overfitting. The maximum tree depth <monospace>max_depth</monospace> was chosen from {3, 5, 10}. For GBRT and LambdaMART, we chose the learning rate parameter from {0.001, 0.01, 0.1, 1.0}. For McRank, we chose the number of bins from {3, 4, &#x02026;, 20}.</p><p>For ridge and RKHS regression, we used the R package <italic>rrBLUP</italic>[<xref rid="pone.0128570.ref054" ref-type="bibr">54</xref>], which can estimate the regularization and kernel parameters automatically by (restricted) maximum likelihood (i.e., without cross-validation). This approach is closely related to gaussian processes in the machine learning community [<xref rid="pone.0128570.ref055" ref-type="bibr">55</xref>].</p><p>For RankSVM, we set <inline-formula id="pone.0128570.e090"><alternatives><graphic xlink:href="pone.0128570.e090.jpg" id="pone.0128570.e090g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M90"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">&#x02223;</mml:mo><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, where &#x02223;<italic>P</italic>&#x02223; is the number of preference pairs, and chose <inline-formula id="pone.0128570.e091"><alternatives><graphic xlink:href="pone.0128570.e091.jpg" id="pone.0128570.e091g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M91"><mml:mrow><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> from 15 log-spaced values between 10<sup>&#x02212;6</sup> and 10<sup>6</sup>. We use the RBF kernel <italic>&#x003ba;</italic>(<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>,<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>) = exp(&#x02212;<italic>&#x003b3;</italic>&#x02016;<bold><italic>x</italic></bold>
<sub><italic>i</italic></sub>&#x02212;<bold><italic>x</italic></bold>
<sub><italic>j</italic></sub>&#x02016;<sup>2</sup>). Following [<xref rid="pone.0128570.ref054" ref-type="bibr">54</xref>], we set <inline-formula id="pone.0128570.e092"><alternatives><graphic xlink:href="pone.0128570.e092.jpg" id="pone.0128570.e092g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M92"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>p</mml:mi><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>p</italic> is the number of markers. We then chose <italic>&#x003c3;</italic> from 10 linearly-spaced values between 0 and 1.</p></sec></sec><sec id="sec041"><title>Cross-validation results</title><p>The general method ranking across six datasets is given in <xref rid="pone.0128570.t001" ref-type="table">Table 1</xref>. Overall, the five best methods for each evaluation measure were as follows:
<list list-type="bullet"><list-item><p>Pearson correlation: RKHS regression, Ordinal McRank, RF, RankSVM, wBSR</p></list-item><list-item><p>Kendall&#x02019;s <italic>&#x003c4;</italic>: RKHS regression, Ordinal McRank, RF, RankSVM, wBSR</p></list-item><list-item><p>NDCG@1: RF, RankSVM, Ordinal McRank, RKHS regression, GBRT</p></list-item><list-item><p>NDCG@5: Ordinal McRank, RF, RankSVM, RKHS, BL</p></list-item><list-item><p>NDCG@10: Ordinal McRank, RKHS regression, RF, RankSVM, GBRT</p></list-item><list-item><p>Mean NDCG@10: Ordinal McRank, RF, RKHS regression, RankSVM, GBRT</p></list-item></list>
</p><table-wrap id="pone.0128570.t001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t001</object-id><label>Table 1</label><caption><title>General method ranking, obtained by sorting methods according to their average ranking across 6 datasets.</title></caption><alternatives><graphic id="pone.0128570.t001g" xlink:href="pone.0128570.t001"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">1</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">2</td></tr><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">1</td><td align="left" rowspan="1" colspan="1">3</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">3</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">4</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">5</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">6</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">7</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">6</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">8</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">9</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">10</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">8</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">11</td><td align="left" rowspan="1" colspan="1">11</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">10</td><td align="left" rowspan="1" colspan="1">9</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">7</td><td align="left" rowspan="1" colspan="1">12</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">4</td><td align="left" rowspan="1" colspan="1">5</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">13</td><td align="left" rowspan="1" colspan="1">13</td></tr></tbody></table></alternatives></table-wrap><p>Detailed results for each dataset are given in Tables <xref rid="pone.0128570.t002" ref-type="table">2</xref>&#x02013;<xref rid="pone.0128570.t007" ref-type="table">7</xref>. For datasets comprising several traits, we report the average scores only, for the purpose of clarity. Bold numbers in parentheses indicate the ranking of the five best methods with respect to the corresponding evaluation measure. We do not include results of LambdaMART with respect to Pearson correlation and Kendall&#x02019;s <italic>&#x003c4;</italic>, since LambdaMART is a method designed to optimize NDCG.</p><table-wrap id="pone.0128570.t002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t002</object-id><label>Table 2</label><caption><title>Cross-validation results on the <italic>Arabidopsis thaliana</italic> dataset, averaged across 3 traits.</title></caption><alternatives><graphic id="pone.0128570.t002g" xlink:href="pone.0128570.t002"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.651 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.481 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.836</td><td align="left" rowspan="1" colspan="1">0.884 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.907 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.883 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.628 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.470 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.855 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.885 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.903 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.881 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.630 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.468 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.844 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.872</td><td align="left" rowspan="1" colspan="1">0.899 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.878 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.628 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.450</td><td align="left" rowspan="1" colspan="1">0.841 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.879 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.899 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.877 <bold>(4)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.814</td><td align="left" rowspan="1" colspan="1">0.870</td><td align="left" rowspan="1" colspan="1">0.904 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.876 <bold>(5)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.627</td><td align="left" rowspan="1" colspan="1">0.468 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.861 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.876</td><td align="left" rowspan="1" colspan="1">0.894</td><td align="left" rowspan="1" colspan="1">0.874</td></tr><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.636 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.455</td><td align="left" rowspan="1" colspan="1">0.833</td><td align="left" rowspan="1" colspan="1">0.879 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.899 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.873</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.617</td><td align="left" rowspan="1" colspan="1">0.462</td><td align="left" rowspan="1" colspan="1">0.840 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.871</td><td align="left" rowspan="1" colspan="1">0.888</td><td align="left" rowspan="1" colspan="1">0.872</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.594</td><td align="left" rowspan="1" colspan="1">0.437</td><td align="left" rowspan="1" colspan="1">0.825</td><td align="left" rowspan="1" colspan="1">0.877</td><td align="left" rowspan="1" colspan="1">0.892</td><td align="left" rowspan="1" colspan="1">0.869</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.619</td><td align="left" rowspan="1" colspan="1">0.442</td><td align="left" rowspan="1" colspan="1">0.802</td><td align="left" rowspan="1" colspan="1">0.863</td><td align="left" rowspan="1" colspan="1">0.896</td><td align="left" rowspan="1" colspan="1">0.866</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.625</td><td align="left" rowspan="1" colspan="1">0.468 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.821</td><td align="left" rowspan="1" colspan="1">0.872</td><td align="left" rowspan="1" colspan="1">0.897</td><td align="left" rowspan="1" colspan="1">0.863</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.630 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.468 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.819</td><td align="left" rowspan="1" colspan="1">0.878 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.873</td><td align="left" rowspan="1" colspan="1">0.857</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.464</td><td align="left" rowspan="1" colspan="1">0.319</td><td align="left" rowspan="1" colspan="1">0.839</td><td align="left" rowspan="1" colspan="1">0.846</td><td align="left" rowspan="1" colspan="1">0.866</td><td align="left" rowspan="1" colspan="1">0.848</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0128570.t003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t003</object-id><label>Table 3</label><caption><title>Cross-validation results on the Barley dataset.</title></caption><alternatives><graphic id="pone.0128570.t003g" xlink:href="pone.0128570.t003"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.581</td><td align="left" rowspan="1" colspan="1">0.436 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.816 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.832 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.850 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.830 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.566</td><td align="left" rowspan="1" colspan="1">0.432</td><td align="left" rowspan="1" colspan="1">0.783 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.803 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.829 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.808 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.729</td><td align="left" rowspan="1" colspan="1">0.824 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.804</td><td align="left" rowspan="1" colspan="1">0.805 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.568</td><td align="left" rowspan="1" colspan="1">0.425</td><td align="left" rowspan="1" colspan="1">0.764 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.809 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.833 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.802 <bold>(4)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.604 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.447 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.766 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.799 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.834 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.795 <bold>(5)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.554</td><td align="left" rowspan="1" colspan="1">0.409</td><td align="left" rowspan="1" colspan="1">0.722</td><td align="left" rowspan="1" colspan="1">0.768</td><td align="left" rowspan="1" colspan="1">0.820 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.775</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.585 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.428</td><td align="left" rowspan="1" colspan="1">0.718</td><td align="left" rowspan="1" colspan="1">0.771</td><td align="left" rowspan="1" colspan="1">0.809</td><td align="left" rowspan="1" colspan="1">0.774</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.572</td><td align="left" rowspan="1" colspan="1">0.421</td><td align="left" rowspan="1" colspan="1">0.700</td><td align="left" rowspan="1" colspan="1">0.756</td><td align="left" rowspan="1" colspan="1">0.820 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.763</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.581</td><td align="left" rowspan="1" colspan="1">0.432</td><td align="left" rowspan="1" colspan="1">0.790 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.782</td><td align="left" rowspan="1" colspan="1">0.813</td><td align="left" rowspan="1" colspan="1">0.762</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.582 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.434 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.745</td><td align="left" rowspan="1" colspan="1">0.765</td><td align="left" rowspan="1" colspan="1">0.805</td><td align="left" rowspan="1" colspan="1">0.759</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.593 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.438 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.682</td><td align="left" rowspan="1" colspan="1">0.765</td><td align="left" rowspan="1" colspan="1">0.814</td><td align="left" rowspan="1" colspan="1">0.756</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.578</td><td align="left" rowspan="1" colspan="1">0.419</td><td align="left" rowspan="1" colspan="1">0.764 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.744</td><td align="left" rowspan="1" colspan="1">0.808</td><td align="left" rowspan="1" colspan="1">0.746</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.592 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.435 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.492</td><td align="left" rowspan="1" colspan="1">0.758</td><td align="left" rowspan="1" colspan="1">0.768</td><td align="left" rowspan="1" colspan="1">0.733</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0128570.t004" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t004</object-id><label>Table 4</label><caption><title>Cross-validation results on the Maize dataset.</title></caption><alternatives><graphic id="pone.0128570.t004g" xlink:href="pone.0128570.t004"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.427 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.298 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.762 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.783 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.795 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.773 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.419 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.283 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.793 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.721</td><td align="left" rowspan="1" colspan="1">0.768 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.768 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.445 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.317 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.780 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.771 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.794 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.765 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.444 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.309 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.726 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.763 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.776 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.757 <bold>(4)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.696</td><td align="left" rowspan="1" colspan="1">0.697</td><td align="left" rowspan="1" colspan="1">0.740</td><td align="left" rowspan="1" colspan="1">0.741 <bold>(5)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.403</td><td align="left" rowspan="1" colspan="1">0.255</td><td align="left" rowspan="1" colspan="1">0.716 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.740</td><td align="left" rowspan="1" colspan="1">0.743</td><td align="left" rowspan="1" colspan="1">0.741 <bold>(5)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.361</td><td align="left" rowspan="1" colspan="1">0.229</td><td align="left" rowspan="1" colspan="1">0.603</td><td align="left" rowspan="1" colspan="1">0.702</td><td align="left" rowspan="1" colspan="1">0.733</td><td align="left" rowspan="1" colspan="1">0.739</td></tr><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.431 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.278 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.675</td><td align="left" rowspan="1" colspan="1">0.736</td><td align="left" rowspan="1" colspan="1">0.761 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.737</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.383</td><td align="left" rowspan="1" colspan="1">0.241</td><td align="left" rowspan="1" colspan="1">0.626</td><td align="left" rowspan="1" colspan="1">0.755 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.725</td><td align="left" rowspan="1" colspan="1">0.725</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.393</td><td align="left" rowspan="1" colspan="1">0.242</td><td align="left" rowspan="1" colspan="1">0.582</td><td align="left" rowspan="1" colspan="1">0.773 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.744</td><td align="left" rowspan="1" colspan="1">0.708</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.404</td><td align="left" rowspan="1" colspan="1">0.258</td><td align="left" rowspan="1" colspan="1">0.360</td><td align="left" rowspan="1" colspan="1">0.716</td><td align="left" rowspan="1" colspan="1">0.719</td><td align="left" rowspan="1" colspan="1">0.705</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.398</td><td align="left" rowspan="1" colspan="1">0.240</td><td align="left" rowspan="1" colspan="1">0.592</td><td align="left" rowspan="1" colspan="1">0.734</td><td align="left" rowspan="1" colspan="1">0.730</td><td align="left" rowspan="1" colspan="1">0.687</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.390</td><td align="left" rowspan="1" colspan="1">0.236</td><td align="left" rowspan="1" colspan="1">0.654</td><td align="left" rowspan="1" colspan="1">0.725</td><td align="left" rowspan="1" colspan="1">0.739</td><td align="left" rowspan="1" colspan="1">0.644</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0128570.t005" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t005</object-id><label>Table 5</label><caption><title>Cross-validation results on the Rice dataset, averaged across 14 traits.</title></caption><alternatives><graphic id="pone.0128570.t005g" xlink:href="pone.0128570.t005"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.719 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.535 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.930 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.941 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.946 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.941 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.717 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.533 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.921 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.940 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.946 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.940 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.702</td><td align="left" rowspan="1" colspan="1">0.525</td><td align="left" rowspan="1" colspan="1">0.921 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.937 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.944 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.937 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.713 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.527 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.917 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.941 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.945 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.937 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.720 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.536 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.916</td><td align="left" rowspan="1" colspan="1">0.937 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.945 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.936 <bold>(5)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.694</td><td align="left" rowspan="1" colspan="1">0.511</td><td align="left" rowspan="1" colspan="1">0.909</td><td align="left" rowspan="1" colspan="1">0.932</td><td align="left" rowspan="1" colspan="1">0.940</td><td align="left" rowspan="1" colspan="1">0.930</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.920 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.931</td><td align="left" rowspan="1" colspan="1">0.934</td><td align="left" rowspan="1" colspan="1">0.929</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.714 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.529 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.896</td><td align="left" rowspan="1" colspan="1">0.924</td><td align="left" rowspan="1" colspan="1">0.938</td><td align="left" rowspan="1" colspan="1">0.924</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.708</td><td align="left" rowspan="1" colspan="1">0.526</td><td align="left" rowspan="1" colspan="1">0.889</td><td align="left" rowspan="1" colspan="1">0.921</td><td align="left" rowspan="1" colspan="1">0.935</td><td align="left" rowspan="1" colspan="1">0.922</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.676</td><td align="left" rowspan="1" colspan="1">0.502</td><td align="left" rowspan="1" colspan="1">0.886</td><td align="left" rowspan="1" colspan="1">0.923</td><td align="left" rowspan="1" colspan="1">0.933</td><td align="left" rowspan="1" colspan="1">0.920</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.686</td><td align="left" rowspan="1" colspan="1">0.506</td><td align="left" rowspan="1" colspan="1">0.889</td><td align="left" rowspan="1" colspan="1">0.918</td><td align="left" rowspan="1" colspan="1">0.932</td><td align="left" rowspan="1" colspan="1">0.916</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.688</td><td align="left" rowspan="1" colspan="1">0.505</td><td align="left" rowspan="1" colspan="1">0.899</td><td align="left" rowspan="1" colspan="1">0.918</td><td align="left" rowspan="1" colspan="1">0.932</td><td align="left" rowspan="1" colspan="1">0.914</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.693</td><td align="left" rowspan="1" colspan="1">0.508</td><td align="left" rowspan="1" colspan="1">0.830</td><td align="left" rowspan="1" colspan="1">0.909</td><td align="left" rowspan="1" colspan="1">0.925</td><td align="left" rowspan="1" colspan="1">0.904</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0128570.t006" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t006</object-id><label>Table 6</label><caption><title>Cross-validation results on the Wheat (CIMMYT) dataset, averaged across 4 traits.</title></caption><alternatives><graphic id="pone.0128570.t006g" xlink:href="pone.0128570.t006"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.503 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.359 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.698 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.752 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.780 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.748 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.486 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.351 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.688 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.745 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.764 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.740 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.482 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.346 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.697 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.739 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.760 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.736 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.463</td><td align="left" rowspan="1" colspan="1">0.329 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.713 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.718</td><td align="left" rowspan="1" colspan="1">0.760 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.733 <bold>(4)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.454</td><td align="left" rowspan="1" colspan="1">0.314</td><td align="left" rowspan="1" colspan="1">0.671</td><td align="left" rowspan="1" colspan="1">0.718</td><td align="left" rowspan="1" colspan="1">0.751</td><td align="left" rowspan="1" colspan="1">0.733 <bold>(4)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.472 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.331 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.690 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.725 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.757 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.732</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.451</td><td align="left" rowspan="1" colspan="1">0.310</td><td align="left" rowspan="1" colspan="1">0.651</td><td align="left" rowspan="1" colspan="1">0.724 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.755</td><td align="left" rowspan="1" colspan="1">0.719</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.464</td><td align="left" rowspan="1" colspan="1">0.320</td><td align="left" rowspan="1" colspan="1">0.650</td><td align="left" rowspan="1" colspan="1">0.697</td><td align="left" rowspan="1" colspan="1">0.732</td><td align="left" rowspan="1" colspan="1">0.711</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.463</td><td align="left" rowspan="1" colspan="1">0.319</td><td align="left" rowspan="1" colspan="1">0.654</td><td align="left" rowspan="1" colspan="1">0.707</td><td align="left" rowspan="1" colspan="1">0.728</td><td align="left" rowspan="1" colspan="1">0.711</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.458</td><td align="left" rowspan="1" colspan="1">0.318</td><td align="left" rowspan="1" colspan="1">0.658</td><td align="left" rowspan="1" colspan="1">0.709</td><td align="left" rowspan="1" colspan="1">0.735</td><td align="left" rowspan="1" colspan="1">0.706</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.448</td><td align="left" rowspan="1" colspan="1">0.312</td><td align="left" rowspan="1" colspan="1">0.675</td><td align="left" rowspan="1" colspan="1">0.690</td><td align="left" rowspan="1" colspan="1">0.735</td><td align="left" rowspan="1" colspan="1">0.699</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.636</td><td align="left" rowspan="1" colspan="1">0.695</td><td align="left" rowspan="1" colspan="1">0.715</td><td align="left" rowspan="1" colspan="1">0.697</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.465 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.322</td><td align="left" rowspan="1" colspan="1">0.524</td><td align="left" rowspan="1" colspan="1">0.627</td><td align="left" rowspan="1" colspan="1">0.677</td><td align="left" rowspan="1" colspan="1">0.666</td></tr></tbody></table></alternatives></table-wrap><table-wrap id="pone.0128570.t007" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t007</object-id><label>Table 7</label><caption><title>Results on the Wheat (P&#x000e9;rez-Rodr&#x000ed;guez) dataset, averaged across 2 traits.</title></caption><alternatives><graphic id="pone.0128570.t007g" xlink:href="pone.0128570.t007"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RKHS</td><td align="left" rowspan="1" colspan="1">0.662 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.448 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.981 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.982 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.981 <bold>(1)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RankSVM</td><td align="left" rowspan="1" colspan="1">0.649 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.448 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.969</td><td align="left" rowspan="1" colspan="1">0.981 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.982 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.980 <bold>(2)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">RF</td><td align="left" rowspan="1" colspan="1">0.658 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.438 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.973 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.980 <bold>(2)</bold>
</td><td align="left" rowspan="1" colspan="1">0.981 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ordinal McRank</td><td align="left" rowspan="1" colspan="1">0.665 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.452 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.975 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.982 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">Ridge</td><td align="left" rowspan="1" colspan="1">0.602</td><td align="left" rowspan="1" colspan="1">0.398</td><td align="left" rowspan="1" colspan="1">0.982 <bold>(1)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.980 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.979 <bold>(3)</bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">GBRT</td><td align="left" rowspan="1" colspan="1">0.649 <bold>(4)</bold>
</td><td align="left" rowspan="1" colspan="1">0.434 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.976 <bold>(3)</bold>
</td><td align="left" rowspan="1" colspan="1">0.977</td><td align="left" rowspan="1" colspan="1">0.980 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.976</td></tr><tr><td align="left" rowspan="1" colspan="1">LambdaMART</td><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">0.973 <bold>(5)</bold>
</td><td align="left" rowspan="1" colspan="1">0.976</td><td align="left" rowspan="1" colspan="1">0.975</td><td align="left" rowspan="1" colspan="1">0.975</td></tr><tr><td align="left" rowspan="1" colspan="1">BL</td><td align="left" rowspan="1" colspan="1">0.608</td><td align="left" rowspan="1" colspan="1">0.398</td><td align="left" rowspan="1" colspan="1">0.972</td><td align="left" rowspan="1" colspan="1">0.977</td><td align="left" rowspan="1" colspan="1">0.975</td><td align="left" rowspan="1" colspan="1">0.974</td></tr><tr><td align="left" rowspan="1" colspan="1">EBL</td><td align="left" rowspan="1" colspan="1">0.596</td><td align="left" rowspan="1" colspan="1">0.387</td><td align="left" rowspan="1" colspan="1">0.959</td><td align="left" rowspan="1" colspan="1">0.969</td><td align="left" rowspan="1" colspan="1">0.975</td><td align="left" rowspan="1" colspan="1">0.969</td></tr><tr><td align="left" rowspan="1" colspan="1">BayesC</td><td align="left" rowspan="1" colspan="1">0.586</td><td align="left" rowspan="1" colspan="1">0.374</td><td align="left" rowspan="1" colspan="1">0.953</td><td align="left" rowspan="1" colspan="1">0.969</td><td align="left" rowspan="1" colspan="1">0.972</td><td align="left" rowspan="1" colspan="1">0.967</td></tr><tr><td align="left" rowspan="1" colspan="1">MIX</td><td align="left" rowspan="1" colspan="1">0.568</td><td align="left" rowspan="1" colspan="1">0.365</td><td align="left" rowspan="1" colspan="1">0.944</td><td align="left" rowspan="1" colspan="1">0.963</td><td align="left" rowspan="1" colspan="1">0.974</td><td align="left" rowspan="1" colspan="1">0.964</td></tr><tr><td align="left" rowspan="1" colspan="1">SSVS</td><td align="left" rowspan="1" colspan="1">0.570</td><td align="left" rowspan="1" colspan="1">0.373</td><td align="left" rowspan="1" colspan="1">0.936</td><td align="left" rowspan="1" colspan="1">0.964</td><td align="left" rowspan="1" colspan="1">0.971</td><td align="left" rowspan="1" colspan="1">0.961</td></tr><tr><td align="left" rowspan="1" colspan="1">wBSR</td><td align="left" rowspan="1" colspan="1">0.578</td><td align="left" rowspan="1" colspan="1">0.381</td><td align="left" rowspan="1" colspan="1">0.915</td><td align="left" rowspan="1" colspan="1">0.951</td><td align="left" rowspan="1" colspan="1">0.965</td><td align="left" rowspan="1" colspan="1">0.959</td></tr></tbody></table></alternatives></table-wrap></sec><sec id="sec042"><title>Comparison of RKHS regression and RankSVM</title><p>We compared RKHS regression and RankSVM when varying the regularization and RBF kernel hyper-parameters. Heatmaps indicating Mean NDCG@10 for various hyper-parameter combinations are shown in <xref ref-type="fig" rid="pone.0128570.g001">Fig 1</xref>. Overall, our results reveal an interesting trend: RankSVM achieves better Mean NDCG@10 than RKHS regression for many different hyper-parameter settings. That is, RankSVM appears to be much more robust than RKHS regression to hyper-parameter choice.</p><fig id="pone.0128570.g001" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.g001</object-id><label>Fig 1</label><caption><title>Comparison of RKHS regression and RankSVM when using the RBF kernel with parameter <inline-formula id="pone.0128570.e093"><alternatives><graphic id="pone.0128570.e093g" xlink:href="pone.0128570.e093"/><mml:math id="M93"><mml:mrow><mml:mi>&#x003b3;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>4</mml:mn><mml:mi>p</mml:mi><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> and when varying the regularization parameter <inline-formula id="pone.0128570.e094"><alternatives><graphic id="pone.0128570.e094g" xlink:href="pone.0128570.e094"/><mml:math id="M94"><mml:mrow><mml:mi>&#x003bb;</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mover accent="true"><mml:mi>&#x003bb;</mml:mi><mml:mo>&#x002dc;</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>N</italic> = <italic>n</italic> (RKHS regression) or <italic>N</italic> = &#x02223;<italic>P</italic>&#x02223; (RankSVM).</title><p>The scores indicated are the test Mean NDCG@10 averaged over 10 CV iterations and across all traits.</p></caption><graphic xlink:href="pone.0128570.g001"/></fig></sec><sec id="sec043"><title>Experiments with tree-based ensemble methods</title><sec id="sec044"><title>Effect of the learning rate and number of trees on GBRT</title><p>An important parameter to prevent overfitting in GBRT is the learning rate parameter. This parameter controls the importance given to the trees added at each stage of the algorithm. We compared GBRT with learning rates {1.0, 0.1, 0.01, 0.001} when varying the number of trees. Results for Mean NDCG@10 are shown in <xref ref-type="fig" rid="pone.0128570.g002">Fig 2</xref>. Our results show that, in order to obtain optimal accuracy, the learning rate must be set neither too large nor too small. This is because large values overfit the training set while small values underfit it. Except on the Maize dataset, the best learning rate was 0.1. This is value is therefore a good rule of thumb for a practical application of GBRT to GS.</p><fig id="pone.0128570.g002" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.g002</object-id><label>Fig 2</label><caption><title>Effect of the learning rate (LR) parameter on GBRT when varying the number of trees.</title><p>The scores indicated are the test Mean NDCG@10 averaged over 10 CV iterations and across all traits.</p></caption><graphic xlink:href="pone.0128570.g002"/></fig></sec><sec id="sec045"><title>Effect of the number of bins on McRank</title><p>Since McRank assumes that &#x1d4e8; is a finite set, we need to discretize the continuous training trait values. To do so, we divided the training trait values into <italic>B</italic> equal-width bins and computed their means <italic>b</italic>
<sub>1</sub>, <italic>b</italic>
<sub>2</sub>, &#x02026;, <italic>b</italic>
<sub><italic>B</italic></sub>. Then, we replaced training trait values <italic>y</italic>
<sub><italic>i</italic></sub> by the mean value of the bin they belong to. Although this discretization might sound like a loss of information, it can be beneficial when the goal is to maximize NDCG on unseen data, i.e., data that does not belong to the training set. We compared the Mean NDCG@10 of multiclass and ordinal McRank when varying the number of bins. The number of trees used was fixed to 300. Results are shown in <xref ref-type="fig" rid="pone.0128570.g003">Fig 3</xref>. For comparison, we also indicate the results of RF regression. Our results clearly show the superiority of ordinal McRank over multiclass McRank. Therefore, modeling the cumulative probabilities, as done in ordinal McRank, seems clearly beneficial.</p><fig id="pone.0128570.g003" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.g003</object-id><label>Fig 3</label><caption><title>Effect of the number of bins on multiclass and ordinal McRank.</title><p>The straight line indicates the results of RF for comparison. The scores indicated are the test Mean NDCG@10 averaged over 10 CV iterations and across all traits.</p></caption><graphic xlink:href="pone.0128570.g003"/></fig></sec></sec><sec id="sec046"><title>Comparison of evaluation measures</title><p>
<xref rid="pone.0128570.t001" ref-type="table">Table 1</xref> shows that the best methods tend to vary depending on the evaluation measure used. To quantify the similarity (or lack thereof) between evaluation measures, we computed their Spearman&#x02019;s correlation coefficient, also known as Spearman&#x02019;s <italic>&#x003c1;</italic>, which is a measure of how well their rankings agree. The results, given in <xref rid="pone.0128570.t008" ref-type="table">Table 8</xref>, indicate several interesting trends.</p><table-wrap id="pone.0128570.t008" orientation="portrait" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0128570.t008</object-id><label>Table 8</label><caption><title>Spearman&#x02019;s rank correlation coefficient of evaluation measures averaged across 6 datasets.</title></caption><alternatives><graphic id="pone.0128570.t008g" xlink:href="pone.0128570.t008"/><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/><col align="left" valign="middle" span="1"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Method</th><th align="left" rowspan="1" colspan="1">Correlation</th><th align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</th><th align="left" rowspan="1" colspan="1">NDCG@1</th><th align="left" rowspan="1" colspan="1">NDCG@5</th><th align="left" rowspan="1" colspan="1">NDCG@10</th><th align="left" rowspan="1" colspan="1">Mean NDCG@10</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">Correlation</td><td align="left" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.899</td><td align="char" char="." rowspan="1" colspan="1">0.604</td><td align="char" char="." rowspan="1" colspan="1">0.774</td><td align="char" char="." rowspan="1" colspan="1">0.775</td><td align="char" char="." rowspan="1" colspan="1">0.744</td></tr><tr><td align="left" rowspan="1" colspan="1">Kendall&#x02019;s <italic>&#x003c4;</italic>
</td><td align="char" char="." rowspan="1" colspan="1">0.899</td><td align="left" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.564</td><td align="char" char="." rowspan="1" colspan="1">0.665</td><td align="char" char="." rowspan="1" colspan="1">0.674</td><td align="char" char="." rowspan="1" colspan="1">0.664</td></tr><tr><td align="left" rowspan="1" colspan="1">NDCG@1</td><td align="char" char="." rowspan="1" colspan="1">0.604</td><td align="char" char="." rowspan="1" colspan="1">0.564</td><td align="left" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.811</td><td align="char" char="." rowspan="1" colspan="1">0.827</td><td align="char" char="." rowspan="1" colspan="1">0.901</td></tr><tr><td align="left" rowspan="1" colspan="1">NDCG@5</td><td align="char" char="." rowspan="1" colspan="1">0.774</td><td align="char" char="." rowspan="1" colspan="1">0.665</td><td align="char" char="." rowspan="1" colspan="1">0.811</td><td align="left" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.923</td><td align="char" char="." rowspan="1" colspan="1">0.920</td></tr><tr><td align="left" rowspan="1" colspan="1">NDCG@10</td><td align="char" char="." rowspan="1" colspan="1">0.775</td><td align="char" char="." rowspan="1" colspan="1">0.674</td><td align="char" char="." rowspan="1" colspan="1">0.827</td><td align="char" char="." rowspan="1" colspan="1">0.923</td><td align="left" rowspan="1" colspan="1">-</td><td align="char" char="." rowspan="1" colspan="1">0.962</td></tr><tr><td align="left" rowspan="1" colspan="1">Mean NDCG@10</td><td align="char" char="." rowspan="1" colspan="1">0.744</td><td align="char" char="." rowspan="1" colspan="1">0.664</td><td align="char" char="." rowspan="1" colspan="1">0.901</td><td align="char" char="." rowspan="1" colspan="1">0.920</td><td align="char" char="." rowspan="1" colspan="1">0.962</td><td align="left" rowspan="1" colspan="1">-</td></tr></tbody></table></alternatives></table-wrap><p>First, Pearson correlation and Kendall&#x02019;s <italic>&#x003c4;</italic> correlate poorly with NDCG@k when <italic>k</italic> is small. This is not surprising since Pearson correlation and Kendall&#x02019;s <italic>&#x003c4;</italic> are evaluation measures for global ranking which treat all individuals equally, regardless of their importance. This confirms that one should not use these measures for evaluation and hyper-parameter selection when one is mostly concerned with ranking accuracy at the top.</p><p>Second, and more surprisingly, Pearson correlation was more correlated with NDCG@k than Kendall&#x02019;s <italic>&#x003c4;</italic>. This suggests that a method designed to maximize Pearson correlation might perform better than a pairwise ranking method.</p><p>Finally, the most correlated measure with NDCG@1, NDCG@5 and NDCG@10 was Mean NDCG@10. This suggests that choosing a model which maximizes Mean NDCG@k is a good compromise, if we want a model which works reasonably well (without retraining) at various positions <italic>k</italic>.</p></sec></sec><sec sec-type="conclusions" id="sec047"><title>Discussion</title><p>The choices of the position <italic>k</italic>, gain function <italic>g</italic>(<italic>y</italic>) and discount function <italic>d</italic>(<italic>i</italic>) used in the DCG and NDCG definitions naturally depend on the usecase. For long-lived perennial plants, such as fruit and forest trees, it is important to select good candidates at their juvenile stage (or even at their small seedling stage) for further field testing. At the same time, it is also important to select a small number <italic>k</italic> of candidates because selected candidates usually become parents for the next generation. If too many candidates are selected, selection intensity becomes low and it is not possible to obtain good improvement of the target trait in the next generation. For these reasons, and since field testing is typically expensive, we chose to evaluate models for small values of <italic>k</italic>: <italic>k</italic> &#x02208; {1, 5, 10}. In the IR literature, the exponential gain function <italic>g</italic>(<italic>y</italic>) = 2<sup><italic>y</italic></sup>&#x02212;1 is frequently used. This is because it is often assumed that more relevant documents are exponentially more useful than irrelevant documents. However, in IR, <italic>y</italic> is usually a small number (say, between 1 and 5) which assesses the relevance of a document to some query. In contrast, in GS, <italic>y</italic> can take on much larger values depending on the trait. For this reason, we choose the linear gain function <italic>g</italic>(<italic>y</italic>) = <italic>y</italic>. For the discount function, a possible choice is to not use any discount at all, i.e., <italic>d</italic>(<italic>i</italic>) = 1. This amounts to completely ignore order in the top-<italic>k</italic> candidates. This choice is only reasonable if we are sure to conduct field testing for all <italic>k</italic> selected candidates. Another possible choice is <inline-formula id="pone.0128570.e095"><alternatives><graphic xlink:href="pone.0128570.e095.jpg" id="pone.0128570.e095g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M95"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, which is also the most common choice in the IR literature. This discount function assigns a monotonically decreasing weight to candidates as a function of their rank. It thus takes into account the fact that a candidate is more likely to be examined if it is placed higher in the ranking. This choice is more reasonable if we need to prioritize field testing of candidates with high breeding value, for example due to budget or time constraints. For this reason, we chose <inline-formula id="pone.0128570.e096"><alternatives><graphic xlink:href="pone.0128570.e096.jpg" id="pone.0128570.e096g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M96"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mtext>log</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> in our experiments.</p><p>Trees are a non-parametric method which can approximate complex functions. However, they tend to severely overfit the training data when used alone. For this reason, trees are usually used as part of an ensemble method. Overall, we found that tree-based ensemble methods perform very well for ranking. This confirms a trend which was also observed during the Yahoo! learning to rank challenge [<xref rid="pone.0128570.ref027" ref-type="bibr">27</xref>]. With respect to Mean NDCG@10, Ordinal McRank, RF and GBRT achieved overall 1st, 2nd and 5th places. Tree-based ensemble methods have a number of other advantages including their ability to handle categorical variables, handle missing values without prior imputation and estimate variable importances and interactions [<xref rid="pone.0128570.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0128570.ref026" ref-type="bibr">26</xref>]. Therefore, while the GS community has until now mainly focused on ridge regression and Bayesian regression methods, we believe that tree-based ensemble methods should be considered a top contender in the context of GS.</p><p>RKHS regression achieved 1st place with respect to Pearson correlation on four out of six datasets. Therefore, our results confirm the good results of RKHS regression previously reported in the literature [<xref rid="pone.0128570.ref010" ref-type="bibr">10</xref>]. With respect to Mean NDCG@10, RKHS regression proved to be a very good method, achieving an overall 3rd place. However, this shows that the best method with respect to Pearson correlation is not necessarily the same as the best method with respect to NDCG or Mean NDCG.</p><p>RankSVM achieved an overall 4th place with respect to Mean NDCG@10. However, with respect to NDCG@1 and NDCG@5, RankSVM outperformed RKHS regression. On the Barley dataset, RankSVM outperformed other methods on all NDCG measures by a very large margin. Our results also show that RankSVM is less sensitive than RKHS regression to hyper-parameter choice.</p><p>LambdaMART achieved a disappointing overall 6th place with respect to Mean NDCG@10. This is worse than regular GBRT, of which LambdaMART is an extension. This is surprising, since LambdaMART achieved the leading results in the Yahoo! learning to rank challenge [<xref rid="pone.0128570.ref027" ref-type="bibr">27</xref>]. However, in the Yahoo! learning to rank challenge, the number of samples <italic>n</italic> was much greater than the number of features <italic>p</italic>, i.e., <italic>n</italic> &#x0226b; <italic>p</italic>. This contrasts with GS, where typically <italic>n</italic> &#x0226a; <italic>p</italic>. We thus hypothesize that LambdaMART does not work well in the <italic>n</italic> &#x0226a; <italic>p</italic> setting.</p><p>Traditional (Bayesian) regression methods overall did not perform well with respect to NDCG and Mean NDCG. For example, although they were suggested as some of the best methods in the recent study of [<xref rid="pone.0128570.ref015" ref-type="bibr">15</xref>], BL and wBSR only achieved overall 7th and 13rd places, with respect to Mean NDCG@10. One exception where traditional regression methods performed well is the <italic>Arabidopsis thaliana</italic> dataset, with RKHS regression, SSVS and MIX achieving 1st, 2nd and 3rd places, respectively, with respect to Mean NDCG@10. In the <italic>Arabidopsis</italic> dataset, genotypes are all recombinant inbred lines (RILs) derived from two homozygous parents. Therefore, quantitative trait loci (QTL) harbored by lines (i.e., RILs) are completely bi-allelic. In contrast, in other datasets, QTL harbored by lines may have allelic variation. In fact, important agronomic traits have multiple alleles in candidate genes [<xref rid="pone.0128570.ref056" ref-type="bibr">56</xref>]. In this case, allelic effects cannot be represented by a single bi-allelic marker. Therefore, there may exist complex relationships between causal polymorphisms and SNPs linked to the polymorphisms. In the <italic>Arabidopsis</italic> dataset, the extent of linkage disequilibrium (LD) between a marker and QTL is simply related to the recombination rate between the marker and QTL. In contrast, in other datasets, the extent of LD between a marker and QTL may be affected by various other factors such as demographic history [<xref rid="pone.0128570.ref057" ref-type="bibr">57</xref>]. In this case, the relationship between markers and QTL becomes complex and may be difficult to model via linear regression models.</p><p>On the rice dataset, the difference between the best (RF) and worst (wBSR) methods with respect to NDCG@10 appears quite small (0.925 vs. 0.946), while it appears larger with respect to NDCG@1 (0.930 vs. 0.830). Similar trends are shown in other datasets, such as barley and wheat. We observed that the NDCG@k score typically increases as a function of <italic>k</italic> (although not monotonically). That is, if <italic>k</italic> &#x0003c; <italic>r</italic> then NDCG@k &#x0003c; NDCG@r will usually hold. Since NDCG@k is always between 0 and 1, this means that NDCG@k gets closer to 1 as <italic>k</italic> increases. This also means that the range of possible values taken by NDCG@k will typically decrease as a function of <italic>k</italic>. This however does not necessarily mean that small values of <italic>k</italic> are better for discriminating between methods. In general, <italic>k</italic> should be set to the number of candidates one wants to select when applying the model.</p><p>Sometimes, both ranking and regression accuracies are important. This is for example the case when predicted trait values are used to determine a selling price (e.g., determine crop price in terms of the predicted grain yield). Because of their overall good ranking accuracy, RF and RKHS regression seem like a good choice in this case. Unfortunately, RankSVM and LambdaMART cannot be used in this case, since the loss function they minimize only guarantees ranking. On the other hand, McRank can be used, since it can compute the expected trait values.</p><p>For simplicity, we assumed throughout this paper that our goal is to select candidates with a high trait value. Of course, it is easy to adapt our framework if the goal is to select candidates with low trait value instead. However, sometimes it may be necessary to select candidates with an appropriate value, rather than highest or lowest value. For example, a certain level of acidity is necessary for fruits, but excessive or insufficient acidity is not preferred. In this case, model evaluation based on mean squared error (MSE) may be more suitable.</p><p>Pearson correlation is a commonly used measure in GS because it enables to predict the response to selection (provided there is no non-genetic cause of resemblance between offspring and parents) [<xref rid="pone.0128570.ref058" ref-type="bibr">58</xref>]. Because the response to selection corresponds to the change of population mean, Pearson correlation is important for breeding schemes that focus on whole population improvement. This is typical in animal breeding such as dairy cattle, where a single head of cattle contributes only little to the total production gain. However, particularly in breeding of plants that can be clonally reproduced (e.g., inbred crops or graftable fruit trees), the aim is often to produce a few excellent lines, rather than improving an entire population. In this case, Pearson correlation may not always be a good choice. Our results using 4 plant species show indeed that Pearson correlation often correlates poorly with NDCG. On the other hand, we found that Mean NDCG was the most correlated with NDCG at various positions.</p><p>Our study suggests two important messages. First, ranking methods are a promising research direction in GS. Second, NDCG and Mean NDCG can be useful evaluation measures for GS, especially in plant breeding.</p></sec></body><back><ref-list><title>References</title><ref id="pone.0128570.ref001"><label>1</label><mixed-citation publication-type="journal">
<name><surname>Meuwissen</surname><given-names>THE</given-names></name>, <name><surname>Hayes</surname><given-names>BJ</given-names></name>, <name><surname>Goddard</surname><given-names>ME</given-names></name> (<year>2001</year>) <article-title>Prediction of Total Genetic Value Using Genome-Wide Dense Marker Maps</article-title>. <source>Genetics</source>
<volume>157</volume>: <fpage>1819</fpage>&#x02013;<lpage>1829</lpage>.
<?supplied-pmid 11290733?><pub-id pub-id-type="pmid">11290733</pub-id></mixed-citation></ref><ref id="pone.0128570.ref002"><label>2</label><mixed-citation publication-type="journal">
<name><surname>Bernardo</surname><given-names>R</given-names></name>, <name><surname>Yu</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Prospects for genome-wide selection for quantitative traits in maize</article-title>. <source>Crop Science</source>
<volume>47</volume>: <fpage>1082</fpage>&#x02013;<lpage>1090</lpage>. <pub-id pub-id-type="doi">10.2135/cropsci2006.11.0690</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref003"><label>3</label><mixed-citation publication-type="journal">
<name><surname>Hayes</surname><given-names>BJ</given-names></name>, <name><surname>Lewin</surname><given-names>HA</given-names></name>, <name><surname>Goddard</surname><given-names>ME</given-names></name> (<year>2013</year>) <article-title>The future of livestock breeding: genomic selection for efficiency, reduced emissions intensity, and adaptation</article-title>. <source>Trends in Genetics</source>
<volume>29</volume>: <fpage>206</fpage>&#x02013;<lpage>214</lpage>. <pub-id pub-id-type="doi">10.1016/j.tig.2012.11.009</pub-id>
<?supplied-pmid 23261029?><pub-id pub-id-type="pmid">23261029</pub-id></mixed-citation></ref><ref id="pone.0128570.ref004"><label>4</label><mixed-citation publication-type="journal">
<name><surname>Goddard</surname><given-names>ME</given-names></name>, <name><surname>Hayes</surname><given-names>BJ</given-names></name> (<year>2007</year>) <article-title>Genomic selection</article-title>. <source>Journal of Animal Breeding and Genetics</source>
<volume>124</volume>: <fpage>323</fpage>&#x02013;<lpage>330</lpage>. <pub-id pub-id-type="doi">10.1111/j.1439-0388.2007.00702.x</pub-id>
<?supplied-pmid 18076469?><pub-id pub-id-type="pmid">18076469</pub-id></mixed-citation></ref><ref id="pone.0128570.ref005"><label>5</label><mixed-citation publication-type="journal">
<name><surname>Gonzales-Recio</surname><given-names>O</given-names></name>, <name><surname>Gianola</surname><given-names>D</given-names></name>, <name><surname>Long</surname><given-names>N</given-names></name>, <name><surname>Wiegel</surname><given-names>K</given-names></name>, <name><surname>Rosa</surname><given-names>G</given-names></name>, <name><surname>Avenda&#x000f1;o</surname><given-names>S</given-names></name>. (<year>2008</year>) <article-title>Non parametric methods for incorporating genomic information into genetic evaluation: an application to mortality in broilers</article-title>. <source>Genetics</source>
<volume>178</volume>: <fpage>2305</fpage>&#x02013;<lpage>2313</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.107.084293</pub-id>
<pub-id pub-id-type="pmid">18430951</pub-id></mixed-citation></ref><ref id="pone.0128570.ref006"><label>6</label><mixed-citation publication-type="journal">
<name><surname>Hayes</surname><given-names>BJ</given-names></name>, <name><surname>Bowman</surname><given-names>PJ</given-names></name>, <name><surname>Chamberlain</surname><given-names>AJ</given-names></name>, <name><surname>Goddard</surname><given-names>ME</given-names></name> (<year>2009</year>) <article-title>Invited review: genomic selection in dairy cattle: progress and challenges</article-title>. <source>Journal of Dairy Science</source>
<volume>92</volume>: <fpage>433</fpage>&#x02013;<lpage>443</lpage>. <pub-id pub-id-type="doi">10.3168/jds.2008-1646</pub-id>
<?supplied-pmid 19164653?><pub-id pub-id-type="pmid">19164653</pub-id></mixed-citation></ref><ref id="pone.0128570.ref007"><label>7</label><mixed-citation publication-type="journal">
<name><surname>de los Campos</surname><given-names>G</given-names></name>, <name><surname>Naya</surname><given-names>H</given-names></name>, <name><surname>Gianola</surname><given-names>D</given-names></name>, <name><surname>Crossa</surname><given-names>J</given-names></name>, <name><surname>Legarra</surname><given-names>A</given-names></name>, <name><surname>Manfredi</surname><given-names>E</given-names></name>
<etal>et al</etal> (<year>2009</year>) <article-title>Predicting quantitative traits with regression models for dense molecular markers and pedigree</article-title>. <source>Genetics</source>
<volume>182</volume>: <fpage>375</fpage>&#x02013;<lpage>385</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.109.101501</pub-id>
<?supplied-pmid 19293140?><pub-id pub-id-type="pmid">19293140</pub-id></mixed-citation></ref><ref id="pone.0128570.ref008"><label>8</label><mixed-citation publication-type="journal">
<name><surname>Piepho</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Ridge regression and extensions for genome-wide selection in maize</article-title>. <source>Crop Science</source>
<volume>49</volume>: <fpage>1165</fpage>&#x02013;<lpage>1176</lpage>. <pub-id pub-id-type="doi">10.2135/cropsci2008.10.0595</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref009"><label>9</label><mixed-citation publication-type="journal">
<name><surname>Jannink</surname><given-names>JL</given-names></name>, <name><surname>Lorenz</surname><given-names>AJ</given-names></name>, <name><surname>Iwata</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Genomic selection in plant breeding: from theory to practice</article-title>. <source>Briefings in Functional Genomics</source>
<volume>9</volume>: <fpage>166</fpage>&#x02013;<lpage>177</lpage>. <pub-id pub-id-type="doi">10.1093/bfgp/elq001</pub-id>
<?supplied-pmid 20156985?><pub-id pub-id-type="pmid">20156985</pub-id></mixed-citation></ref><ref id="pone.0128570.ref010"><label>10</label><mixed-citation publication-type="journal">
<name><surname>Crossa</surname><given-names>J</given-names></name>, <name><surname>de los Campos</surname><given-names>G</given-names></name>, <name><surname>P&#x000e9;rez</surname><given-names>P</given-names></name>, <name><surname>Gianola</surname><given-names>D</given-names></name>, <name><surname>Burgue&#x000f1;o</surname><given-names>J</given-names></name>, <name><surname>Araus</surname><given-names>JL</given-names></name>
<etal>et al</etal> (<year>2010</year>) <article-title>Prediction of genetic values of quantitative traits in plant breeding using pedigree and molecular markers</article-title>. <source>Genetics</source>
<volume>186</volume>: <fpage>713</fpage>&#x02013;<lpage>724</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.110.118521</pub-id>
<?supplied-pmid 20813882?><pub-id pub-id-type="pmid">20813882</pub-id></mixed-citation></ref><ref id="pone.0128570.ref011"><label>11</label><mixed-citation publication-type="journal">
<name><surname>Park</surname><given-names>T</given-names></name>, <name><surname>Casella</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>The Bayesian Lasso</article-title>. <source>Journal of the American Statistical Association</source>
<volume>103</volume>: <fpage>681</fpage>&#x02013;<lpage>686</lpage>. <pub-id pub-id-type="doi">10.1198/016214508000000337</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref012"><label>12</label><mixed-citation publication-type="journal">
<name><surname>Yi</surname><given-names>N</given-names></name>, <name><surname>Xu</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Bayesian lasso for quantitative trait loci mapping</article-title>. <source>Genetics</source>
<volume>179</volume>: <fpage>1045</fpage>&#x02013;<lpage>1055</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.107.085589</pub-id>
<?supplied-pmid 18505874?><pub-id pub-id-type="pmid">18505874</pub-id></mixed-citation></ref><ref id="pone.0128570.ref013"><label>13</label><mixed-citation publication-type="journal">
<name><surname>Hayashi</surname><given-names>T</given-names></name>, <name><surname>Iwata</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Em algorithm for bayesian estimation of genomic breeding values</article-title>. <source>BMC genetics</source>
<volume>11</volume>: <fpage>3</fpage>
<pub-id pub-id-type="doi">10.1186/1471-2156-11-3</pub-id>
<?supplied-pmid 20092655?><pub-id pub-id-type="pmid">20092655</pub-id></mixed-citation></ref><ref id="pone.0128570.ref014"><label>14</label><mixed-citation publication-type="journal">
<name><surname>Gianola</surname><given-names>D</given-names></name>, <name><surname>van Kaam</surname><given-names>JB</given-names></name> (<year>2008</year>) <article-title>Reproducing kernel hilbert spaces regression methods for genomic assisted prediction of quantitative traits</article-title>. <source>Genetics</source>
<volume>178</volume>: <fpage>2289</fpage>&#x02013;<lpage>2303</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.107.084285</pub-id>
<?supplied-pmid 18430950?><pub-id pub-id-type="pmid">18430950</pub-id></mixed-citation></ref><ref id="pone.0128570.ref015"><label>15</label><mixed-citation publication-type="journal">
<name><surname>Heslot</surname><given-names>N</given-names></name>, <name><surname>Yang</surname><given-names>H</given-names></name>, <name><surname>Sorrells</surname><given-names>M</given-names></name>, <name><surname>Jannink</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Genomic selection in plant breeding: A comparison of models</article-title>. <source>Crop Science</source>
<volume>52</volume>: <fpage>146</fpage>&#x02013;<lpage>160</lpage>. <pub-id pub-id-type="doi">10.2135/cropsci2011.09.0297</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref016"><label>16</label><mixed-citation publication-type="other">J&#x000e4;rvelin K, Kek&#x000e4;l&#x000e4;inen J (2000) Ir evaluation methods for retrieving highly relevant documents. In: Proceedings of the international ACM SIGIR conference on Research and development in information retrieval. pp. 41&#x02013;48.</mixed-citation></ref><ref id="pone.0128570.ref017"><label>17</label><mixed-citation publication-type="book">
<name><surname>Wahba</surname><given-names>G</given-names></name> (<year>1990</year>) <source>Spline models for observational data</source>, <volume>volume 59</volume>
<publisher-name>Siam</publisher-name>.</mixed-citation></ref><ref id="pone.0128570.ref018"><label>18</label><mixed-citation publication-type="other">Sch&#x000f6;lkopf B, Herbrich R, Smola AJ (2001) A generalized representer theorem. In: Computational learning theory. pp. 416&#x02013;426.</mixed-citation></ref><ref id="pone.0128570.ref019"><label>19</label><mixed-citation publication-type="journal">
<name><surname>Tibshirani</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>: <fpage>267</fpage>&#x02013;<lpage>288</lpage>.</mixed-citation></ref><ref id="pone.0128570.ref020"><label>20</label><mixed-citation publication-type="journal">
<name><surname>Li</surname><given-names>Z</given-names></name>, <name><surname>Sillanp&#x000e4;&#x000e4;</surname><given-names>MJ</given-names></name> (<year>2012</year>) <article-title>Estimation of quantitative trait locus effects with epistasis by variational bayes algorithms</article-title>. <source>Genetics</source>
<volume>190</volume>: <fpage>231</fpage>&#x02013;<lpage>249</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.111.134866</pub-id>
<?supplied-pmid 22042575?><pub-id pub-id-type="pmid">22042575</pub-id></mixed-citation></ref><ref id="pone.0128570.ref021"><label>21</label><mixed-citation publication-type="journal">
<name><surname>Mutshinda</surname><given-names>CM</given-names></name>, <name><surname>Sillanp&#x000e4;&#x000e4;</surname><given-names>MJ</given-names></name> (<year>2010</year>) <article-title>Extended bayesian lasso for multiple quantitative trait loci mapping and unobserved phenotype prediction</article-title>. <source>Genetics</source>
<volume>186</volume>: <fpage>1067</fpage>&#x02013;<lpage>1075</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.110.119586</pub-id>
<?supplied-pmid 20805559?><pub-id pub-id-type="pmid">20805559</pub-id></mixed-citation></ref><ref id="pone.0128570.ref022"><label>22</label><mixed-citation publication-type="journal">
<name><surname>Habier</surname><given-names>D</given-names></name>, <name><surname>Fernando</surname><given-names>RL</given-names></name>, <name><surname>Kizilkaya</surname><given-names>K</given-names></name>, <name><surname>Garrick</surname><given-names>DJ</given-names></name> (<year>2011</year>) <article-title>Extension of the bayesian alphabet for genomic selection</article-title>. <source>BMC bioinformatics</source>
<volume>12</volume>: <fpage>186</fpage>
<pub-id pub-id-type="doi">10.1186/1471-2105-12-186</pub-id>
<?supplied-pmid 21605355?><pub-id pub-id-type="pmid">21605355</pub-id></mixed-citation></ref><ref id="pone.0128570.ref023"><label>23</label><mixed-citation publication-type="journal">
<name><surname>George</surname><given-names>EI</given-names></name>, <name><surname>McCulloch</surname><given-names>RE</given-names></name> (<year>1993</year>) <article-title>Variable selection via gibbs sampling</article-title>. <source>Journal of the American Statistical Association</source>
<volume>88</volume>: <fpage>881</fpage>&#x02013;<lpage>889</lpage>. <pub-id pub-id-type="doi">10.1080/01621459.1993.10476353</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref024"><label>24</label><mixed-citation publication-type="journal">
<name><surname>Luan</surname><given-names>T</given-names></name>, <name><surname>Woolliams</surname><given-names>JA</given-names></name>, <name><surname>Lien</surname><given-names>S</given-names></name>, <name><surname>Kent</surname><given-names>M</given-names></name>, <name><surname>Svendsen</surname><given-names>M</given-names></name>, <name><surname>Meuwissen</surname><given-names>THE</given-names></name> (<year>2009</year>) <article-title>The accuracy of genomic selection in norwegian red cattle assessed by cross-validation</article-title>. <source>Genetics</source>
<volume>183</volume>: <fpage>1119</fpage>&#x02013;<lpage>1126</lpage>. <pub-id pub-id-type="doi">10.1534/genetics.109.107391</pub-id>
<?supplied-pmid 19704013?><pub-id pub-id-type="pmid">19704013</pub-id></mixed-citation></ref><ref id="pone.0128570.ref025"><label>25</label><mixed-citation publication-type="journal">
<name><surname>Breiman</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Random forests</article-title>. <source>Machine Learning</source>
<volume>45</volume>: <fpage>5</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref026"><label>26</label><mixed-citation publication-type="journal">
<name><surname>Friedman</surname><given-names>JH</given-names></name> (<year>2000</year>) <article-title>Greedy function approximation: A gradient boosting machine</article-title>. <source>Annals of Statistics</source>
<volume>29</volume>: <fpage>1189</fpage>&#x02013;<lpage>1232</lpage>. <pub-id pub-id-type="doi">10.1214/aos/1013203451</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref027"><label>27</label><mixed-citation publication-type="other">Chapelle O, Chang Y (2011) Yahoo! learning to rank challenge overview. In: Yahoo! Learning to Rank Challenge. pp. 1&#x02013;24.</mixed-citation></ref><ref id="pone.0128570.ref028"><label>28</label><mixed-citation publication-type="journal">
<name><surname>Lee</surname><given-names>CP</given-names></name>, <name><surname>Lin</surname><given-names>CJ</given-names></name> (<year>2014</year>) <article-title>Large-scale linear rankSVM</article-title>. <source>Neural Computation</source>
<volume>26</volume>: <fpage>781</fpage>&#x02013;<lpage>817</lpage>. <pub-id pub-id-type="doi">10.1162/NECO_a_00571</pub-id>
<?supplied-pmid 24479776?><pub-id pub-id-type="pmid">24479776</pub-id></mixed-citation></ref><ref id="pone.0128570.ref029"><label>29</label><mixed-citation publication-type="other">Hanley J, McNeil B (1982) The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology: 29&#x02013;36.</mixed-citation></ref><ref id="pone.0128570.ref030"><label>30</label><mixed-citation publication-type="book">
<name><surname>Herbrich</surname><given-names>R</given-names></name>, <name><surname>Graepel</surname><given-names>T</given-names></name>, <name><surname>Obermayer</surname><given-names>K</given-names></name> (<year>2000</year>) <chapter-title>Large margin rank boundaries for ordinal regression</chapter-title> In: <source>Advances in Large Margin Classifiers</source>. <publisher-name>MIT Press</publisher-name>, pp. <fpage>115</fpage>&#x02013;<lpage>132</lpage>.</mixed-citation></ref><ref id="pone.0128570.ref031"><label>31</label><mixed-citation publication-type="journal">
<name><surname>Freund</surname><given-names>Y</given-names></name>, <name><surname>Iyer</surname><given-names>R</given-names></name>, <name><surname>Schapire</surname><given-names>RE</given-names></name>, <name><surname>Singer</surname><given-names>Y</given-names></name> (<year>2003</year>) <article-title>An efficient boosting algorithm for combining preferences</article-title>. <source>Journal of Machine Learning Research</source>
<volume>4</volume>: <fpage>933</fpage>&#x02013;<lpage>969</lpage>.</mixed-citation></ref><ref id="pone.0128570.ref032"><label>32</label><mixed-citation publication-type="other">Burges C, Shaked T, Renshaw E, Lazier A, Deeds M, Hamilton N et al. (2005) Learning to rank using gradient descent. In: Proceedings of the 22Nd International Conference on Machine Learning. pp. 89&#x02013;96.</mixed-citation></ref><ref id="pone.0128570.ref033"><label>33</label><mixed-citation publication-type="other">Kendall MG (1938) A new measure of rank correlation. Biometrika.</mixed-citation></ref><ref id="pone.0128570.ref034"><label>34</label><mixed-citation publication-type="journal">
<name><surname>Cossock</surname><given-names>D</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>Statistical analysis of bayes optimal subset ranking</article-title>. <source>IEEE Transactions on Information Theory</source>
<volume>54</volume>: <fpage>5140</fpage>&#x02013;<lpage>5154</lpage>. <pub-id pub-id-type="doi">10.1109/TIT.2008.929939</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref035"><label>35</label><mixed-citation publication-type="journal">
<name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Wu</surname><given-names>Q</given-names></name>, <name><surname>Burges</surname><given-names>CJ</given-names></name> (<year>2008</year>) <article-title>Mcrank: Learning to rank using multiple classification and gradient boosting</article-title>. In: <source>Advances in Neural Information Processing Systems</source>
<volume>20</volume> pp. <fpage>897</fpage>&#x02013;<lpage>904</lpage>.</mixed-citation></ref><ref id="pone.0128570.ref036"><label>36</label><mixed-citation publication-type="other">Wu Q, Burges CJ, Svore K, Gao J (2008) Ranking, boosting, and model adaptation. Technical report, Microsoft Research.</mixed-citation></ref><ref id="pone.0128570.ref037"><label>37</label><mixed-citation publication-type="journal">
<name><surname>Liu</surname><given-names>TY</given-names></name> (<year>2009</year>) <article-title>Learning to rank for information retrieval</article-title>. <source>Foundations and Trends in Information Retrieval</source>
<volume>3</volume>: <fpage>225</fpage>&#x02013;<lpage>331</lpage>. <pub-id pub-id-type="doi">10.1561/1500000016</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref038"><label>38</label><mixed-citation publication-type="other">Li H (2011) A short introduction to learning to rank. IEICE Transactions 94-D: 1854&#x02013;1862.</mixed-citation></ref><ref id="pone.0128570.ref039"><label>39</label><mixed-citation publication-type="other">Kuo TM, Lee CP, Lin CJ (2014) Large-scale kernel rankSVM. In: Proceedings of SIAM International Conference on Data Mining.</mixed-citation></ref><ref id="pone.0128570.ref040"><label>40</label><mixed-citation publication-type="journal">
<name><surname>Liu</surname><given-names>DC</given-names></name>, <name><surname>Nocedal</surname><given-names>J</given-names></name> (<year>1989</year>) <article-title>On the limited memory bfgs method for large scale optimization</article-title>. <source>Mathematical programming</source>
<volume>45</volume>: <fpage>503</fpage>&#x02013;<lpage>528</lpage>. <pub-id pub-id-type="doi">10.1007/BF01589116</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref041"><label>41</label><mixed-citation publication-type="other">Burges CJ (2010) From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82.</mixed-citation></ref><ref id="pone.0128570.ref042"><label>42</label><mixed-citation publication-type="other">Louppe G (2014) Understanding Random Forests. Ph.D. thesis, University of Li&#x000e8;ge.</mixed-citation></ref><ref id="pone.0128570.ref043"><label>43</label><mixed-citation publication-type="journal">
<name><surname>Loudet</surname><given-names>O</given-names></name>, <name><surname>Chaillou</surname><given-names>S</given-names></name>, <name><surname>Camilleri</surname><given-names>C</given-names></name>, <name><surname>Bouchez</surname><given-names>D</given-names></name>, <name><surname>Daniel-Vedele</surname><given-names>F</given-names></name> (<year>2002</year>) <article-title>Bay-0 &#x000d7;shahdara recombinant inbred line population: a powerful tool for the genetic dissection of complex traits in arabidopsis</article-title>. <source>Theoretical and Applied Genetics</source>
<volume>104</volume>: <fpage>1173</fpage>&#x02013;<lpage>1184</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-001-0825-9</pub-id>
<?supplied-pmid 12582628?><pub-id pub-id-type="pmid">12582628</pub-id></mixed-citation></ref><ref id="pone.0128570.ref044"><label>44</label><mixed-citation publication-type="journal">
<name><surname>Lorenzana</surname><given-names>RE</given-names></name>, <name><surname>Bernardo</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Accuracy of genotypic value predictions for marker-based selection in biparental plant populations</article-title>. <source>Theoretical and Applied Genetics</source>
<volume>120</volume>: <fpage>151</fpage>&#x02013;<lpage>161</lpage>. <pub-id pub-id-type="doi">10.1007/s00122-009-1166-3</pub-id>
<?supplied-pmid 19841887?><pub-id pub-id-type="pmid">19841887</pub-id></mixed-citation></ref><ref id="pone.0128570.ref045"><label>45</label><mixed-citation publication-type="journal">
<name><surname>Broman</surname><given-names>KW</given-names></name>, <name><surname>Wu</surname><given-names>H</given-names></name>, <name><surname>Sen</surname><given-names>&#x0015a;</given-names></name>, <name><surname>Churchill</surname><given-names>GA</given-names></name> (<year>2003</year>) <article-title>R/qtl: Qtl mapping in experimental crosses</article-title>. <source>Bioinformatics</source>
<volume>19</volume>: <fpage>889</fpage>&#x02013;<lpage>890</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btg112</pub-id>
<?supplied-pmid 12724300?><pub-id pub-id-type="pmid">12724300</pub-id></mixed-citation></ref><ref id="pone.0128570.ref046"><label>46</label><mixed-citation publication-type="journal">
<name><surname>Browning</surname><given-names>BL</given-names></name>, <name><surname>Browning</surname><given-names>SR</given-names></name> (<year>2009</year>) <article-title>A unified approach to genotype imputation and haplotype-phase inference for large data sets of trios and unrelated individuals</article-title>. <source>The American Journal of Human Genetics</source>
<volume>84</volume>: <fpage>210</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/j.ajhg.2009.01.005</pub-id>
<pub-id pub-id-type="pmid">19200528</pub-id></mixed-citation></ref><ref id="pone.0128570.ref047"><label>47</label><mixed-citation publication-type="journal">
<name><surname>Hofheinz</surname><given-names>N</given-names></name>, <name><surname>Frisch</surname><given-names>M</given-names></name> (<year>2014</year>) <article-title>Heteroscedastic ridge regression approaches for genome-wide prediction with a focus on computational efficiency and accurate effect estimation</article-title>. <source>G3: Genes&#x02014;Genomes&#x02014;Genetics</source>
<volume>4</volume>: <fpage>539</fpage>&#x02013;<lpage>546</lpage>. <pub-id pub-id-type="doi">10.1534/g3.113.010025</pub-id>
<?supplied-pmid 24449687?><pub-id pub-id-type="pmid">24449687</pub-id></mixed-citation></ref><ref id="pone.0128570.ref048"><label>48</label><mixed-citation publication-type="journal">
<name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Wright</surname><given-names>M</given-names></name>, <name><surname>Kimball</surname><given-names>J</given-names></name>, <name><surname>Eizenga</surname><given-names>G</given-names></name>, <name><surname>McClung</surname><given-names>A</given-names></name>, <name><surname>Kovach</surname><given-names>M</given-names></name>
<etal>et al</etal> (<year>2010</year>) <article-title>Genomic diversity and introgression in o. sativa reveal the impact of domestication and breeding on the rice genome</article-title>. <source>PLoS One</source>
<volume>5</volume>: <fpage>e10780</fpage>
<pub-id pub-id-type="doi">10.1371/journal.pone.0010780</pub-id>
<?supplied-pmid 20520727?><pub-id pub-id-type="pmid">20520727</pub-id></mixed-citation></ref><ref id="pone.0128570.ref049"><label>49</label><mixed-citation publication-type="journal">
<name><surname>Zhao</surname><given-names>K</given-names></name>, <name><surname>Tung</surname><given-names>CW</given-names></name>, <name><surname>Eizenga</surname><given-names>GC</given-names></name>, <name><surname>Wright</surname><given-names>MH</given-names></name>, <name><surname>Ali</surname><given-names>ML</given-names></name>, <name><surname>Price</surname><given-names>AH</given-names></name>
<etal>et al</etal> (<year>2011</year>) <article-title>Genome-wide association mapping reveals a rich genetic architecture of complex traits in oryza sativa</article-title>. <source>Nature communications</source>
<volume>2</volume>: <fpage>467</fpage>
<pub-id pub-id-type="doi">10.1038/ncomms1467</pub-id>
<?supplied-pmid 21915109?><pub-id pub-id-type="pmid">21915109</pub-id></mixed-citation></ref><ref id="pone.0128570.ref050"><label>50</label><mixed-citation publication-type="journal">
<name><surname>P&#x000e9;rez-Rodr&#x000ed;guez</surname><given-names>P</given-names></name>, <name><surname>Gianola</surname><given-names>D</given-names></name>, <name><surname>Gonz&#x000e1;lez-Camacho</surname><given-names>J</given-names></name>, <name><surname>Crossa</surname><given-names>J</given-names></name>, <name><surname>Man&#x000e8;</surname><given-names>Ys</given-names></name>, <etal>et al</etal> (<year>2012</year>) <article-title>Comparison between linear and non-parametric regression models for genome-enabled prediction in wheat</article-title>. <source>G3 (Bethesda)</source>
<volume>2</volume>: <fpage>1595</fpage>&#x02013;<lpage>1605</lpage>.<pub-id pub-id-type="pmid">23275882</pub-id></mixed-citation></ref><ref id="pone.0128570.ref051"><label>51</label><mixed-citation publication-type="journal">
<name><surname>Carbonetto</surname><given-names>P</given-names></name>, <name><surname>Stephens</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies</article-title>. <source>Bayesian Analysis</source>
<volume>7</volume>: <fpage>73</fpage>&#x02013;<lpage>108</lpage>. <pub-id pub-id-type="doi">10.1214/12-BA703</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref052"><label>52</label><mixed-citation publication-type="journal">
<name><surname>Pedregosa</surname><given-names>F</given-names></name>, <name><surname>Varoquaux</surname><given-names>G</given-names></name>, <name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>V</given-names></name>, <name><surname>Thirion</surname><given-names>B</given-names></name>, <name><surname>Grisel</surname><given-names>O</given-names></name>
<etal>et al</etal> (<year>2011</year>) <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source>
<volume>12</volume>: <fpage>2825</fpage>&#x02013;<lpage>2830</lpage>.</mixed-citation></ref><ref id="pone.0128570.ref053"><label>53</label><mixed-citation publication-type="other">Buitinck L, Louppe G, Blondel M, Pedregosa F, Mueller A, Grisel O et al. (2013) Api design for machine learning software: experiences from the scikit-learn project. arXiv preprint arXiv:13090238.</mixed-citation></ref><ref id="pone.0128570.ref054"><label>54</label><mixed-citation publication-type="journal">
<name><surname>Endelman</surname><given-names>JB</given-names></name> (<year>2011</year>) <article-title>Ridge regression and other kernels for genomic selection with r package rrblup</article-title>. <source>The Plant Genome</source>
<volume>4</volume>: <fpage>250</fpage>&#x02013;<lpage>255</lpage>. <pub-id pub-id-type="doi">10.3835/plantgenome2011.08.0024</pub-id>
</mixed-citation></ref><ref id="pone.0128570.ref055"><label>55</label><mixed-citation publication-type="book">
<name><surname>Rasmussen</surname><given-names>CE</given-names></name>, <name><surname>Williams</surname><given-names>CKI</given-names></name> (<year>2005</year>) <source>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</source>. <publisher-name>The MIT Press</publisher-name>.</mixed-citation></ref><ref id="pone.0128570.ref056"><label>56</label><mixed-citation publication-type="journal">
<name><surname>Kumar</surname><given-names>GR</given-names></name>, <name><surname>Sakthivel</surname><given-names>K</given-names></name>, <name><surname>Sundaram</surname><given-names>RM</given-names></name>, <name><surname>Neeraja</surname><given-names>CN</given-names></name>, <name><surname>Balachandran</surname><given-names>S</given-names></name>, <name><surname>Rani</surname><given-names>NS</given-names></name>
<etal>et al</etal> (<year>2010</year>) <article-title>Allele mining in crops: prospects and potentials</article-title>. <source>Biotechnology advances</source>
<volume>28</volume>: <fpage>451</fpage>&#x02013;<lpage>461</lpage>. <pub-id pub-id-type="doi">10.1016/j.biotechadv.2010.02.007</pub-id>
<?supplied-pmid 20188810?><pub-id pub-id-type="pmid">20188810</pub-id></mixed-citation></ref><ref id="pone.0128570.ref057"><label>57</label><mixed-citation publication-type="journal">
<name><surname>Gupta</surname><given-names>PK</given-names></name>, <name><surname>Rustgi</surname><given-names>S</given-names></name>, <name><surname>Kulwal</surname><given-names>PL</given-names></name> (<year>2005</year>) <article-title>Linkage disequilibrium and association studies in higher plants: present status and future prospects</article-title>. <source>Plant molecular biology</source>
<volume>57</volume>: <fpage>461</fpage>&#x02013;<lpage>485</lpage>. <pub-id pub-id-type="doi">10.1007/s11103-005-0257-z</pub-id>
<?supplied-pmid 15821975?><pub-id pub-id-type="pmid">15821975</pub-id></mixed-citation></ref><ref id="pone.0128570.ref058"><label>58</label><mixed-citation publication-type="book">
<name><surname>Falconer</surname><given-names>DS</given-names></name> (<year>1981</year>) <source>Introduction to Quantitative Genetics</source>. <publisher-name>Longmans Green</publisher-name>, <edition>2th edition</edition>.</mixed-citation></ref></ref-list></back></article>