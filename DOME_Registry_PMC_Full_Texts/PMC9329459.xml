<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" dtd-version="1.3"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Nat Commun</journal-id><journal-id journal-id-type="iso-abbrev">Nat Commun</journal-id><journal-title-group><journal-title>Nature Communications</journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">9329459</article-id><article-id pub-id-type="pmid">35896542</article-id><article-id pub-id-type="publisher-id">32007</article-id><article-id pub-id-type="doi">10.1038/s41467-022-32007-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ProtGPT2 is a deep unsupervised language model for protein design</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4172-8201</contrib-id><name><surname>Ferruz</surname><given-names>Noelia</given-names></name><address><email>noelia.ferruz-capapey@uni-bayreuth.de</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9077-6010</contrib-id><name><surname>Schmidt</surname><given-names>Steffen</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8250-9462</contrib-id><name><surname>H&#x000f6;cker</surname><given-names>Birte</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.7384.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 0467 6972</institution-id><institution>Department of Biochemistry, </institution><institution>University of Bayreuth, </institution></institution-wrap>Bayreuth, Germany </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.7384.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 0467 6972</institution-id><institution>Computational Biochemistry, </institution><institution>University of Bayreuth, </institution></institution-wrap>95447 Bayreuth, Germany </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5319.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2179 7512</institution-id><institution>Present Address: Institute of Informatics and Applications, </institution><institution>University of Girona, </institution></institution-wrap>Girona, Spain </aff></contrib-group><pub-date pub-type="epub"><day>27</day><month>7</month><year>2022</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>7</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>13</volume><elocation-id>4348</elocation-id><history><date date-type="received"><day>1</day><month>4</month><year>2022</year></date><date date-type="accepted"><day>13</day><month>7</month><year>2022</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2022</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.</p></abstract><abstract id="Abs2" abstract-type="web-summary"><p id="Par2">Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Here the authors apply some of the latest advances in natural language processing, generative Transformers, to train ProtGPT2, a language model that explores unseen regions of the protein space while designing proteins with nature-like properties.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Machine learning</kwd><kwd>Computational science</kwd></kwd-group><funding-group><award-group><funding-source><institution>AGAUR. Beatriu de Pin&#x000f3;s Program MSCA Actions.</institution></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2022</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Natural language processing (NLP) has seen extraordinary advances in recent years. Large pre-trained language models have drastically transformed the NLP field and with it, many of the tools we use in our daily lives, such as chatbots, smart assistants, or translation machines. Analogies between protein sequences and human languages have long been noted by us and others<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Protein sequences can be described as a concatenation of letters from a chemically defined alphabet, the natural amino acids, and like human languages, these letters arrange to form secondary structural elements (&#x0201c;words&#x0201d;), which assemble to form domains (&#x0201c;sentences&#x0201d;) that undertake a function (&#x0201c;meaning&#x0201d;). One of the most attractive similarities is that protein sequences, like natural languages, are information-complete: they store structure and function entirely in their amino acid order with extreme efficiency. With the extraordinary advances in the NLP field in understanding and generating language with near-human capabilities, we hypothesized that these methods open a new door to approach protein-related problems from sequence alone, such as protein design.</p><p id="Par4">Although protein sequences and human languages are not without dissimilarities, their analogies have stimulated applying NLP methods to solve protein research problems for decades<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Supervised NLP methods, where the input sequences are trained jointly with their labels to produce predictive models, have been applied to various tasks, such as detecting structural similarity or predicting stability<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>. A remarkable collection of supervised language models applied to biomolecules is available in the BioSeq-BLM platform<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. Nevertheless, since the inception of the Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, unsupervised learning, where the training occurs on unlabeled data, has emerged as a versatile tool for language modeling. Several Transformer-based models, such as TCR-BERT<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, epiBERTope<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, ESM<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, ProtTrans<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, or ProteinBERT<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, have shown to be very competitive with other methods<sup><xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>. Most of these models use BERT-like<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> architectures and denoising autoencoding training objectives, i.e., they are pre-trained by corrupting the input tokens in some way and trying to reconstruct the original sentence<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Although these models could be adjusted for generation<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, their most direct application is sequence embedding.</p><p id="Par5">Another important branch of language models benefits from autoregressive training, i.e., models are trained to predict subsequent words given a context. These models, the most well-known of which are possibly the GPT-x series<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, excel at generating long, coherent text&#x02014;sometimes to the extent that much debate has been raised about their potential misuse<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. Protein autoregressive language models, such as ProGen<sup><xref ref-type="bibr" rid="CR19">19</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref></sup>, RITA<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>,&#x000a0;and&#x000a0;DARK<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> have also been studied, and show the potential of autoregressive Transformers for protein design. Motivated by these works and the ever-increasing capabilities of English-speaking models such as the GPT-x series, we wondered whether we could train a generative model to (i) effectively learn the protein language, (ii) generate fit, stable proteins, and (iii) understand how these sequences relate to natural ones, including whether they sample unseen regions of the protein space.</p><p id="Par6">Here, we introduce ProtGPT2, an autoregressive Transformer model with 738 million parameters capable of generating de novo protein sequences in a high-throughput fashion. ProtGPT2 has effectively learned the protein language upon being trained on about 50 non-annotated million sequences spanning the entire protein space. ProtGPT2 generates protein sequences with amino acid and disorder propensities on par with natural ones while being &#x0201c;evolutionarily&#x0201d; distant from the current protein space. Secondary structure prediction calculates 88% of the sequences to be globular, in line with natural proteins. Representation of the protein space using similarity networks reveals that ProtGPT2 sequences explore &#x02018;dark&#x02019; areas of the protein space by expanding natural superfamilies. The generated sequences show predicted stabilities and dynamic properties akin to their natural counterparts. Since ProtGPT2 has been already pre-trained, it can be used to generate sequences on standard workstations in a matter of seconds or be further finetuned on sequence sets of a user&#x02019;s choice to augment specific protein families. The model and datasets are available in the HuggingFace repository<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> at (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/nferruz/ProtGPT2">https://huggingface.co/nferruz/ProtGPT2</ext-link>). Since protein design has an enormous potential to solve problems in fields ranging from biomedical to environmental sciences<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>, we believe that ProtGPT2 is a timely advance towards efficient high-throughput protein engineering and design.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Learning the protein language</title><p id="Par7">The major advances in the NLP field can be partially attributed to the scale-up of unsupervised language models. Unlike supervised learning, which requires the labeling of each data point, self-supervised (or often named unsupervised) methods do not require annotated data, thus promoting the use of ever-growing datasets such as Wikipedia or the C4 Corpus<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Given both the growth of protein sequence databases and the lack of annotation for a significant part of the protein space, protein sequences have become great candidates for unsupervised training<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup> and now offer the opportunity to encode and generate protein sequences.</p><p id="Par8">To achieve this goal, we trained a Transformer<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> to produce a model that generates protein sequences. Language models are statistical models that assign probabilities to words and sentences. We are interested in a model that assigns high probability to sentences (W) that are semantically and syntactically correct or fit and functional, in the case of proteins. Because we are interested in a generative language model, we trained the model using an autoregressive strategy. In autoregressive models, the probability of a particular token or word (w<sub>i</sub>) in a sequence depends solely on its context, namely the previous tokens in the sequence. The total probability of a sentence (W) is the combination of the individual probabilities for each word (w<sub>i</sub>):<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p\,\left(W\right)=\mathop{\prod }\limits_{i}^{n}p\left({w}_{i}|{w}_{ &#x0003c; i}\right)$$\end{document}</tex-math><mml:math id="M2"><mml:mi>p</mml:mi><mml:mspace width="0.25em"/><mml:mfenced close=")" open="("><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x0220f;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02223;</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2022_32007_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par9">We trained the Transformer by minimizing the negative log-likelihood over the entire dataset. More intuitively, the model must learn the relationships between a word w<sub>i</sub> &#x02014;or amino acid&#x02014;and all the previous ones in the sequence, and must do so for each sequence <italic>k</italic> in dataset (D):<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{{{{{\mathscr{L}}}}}}}}}_{{{{{{{{{\mathrm{CLM}}}}}}}}}}=\,-\mathop{\sum }\limits_{k=1}^{D}{log}\,{p}_\theta\left({w}_{i}^{k}|{w}_{ &#x0003c; i}^{k}\right)$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">CLM</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mo>&#x02212;</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003b8;</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#x02223;</mml:mo><mml:msubsup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41467_2022_32007_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par10">To learn the protein language, we used UniRef50 (UR50) (version 2021_04), a clustering of UniProt at 50% identity. We chose this dataset versus larger versions of UniParc (such as UR100) as it was previously shown to improve generalization and performance for the ESM Transformers<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Uniref50&#x02019;s sequences populate the entire protein space, including the dark proteome, regions of the protein space whose structure is not accessible via experimental methods or homology modeling<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. For evaluation, we randomly excluded 10% of the dataset sequences&#x02014;these sequences are not seen by ProtGPT2 during the training process. The final training datasets contained 44.9 and 4.9 million sequences for training and evaluation, respectively. We tokenized our dataset using the BPE algorithm<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The final model is a decoder-only architecture of 36 layers and 738 million parameters.</p><p id="Par11">Analogous to the GLUE benchmark<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>&#x02014;a collection of tools that computational linguists use to evaluate language models on different tasks such as question answering or translation&#x02014;we also developed a series of extrinsic tests to assess the quality of ProtGPT2-generated sequences. The following sections elaborate on how ProtGPT2 generates de novo sequences with properties that resemble modern protein space.</p></sec><sec id="Sec4"><title>Statistical sampling of natural amino acid propensities</title><p id="Par12">Autoregressive language generation is based on the assumption that the probability distribution of a sequence can be decomposed into the product of conditional next-word distributions (Eq.&#x000a0;<xref rid="Equ1" ref-type="">1</xref>). However, there is still considerable debate about the best decoding strategy to emit sequences from a model<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. It is not uncommon that well-trained generic language models that perform well in GLUE tasks generate incoherent gibberish or repetitive text depending on the sampling procedure<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. We briefly summarize here the most used sampling strategies for language generation that we applied in this study.</p><p id="Par13">Greedy search strategy selects the word with the highest probability at each timestep. Although algorithmically simple, the generated sequences are deterministic and soon also become repetitive (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1a</xref>). Beam search tries to alleviate this problem by retaining the most probable candidates, although the resulting texts still suffer from repetitiveness and are not as surprising as those from humans, which tend to alternate low and high probability tokens<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1b</xref>). Lastly, random sampling moves away from deterministic sampling by randomly picking a word out of the top-k most probable ones (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1c,&#x000a0;d</xref>).<fig id="Fig1"><label>Fig. 1</label><caption><title>Examples with different sampling parameters for GPT2-large after the context input: &#x02018;ten best things to do in Lisbon&#x02019; (a&#x02013;d) and ProtGPT2 without context (e&#x02013;h).</title><p>While greedy and beam search produce repetitive sentences (<bold>a</bold>, <bold>b</bold>) and protein sequences (<bold>e</bold>, <bold>f</bold>), sampling generates creative texts, which, however, can be degenerate (<bold>c</bold>) or not sample natural sequence propensities (<bold>g</bold>) for small values of k. Larger values of k produce quality text (<bold>d</bold>) and sequences whose propensities match natural ones. Repetitive and degenerative text are shown in blue and orange, respectively.</p></caption><graphic xlink:href="41467_2022_32007_Fig1_HTML" id="d32e621"/></fig></p><p id="Par14">In a recent study, Holtzman et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> investigated several sampling strategies to find the best parameters for text generation. Inspired by this work, we systematically generated sequences following different sampling strategies and parameters (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). To assess what sampling procedure generates the most natural-like sequences, we compared the amino acid propensities of the generated set to that found in natural protein sequences&#x000a0;(Methods). As stated by Hoffmann et al., we also observe greedy and beam search to produce repetitive, deterministic sequences, while random sampling dramatically improves the generated propensities (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>). Moreover, we also observe that high values of k are needed to generate sequences that resemble natural ones, i.e., our best results occur in the range of <italic>k</italic>&#x02009;&#x0003e;&#x02009;800 and we specifically chose <italic>k</italic>&#x02009;=&#x02009;950 in this work (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1h</xref>). As observed with other generative models<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, our sampling improves when applying a repetition penalty of 1.2. Consequently, we used these sampling parameters for the rest of this work.</p></sec><sec id="Sec5"><title>ProtGPT2 sequences encode globular proteins</title><p id="Par15">In order to evaluate ProtGPT2&#x02019;s generated sequences in the context of sequence and structural properties, we created two datasets, one with sequences generated from ProtGPT2 using the previously described inference parameters, and the other with randomly chosen sequences from UR50. Each dataset consists of 10,000 sequences. Since ProtGPT2 was trained in an unsupervised manner, i.e., without including functional annotations, our analyses focus on validating the structural and biochemical properties of ProtGPT2 sequences.</p><p id="Par16">We first studied disordered and secondary structural content in the datasets. It has been previously shown that approximately 14% of the proteins found in bacteria and archaea are disordered<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. To this end, we ran IUPred3<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> to analyze if the ProtGPT2-generated sequences are more prone to be disordered than a set of natural sequences. Interestingly, our analysis shows a similar number of globular domains among the ProtGPT2-generated sequences (87.59%) and natural sequences (88.40%). Several methods have been reported that detect short intrinsically disorder regions<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Since our goal is to provide high-level comparisons of globularity and prevalent disorder across datasets, we further performed an analysis of the protein sequences at the amino acid level using IUPred3. Remarkably, our results show a similar distribution of ordered/disordered regions for the two datasets, with 79.71 and 82.59% of ordered amino acids in the ProtGPT2 and natural datasets, respectively (Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Disorder and secondary structure predictions of the natural and ProtGPT2 dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Natural dataset</th><th>ProtGPT2 dataset</th></tr></thead><tbody><tr><td><bold>IUPred3 (globular domains)</bold></td><td>88.40%</td><td>87.59%</td></tr><tr><td><bold>Ordered content</bold></td><td>79.71%</td><td>82.59%</td></tr><tr><td><bold>Alpha-helical content</bold></td><td>45.19%</td><td>48.64%</td></tr><tr><td><bold>Beta-sheet content</bold></td><td>41.87%</td><td>39.70%</td></tr><tr><td><bold>Coil content</bold></td><td>12.93%</td><td>11.66%</td></tr></tbody></table><table-wrap-foot><p>(<italic>n</italic>&#x02009;=&#x02009;10,000 independent sequences/dataset).</p></table-wrap-foot></table-wrap></p><p id="Par17">We next investigated whether the similarities in disorder are a consequence of equivalent secondary structure element content. To this end, we computed PSIPRED<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> predictions for the ProtGPT2 and natural sequence datasets. The natural sequences display alpha-helical, beta-sheet, and coil contents of 45.19, 41.87, and 12.93%, respectively. The ProtGPT2 dataset presented percentages of 48.64, 39.70, and 11.66%, respectively.</p><p id="Par18">These results indicate that ProtGPT2 generates sequences that resemble globular domains whose secondary structure contents are comparable to those found in the natural space.</p></sec><sec id="Sec6"><title>ProtGPT2 sequences are similar yet distant to natural ones</title><p id="Par19">Proteins have diversified immensely in the course of evolution via point mutations as well as duplication and recombination. Using sequence comparisons, it is, however, possible to detect similarities between two proteins even when their sequences have significantly diverged. We wondered how related ProtGPT2 sequences are to natural ones. To this end, we utilized HHblits, a sensitive remote homology detection tool that uses profile hidden Markov models to search query sequences against a database<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. We searched for homologs of the 10,000 sequences in ProtGPT2&#x02019;s dataset against the Uniclust30 database<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. For comparison purposes, we also performed the same search with the natural dataset using the same settings. In addition, to analyze how completely random sequences would compare against ProtGPT2 ones, we also crafted a third dataset by randomly concatenating the 25 letters in the vocabulary.</p><p id="Par20">Because we want to provide a quantitative comparison of the datasets&#x02019; relatedness to modern protein space, we produced identity vs sequence length plots (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). In detail, for each of the alignments found in Uniclust30, we depict the one with the highest identity and length. As a reference point in this sequence identity-length space, we use the HSSP curve<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>, a boundary set to define the confidence of protein sequence relatedness. Proteins whose identity falls below this curve, an area known as the &#x0201c;twilight zone&#x0201d;, do not necessarily have similar 3D structures nor are likely homologous. Since the sequences in the ProtGPT2 and random datasets are not the consequence of protein evolution, we use the curve as a well-known threshold to compare the datasets.<fig id="Fig2"><label>Fig. 2</label><caption><title>Pairwise sequence identities vs. alignment length for each of the datasets (a: natural (yellow), b: ProtGPT2 (green), and c: random (red)) as computed with HHblits against the Uniclust30 database.</title><p>The lines depicted in red on each plot represent the HSSP curve, which we use as a reference to compare the three datasets<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Each plot shows a hexbin compartmentalization of the best-scoring identities and their distributions. While natural (<bold>a</bold>) and protGPT2 (<bold>b</bold>) sequences show similar percentages below the curve, 93% of the sequences in the random dataset (<bold>c</bold>) do not have significantly similar sequences in the Uniclust30 database. Natural and ProtGPT2 datasets show significant differences in the high-identity range (<italic>n</italic>&#x02009;=&#x02009;10,000 independent sequences/dataset).</p></caption><graphic xlink:href="41467_2022_32007_Fig2_HTML" id="d32e811"/></fig></p><p id="Par21">When looking at the distribution of hits above and below the curve, we observe that HHblits finds many hits in the Uniclust30 database that are related to the dataset of natural sequences (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a</xref>). Specifically, out of the 10,000 dataset sequences, 9621 (96.2%) showed identities above the HSSP curve. Similarly, 9295 ProtGPT2-generated sequences (93%) also have counterparts in the Uniclust30 database that align above the HSSP curve (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2b</xref>). Conversely, 93% of the randomly generated sequences fall below this threshold (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2c</xref>). Despite these similar patterns for the natural and ProtGPT2 datasets, the two datasets show differences in their distribution of hits. With a one-standard-deviation range of 31.5&#x02013;69.7%, the natural dataset has a higher mean identity than the ProtGPT2 set, with a range of 32.9&#x02013;64.1% (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2a, b</xref>). The differences between the natural and ProtGPT2 sequence distributions are not statistically significant (<italic>p</italic> value &#x0003c;0.05 Kolmogorov&#x02013;Smirnoff). However, substantial differences between the natural and ProtGPT2 datasets occur in the high-identity range (&#x0003e;90%). Although 365 sequences in the ProtGPT2 dataset have high-identity sequences in Uniclust30, they correspond in all cases to alignments below 15 amino acids, whereas the natural dataset displays 760 sequences over 90% with an alignment length in the one-standard-deviation range of 14.8&#x02013;77.3 amino acids. These results suggest that ProtGPT2 effectively generates sequences that are distantly related to natural ones but are not a consequence of memorization and repetition.</p></sec><sec id="Sec7"><title>ProtGPT2 generates ordered structures</title><p id="Par22">One of the most important features when designing de novo sequences is their ability to fold into stable ordered structures. We have evaluated the potential fitness of ProtGPT2 sequences in comparison to natural and random sequences in the context of AlphaFold predictions, Rosetta Relax scores, and molecular dynamics (MD) simulations.</p><p id="Par23">AlphaFold<sup><xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> produces a per-residue estimate of its confidence on a scale from 0&#x02013;100 (pLDDT). This score has been shown to correlate with order<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>: Low scores (pLDDT &#x0003e; 50) tend to appear in disordered regions, while excellent scores (pLDDT &#x0003e; 90) appear in ordered ones<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Here we produced five structure predictions per sequence. The mean pLDDT of the dataset is 63.2 when taking the best-scoring structure per sequence and 59.6 when averaging across all five predictions per sequence. Moreover, 37% of sequences show pLDDT values over 70, in agreement with other recent studies<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. A representation of all data points is shown in Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2a</xref>. Since pLDDT scores are a proxy for structural order, we turned to the natural and random datasets to see how they compare to ProtGPT2 sequences. In agreement with previous works, 66% of the sequences in the natural dataset were predicted with pLDDT values greater than 70<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, giving an average value of 75.3 for the whole dataset (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2b</xref>). In contrast, the predictions in the random dataset revealed a mean pLDDT value of 44, with only 7.4% of sequences with pLDDT values over 70 (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">2c</xref>).</p><p id="Par24">To further validate the quality of the model, we performed Rosetta-RelaxBB runs on the three datasets<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Rosetta Relax performs a Monte Carlo optimization over the Rosetta energy function, which results in different backbone and rotamer conformations. Lower Rosetta Energy conformers correlate with more relaxed structures<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. The most recent Rosetta Energy Forcefield (REF2015) strongly correlates with experimental variables such as heat capacity, density, and enthalpy<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. This scoring function reflects the thermodynamic stability of one static protein conformation. Here we have performed Rosetta Relax experiments for the 30,000 sequences of the three datasets (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3a</xref>). A broad rule of thumb is that the total score (Rosetta Energy Units, REU) should lie between &#x02212;1 and &#x02212;3 per residue<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. We observe such distribution in the natural and ProtGPT2 datasets, with averages of 1.90 and 1.73 REU/residue, respectively. As expected, the dataset of random sequences showed an average value of 0.13 REU/residue.<fig id="Fig3"><label>Fig. 3</label><caption><title>Comparison of Rosetta and molecular dynamics calculations among the three datasets.</title><p><bold>a</bold> Average Rosetta energy units per residue for the three datasets. AlphaFold prediction structures were used as input for the Rosetta RelaxBB protocol. 10,000 structures were run per dataset, one replica per system. <bold>b</bold> Root mean square deviation (RMSD) distribution for each MD dataset as computed by averaging RMSDs independently for each trajectory, represented as a boxplot. Twelve structures were simulated per dataset, three replicas per system. In both plots, the median is indicated as a black line; boxes depict the interquartile range (IQR), and whiskers represent 1.5 x IQR. Points outside this range are displayed as individual data points.</p></caption><graphic xlink:href="41467_2022_32007_Fig3_HTML" id="d32e906"/></fig></p><p id="Par25">We further tested if ProtGPT2 sequences show similar dynamic properties as natural sequences. Proteins are dynamic entities; without their inherent flexibility, they would not be capable of interacting with other biomolecules and performing their functions in the cell<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. To evaluate whether ProtGPT2 sequences show flexibility patterns in the same range as natural proteins, we randomly selected 12 sequences per dataset and ran three replicas of molecular dynamics (MD) of 100&#x02009;ns each, totaling 108 trajectories and an aggregate time of 10.8 microseconds (Methods). To ensure that the dynamics observed during the simulations were not an artifact of different pLDDT values&#x02014;and hence possible different disorder predictions&#x02014;we made sure that differences among dataset-pLDDT mean values were not statistically different (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">3</xref>). The Root Mean Square Deviation means for each of the trajectories in the natural and ProtGPT2 datasets resulted in average values of 2.93 and 3.12&#x02009;&#x000c5;, respectively (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3b</xref>). As expected, the random sequences showed significant deviations during the trajectories, with an average of 9.41&#x02009;&#x000c5;. While ProtGPT2 sequences showed higher values than the natural ones, the distributions are not significantly different (Mann&#x02013;Whitney <italic>U</italic>-test, <italic>p</italic> value 0.39). The results indicate that ProtGPT2 sequences might have similar dynamic properties as proteins found in nature. The complete list of the trajectories&#x02019; RMSD is presented in Supplementary Figs.&#x000a0;<xref rid="MOESM1" ref-type="media">4,</xref><xref rid="MOESM1" ref-type="media">5</xref>.</p></sec><sec id="Sec8"><title>ProtGPT2 transcends the boundaries of the current protein space</title><p id="Par26">Several studies tried to reduce the large dimensionality of protein sequences into a few discernible dimensions for their analysis. Most representation methods consist of (i) hierarchical classifications of protein structures such as the ECOD and CATH databases<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>, (ii) Cartesian representations<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>, and similarity networks<sup><xref ref-type="bibr" rid="CR52">52</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>. We recently represented the structural space in a network that showed proteins as nodes, linked when they have a homologous and structurally-similar fragment in common<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> and made the results available in the Fuzzle database<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. The network represented 25,000 domains from the seven major SCOP classes and showed that the modern known protein space has both connected and &#x0201c;island-like&#x0201d; regions.</p><p id="Par27">It is implausible that evolution has explored all possible protein sequences<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Therefore, the challenge has been posed whether we can design proteins that populate unexplored&#x02014;or dark&#x02014;regions of the protein space and if, by doing so, we can design novel topologies and functions<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Here, we integrated the ProtGPT2 sequences into our network representation of the protein space. To this end, we generated an HMM profile for each SCOPe2.07 and ProtGPT2 sequence, compared them in an all-against-all fashion using HHsearch and represented the networks with Protlego<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. To avoid that specific sequences with several alignments end up represented by the same node in the network, we duplicate entries with two non-overlapping alignments, as previously described<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p><p id="Par28">The network contains 59,612 vertices and 427,378 edges, comprising 1847 components or &#x02018;island-like&#x02019; clusters (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). The major component accumulates more than half of the nodes (30,690)&#x02014;a number significantly higher than the number observed in a network produced with the same settings but excluding ProtGPT2 sequences (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">6</xref>)&#x02014; strongly suggesting that ProtGPT2 generates sequences that bridge separate islands in protein space. We select six examples across different areas of the network from topologically different SCOPe classes to showcase ProtGPT2 sequences at the structural level (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). In particular, we report an all-&#x003b2; (<bold>751</bold>), two &#x003b1;/&#x003b2; (<bold>4266</bold>, <bold>1068</bold>), one membrane protein (<bold>4307</bold>), an &#x003b1;&#x02009;+&#x02009;&#x003b2; (<bold>486</bold>) and all-&#x003b1; (<bold>785</bold>) structures. These structures illustrate ProtGPT2&#x02019;s versatility at generating de novo structures. For each case, we searched the most similar protein structure found in the PDB database using FoldSeek<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. ProtGPT2 generates well-folded all-&#x003b2; structures (<bold>751</bold>, <bold>4307</bold>), which despite recent impressive advances<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>, have for long remained very challenging<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. ProtGPT2 also produces membrane proteins (<bold>4307</bold>), which pose a difficult target for protein design due to the challenges at specifying structure within the membrane and the laborious experimental characterizations<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. Besides the generation of natural fold representatives, ProtGPT2 also produces previously unreported topologies. For example, we report protein <bold>4266</bold>, whose topology does not match any of the currently reported structures in the PDB, with a low DALI Z-score of 5.4 and an RMSD of 3.0&#x02009;&#x000c5; to PDB <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.2210/pdb5B48/pdb">5B48</ext-link> over 67 residues (identity 9%).<fig id="Fig4"><label>Fig. 4</label><caption><title>An overview of the protein space and examples of proteins generated by ProtGPT2.</title><p>Each node represents a sequence. Two nodes are linked when they have an alignment of at least 20 amino acids and 70% HHsearch probability. Colors depict the different SCOPe classes, and ProtGPT2 sequences are shown in white. As examples, we select proteins of each of the major five SCOP classes: all-&#x003b2; structures (751), &#x003b1;/&#x003b2; (4266 and 1068), membrane protein (4307), &#x003b1;+&#x003b2; (486), and all-&#x003b1; (785). The selected structures are colored according to the class of their most similar hit. The structures were predicted with AlphaFold, and we indicate the code of the most similar structure in the PDB as found by FoldSeek<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>, except for protein 4266, where no structures were found.</p></caption><graphic xlink:href="41467_2022_32007_Fig4_HTML" id="d32e1059"/></fig></p><p id="Par29">Nevertheless, possibly the most remarkable property of ProtGPT2 sequences is their significant deviation from all previously designed de novo structures, which often feature idealized topologies with loops and minimal structural elements. De novo proteins have the advantage of not carrying any evolutionary history and are thus amenable as a scaffold for virtually any function, but in practice, the lack of embodiments and longer loops hamper the design of crevices, surfaces, and cavities&#x02014;necessary for the interaction with other molecules and function realization. ProtGPT2 sequences resemble the complexity of natural proteins, with multifaceted surfaces capable of allocating interacting molecules and substrates, thus paving the way for functionalization. In Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, we show structures <bold>486</bold> and <bold>1060</bold>, two examples of such complex structures. In particular, <bold>1068</bold> shows a TIM-barrel fold, a topology which to date has met impressive success in de novo design<sup><xref ref-type="bibr" rid="CR62">62</xref>&#x02013;<xref ref-type="bibr" rid="CR64">64</xref></sup>, but whose idealized structure has nevertheless proven challenging to extend via additional secondary elements and longer loops<sup><xref ref-type="bibr" rid="CR65">65</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup>.</p></sec><sec id="Sec9"><title>Preserved functional hotspots</title><p id="Par30">Visual inspection of the structural superimposition of the best hits found with FoldSeek revealed several instances where the sidechains of ligand-interacting residues are conserved. Two examples are shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. The natural structure most similar to sequence <bold>357</bold> (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5a</xref>) corresponds to PDB code <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.2210/pdb1X0P/pdb">1X0P</ext-link> (chain A), a blue-light sensor domain that binds FAD. When superimposing the structures, we observe that <bold>357</bold> has retained the sidechain binding hotspots, with three residues identical (D169, Q150, and N131) and two different but capable of forming the same interactions, Lysine at position R165 and Histidine at position K127. Sequence <bold>475</bold> (Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5b</xref>) is most similar to PDB code <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.2210/pdb5M1T/pdb">5M1T</ext-link> (chain A), a phosphodiesterase that folds into a TIM-barrel and binds to the bacterial second messenger cyclic di-3&#x02032;,5&#x02032;-guanosine monophosphate (PDB three-letter code C2E). Out of the five sidechain-interacting residues, the ProtGPT2 sequence preserves three residues (Q455, R473, and E469), and includes one substitution for another residue capable of hydrogen-bonding (aspartic acid for Q513). It is remarkable to note that ProtGPT2 has generated these sequences in a zero-shot fashion, i.e., without further finetuning in these two particular folds. These results have impactful consequences for protein engineering because ProtGPT2 appears to preserve binding positions in the generated sequences, despite the low identities (31.1 and 29.2% for 357 and 45, respectively), and can be used to augment the repertoires of specific folds and families.<fig id="Fig5"><label>Fig. 5</label><caption><title>Superimposition of the predicted structures for sequences 357 and 475 and the respective top scoring proteins in FoldSeek.</title><p><bold>a</bold> Structural alignment of 357 with pdb <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.2210/pdb1X0P/pdb">1X0P</ext-link> (chain A, blue). Shown are five residues in 1X0P that interact via their sidechains with the ligand FAD. Of these, three are identical in <bold>357</bold>, and another two correspond to substitutions to the same amino acid type (R165 to lysine and Q150 to histidine). <bold>b</bold> Structural alignment of <bold>475</bold> with pdb <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.2210/pdb5M1T/pdb">5M1T</ext-link> (chain A) depicting five sidechain-interacting residues with ligand C2E. All amino acids in <bold>475</bold> are conserved except for residue R614, which was substituted by a glycine. The PDB structures are shown in color with their sidechains in a thinner representation.</p></caption><graphic xlink:href="41467_2022_32007_Fig5_HTML" id="d32e1158"/></fig></p></sec></sec><sec id="Sec10" sec-type="discussion"><title>Discussion</title><p id="Par31">The design of de novo proteins harnessing artificial intelligence methods has been meeting incredible success in the last 2 years<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR67">67</xref>,<xref ref-type="bibr" rid="CR68">68</xref></sup>. Motivated by the unprecedented advances in NLP, we have implemented a generative language model, ProtGPT2, which has effectively learned the protein language. ProtGPT2 can generate sequences that are distantly related to natural ones and whose structures resemble the known structural space, with non-idealized complex structures. Since ProtGPT2 has been trained on the entire sequence space, the sequences produced by the model can sample any region, including the dark proteome and areas traditionally regarded as very challenging in the protein design field, such as all-&#x003b2; structures and membrane proteins. Visual superimposition of ProtGPT2 proteins with distantly related natural protein structures reveals that ProtGPT2 has also captured functional determinants, preserving ligand-binding interactions. As the design of artificial proteins can solve many biomedical and environmental problems, we see extraordinary potential in our protein language model. ProtGPT2 designs fit globular proteins in a matter of seconds without requiring further training on a standard workstation. ProtGPT2 can be conditioned towards a particular family, function, or fold by finetuning the model on a set of sequences of a user&#x02019;s choice. In this context, ProtGPT2 will enable the screening for proteins with similarities to natural proteins in order to improve, fine-tune or alter a specific biochemical function of a natural protein. Large-scale screening of ProtGPT2-designed protein libraries might identify proteins with folds not captured in structural databases and functions that have no related counterpart in the natural space. ProtGPT2 constitutes a big step forward towards efficient protein design and generation, and lays the groundwork for future experimental studies exploring the structural and functional parameters of designed proteins, and their subsequent real-world applications. Future efforts include the inclusion of conditional tags, which will enable the controlled generation of specific functions.</p></sec><sec id="Sec11"><title>Methods</title><sec id="Sec12"><title>Vocabulary encoding</title><p id="Par32">We use a BPE<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> tokenizer to train the vocabulary of our dataset. BPE is a sub-word tokenization algorithm that finds the most frequently used word roots, ensuring better performance than one-hot tokenization and avoiding the out-of-vocabulary problem. Given the size of Uniref50, we used Swiss-Prot (2021_04) containing &#x0003e;0.5&#x02009;M sequences to train our tokenizer. Following the training strategy of GPT2<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, our final vocabulary contained 50,256 tokens that correspond to the most widely reused oligomers in protein space, with an average size of four amino acids per token (Supplementary Fig.&#x000a0;<xref rid="MOESM1" ref-type="media">1</xref>). Learned positional embeddings were used as in the original GPT2.</p></sec><sec id="Sec13"><title>Dataset preparation</title><p id="Par33">We took Uniref50 version 2021_04 as the dataset for training, containing 49,874,565 sequences. 10% of the sequences were randomly selected to produce the validation dataset. The final training and validation datasets contained 44.88 and 4.99 million sequences, respectively. We produced two datasets, one using a block size of 512 tokens, and another one with 1024 tokens. The results shown in this work correspond to a model trained with a block size of 512 tokens.</p></sec><sec id="Sec14"><title>Model pre-training</title><p id="Par34">We use a Transformer decoder model as architecture for our training which processes input sequences tokenized with a BPE strategy. The model uses during training the original dot-scale self-attention as introduced by ref. <xref ref-type="bibr" rid="CR7">7</xref>. The model consist of 36 layers with a model dimensionality of 1280. The architecture matches that of the previously released GPT2-large Transformer<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, which was downloaded from HuggingFace<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Model weights were reinitialized prior to training. The model was optimized using Adam (&#x003b2;<sub>1</sub>&#x02009;=&#x02009;0.9, &#x003b2;<sub>2</sub>&#x02009;=&#x02009;0.999) with a learning rate of 1e-03. For our main model, we trained 65,536 tokens per batch (128 GPUs&#x02009;&#x000d7;&#x02009;512 tokens). A batch size of 8 per device was used, totaling 1024. The model trained on 128 NVIDIA A100s in 4 days. Parallelism of the model was handled with DeepSpeed<sup><xref ref-type="bibr" rid="CR69">69</xref></sup>.</p></sec><sec id="Sec15"><title>Model inference</title><p id="Par35">We systematically sampled sequences using our main model using different inference parameters. In particular, we varied the&#x000a0;repetition penalty from a range of 1.1 to 3.0 at each 0.1 units, top_k from 250 to 1000 sampling every 50 units, and a top_p from 0.7 to 1.0 with a window of 0.05 units. 100 sequences were produced for each sampling parameter set and the frequency of their amino acids compared to natural sequences. We observed which parameters produced fewer differences in the set of the&#x000a0;seven most common amino acids in natural sequences. We also explored the beam search algorithm for beams in the range 50 to 100 using a window of 1 unit but it produced worse matches in all cases. To determine amino acid frequencies in natural sequences for comparison to ProtGPT2 samples, we randomly picked 1 million sequences from the Uniref50 dataset. The best matching parameters were further downsampled with finer windows and their frequencies compared with radar plots, as shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref> in the main text. The best performing parameters in our dataset were top_k 950, repetition penalty of 1.2, and default temperature and top_p values of 1.</p></sec><sec id="Sec16"><title>Sequence dataset generation</title><p id="Par36">Three sequence datasets were produced to compare their properties. The ProtGPT2 dataset was generated by sampling 1000 batches of 100 sequences, each with the selected inference parameters and a window context of 250 tokens. This step produced 100,000 sequences. We filtered from this set those sequences whose length had been cut due to the window context, giving a total of 29,876 sequences. From this set, we randomly selected 10,000 sequences. Their average length is 149.2&#x02009;&#x000b1;&#x02009;50.9 amino acids. The natural dataset was created by randomly sampling 100,000 sequences from Uniref50. 10,000 of these sequences were further chosen to ensure their average and standard deviation lengths matched that of the ProtGPT2 dataset sequences. The random dataset was created by concatenating the 25 amino acids that appear in UniRef50, which includes the 20 standard amino acids and other IUPAC codes such as &#x0201c;X&#x0201d;, &#x0201c;B&#x0201d;, &#x0201c;U&#x0201d;, &#x0201c;O&#x0201d;, and &#x0201c;Z&#x0201d;, by randomly concatenating them into sequences with a length taken from a normal distribution between 5 and 267 amino acids.</p></sec><sec id="Sec17"><title>Homology detection</title><p id="Par37">Each sequence in the three 10k datasets was searched for similarity against the PDB70 and uniclust30 databases using HHblits<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>. We used the Uniclust30 database version 2018_08 and the pdb70 version 2021_04. As HHblits produces a list of alignments we selected all those over the HSSP curve as possible matches, and from these, selected the largest alignment. Thus, for each sequence in each dataset, the longest and the highest identity scoring alignment was selected and represented in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>.</p></sec><sec id="Sec18"><title>Disorder prediction</title><p id="Par38">IUPred3 was run on ProtGPT2 and natural datasets using all three possible options to detect shorter (&#x0201c;short&#x0201d;) or longer (&#x0201c;longer&#x0201d;) unstructured regions, as well as structured regions (&#x0201c;glob&#x0201d;)<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Ordered content was determined with the &#x0201c;short&#x0201d; option. The output of the &#x0201c;glob&#x0201d; analysis also reports if any structured, globular domain was found, as shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. We ran secondary structure prediction using PSIPRED v4.0 for each sequence in natural and ProtGPT2 datasets<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. The alignments of the abovementioned HHblits searches were used as multiple sequence alignments. We computed the percentages for each secondary element by dividing the number of amino acids with a certain prediction by the total number of amino acids with a confidence value of 5 or more.</p></sec><sec id="Sec19"><title>AlphaFold2 structure prediction</title><p id="Par39">We predicted five structures for each sequence in the ProtGPT2 dataset using AlphaFold ColabFold batch v1.2<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>.</p></sec><sec id="Sec20"><title>Network construction</title><p id="Par40">Sequences in the ProtGPT2 and SCOP 2.07 filtered at 95% datasets were joined. For each sequence, we produced a multiple sequence alignment (MSA) using HHblits against the database Uniclust 2018_08. Hidden Markov model profiles were produced for each MSA using HHblits<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, and an all-against-all search for each profile was performed using HHsearch<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. The network was constructed by representing every sequence as a node, and linking two nodes whenever they have an alignment of at least 20 amino acids with 70% HHsearch probability. Extensive details on the all-against-all comparison and network construction, and tools to generate the networks can be found in our previous works Fuzzle<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup> and Protlego<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Detection of similar topologies was determined with FoldSeek<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>.</p></sec><sec id="Sec21"><title>Molecular dynamics simulations</title><p id="Par41">Simulation systems were built and run with the software HTMD<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>. In all cases, systems comprised solvated all-atom cubic boxes. Simulation boxes consisted of a protein centered at the origin of coordinates and explicit solvent molecules and neutralizing NaCl ions were added to each box. The Amber 19SB forcefield was used<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>. Three replicas were constructed per sequence. All systems were minimized, equilibrated, and run with ACEMD<sup><xref ref-type="bibr" rid="CR73">73</xref></sup> using default parameters: each system was minimized and relaxed under NPT conditions for 1&#x02009;ns at 1&#x02009;atm and 300&#x02009;K using a time-step of 4&#x02009;fs, rigid bonds, cutoff of 9&#x02009;&#x000c5;, and PME for long-range electrostatics. Heavy protein and ligand atoms were constrained by a 10&#x02009;kcal/mol/&#x000c5;2 spring constant. Production simulations were run in the NVT ensemble using a Langevin thermostat with a damping of 0.1&#x02009;ps<sup>&#x02212;1</sup> and a hydrogen mass repartitioning scheme to achieve timesteps of 4&#x02009;fs<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>.</p></sec><sec id="Sec22"><title>Rosetta calculations</title><p id="Par42">Rosetta Relax runs were produced with the Rosetta Software Suite v3.12<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> using as input structure the best-scoring prediction from AlphaFold.</p></sec><sec id="Sec23"><title>Reporting summary</title><p id="Par43">Further information on research design is available in the&#x000a0;<xref rid="MOESM2" ref-type="media">Nature Research Reporting Summary</xref> linked to this article.</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary information</title><sec id="Sec24"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41467_2022_32007_MOESM1_ESM.pdf"><caption><p>Supplementary Information</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="41467_2022_32007_MOESM2_ESM.pdf"><caption><p>Reporting Summary</p></caption></media></supplementary-material>
<supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="41467_2022_32007_MOESM3_ESM.pdf"><caption><p>Peer Review File</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#x02019;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41467-022-32007-7.</p></sec><ack><title>Acknowledgements</title><p>The authors gratefully acknowledge the scientific support and HPC resources provided by the Erlangen National High-Performance Computing Center (NHR@FAU) of the Friedrich-Alexander-Universit&#x000e4;t Erlangen-N&#x000fc;rnberg (FAU) under an early-access NHR project. NHR funding is provided by federal and Bavarian state authorities. NHR@FAU hardware is partially funded by the German Research Foundation (DFG)&#x02014;440719683. We thank Thomas Zeiser for his considerate support and Surbhi Dhingra for feedback on the manuscript. N.F. acknowledges support from an AGAUR Beatriu de Pin&#x000f3;s MSCA-COFUND Fellowship (project 2020-BP-00130). The authors thank funding from the German Research Foundation (DFG) - 491183248 and the Open Access Publishing Fund of the University of Bayreuth.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>N.F conceived the work, trained the model, analyzed the data, and wrote the manuscript. S.S produced the IUPred3 disorder predictions and analysis and wrote the manuscript. B.H analyzed the data and wrote the manuscript. The three authors discussed the results and supervised the work.</p></notes><notes notes-type="peer-review"><title>Peer review</title><sec id="FPar1"><title>Peer review information</title><p id="Par44"><italic>Nature Communications</italic> thanks the anonymous reviewer(s) for their contribution to the peer review of this work.&#x000a0;<xref rid="MOESM3" ref-type="media">Peer reviewer reports</xref> are available.</p></sec></notes><notes notes-type="funding-information"><title>Funding</title><p>Open Access funding enabled and organized by Projekt DEAL.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The model weights are publicly available in the HuggingFace repository: <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/nferruz/ProtGPT2">https://huggingface.co/nferruz/ProtGPT2</ext-link> and Zenodo: 10.5281/zenodo.6796843 [<ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6796843#.YswB9XbMIVA">https://zenodo.org/record/6796843#.YswB9XbMIVA</ext-link>]. The dataset for training is available at: <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/nferruz/UR50_2021_04">https://huggingface.co/datasets/nferruz/UR50_2021_04</ext-link>. The three sequence datasets in this work are available at: <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/nferruz/dataset_fastas">https://huggingface.co/datasets/nferruz/dataset_fastas</ext-link>. The AlphaFold predictions for the three datasets are available at <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/datasets/nferruz/dataset_alphafold">https://huggingface.co/datasets/nferruz/dataset_alphafold</ext-link>. The Uniref50 original database version 21_04 is available at <ext-link ext-link-type="uri" xlink:href="https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2021_04/">https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2021_04/</ext-link>. The Uniclust30 database version 2018_08 is available at <ext-link ext-link-type="uri" xlink:href="http://gwdu111.gwdg.de/~compbiol/uniclust/2018_08/uniclust30_2018_08_hhsuite.tar.gz">http://gwdu111.gwdg.de/~compbiol/uniclust/2018_08/uniclust30_2018_08_hhsuite.tar.gz</ext-link>.</p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The model was trained with the HugginFace transformers Trainer version 4.14.1. The code and documentation are available here: <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/docs/transformers/main_classes/trainer">https://huggingface.co/docs/transformers/main_classes/trainer</ext-link>.</p></notes><notes id="FPar2" notes-type="COI-statement"><title>Competing interests</title><p id="Par45">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>KK</given-names></name><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Arnold</surname><given-names>FH</given-names></name></person-group><article-title>Machine-learning-guided directed evolution for protein engineering</article-title><source>Nat. Methods</source><year>2019</year><volume>16</volume><fpage>687</fpage><lpage>694</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0496-6</pub-id><?supplied-pmid 31308553?><pub-id pub-id-type="pmid">31308553</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>H&#x000f6;cker</surname><given-names>B</given-names></name></person-group><article-title>Controllable protein design with language models</article-title><source>Nat. Mach. Intell.</source><year>2022</year><volume>4</volume><fpage>521</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00499-z</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bepler</surname><given-names>T</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><article-title>Learning the protein language: evolution, structure, and function</article-title><source>Cell Syst.</source><year>2021</year><volume>12</volume><fpage>654</fpage><lpage>669.e3</lpage><pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id><?supplied-pmid 34139171?><pub-id pub-id-type="pmid">34139171</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alley</surname><given-names>EC</given-names></name><name><surname>Khimulya</surname><given-names>G</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>AlQuraishi</surname><given-names>M</given-names></name><name><surname>Church</surname><given-names>GM</given-names></name></person-group><article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title><source>Nat. Methods</source><year>2019</year><volume>16</volume><fpage>1315</fpage><lpage>1322</lpage><pub-id pub-id-type="doi">10.1038/s41592-019-0598-1</pub-id><?supplied-pmid 31636460?><pub-id pub-id-type="pmid">31636460</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HL</given-names></name><name><surname>Pang</surname><given-names>YH</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name></person-group><article-title>BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models</article-title><source>Nucleic Acids Res.</source><year>2021</year><volume>49</volume><fpage>e129</fpage><lpage>e129</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab829</pub-id><?supplied-pmid 34581805?><pub-id pub-id-type="pmid">34581805</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Gao</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>BioSeq-Analysis2.0: an updated platform for analyzing DNA, RNA and protein sequences at sequence level and residue level based on machine learning approaches</article-title><source>Nucleic Acids Res.</source><year>2019</year><volume>47</volume><fpage>e127</fpage><lpage>e127</lpage><pub-id pub-id-type="doi">10.1093/nar/gkz740</pub-id><?supplied-pmid 31504851?><pub-id pub-id-type="pmid">31504851</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Vaswani, A. et al. Transformer: attention is all you need. In <italic>Advances in Neural Information Processing Systems</italic> 5999&#x02013;6009 (2017).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Wu, K. et al. TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-xbinding analyses. Preprint at <italic>bioRxiv</italic>10.1101/2021.11.18.469186 (2021).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Park, M., Seo, S., Park, E. &#x00026; Kim, J. EpiBERTope: a sequence-based pre-trained BERT model improves linear and structural epitope prediction by learning long-distance protein interactions effectively. Preprint at <italic>bioRxiv</italic>10.1101/2022.02.27.481241 (2022).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. <italic>Proc. Natl. Acad. Sci</italic>. <bold>118</bold>, e2016239118 (2021).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Elnaggar, A. et al. ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing. In <italic>IEEE Transactions on Pattern Analysis and Machine Intelligence</italic>. 10.1109/TPAMI.2021.3095381.</mixed-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>N</given-names></name><name><surname>Ofer</surname><given-names>D</given-names></name><name><surname>Peleg</surname><given-names>Y</given-names></name><name><surname>Rappoport</surname><given-names>N</given-names></name><name><surname>Linial</surname><given-names>M</given-names></name></person-group><article-title>ProteinBERT: a universal deep-learning model of protein sequence and function</article-title><source>Bioinformatics</source><year>2022</year><volume>38</volume><fpage>2102</fpage><lpage>2110</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btac020</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Yang, K. K., Lu, A. X. &#x00026; Fusi, N. K. Convolutions are competitive with transformers for protein sequence pretraining. Preprint at <italic>bioRxiv</italic>10.1101/2022.05.19.492714 (2022).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Rao, R. et al. Evaluating protein transfer learning with TAPE. <italic>Adv. Neural Inf. Process. Syst</italic>. <bold>32</bold>, 9689&#x02013;9701 (2019).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &#x00026; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. Preprint at arXiv:1810.04805 (2018).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Johnson, S. R., Monaco, S., Massie, K. &#x00026; Syed, Z. Generating novel protein sequences using Gibbs sampling of masked language models. Preprint at <italic>bioRxiv</italic>10.1101/2021.01.26.428322 (2021).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Radford, A. et al. Language models are unsupervised multitask learners. <ext-link ext-link-type="uri" xlink:href="https://github.com/codelucas/newspaper">https://github.com/codelucas/newspaper</ext-link> (2018).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">OpenAI says its text-generating algorithm GPT-2 is too dangerous to release. <ext-link ext-link-type="uri" xlink:href="https://slate.com/technology/2019/02/openai-gpt2-text-generating-algorithm-ai-dangerous.html">https://slate.com/technology/2019/02/openai-gpt2-text-generating-algorithm-ai-dangerous.html</ext-link> (2019).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Madani, A. et al. ProGen: language modeling for protein generation. (2020).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Madani, A. et al. Deep neural language modeling enables functional protein generation across families. Preprint at <italic>bioRxiv</italic>10.1101/2021.07.18.452833 (2021).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Nijkamp, E. et al. ProGen2: exploring the boundaries of protein language models. Preprint at arxiv 10.48550/arxiv.2206.13517 (2022).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Hesslow, D. et al. RITA: a Study on Scaling Up Generative Protein Sequence Models. Preprint at arXiv 2205.05789 (2022).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Moffat, L., Kandathil, S. M. &#x00026; Jones, D. T. Design in the DARK: learning deep generative models for de novo protein design. Preprint at <italic>bioRxiv</italic>10.1101/2022.01.27.478087 (2022).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Wolf, T. et al. HuggingFace&#x02019;s transformers: state-of-the-art natural language processing. Preprint at arXiv 1910.03771 (2019).</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campeotto</surname><given-names>I</given-names></name><etal/></person-group><article-title>One-step design of a stable variant of the malaria invasion protein RH5 for use as a vaccine immunogen</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2017</year><volume>114</volume><fpage>998</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1073/pnas.1616903114</pub-id><?supplied-pmid 28096331?><pub-id pub-id-type="pmid">28096331</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Machine learning-aided engineering of hydrolases for PET depolymerization</article-title><source>Nature</source><year>2022</year><volume>604</volume><fpage>662</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-04599-z</pub-id><?supplied-pmid 35478237?><pub-id pub-id-type="pmid">35478237</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C</given-names></name><etal/></person-group><article-title>Exploring the limits of transfer learning with a unified text-to-text transformer</article-title><source>J. Mach. Learn. Res.</source><year>2020</year><volume>21</volume><fpage>1</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">34305477</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdig&#x000e3;o</surname><given-names>N</given-names></name><etal/></person-group><article-title>Unexpected features of the dark proteome</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2015</year><volume>112</volume><fpage>15898</fpage><lpage>15903</lpage><pub-id pub-id-type="doi">10.1073/pnas.1508380112</pub-id><?supplied-pmid 26578815?><pub-id pub-id-type="pmid">26578815</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Perdig&#x000e3;o, N., Rosa, A. C. &#x00026; O&#x02019;Donoghue, S. I. The Dark Proteome Database. <italic>BioData Min</italic>. <bold>10</bold>, 24 (2017).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Gage, P. A new algorithm for data compression. 10.5555/177910.177914.</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Wang, A. et al. GLUE: a multi-task benchmark and analysis platform for natural language understanding. Preprint at <italic>arXiv</italic> (2018).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Holtzman, A., Buys, J., Du, L., Forbes, M. &#x00026; Choi, Y. The curious case of neural text degeneration. <italic>CEUR Workshop Proc</italic>. <bold>2540</bold>, (2019).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C. &#x00026; Socher, R. CTRL: a conditional transformer language model for controllable generation. Preprint at arxiv (2019).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Madani, A. et al. ProGen: language modeling for protein generation. Preprint at <italic>bioRxiv</italic>10.1101/2020.03.07.982272 (2020).</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abor Erd</surname><given-names>G&#x000b4;</given-names></name><name><surname>Os</surname></name><name><surname>Aty&#x000e1;saty&#x000b4;aty&#x000e1;s Pajkos</surname><given-names>M&#x000b4;</given-names></name><name><surname>Doszt&#x000e1;nyi</surname><given-names>Z</given-names></name><name><surname>Doszt&#x000e1;nyi</surname><given-names>D</given-names></name></person-group><article-title>IUPred3: prediction of protein disorder enhanced with unambiguous experimental annotation and visualization of evolutionary conservation</article-title><source>Nucleic Acids Res.</source><year>2021</year><volume>49</volume><fpage>W297</fpage><lpage>W303</lpage><pub-id pub-id-type="doi">10.1093/nar/gkab408</pub-id><pub-id pub-id-type="pmid">34048569</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>YJ</given-names></name><name><surname>Pang</surname><given-names>YH</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name></person-group><article-title>DeepIDP-2L: protein intrinsically disordered region prediction by combining convolutional attention network and hierarchical attention network</article-title><source>Bioinformatics</source><year>2022</year><volume>38</volume><fpage>1252</fpage><lpage>1260</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btab810</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchan</surname><given-names>DWA</given-names></name><name><surname>Jones</surname><given-names>DT</given-names></name></person-group><article-title>The PSIPRED protein analysis workbench: 20 years on</article-title><source>Nucleic Acids Res.</source><year>2019</year><volume>47</volume><fpage>W402</fpage><lpage>W407</lpage><pub-id pub-id-type="doi">10.1093/nar/gkz297</pub-id><?supplied-pmid 31251384?><pub-id pub-id-type="pmid">31251384</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>S&#x000f6;ding</surname><given-names>J</given-names></name></person-group><article-title>Protein homology detection by HMM-HMM comparison</article-title><source>Bioinformatics</source><year>2005</year><volume>21</volume><fpage>951</fpage><lpage>960</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bti125</pub-id><?supplied-pmid 15531603?><pub-id pub-id-type="pmid">15531603</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirdita</surname><given-names>M</given-names></name><etal/></person-group><article-title>Uniclust databases of clustered and deeply annotated protein sequences and alignments</article-title><source>Nucleic Acids Res.</source><year>2017</year><volume>45</volume><fpage>D170</fpage><pub-id pub-id-type="doi">10.1093/nar/gkw1081</pub-id><?supplied-pmid 27899574?><pub-id pub-id-type="pmid">27899574</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rost</surname><given-names>B</given-names></name></person-group><article-title>Twilight zone of protein sequence alignments</article-title><source>Protein Eng. Des. Sel.</source><year>1999</year><volume>12</volume><fpage>85</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1093/protein/12.2.85</pub-id></element-citation></ref><ref id="CR41"><label>41.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirdita</surname><given-names>M</given-names></name><etal/></person-group><article-title>ColabFold: making protein folding accessible to all</article-title><source>Nat. Methods</source><year>2022</year><volume>19</volume><fpage>679</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1038/s41592-022-01488-1</pub-id><?supplied-pmid 35637307?><pub-id pub-id-type="pmid">35637307</pub-id></element-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><etal/></person-group><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><year>2021</year><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><?supplied-pmid 34265844?><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><etal/></person-group><article-title>Highly accurate protein structure prediction for the human proteome</article-title><source>Nature</source><year>2021</year><volume>596</volume><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03828-1</pub-id><?supplied-pmid 34293799?><pub-id pub-id-type="pmid">34293799</pub-id></element-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">DiMaio, F., Leaver-Fay, A., Bradley, P., Baker, D. &#x00026; Andr&#x000e9;, I. Modeling symmetric macromolecular structures in Rosetta3. <italic>PLoS ONE</italic><bold>6</bold>, e20450 (2011).</mixed-citation></ref><ref id="CR45"><label>45.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauer</surname><given-names>MF</given-names></name><name><surname>Sevy</surname><given-names>AM</given-names></name><name><surname>Crowe</surname><given-names>JE</given-names></name><name><surname>Meiler</surname><given-names>J</given-names></name></person-group><article-title>Multi-state design of flexible proteins predicts sequences optimal for conformational change</article-title><source>PLOS Comput. Biol.</source><year>2020</year><volume>16</volume><fpage>e1007339</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007339</pub-id><?supplied-pmid 32032348?><pub-id pub-id-type="pmid">32032348</pub-id></element-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alford</surname><given-names>RF</given-names></name><etal/></person-group><article-title>The Rosetta all-atom energy function for macromolecular modeling and design</article-title><source>J. Chem. Theory Comput.</source><year>2017</year><volume>13</volume><fpage>3031</fpage><pub-id pub-id-type="doi">10.1021/acs.jctc.7b00125</pub-id><?supplied-pmid 28430426?><pub-id pub-id-type="pmid">28430426</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wedemeyer</surname><given-names>MJ</given-names></name><name><surname>Mueller</surname><given-names>BK</given-names></name><name><surname>Bender</surname><given-names>BJ</given-names></name><name><surname>Meiler</surname><given-names>J</given-names></name><name><surname>Volkman</surname><given-names>BF</given-names></name></person-group><article-title>Modeling the complete chemokine-receptor interaction</article-title><source>Methods Cell Biol.</source><year>2019</year><volume>149</volume><fpage>289</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1016/bs.mcb.2018.09.005</pub-id><?supplied-pmid 30616825?><pub-id pub-id-type="pmid">30616825</pub-id></element-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Miller, M. D. &#x00026; Phillips, G. N. Moving beyond static snapshots: protein dynamics and the protein Data Bank. <italic>J. Biol. Chem</italic>. <bold>296</bold>, 100749 (2021).</mixed-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>H</given-names></name><etal/></person-group><article-title>ECOD: an evolutionary classification of protein domains</article-title><source>PLoS Comput. Biol.</source><year>2014</year><volume>10</volume><fpage>e1003926</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003926</pub-id><?supplied-pmid 25474468?><pub-id pub-id-type="pmid">25474468</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sillitoe</surname><given-names>I</given-names></name><etal/></person-group><article-title>CATH: increased structural coverage of functional space</article-title><source>Nucleic Acids Res.</source><year>2021</year><volume>49</volume><fpage>D266</fpage><lpage>D273</lpage><pub-id pub-id-type="doi">10.1093/nar/gkaa1079</pub-id><?supplied-pmid 33237325?><pub-id pub-id-type="pmid">33237325</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osadchy</surname><given-names>M</given-names></name><name><surname>Kolodny</surname><given-names>R</given-names></name></person-group><article-title>Maps of protein structure space reveal a fundamental relationship between protein structure and function</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2011</year><volume>108</volume><fpage>12301</fpage><lpage>12306</lpage><pub-id pub-id-type="doi">10.1073/pnas.1102727108</pub-id><?supplied-pmid 21737750?><pub-id pub-id-type="pmid">21737750</pub-id></element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alva</surname><given-names>V</given-names></name><name><surname>Remmert</surname><given-names>M</given-names></name><name><surname>Biegert</surname><given-names>A</given-names></name><name><surname>Lupas</surname><given-names>AN</given-names></name><name><surname>S&#x000f6;ding</surname><given-names>J</given-names></name></person-group><article-title>A galaxy of folds</article-title><source>Protein Sci.</source><year>2010</year><volume>19</volume><fpage>124</fpage><lpage>130</lpage><?supplied-pmid 19937658?><pub-id pub-id-type="pmid">19937658</pub-id></element-citation></ref><ref id="CR53"><label>53.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nepomnyachiy</surname><given-names>S</given-names></name><name><surname>Ben-Tal</surname><given-names>N</given-names></name><name><surname>Kolodny</surname><given-names>R</given-names></name></person-group><article-title>Global view of the protein universe</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2014</year><volume>111</volume><fpage>11691</fpage><lpage>11696</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403395111</pub-id><?supplied-pmid 25071170?><pub-id pub-id-type="pmid">25071170</pub-id></element-citation></ref><ref id="CR54"><label>54.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><etal/></person-group><article-title>Identification and analysis of natural building blocks for evolution-guided fragment-based protein design</article-title><source>J. Mol. Biol.</source><year>2020</year><volume>432</volume><fpage>3898</fpage><lpage>3914</lpage><pub-id pub-id-type="doi">10.1016/j.jmb.2020.04.013</pub-id><?supplied-pmid 32330481?><pub-id pub-id-type="pmid">32330481</pub-id></element-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>Michel</surname><given-names>F</given-names></name><name><surname>Lobos</surname><given-names>F</given-names></name><name><surname>Schmidt</surname><given-names>S</given-names></name><name><surname>H&#x000f6;cker</surname><given-names>B</given-names></name></person-group><article-title>Fuzzle 2.0: ligand binding in natural protein building blocks</article-title><source>Front. Mol. Biosci.</source><year>2021</year><volume>8</volume><fpage>805</fpage><pub-id pub-id-type="doi">10.3389/fmolb.2021.715972</pub-id></element-citation></ref><ref id="CR56"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>PS</given-names></name><name><surname>Boyken</surname><given-names>SE</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><article-title>The coming of age of de novo protein design</article-title><source>Nature</source><year>2016</year><volume>537</volume><fpage>320</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1038/nature19946</pub-id><?supplied-pmid 27629638?><pub-id pub-id-type="pmid">27629638</pub-id></element-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Ferruz, N., Noske, J. &#x00026; H&#x000f6;cker, B. Protlego: a Python package for the analysis and design of chimeric proteins. <italic>Bioinformatics</italic>10.1093/bioinformatics/btab253 (2021).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Kempen, M. van et al. Foldseek: fast and accurate protein structure search. Preprint at <italic>bioRxiv</italic>10.1101/2022.02.07.479398 (2022).</mixed-citation></ref><ref id="CR59"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcos</surname><given-names>E</given-names></name><etal/></person-group><article-title>De novo design of a non-local &#x003b2;-sheet protein with high stability and accuracy</article-title><source>Nat. Struct. Mol. Biol.</source><year>2018</year><volume>25</volume><fpage>1028</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1038/s41594-018-0141-6</pub-id><?supplied-pmid 30374087?><pub-id pub-id-type="pmid">30374087</pub-id></element-citation></ref><ref id="CR60"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X</given-names></name><name><surname>Kortemme</surname><given-names>T</given-names></name></person-group><article-title>Recent advances in de novo protein design: Principles, methods, and applications</article-title><source>J. Biol. Chem.</source><year>2021</year><volume>296</volume><fpage>100558</fpage><pub-id pub-id-type="doi">10.1016/j.jbc.2021.100558</pub-id><?supplied-pmid 33744284?><pub-id pub-id-type="pmid">33744284</pub-id></element-citation></ref><ref id="CR61"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>C</given-names></name><etal/></person-group><article-title>Computational design of transmembrane pores</article-title><source>Nature</source><year>2020</year><volume>585</volume><fpage>129</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2646-5</pub-id><?supplied-pmid 32848250?><pub-id pub-id-type="pmid">32848250</pub-id></element-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Romero-Romero, S. et al. The Stability Landscape of de novo TIM Barrels Explored by a Modular Design Approach. <italic>J. Mol. Biol</italic>. <bold>433</bold>, 167153 (2021).</mixed-citation></ref><ref id="CR63"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>PS</given-names></name><etal/></person-group><article-title>De novo design of a four-fold symmetric TIM-barrel protein with atomic-level accuracy</article-title><source>Nat. Chem. Biol.</source><year>2016</year><volume>12</volume><fpage>29</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1038/nchembio.1966</pub-id><?supplied-pmid 26595462?><pub-id pub-id-type="pmid">26595462</pub-id></element-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anand</surname><given-names>N</given-names></name><etal/></person-group><article-title>Protein sequence design with a learned potential</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-022-28313-9</pub-id><pub-id pub-id-type="pmid">34983933</pub-id></element-citation></ref><ref id="CR65"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kordes</surname><given-names>S</given-names></name><name><surname>Romero-Romero</surname><given-names>S</given-names></name><name><surname>Lutz</surname><given-names>L</given-names></name><name><surname>H&#x000f6;cker</surname><given-names>B</given-names></name></person-group><article-title>A newly introduced salt bridge cluster improves structural and biophysical properties of de novo TIM barrels</article-title><source>Protein Sci.</source><year>2022</year><volume>31</volume><fpage>513</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1002/pro.4249</pub-id><?supplied-pmid 34865275?><pub-id pub-id-type="pmid">34865275</pub-id></element-citation></ref><ref id="CR66"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiese</surname><given-names>JG</given-names></name><name><surname>Shanmugaratnam</surname><given-names>S</given-names></name><name><surname>H&#x000f6;cker</surname><given-names>B</given-names></name></person-group><article-title>Extension of a de novo TIM barrel with a rationally designed secondary structure element</article-title><source>Protein Sci.</source><year>2021</year><volume>30</volume><fpage>982</fpage><lpage>989</lpage><pub-id pub-id-type="doi">10.1002/pro.4064</pub-id><?supplied-pmid 33723882?><pub-id pub-id-type="pmid">33723882</pub-id></element-citation></ref><ref id="CR67"><label>67.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senior</surname><given-names>AW</given-names></name><etal/></person-group><article-title>Improved protein structure prediction using potentials from deep learning</article-title><source>Nature</source><year>2020</year><volume>577</volume><fpage>706</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1923-7</pub-id><?supplied-pmid 31942072?><pub-id pub-id-type="pmid">31942072</pub-id></element-citation></ref><ref id="CR68"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>H&#x000f6;cker</surname><given-names>B</given-names></name></person-group><article-title>Dreaming ideal protein structures</article-title><source>Nat. Biotechnol.</source><year>2022</year><volume>40</volume><fpage>171</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1038/s41587-021-01196-9</pub-id><?supplied-pmid 35075248?><pub-id pub-id-type="pmid">35075248</pub-id></element-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Rasley, J., Rajbhandari, S., Ruwase, O. &#x00026; He, Y. DeepSpeed: system optimizations enable training deep learning models with over 100 billion parameters. In <italic>Proc. 26th ACM SIGKDD International Conference on Knowledge Discovery &#x00026; Data Mining</italic>. 3505&#x02013;3506 (Association for Computing Machinery, 2020).</mixed-citation></ref><ref id="CR70"><label>70.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remmert</surname><given-names>M</given-names></name><name><surname>Biegert</surname><given-names>A</given-names></name><name><surname>Hauser</surname><given-names>A</given-names></name><name><surname>S&#x000f6;ding</surname><given-names>J</given-names></name></person-group><article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title><source>Nat. Methods</source><year>2011</year><volume>9</volume><fpage>173</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1818</pub-id><?supplied-pmid 22198341?><pub-id pub-id-type="pmid">22198341</pub-id></element-citation></ref><ref id="CR71"><label>71.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerr</surname><given-names>S</given-names></name><name><surname>Harvey</surname><given-names>MJ</given-names></name><name><surname>No&#x000e9;</surname><given-names>F</given-names></name><name><surname>De Fabritiis</surname><given-names>G</given-names></name></person-group><article-title>HTMD: high-throughput molecular dynamics for molecular discovery</article-title><source>J. Chem. Theory Comput.</source><year>2016</year><volume>12</volume><fpage>1845</fpage><lpage>1852</lpage><pub-id pub-id-type="doi">10.1021/acs.jctc.6b00049</pub-id><?supplied-pmid 26949976?><pub-id pub-id-type="pmid">26949976</pub-id></element-citation></ref><ref id="CR72"><label>72.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>C</given-names></name><etal/></person-group><article-title>Ff19SB: amino-acid-specific protein backbone parameters trained against quantum mechanics energy surfaces in solution</article-title><source>J. Chem. Theory Comput.</source><year>2020</year><volume>16</volume><fpage>528</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1021/acs.jctc.9b00591</pub-id><?supplied-pmid 31714766?><pub-id pub-id-type="pmid">31714766</pub-id></element-citation></ref><ref id="CR73"><label>73.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>MJ</given-names></name><name><surname>Giupponi</surname><given-names>G</given-names></name><name><surname>De Fabritiis</surname><given-names>G</given-names></name></person-group><article-title>ACEMD: accelerating biomolecular dynamics in the microsecond time scale</article-title><source>J. Chem. Theory Comput.</source><year>2009</year><volume>5</volume><fpage>1632</fpage><lpage>1639</lpage><pub-id pub-id-type="doi">10.1021/ct9000685</pub-id><?supplied-pmid 26609855?><pub-id pub-id-type="pmid">26609855</pub-id></element-citation></ref><ref id="CR74"><label>74.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferruz</surname><given-names>N</given-names></name><name><surname>Harvey</surname><given-names>MJ</given-names></name><name><surname>Mestres</surname><given-names>J</given-names></name><name><surname>De Fabritiis</surname><given-names>G</given-names></name></person-group><article-title>Insights from fragment hit binding assays by molecular simulations</article-title><source>J. Chem. Inf. Model.</source><year>2015</year><volume>55</volume><fpage>2200</fpage><lpage>2205</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.5b00453</pub-id><?supplied-pmid 26376295?><pub-id pub-id-type="pmid">26376295</pub-id></element-citation></ref></ref-list></back></article>