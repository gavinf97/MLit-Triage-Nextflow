<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">10821710</article-id><article-id pub-id-type="doi">10.1093/gigascience/giad120</article-id><article-id pub-id-type="publisher-id">giad120</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Note</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject><subject>AcademicSubjects/SCI02254</subject></subj-group></article-categories><title-group><article-title>
<italic toggle="yes">MMV_Im2Im</italic>: an open-source microscopy machine vision toolbox for image-to-image transformation</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1640-3045</contrib-id><name><surname>Sonneck</surname><given-names>Justin</given-names></name><aff>
<institution>Leibniz-Institut f&#x000fc;r Analytische Wissenschaften &#x02013; ISAS &#x02013; e.V.</institution>, <addr-line>Bunsen-Kirchhoff-Str. 11, Dortmund 44139</addr-line>, <country country="DE">Germany</country></aff><aff>
<institution>Faculty of Computer Science, Ruhr-University Bochum</institution>, <addr-line>Universit&#x000e4;tsstra&#x000df;e 150, Bochum 44801</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0002-3914-1102</contrib-id><name><surname>Zhou</surname><given-names>Yu</given-names></name><aff>
<institution>Leibniz-Institut f&#x000fc;r Analytische Wissenschaften &#x02013; ISAS &#x02013; e.V.</institution>, <addr-line>Bunsen-Kirchhoff-Str. 11, Dortmund 44139</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8500-1357</contrib-id><name><surname>Chen</surname><given-names>Jianxu</given-names></name><!--jianxu.chen@isas.de--><aff>
<institution>Leibniz-Institut f&#x000fc;r Analytische Wissenschaften &#x02013; ISAS &#x02013; e.V.</institution>, <addr-line>Bunsen-Kirchhoff-Str. 11, Dortmund 44139</addr-line>, <country country="DE">Germany</country></aff><xref rid="cor1" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">Correspondence address. Jianxu Chen, Bunsen-Kirchhoff-Str. 11, 44139 Dortmund, Germany. E-mail: <email>jianxu.chen@isas.de</email></corresp></author-notes><pub-date pub-type="epub" iso-8601-date="2024-01-27"><day>27</day><month>1</month><year>2024</year></pub-date><pub-date pub-type="collection"><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>1</month><year>2024</year></pub-date><volume>13</volume><elocation-id>giad120</elocation-id><history><date date-type="received"><day>06</day><month>4</month><year>2023</year></date><date date-type="rev-recd"><day>30</day><month>9</month><year>2023</year></date><date date-type="accepted"><day>28</day><month>12</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024. Published by Oxford University Press GigaScience.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giad120.pdf"/><abstract><title>Abstract</title><p>Over the past decade, deep learning (DL) research in computer vision has been growing rapidly, with many advances in DL-based image analysis methods for biomedical problems. In this work, we introduce <italic toggle="yes">MMV_Im2Im</italic>, a new open-source Python package for image-to-image transformation in bioimaging applications. <italic toggle="yes">MMV_Im2Im</italic> is designed with a generic image-to-image transformation framework that can be used for a wide range of tasks, including semantic segmentation, instance segmentation, image restoration, image generation, and so on. Our implementation takes advantage of state-of-the-art machine learning engineering techniques, allowing researchers to focus on their research without worrying about engineering details. We demonstrate the effectiveness of <italic toggle="yes">MMV_Im2Im</italic> on more than 10 different biomedical problems, showcasing its general potentials and applicabilities. For computational biomedical researchers, <italic toggle="yes">MMV_Im2Im</italic> provides a starting point for developing new biomedical image analysis or machine learning algorithms, where they can either reuse the code in this package or fork and extend this package to facilitate the development of new methods. Experimental biomedical researchers can benefit from this work by gaining a comprehensive view of the image-to-image transformation concept through diversified examples and use cases. We hope this work can give the community inspirations on how DL-based image-to-image transformation can be integrated into the assay development process, enabling new biomedical studies that cannot be done only with traditional experimental assays. To help researchers get started, we have provided source code, documentation, and tutorials for <italic toggle="yes">MMV_Im2Im</italic> at [<ext-link xlink:href="https://github.com/MMV-Lab/mmv_im2im" ext-link-type="uri">https://github.com/MMV-Lab/mmv_im2im</ext-link>] under MIT license.</p></abstract><kwd-group kwd-group-type="keywords"><kwd>deep learning</kwd><kwd>microscopy image analysis</kwd><kwd>open-source</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Bundesministerium f&#x000fc;r Bildung und Frauen</institution><institution-id institution-id-type="DOI">10.13039/501100006603</institution-id></institution-wrap>
</funding-source><award-id>161L0272</award-id></award-group></funding-group><counts><page-count count="20"/></counts></article-meta></front><body><sec sec-type="intro" id="sec1"><title>Introduction</title><p>With the rapid advancements in the fields of machine learning (ML) and computer vision, computers can now transform images into new forms, enabling better visualization [<xref rid="bib2" ref-type="bibr">1</xref>], better animation [<xref rid="bib3" ref-type="bibr">2</xref>], and better information extraction [<xref rid="bib4" ref-type="bibr">3</xref>] with unprecedented and continuously growing accuracy and efficiency compared to conventional digital image processing. These techniques have recently been adapted for bioimaging applications and have revolutionized image-based biomedical research [<xref rid="bib5" ref-type="bibr">4&#x02013;7</xref>]. In principle, these techniques and applications can be formulated as a general image-to-image transformation problem, as depicted in the central panel in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>. Deep neural networks are trained to perceive the information from the source image(s) and reconstruct the learned knowledge from source images(s) in the form of a new image(s) of the target type. The source and target images can be real or simulated microscopy images, segmentation masks, or their combinations, as exemplified in Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>. Since these underlying methods share the same essential spirit, a natural question arises: is it possible to develop a single generic codebase for deep learning (DL)&#x02013;based image-to-image transformation applicable to various biomedical studies?</p><fig position="float" id="fig1"><label>Figure 1:</label><caption><p>Overview of the image-to-image transformation concept and its example applications.</p></caption><graphic xlink:href="giad120fig1" position="float"/></fig><p>In this article, we introduce <italic toggle="yes">MMV_Im2Im</italic>, an open-source microscopy machine vision (MMV) toolbox for image-to-image transformation, and demonstrate its applications in over 10 biomedical tasks of various types by performing more than 30 experiments. Currently, <italic toggle="yes">MMV_Im2Im</italic> supports handling 2D to 5D microscopy images for supervised image-to-image translation (e.g., label-free determination [<xref rid="bib5" ref-type="bibr">4</xref>], imaging modality transformation [<xref rid="bib6" ref-type="bibr">5</xref>, <xref rid="bib9" ref-type="bibr">8</xref>]), supervised image restoration [<xref rid="bib7" ref-type="bibr">6</xref>], supervised semantic segmentation [<xref rid="bib10" ref-type="bibr">9</xref>], supervised instance segmentation [<xref rid="bib11" ref-type="bibr">10</xref>, <xref rid="bib12" ref-type="bibr">11</xref>], unsupervised semantic segmentation [<xref rid="bib13" ref-type="bibr">12</xref>], and unsupervised image-to-image translation and synthetization [<xref rid="bib14" ref-type="bibr">13</xref>]. The toolbox will continuously grow with more and more methods, such as self-supervised learning-based methods, ideally also with contributions from the open-source community.</p><p>Why do we need such a single generic codebase for all DL-based microscopy image-to-image transformation? <italic toggle="yes">MMV_Im2Im</italic> is not simply a collection of many existing methods but rather has a systematic design for generality, flexibility, simplicity, and reusability, attempting to address several fundamental bottlenecks for image-to-image transformation in biomedical applications, as highlighted below.</p><sec id="sec1-1"><title>Feature 1: universal boilerplate with state-of-the-art ML engineering</title><sec id="sec1-1-1"><title>Bottleneck: existing code not easy to understand or to extend or reuse</title><p>Our package <italic toggle="yes">MMV_Im2Im</italic> employs pytorch-lightning [<xref rid="bib15" ref-type="bibr">14</xref>] as the core in the backend, which offers numerous benefits, such as readability, flexibility, simplicity, and reusability. First of all, have you ever had the moment when you wanted to extend someone&#x02019;s open-source code to suit your special ML needs but found it so difficult to figure out where and how to extend, especially for complex methods? Or, have you ever encountered the situation where you want to compare the methods and code from 2 different papers, even solving the same problem (e.g., semantic segmentation), but not quite easy to grasp quickly since the 2 repositories are implemented in very different ways? It is not rare that even different researchers from the same group may implement similar methods in very different manners. This is not only a barrier for other people to learn and reuse the open-source code, but it also poses challenges for developers in maintenance, further development, and interoperability among different packages. We follow the pytorch-lightning framework and carefully design a universal boilerplate for image-to-image transformation for biomedical applications, where the implementation of all the methods shares the same modularized code structure. For all PyTorch users, this greatly lowers the learning curve for people to read and understand the code and makes implementing new methods or extending existing methods simple and fast, at least from an engineering perspective.</p><p>Moreover, as ML scientists, have you ever been overwhelmed by different training tricks for different methods or been curious about if certain state-of-the-art training methods can boost the performance of existing models? With the pytorch-lightning backend, <italic toggle="yes">MMV_Im2Im</italic> allows you to enjoy different state-of-the-art ML training engineering techniques without changing the code, for example, stochastic weight averaging [<xref rid="bib16" ref-type="bibr">15</xref>], single precision training, automatic batch size determination, different optimizers, different learning rate schedulers, easy deployment on different devices, distributed training on multi-GPU (even multinode), logging with common loggers such as Tensorboard, and so on. In short, with the pytorch-lightning&#x02013;based universal boilerplate, bioimaging researchers can really focus on research and develop novel methods for their biomedical applications, without worrying about the ML engineering works (which are usually lacking in non&#x02013;computer science labs).</p></sec></sec><sec id="sec1-2"><title>Feature 2: modularization and human-readable configuration system</title><sec id="sec1-2-1"><title>Bottleneck: dilemma between simplicity and flexibility</title><p>The toolbox is designed for people with or without extensive experience with ML and Python programming. It is not rare to find biomedical image analysis software that is very easy to use on a set of problems, but very hard to extend or adjust to other different but essentially related problems, or find some with great flexibility with tunable knobs at all levels, but unfortunately not easy for inexperienced users. To address this issue, we designed the toolbox in a systematically modularized way with various levels of configurability. One can use the toolbox with a single command as simple as <monospace>run_im2im &#x02013;config train_semanticseg_3d &#x02013;data.data_path/path/to/data</monospace> or make customization on details directly from a human-readable configuration file, such as choosing batch normalization or instance normalization in certain layers of the model, or adding extra data augmentation steps, and so on. We provide an extensive list of more than 20 example configurations for various applications and comprehensive documentation to address common questions for users as reference. For users preferring graphical interface, another napari plugin for the MMV toolbox has been planned as the extension of <italic toggle="yes">MMV_Im2Im</italic> (see Discussion for details).</p><p>In addition, the modularization and configuration system is designed to allow not only configuring with the elements offered by the package itself but also any compatible elements from a third-party package or from a public repository on GitHub. For example, one can easily switch the 3D neural network in the original <italic toggle="yes">Embedseg</italic> method to any customized U-Net from FastAI by specifying the network as <monospace>fastai.vision.models.unet</monospace>. Such painless extendability releases the power of the toolbox, amplifies the benefit of the open-source ML community, and upholds our philosophy of open science.</p></sec></sec><sec id="sec1-3"><title>Feature 3: customization for biomedical imaging applications</title><sec id="sec1-3-1"><title>Bottleneck: not enough consideration for specific challenges in microscopy images in general DL toolboxes</title><p>The original idea of a general toolbox actually stemmed from the OpenMMLab project [<xref rid="bib17" ref-type="bibr">16</xref>], which provides generic codebases for a wide range of computer vision research topics. For instance, <italic toggle="yes">MMSegmentation</italic> [<xref rid="bib18" ref-type="bibr">17</xref>] is an open-source toolbox for semantic segmentation, supporting unified benchmarking and state-of-the-art models ready to use out-of-box. It has become one of most widely used codebases for research in semantic segmentation (2.3&#x000a0;K forks and 6.5&#x000a0;K stars on GitHub as of 29 September 2023). This inspires us to develop <italic toggle="yes">MMV_Im2Im</italic> to facilitate research in image-to-image transformation with a special focus on biomedical applications.</p><p>First of all, different from general computer vision datasets, such as ImageNet [<xref rid="bib19" ref-type="bibr">18</xref>], where the images are usually small 2D RGB images (e.g., 3 &#x000d7; 256 &#x000d7; 256 pixels), biomedical applications usually involve large-scale high-dimensional data (e.g., 500 images of 4 &#x000d7; 128 &#x000d7; 2,048 &#x000d7; 2,048 voxels). To deal with this issue, we employ the PersistentDataset in MONAI [<xref rid="bib20" ref-type="bibr">19</xref>] with partial loading and sampling support, as well as delayed image reading powered by aicsimageio [<xref rid="bib21" ref-type="bibr">20</xref>] as default (configurable if another dataloader is preferred). As a result, in our stress test, training a 3D nuclei instance segmentation model with more than 125,000 3D images can be conducted efficiently on a day, even with limited resources.</p><p>Second, because microscopy data are not restricted to 2D, we reimplement common frameworks, such as fully convolutional networks (FCNs), conditional generative models, cycle-consistent generative models, and so on, in a generic way to easily switch between different dimensionalities for training. During inference, up to 5D images (channel &#x000d7; time &#x000d7; Z &#x000d7; Y &#x000d7; X) can be directly loaded as the input without presplitting into smaller 2D/3D chunks.</p><p>Third, the toolbox prepacks common functionalities specific to microscopy images. For example, we incorporate the special image normalization method introduced in [<xref rid="bib5" ref-type="bibr">4</xref>], where only the middle chunk along the Z dimension of 3D microscopy images will be used for calculating the mean and standard deviation of image intensity for standard normalization. Also, 3D light microscopy images are usually anisotropic (i.e., much lower resolution along Z than XY dimension). So, we adopt the anisotropic variation of UNet as proposed in [<xref rid="bib22" ref-type="bibr">21</xref>].</p><p>Finally, to deploy the model in production, a model trained on small 3D patches sometimes needs to be applied not only on much larger images. Combining the efficient data handling of aicsimageio [<xref rid="bib21" ref-type="bibr">20</xref>] and the sliding window inference with Gaussian weighted blending, the toolbox can yield efficient inference without visible stitching artifacts in production.</p><p>All in all, the <italic toggle="yes">MMV_Im2Im</italic> toolbox stands on the shoulders of many giants in the open-source software and ML engineering communities (pytorch-lightning, MONAI, aicsimageio, etc.) and is systematically designed for image-to-image transformation R&#x00026;D for biomedical applications. The source code of <italic toggle="yes">MMV_Im2Im</italic> is available at [<xref rid="bib1" ref-type="bibr">22</xref>]. This manuscript is generated with the open-source package Manubot [<xref rid="bib23" ref-type="bibr">23</xref>]. The manuscript source code is available at [<xref rid="bib24" ref-type="bibr">24</xref>].</p></sec></sec></sec><sec sec-type="results" id="sec2"><title>Results</title><p>In this section, we showcase the versatility of the <italic toggle="yes">MMV_Im2Im</italic> toolbox by presenting over 10 different biomedical applications across various R&#x00026;D use cases and scales. All experiments and results in this section were conducted on publicly available datasets released with other publications and our scripts (for pulling the public dataset online and data wrangling) and configuration files (for setting up training and inference details), both included in the <italic toggle="yes">MMV_Im2Im</italic> package. Our aim is to make it easy to reproduce all of the results in this article and, more important, use these data and scripts to get familiar with the package and adapt to new problems of users&#x02019; interest. It is important to note that the aim of these experiments was not to achieve the best performance on each individual task, as this may require further hyperparameter tuning (see Discussion section for more details). Rather, the experiments were intended to demonstrate the package&#x02019;s different features and general applicability, providing a holistic view of image-to-image transformation concepts to biomedical researchers. We hope that these concepts will help researchers integrate AI into traditional assay development strategies and inspire computational and experimental codesign methods, enabling new biomedical studies that were previously unfeasible.</p><sec id="sec2-1"><title>Label-free prediction of nuclear structure from 2D/3D brightfield images</title><p>The label-free method refers to a DL method that can predict fluorescent images directly from transmitted light brightfield images [<xref rid="bib5" ref-type="bibr">4</xref>]. Compared to brightfield images, fluorescent images can resolve subcellular structures in living cells at high resolution but with the cost of expensive and slow procedures and high phototoxicity. The label-free method provides a new perspective in assay development to conduct integrated computational analysis of multiple organelles only with a single brightfield image acquisition. In our first demonstration, we applied <italic toggle="yes">MMV_Im2Im</italic> to build 2D/3D models that can predict fluorescent images of nuclear structures from brightfield images. For 3D models, we also compared (i) different image normalization methods, (ii) different network backbones, and (iii) different types of models.</p><p>It should be noted that while we recognize the importance of systematically evaluating the predictions, such an analysis falls outside the scope of this article. We argue that an appropriate evaluation methodology should depend on specific downstream quantitative analysis goals (e.g., [<xref rid="bib25" ref-type="bibr">25&#x02013;27</xref>]). For example, if our aim is to quantify the size of nucleoli, we must compare the segmentation derived from real nucleoli signals to that of the predicted nucleoli segmentation, ensuring that measurements from both are consistent. Alternatively, if the goal is to localize the nucleoli roughly within the cell, Pearson correlation may be a more appropriate metric. In this work, we concentrate on visual inspection, using Pearson correlation and structural similarity as a rough quantitative reference. Our intent is to demonstrate the utility of our <italic toggle="yes">MMV_Im2Im</italic> package and leave appropriate evaluations to users in their specific problems in real studies.</p><sec id="sec2-1-1"><title>2D label-free</title><p>We started with a simple problem using 2D images from the HeLa &#x0201c;Kyoto&#x0201d; cells dataset [<xref rid="bib28" ref-type="bibr">28</xref>]. For all images, we took the brightfield channel and the mCherry-H2B channel out of the multichannel time-lapse movies. The 2D images were acquired at 20&#x000d7; with 0.8&#x000a0;NA and then downscaled by 4 (pixel size: 0.299&#x000a0;&#x000d7; 0.299&#x000a0;nm). Example predictions can be found in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2A</xref>. We compared a basic UNet model [<xref rid="bib10" ref-type="bibr">9</xref>] and a 2D version of the fnet model in [<xref rid="bib5" ref-type="bibr">4</xref>]. The fnet model achieved slightly more accurate predictions than the basic UNet, as seen in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2A</xref>.</p><fig position="float" id="fig2"><label>Figure 2:</label><caption><p>(A) Example of 2D label-free results. (B) Overview of various 3D label-free results obtained by different training strategies. p/c/s refers to percentile normalization, center normalization, and standard normalization, respectively (see main text for details). (The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.)</p></caption><graphic xlink:href="giad120fig2" position="float"/></fig></sec><sec id="sec2-1-2"><title>3D label-free</title><p>We tested with 3D images from the human induced pluripotent stem cell (hiPSC) single-cell image dataset [<xref rid="bib29" ref-type="bibr">29</xref>]. Specifically, we extracted the brightfield channel and the structure channel from the full field-of-view (FOV) multichannel images from the HIST1H2BJ, FBL, NPM1, and LMNB1 cell lines, so as to predict from one brightfield image various nuclear structures, histones, nucleoli (dense fibrillar component via fibrillarin), nucleoli (granular component via nucleophosmin), and nuclear envelope, respectively. Images were acquired at 100&#x000d7; with 1.25 NA (voxel size: 0.108 &#x000d7; 0.108 &#x000d7; 0.29 microns).</p><p>We conducted 3 groups of comparisons (see results in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2B</xref>). First, we compared 3 different image normalization methods for 3D images: percentile normalization, standard normalization, and center normalization [<xref rid="bib5" ref-type="bibr">4</xref>]. Percentile normalization refers to cutting the intensity out of the range of [0.5, 99.5] percentile of the image intensity and then rescaling the values to the range of [&#x02212;1, 1], while the standard normalization is simply subtracting mean intensity and then dividing by the standard deviation of all pixel intensities. Center normalization is similar to standard normalization, but the statistics are calculated only around the center along the Z-axis [<xref rid="bib5" ref-type="bibr">4</xref>]. One could easily test different percentiles or rescaling to [0, 1] instead of [&#x02212;1, 1]. Qualitatively, we found center normalization slightly more accurate and more robust than the other two (see first row of Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3B</xref>).</p><fig position="float" id="fig3"><label>Figure 3:</label><caption><p>(A) Comparison of predictions of different 3D label-free models and ground truth. (B) Predictions of the different label-free models using the same brightfield image as input, which provides a deep insight into the nuclear structure. This would not be possible with brightfield imaging alone and is enabled by the application of the label-free approach. (The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.)</p></caption><graphic xlink:href="giad120fig3" position="float"/></fig><p>Second, we compared different network backbone architectures, including the original fnet model [<xref rid="bib5" ref-type="bibr">4</xref>], an enhanced UNet [<xref rid="bib30" ref-type="bibr">30</xref>], the attention UNet [<xref rid="bib31" ref-type="bibr">31</xref>], and 2 transformer-based models, SwinUNETR [<xref rid="bib32" ref-type="bibr">32</xref>] and UNETR [<xref rid="bib33" ref-type="bibr">33</xref>] (all with center normalization). Inspecting the predictions on a holdout validation set suggested that fnet achieved the best performance.</p><p>Finally, we showed the comparison between 3 different types of models, an FCN-type model (i.e., fnet), a pix2pix-type model, and a CycleGAN-type model. For fair comparison, we used fnet as the same backbone for all 3 types of models. In theory, the pix2pix-type model can be trained in 2 different ways: from scratch or initializing the generator with a pretrained fnet (trained as FCN). Examples of the comparison results are shown in the last 2 rows in Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3B</xref>. Visually, it is evident that the additional adversarial components (i.e., the discriminator) could generate images with a more realistic appearance than a typical FCN-type model alone, but again, we leave the appropriate quantitative evaluations to users&#x02019; specific biomedical studies.</p><p>From the experiments above, we found that center normalization + pix2pix with fnet as the generator achieved the best overall performance qualitatively. So, we employed the same strategy on all other nuclear structures. At the end, we had 4 different label-free models, each predicting one different nuclear structure from 3D brightfield images. As an example of evaluation, we calculated the Pearson correlation, the structural similarity, and the peak signal-to-noise ratio on holdout validation sets. The results are summarized in Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref>. Again, these numbers were merely examples of evaluation; systematic evaluation based on each specific biological problem would be necessary before deployment. Figure&#x000a0;<xref rid="fig3" ref-type="fig">3A</xref> shows the comparison of each predicted structure and its ground truth, while Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3B</xref> shows one example of all 4 different structures predicted from a single unseen brightfield image. This would permit an integrated analysis of 4 different nuclear components that could hardly be acquired simultaneously in real experiments and real images.</p><table-wrap position="float" id="tbl1"><label>Table 1:</label><caption><p>Evaluation of the final 3D label-free models for 4 different nuclear structures</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Dataset</th><th rowspan="1" colspan="1">Pearson correlation</th><th rowspan="1" colspan="1">Structural similarity</th><th rowspan="1" colspan="1">Peak signal-to-noise ratio</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">FBL</td><td rowspan="1" colspan="1">0.902 &#x000b1; 0.014</td><td rowspan="1" colspan="1">0.864 &#x000b1; 0.029</td><td rowspan="1" colspan="1">33.559 &#x000b1; 1.182</td></tr><tr><td rowspan="1" colspan="1">HIST1H2BJ</td><td rowspan="1" colspan="1">0.880 &#x000b1; 0.022</td><td rowspan="1" colspan="1">0.735 &#x000b1; 0.070</td><td rowspan="1" colspan="1">27.307 &#x000b1; 2.832</td></tr><tr><td rowspan="1" colspan="1">LMNB1</td><td rowspan="1" colspan="1">0.883 &#x000b1; 0.020</td><td rowspan="1" colspan="1">0.703 &#x000b1; 0.060</td><td rowspan="1" colspan="1">29.582 &#x000b1; 1.793</td></tr><tr><td rowspan="1" colspan="1">NPM1</td><td rowspan="1" colspan="1">0.939 &#x000b1; 0.009</td><td rowspan="1" colspan="1">0.846 &#x000b1; 0.027</td><td rowspan="1" colspan="1">32.636 &#x000b1; 1.040</td></tr></tbody></table></table-wrap></sec></sec><sec id="sec2-2"><title>2D semantic segmentation of tissues from H&#x00026;E images</title><p>Segmentation is a common image-processing task and can be considered a special type of image-to-image transformation, where the generated images are segmentation masks. DL-based methods have achieved huge success in semantic segmentation in biomedical images. In this example, we demonstrated <italic toggle="yes">MMV_Im2Im</italic> on a pathology application to segment glands from hematoxylin and eosin (H&#x00026;E)&#x02013;stained tissue images from the 2015 Gland Segmentation challenge [<xref rid="bib34" ref-type="bibr">34</xref>, <xref rid="bib35" ref-type="bibr">35</xref>]. Stain normalization is an important preprocessing step in order to develop models robust to stain variation and tissue variations. <italic toggle="yes">MMV_Im2Im</italic> included a classic stain normalization method [<xref rid="bib36" ref-type="bibr">36</xref>] as a preprocessing step. The effect of stain normalization can be observed in Fig.&#x000a0;<xref rid="fig4" ref-type="fig">4A</xref>, <xref rid="fig4" ref-type="fig">B</xref>. We trained a simple attention UNet model [<xref rid="bib31" ref-type="bibr">31</xref>]. Evaluated on the 2 different holdout test sets, the model achieved an F1-score of 0.904 &#x000b1; 0.060 and 0.861 &#x000b1; 0.117 on test set A and test set B, respectively. The performance was competitive compared to the methods reported in the challenge report [<xref rid="bib35" ref-type="bibr">35</xref>], especially with much more consistent performance across the 2 different test sets. Example results can be found in Fig.&#x000a0;<xref rid="fig4" ref-type="fig">4C</xref>.</p><fig position="float" id="fig4"><label>Figure 4:</label><caption><p>Example results of 2D semantic segmentation of a gland in H&#x00026;E images. A and B provide insight into the stain normalization implemented in <italic toggle="yes">MMV_Im2Im</italic>. C compares a raw example image before stain normalization and prediction to the ground truth for each test set.</p></caption><graphic xlink:href="giad120fig4" position="float"/></fig></sec><sec id="sec2-3"><title>Instance segmentation in microscopy images</title><p>Instance segmentation is a type of segmentation problem that goes beyond semantic segmentation. The goal is to differentiate not only between different types of objects but also different instances of the same type of objects. Currently, the <italic toggle="yes">MMV_Im2Im</italic> package supports EmbedSeg-type models. The major benefit of EmbedSeg-type models is their agnosticism to the morphology and dimensionality of the object instances, compared to other models such as StarDist [<xref rid="bib37" ref-type="bibr">37</xref>, <xref rid="bib38" ref-type="bibr">38</xref>], SplineDist [<xref rid="bib39" ref-type="bibr">39</xref>], and Cellpose [<xref rid="bib40" ref-type="bibr">40</xref>]. For example, different from the others, EmbedSeg-type models are even able to generate instance segmentation where each instance contains multiple connected components. Additional frameworks such as Omnipose [<xref rid="bib41" ref-type="bibr">41</xref>] will be supported in future versions. Another mainstream category of instance segmentation methods are detection-based models, such as Mask-RCNN [<xref rid="bib42" ref-type="bibr">42</xref>]. However, these models are better suited to the detection framework rather than image-to-image transformation (see Discussion section for details).</p><p>The <italic toggle="yes">EmbedSeg</italic>-type models were reimplemented according to the original study [<xref rid="bib11" ref-type="bibr">10</xref>, <xref rid="bib12" ref-type="bibr">11</xref>] following the generic boilerplate in <italic toggle="yes">MMV_Im2Im</italic>, with significant improvement. First of all, following the modular design of <italic toggle="yes">MMV_Im2Im</italic>, it is flexible to use different neural network models as the backbone. For 3D anisotropic microscopy images, the original backbone ERFNet [<xref rid="bib43" ref-type="bibr">43</xref>] does not take the anisotropic dimensions into account and therefore may not perform well or even be applicable. In this scenario, it is straightforward to employ another anisotropic neural network bone, such as the anisotropic U-Net in [<xref rid="bib22" ref-type="bibr">21</xref>] or the anisotropic version of Dynamic U-Net in MONAI. Second, we significantly improved the training strategy. The original version requires precropping patches centered on each instance and precalculating the center images and class images. This may generate a massive amount of additional data on the disk. More important, such precropping makes data augmentation nearly impossible, except the simple ones like flipping (otherwise, the precalculated centers might be wrong), and also greatly undersamples around negative cases (e.g., background). For example, we have observed that for an EmbedSeg model training only with patches centered on instances, the model may suffer from degraded performance during inference when there are a large amount of background areas without any instances. Again, following the modular design of <italic toggle="yes">MMV_Im2Im</italic>, it is now possible to do on-the-fly data augmentation and patch sampling, even weighted patch sampling. Third, our improved <italic toggle="yes">EmbedSeg</italic>-type models can accept an exclusion mask so that certain parts of the images can be ignored during training. This is especially useful for partially annotated ground truth. For large images, it could be extremely time-consuming to require every single instance to be annotated. The exclusion masks can address this bottleneck. Another extension compared to the original implementation was that the <italic toggle="yes">MMV_Im2Im</italic> package made sliding windowing inference straightforward and therefore permitted easy handling of images of any size during inference.</p><p>In this work, we tested on both 2D and 3D instance segmentation problems. Going from 2D to 3D is not a simple generalization from 2D models by switching 2D operations with 3D operations, but with many practical challenges. Large GPU footprint is one of the biggest issues, which makes many training strategies common in 2D not feasible in 3D (e.g., limited mini-batch size). <italic toggle="yes">MMV_Im2Im</italic> is able to take advantage of state-of-the-art ML engineering methods to efficiently handle 3D problems. For example, by using effective half-precision training, one can greatly reduce GPU memory workload for each sample and therefore increase the batch size or the patch size. When multiple GPUs are available, it is also possible to easily take advantage of the additional resources to scale up the training to multiple GPU cards, even multiple GPU nodes. As a demonstration, we applied <italic toggle="yes">EmbedSeg</italic>-like models to a 2D problem of segmenting <italic toggle="yes">Caenorhabditis elegans</italic> from widefield images [<xref rid="bib44" ref-type="bibr">44</xref>], as well as a 3D problem of nuclear segmentation from fluorescent and brightfield images from the hiPSC single-cell image dataset [<xref rid="bib29" ref-type="bibr">29</xref>].</p><p>For the 2D problem, we adopted the same network backbone as in the original <italic toggle="yes">EmbedSeg</italic> paper. Example results on a small holdout set of 5 images are shown in Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5A</xref> (average precision at 50&#x000a0;=&#x000a0;0.866 &#x000b1; 0.163), which is comparable to the original published results [<xref rid="bib12" ref-type="bibr">11</xref>]. For the 3D problem, the original backbone is not directly applicable, due to the before mentioned anisotropic issue, and the images in the dataset do not contain enough Z-slices to run through all down-sampling blocks in 3D. The anisotropic UNet [<xref rid="bib22" ref-type="bibr">21</xref>] is used here. The segmentation results obtained from the public dataset [<xref rid="bib29" ref-type="bibr">29</xref>] contain nuclear instance segmentation of all cells. But, the cells touching the image borders are ignored from downstream analysis [<xref rid="bib29" ref-type="bibr">29</xref>] and therefore not curated. In other words, the segmentation from this public dataset can only be used as high-quality nuclear instance segmentation ground truth after excluding the areas covered by cells touching the image borders [<xref rid="bib29" ref-type="bibr">29</xref>]. Therefore, the exclusion masking function in <italic toggle="yes">MMV_Im2Im</italic> is very helpful in this example.</p><fig position="float" id="fig5"><label>Figure 5:</label><caption><p>(A) Results of 2D instance segmentation of <italic toggle="yes">C. elegans</italic>. A minor error can be observed in the zoom-in window. (B) Results of 3D nuclear instance segmentation from fluorescent images and brightfield images. The green box in the fluorescent image highlights a mitotic example. The side view panel shows the segmentation of one specific nucleus along the line annotated in the fluorescent image from the side. The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.</p></caption><graphic xlink:href="giad120fig5" position="float"/></fig><p>Example results are presented in Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5B</xref>. The green box highlighted a mitotic cell (the DNA signals forming &#x0201c;spaghetti&#x0201d; shapes). The average precision at 50 for the fluorescence model is 0.827 &#x000b1; 0.082, and it can be seen that the fluorescence model is able to distinguish the complex DNA signal from the background. Even holes can appear in the predicted segmentation, allowing the prediction of very complex shapes that are theoretically not feasible for other instance segmentation models like StarDist or Cellpose. Additionally, <italic toggle="yes">EmbedSeg</italic>-type models are able to assign spatially unrelated structures to the same instance (see Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5</xref>, bottom). Nuclear instance segmentation from brightfield images was much more challenging than from fluorescent images (average precision at 50&#x000a0;=&#x000a0;0.622 &#x000b1; 0.101).</p></sec><sec id="sec2-4"><title>Comparing semantic segmentation and instance segmentation of organelles from 3D confocal microscopy images</title><p>We did a special comparison in this subsection to further illustrate the difference between semantic and instance segmentations. We took the 3D fibrillarin dataset from [<xref rid="bib29" ref-type="bibr">29</xref>]. There are multiple channels in each 3D image, including DNA dye, membrane dye, and the structure channel (i.e., fibrillarin in this case). The original fibrillarin segmentation released with the dataset is a semantic segmentation (0 = background, 1 = fibrillarin). With the additional cell segmentation available in the dataset, we can know which groups of segmented fibrillarin belong to the same cell. Then, we can convert the original 3D fibrillarin semantic segmentation ground truth into 3D instance segmentation ground truth (fibrillarin pixels belonging to the same cell are grouped as a unique instance). Sample images and results are shown in Fig.&#x000a0;<xref rid="fig6" ref-type="fig">6</xref>. We can observe that the semantic segmentation model is able to achieve good accuracy in determining pixels from the fibrillarin signals (F1&#x000a0;=&#x000a0;0.958 &#x000b1; 0.008). Meanwhile, the instance segmentation can group them properly (average precision at 50&#x000a0;=&#x000a0;0.795 &#x000b1; 0.055) so that fibrillarin masks from the same cell are successfully identified as unique instances, even without referring to the cell membrane channel or cell segmentation results. This is not a simple grouping step based on distance, since the fibrillarin signals from tightly touching nuclei may exist very close to each other.</p><fig position="float" id="fig6"><label>Figure 6:</label><caption><p>Comparing 3D semantic segmentation and 3D instance segmentation results on confocal microscopy images of fibrillarin (showing a middle Z-slice of a 3D stack), showing true-positive, false-negative, and false-positive pixels.</p></caption><graphic xlink:href="giad120fig6" position="float"/></fig></sec><sec id="sec2-5"><title>Unsupervised semantic segmentation of intracellular structures from 2D/3D confocal microscopy images</title><p>Large amounts of high-quality segmentation ground truth are not always available or may require endless effort to collect for a segmentation task. CycleGAN-based methods have opened up a new avenue for segmentation without the need for pixel-wise ground truth [<xref rid="bib13" ref-type="bibr">12</xref>]. In this subsection, we demonstrate an unsupervised learning-based segmentation method on 4 examples: 2D tight-junction (via ZO1) segmentation from 2D FP-tagged ZO1 images (max-projected from 3D stacks) and segmentation of nuclei, mitochondria, and Golgi from 3D confocal microscopy images.</p><p>To perform unsupervised learning, we used raw images from the hiPSC single-cell image dataset [<xref rid="bib29" ref-type="bibr">29</xref>], as well as their corresponding segmentations (may not be absolute pixel-wise ground truth but have gone through systematic evaluation to ensure the overall quality). We shuffled the raw images and their segmentations to generate a set of simulated segmentation masks. A demonstration of the concept is illustrated in Fig.&#x000a0;<xref rid="fig7" ref-type="fig">7A</xref>. Example results for all 3D models are shown in Fig.&#x000a0;<xref rid="fig7" ref-type="fig">7B</xref>, and the F1-scores on the test set are summarized in Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref>.</p><fig position="float" id="fig7"><label>Figure 7:</label><caption><p>(A) Illustration of the unsupervised learning scheme and results in the 2D tight-junction segmentation problem. (B) Example 3D segmentation results (only showing a middle Z-slice) from models obtained by unsupervised learning. The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.</p></caption><graphic xlink:href="giad120fig7" position="float"/></fig><table-wrap position="float" id="tbl2"><label>Table 2:</label><caption><p>F1-scores of the unsupervised semantic segmentation predictions</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Dimensionality</th><th rowspan="1" colspan="1">Dataset</th><th rowspan="1" colspan="1">F1-score</th><th rowspan="1" colspan="1"># of test data</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">2D</td><td rowspan="1" colspan="1">Tight junction</td><td rowspan="1" colspan="1">0.906 &#x000b1; 0.011</td><td rowspan="1" colspan="1">18</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">Nucleus</td><td rowspan="1" colspan="1">0.836 &#x000b1; 0.081</td><td rowspan="1" colspan="1">31</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">Golgi</td><td rowspan="1" colspan="1">0.689 &#x000b1; 0.057</td><td rowspan="1" colspan="1">44</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">Mitochondria</td><td rowspan="1" colspan="1">0.804 &#x000b1; 0.015</td><td rowspan="1" colspan="1">54</td></tr></tbody></table></table-wrap><p>For the 2D example, we saw that the unsupervised training provides a valuable segmentation, which is reflected by the F1-score in Table&#x000a0;<xref rid="tbl2" ref-type="table">2</xref>. For the 3D examples, it has been suggested that the quality of unsupervised nuclei segmentation could be further improved with additional simulation strategies [<xref rid="bib13" ref-type="bibr">12</xref>]. Overall, we believe that unsupervised learning offers an effective way to generate preliminary segmentation, which can be further refined through active learning such as the iterative DL workflow described in [<xref rid="bib22" ref-type="bibr">21</xref>].</p></sec><sec id="sec2-6"><title>Generating synthetic microscopy images from binary masks</title><p>Generating a large amount of synthetic microscopy images can be an important step in developing image analysis methods. Synthetic images offer a way to train other DL models, such as self-supervised pretraining, using a diverse set of images without the need for large amounts of real-world data. As long as the synthetic images are generated with sufficient quality, it is possible to have an unlimited amount of training data for certain applications. Moreover, synthetic images can be used to evaluate other models when validation data are difficult to obtain. In this study, we demonstrate that <italic toggle="yes">MMV_Im2Im</italic> can generate 2D/3D synthetic microscopy images with high realism and validity, using a subset of data collected from the hiPSC single-cell image dataset [<xref rid="bib29" ref-type="bibr">29</xref>], in either a supervised or an unsupervised manner.</p><p>For 2D demonstration, we extracted the middle Z-slice from NPM1 images as the training target, while using the NPM1 segmentation results as the input binary masks. With the paired &#x0201c;mask + microscopy image&#x0201d; data, we could train the model in a supervised fashion or randomly shuffle the data to simulate the situation without paired data, which can be trained in an unsupervised fashion using the CycleGAN-type framework implemented in <italic toggle="yes">MMV_Im2Im</italic>. Example results can be found in Fig.&#x000a0;<xref rid="fig8" ref-type="fig">8A</xref> and Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref>. In general, the supervised synthesization can generate more realistic images than the unsupervised model.</p><fig position="float" id="fig8"><label>Figure 8:</label><caption><p>Example results of (A) 2D synthetic fluorescent images of nucleoli (via NPM1) and (B) 3D synthetic fluorescent images of H2B (middle Z-slices of a Z-stack) with a coarse mask and a fine mask as the input.</p></caption><graphic xlink:href="giad120fig8" position="float"/></fig><table-wrap position="float" id="tbl3"><label>Table 3:</label><caption><p>Results of the synthetic generation of microscopy images from binary masks</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Dimensionality</th><th rowspan="1" colspan="1">Dataset</th><th rowspan="1" colspan="1">Training</th><th rowspan="1" colspan="1">Pearson correlation</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">2D</td><td rowspan="1" colspan="1">NPM1</td><td rowspan="1" colspan="1">Supervised</td><td rowspan="1" colspan="1">0.925 &#x000b1; 0.019</td></tr><tr><td rowspan="1" colspan="1">2D</td><td rowspan="1" colspan="1">NPM1</td><td rowspan="1" colspan="1">Unsupervised</td><td rowspan="1" colspan="1">0.913 &#x000b1; 0.023</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">H2B_coarse</td><td rowspan="1" colspan="1">Supervised</td><td rowspan="1" colspan="1">0.841 &#x000b1; 0.023</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">H2B_coarse</td><td rowspan="1" colspan="1">Unsupervised</td><td rowspan="1" colspan="1">0.796 &#x000b1; 0.035</td></tr><tr><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">H2B_fine</td><td rowspan="1" colspan="1">Supervised</td><td rowspan="1" colspan="1">0.939 &#x000b1; 0.009</td></tr></tbody></table></table-wrap><p>For 3D demonstration, we use 3D H2B images with 2 different types of input masks. First, we attempted to generate synthetic images from a coarse mask (i.e., only the overall shape of the nucleus, available as nuclear segmentation from the dataset) with both supervised training and unsupervised training. The unsupervised model in <italic toggle="yes">MMV_Im2Im</italic> uses the CycleGAN-based approaches. So, the unsupervised training is actually already done within the unsupervised segmentation experiments. In other words, the unsupervised model works in a bidirectional way, from real microscopy images to binary masks, and also from binary masks to simulated microscopy images. Here, we could also do the inference in a different direction (from binary to simulated microscopy) using the model trained in the unsupervised segmentation section. The results are shown in Fig.&#x000a0;<xref rid="fig8" ref-type="fig">8B</xref> (row 1). The unsupervised synthesization can mostly &#x0201c;paint&#x0201d; the mask with homogeneous grayscale intensity, while the supervised model can simulate the textures to some extent. For a relatively large mask, it could be challenging for a model to fill in sufficient details to simulate real microscopy images (might be improved with diffusion-based models; see Discussion section).</p><p>We made another attempt with 3D masks containing finer details beyond the overall shapes. So, we employed the H2B structure segmentation results from the dataset (capturing the detailed nuclear components marked by histone H2B) as the input for supervised synthesization. The result is shown in Fig.&#x000a0;<xref rid="fig8" ref-type="fig">8B</xref> (row 2). Compared to the synthesization with coarse masks, the images simulated from fine masks exhibit a much more realistic appearance. As we can see, it is important to design the solutions with proper data. Preliminary quantitative evaluations on all synthesization experiments are summarized in Table&#x000a0;<xref rid="tbl3" ref-type="table">3</xref>.</p></sec><sec id="sec2-7"><title>Image denoising for microscopy images</title><p>
<italic toggle="yes">MMV_Im2Im</italic> can also be used to computationally reduce image noise or restore the data from various sources of imaging artifacts, so as to increase the feasibility and efficiency in downstream analysis. In the current version of <italic toggle="yes">MMV_Im2Im</italic>, the restoration model can only be trained in a fully supervised manner. Therefore, aligned low-quality and high-quality images are required for supervision, even though such paired data can be partially simulated [<xref rid="bib7" ref-type="bibr">6</xref>]. Other methods, such as unsupervised learning-based solutions [<xref rid="bib45" ref-type="bibr">45</xref>], will be made available within <italic toggle="yes">MMV_Im2Im</italic> in future versions.</p><p>In this example, we presented an image denoising demonstration with sample data from [<xref rid="bib46" ref-type="bibr">46</xref>]. The goal was to increase the quality of low signal-to-noise ratio (SNR) images of nucleus-stained flatworms (<italic toggle="yes">Schmidtea mediterranea</italic>, planaria) and lightsheet images of <italic toggle="yes">Tribolium castaneum</italic> (red flour beetle) embryos. The models were trained with paired data acquired with low and high laser intensity on fixed samples and then applied on live imaging data. For the nucleus-stained flatworm data (a test set of 20 images is available), the model achieved a Pearson correlation of 0.392 &#x000b1; 0.065, while the Pearson correlation between the noisy raw and ground-truth images was 0.065 &#x000b1; 0.057. For the red flour beetle dataset, the model has improved the Pearson correlation from 0.219 &#x000b1; 0.045 to 0.444 &#x000b1; 0.077 (6 images). Based on this and the results in Fig.&#x000a0;<xref rid="fig9" ref-type="fig">9</xref>, it can be observed that the low SNR images can be greatly improved. Systematic quantitative evaluations would be necessary to confirm the biological validity but beyond the scope of this article.</p><fig position="float" id="fig9"><label>Figure 9:</label><caption><p>Denoising results of 3D images of nucleus-stained flatworm (planaria) and <italic toggle="yes">Tribolium castaneum</italic> embryos at a single Z-slice each. It can be seen that the predicted images have a greatly reduced SNR. Left: raw images (low SNR), middle: reference images (high SNR), right: predictions. The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.</p></caption><graphic xlink:href="giad120fig9" position="float"/></fig></sec><sec id="sec2-8"><title>Imaging modality transformation from 3D confocal microscopy images to stimulated emission depletion microscopy images</title><p>Another important application of image-to-image transformation is imaging modality transformation [<xref rid="bib9" ref-type="bibr">8</xref>], usually from one &#x0201c;cheaper&#x0201d; modality with lower resolution (e.g., with larger FOV, easier to acquire and scale up) to another modality with higher resolution but expensive to obtain. Such models will permit a new way in assay development strategy to take advantage of all the benefits of the cheaper modality with lower resolution and still be able to enhance the resolution computationally post hoc. To demonstrate the application of <italic toggle="yes">MMV_Im2Im</italic> in this scenario, we took an example dataset with paired 3D confocal and stimulated emission depletion (STED) images of 2 different cellular structures, microtubule and nuclear pore [<xref rid="bib9" ref-type="bibr">8</xref>]. Sample results are summarized in Figs.&#x000a0;<xref rid="fig10" ref-type="fig">10</xref> and&#x000a0;<xref rid="fig11" ref-type="fig">11</xref>. The corresponding error plots show pixel-based absolute differences between ground truth and prediction. Intensities were normalized to the interval from &#x02212;1 to 1 for training, with intensity limits restricted to the 0.01 percentile and 99.99 percentile values of the intensity distribution.</p><fig position="float" id="fig10"><label>Figure 10:</label><caption><p>Example results of confocal-to-STED modality transformation of microtubule in 3 consecutive Z-slices. From top to bottom: raw images, reference STED images, predicted images, error plots. For the error plots, the ground-truth images were normalized as described in the main text. The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.</p></caption><graphic xlink:href="giad120fig10" position="float"/></fig><fig position="float" id="fig11"><label>Figure 11:</label><caption><p>Example results of confocal-to-STED modality transformation nuclear pore in 3 consecutive Z-slices. From top to bottom: raw images, reference STED images, predicted images, error plots. For the error plots, the ground-truth images were normalized as described in the main text. The contrast of grayscale images was adjusted using ImageJ&#x02019;s autoscale.</p></caption><graphic xlink:href="giad120fig11" position="float"/></fig><p>For microtubule, the model achieved Pearson correlation of 0.786 &#x000b1; 0.020 and a peak signal-to-noise ratio of 21.201 &#x000b1; 0.586, while for nuclear pore complex, the Pearson correlation was 0.744 &#x000b1; 0.025 and the peak signal-to-noise ratio was 22.939 &#x000b1; 1.896. Considering a Pearson correlation of 0.699 &#x000b1; 0.030 and a peak signal-to-noise ratio of 18.847 &#x000b1; 0.649 for the microtubule dataset and a Pearson correlation of 0.656 &#x000b1; 0.033 and a peak signal-to-noise ratio of 20.352 &#x000b1; 1.009 of the lower-resolution raw images with the higher-resolution ground truth, this approach improved data quality. Also, visual inspection can confirm the effectiveness of the models. Again, it would be necessary to conduct further quantitative evaluation to ensure the validity of users&#x02019; specific problems.</p></sec><sec id="sec2-9"><title>Staining transformation in multiplex experiments</title><p>DL has emerged as a powerful tool for multiplex imaging, a powerful technique that enables the simultaneous detection and visualization of multiple biomolecules within a single tissue sample. This technique is increasingly being used in biomedical experiments but demands efficient image analysis solutions to accurately identify and quantify the different biomolecules of interest at scale. DL has demonstrated great potential in analyzing multiplex datasets, as it can automatically learn the complex relationships between different biomolecules and their spatial distribution within tissues. Specifically, in this study, we present the effectiveness of <italic toggle="yes">MMV_Im2Im</italic> in transforming tissue images from one staining to another, which will permit efficient coregistration, colocalization, and quantitative analysis of multiplex datasets. We used the sample dataset from [<xref rid="bib6" ref-type="bibr">5</xref>]. In this example, we trained 3 different models to transform IHC (Immunohistochemistry) images to images of standard hematoxylin stain, mpIF nuclear (DAPI), and mpIF LAP2beta (a nuclear envelope stain). Example results can be observed in Fig.&#x000a0;<xref rid="fig12" ref-type="fig">12</xref> to verify the results qualitatively, and the respective metrics can be found in Table&#x000a0;<xref rid="tbl4" ref-type="table">4</xref>. It is worth mentioning that there is a pixel shift in the mpIF LAP2beta holdout dataset, but image registration is beyond the scope of this article. We show the metrics as an example of an evaluation of the transformed images, but we leave an application-specific evaluation to the appropriate researchers. But it is evident that these transformed images can provide valuable insights into the localization and expression patterns of specific biomolecules spatially.</p><fig position="float" id="fig12"><label>Figure 12:</label><caption><p>Qualitative visualization of staining transformation results with the <italic toggle="yes">MMV_Im2Im</italic> package. The top row refers to the input image (IHC) and to the respective ground truth for hematoxylin, DAPI, and LAP2beta, while the bottom row shows the respective prediction.</p></caption><graphic xlink:href="giad120fig12" position="float"/></fig><table-wrap position="float" id="tbl4"><label>Table 4:</label><caption><p>Results of the staining transformation in multiplex experiments, derived from 51 holdout images each</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Dataset</th><th rowspan="1" colspan="1">Pearson correlation</th><th rowspan="1" colspan="1">Structural similarity</th><th rowspan="1" colspan="1">Peak signal-to-noise ratio</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Hematoxylin</td><td rowspan="1" colspan="1">0.860 &#x000b1; 0.075</td><td rowspan="1" colspan="1">0.453 &#x000b1; 0.063</td><td rowspan="1" colspan="1">23.855 &#x000b1; 1.742</td></tr><tr><td rowspan="1" colspan="1">DAPI</td><td rowspan="1" colspan="1">0.920 &#x000b1; 0.049</td><td rowspan="1" colspan="1">0.770 &#x000b1; 0.067</td><td rowspan="1" colspan="1">26.754 &#x000b1; 2.129</td></tr><tr><td rowspan="1" colspan="1">LAP2beta</td><td rowspan="1" colspan="1">0.435 &#x000b1; 0.087</td><td rowspan="1" colspan="1">0.597 &#x000b1; 0.083</td><td rowspan="1" colspan="1">22.415 &#x000b1; 1.586</td></tr></tbody></table></table-wrap></sec><sec id="sec2-10"><title>Overview of used frameworks</title><p>From all experiments above (37 in total), we want to demonstrate the great flexibility of <italic toggle="yes">MMV_Im2Im</italic> and not to optimize every task in detail. Presenting all detailed configurations in these 37 experiments in the article could lead to more confusion than clarity. To this end, we give a high-level overview of the key information of each task in Table&#x000a0;<xref rid="tbl5" ref-type="table">5</xref>, hoping to serve as a valuable starting point for researchers to optimize their DL-based image-to-image transformation using <italic toggle="yes">MMV_Im2Im</italic>. The full configuration details are available in human-readable formats in our GitHub repository [<xref rid="bib1" ref-type="bibr">22</xref>].</p><table-wrap position="float" id="tbl5"><label>Table 5:</label><caption><p>Overview of the used frameworks for the demonstrated tasks</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Task</th><th rowspan="1" colspan="1">Dimension</th><th rowspan="1" colspan="1">Framework</th><th rowspan="1" colspan="1">Backbone</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Label-free</td><td rowspan="1" colspan="1">2D/3D</td><td rowspan="1" colspan="1">FCN, Pix2pix, CycleGAN</td><td rowspan="1" colspan="1">fnet, UNet, AttentionUnet, SwinUNETR, &#x02026;</td></tr><tr><td rowspan="1" colspan="1">Semantic segmentation</td><td rowspan="1" colspan="1">2D/3D</td><td rowspan="1" colspan="1">FCN, CycleGAN</td><td rowspan="1" colspan="1">AttentionUnet, DynUnet, UNet3D</td></tr><tr><td rowspan="1" colspan="1">Instance segmentation</td><td rowspan="1" colspan="1">2D/3D</td><td rowspan="1" colspan="1">EmbedSeg</td><td rowspan="1" colspan="1">BranchedERFNet_2d, UNet3D</td></tr><tr><td rowspan="1" colspan="1">Synthetic</td><td rowspan="1" colspan="1">2D/3D</td><td rowspan="1" colspan="1">Pix2pix</td><td rowspan="1" colspan="1">AttentionUnet, fnet</td></tr><tr><td rowspan="1" colspan="1">Denoising</td><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">FCN</td><td rowspan="1" colspan="1">UNet</td></tr><tr><td rowspan="1" colspan="1">Modality transformation</td><td rowspan="1" colspan="1">3D</td><td rowspan="1" colspan="1">FCN</td><td rowspan="1" colspan="1">UNet3D</td></tr><tr><td rowspan="1" colspan="1">Staining transformation</td><td rowspan="1" colspan="1">2D</td><td rowspan="1" colspan="1">Pix2pix</td><td rowspan="1" colspan="1">predefined_unet</td></tr></tbody></table></table-wrap></sec></sec><sec sec-type="materials|methods" id="sec3"><title>Methods</title><sec id="sec3-1"><title>Overview of the codebase</title><p>Overall, the package inherited the boilerplate concept from pytorch-lightning [<xref rid="bib15" ref-type="bibr">14</xref>] and was made fully configurable via yaml files supported by pyrallis [<xref rid="bib47" ref-type="bibr">47</xref>], as well as largely employed state-of-the-art DL components from MONAI [<xref rid="bib20" ref-type="bibr">19</xref>]. The 3 key parts in the package, <monospace>mmv_im2im.models, mmv_im2im.data_modules</monospace>, and <monospace>Trainers</monospace>, will be further described below.</p></sec><sec id="sec3-2"><title>Main frameworks for <italic toggle="yes">mmv_im2im.models</italic></title><p>
<italic toggle="yes">mmv_im2im.models</italic> is the core module defining the DL framework for your problem, where we can instantiate the neural network architecture and define what to do before training starts, what to do in each training and validation step, what to do at the end of each epoch, and so on. All are implemented following the same lightning module from pytorch-lightning, which makes the code very easy to read, to understand, and even to extend.</p><p>In general, there are mainly 4 major DL frameworks that can be applied to microscopy image-to-image transformation: supervised learning with a fully convolutional network (FCN)&#x02013;type model, supervised learning with pix2pix-type models, unsupervised learning to learn mapping between visual domains, and Self2Self-type self-supervised learning [<xref rid="bib48" ref-type="bibr">48</xref>]. The major difference between FCN-based supervised learning and pix2pix-based supervised learning is that the pix2pix framework extends an FCN model with an adversarial head as a discriminator to further improve the realism of the prediction. The major difference between the unsupervised framework and the self-supervised framework is that the unsupervised methods still require examples of the target images, even though the source images and target images do not need to be from the same sample or pixel-wise aligned. But, the self-supervised framework would only need the original images, which could be really helpful when it is impossible to acquire the target images (e.g., there is no truly noise-free or artifact-free image).</p><p>Currently, for supervised frameworks, both the FCN type and pix2pix type are well supported in the <italic toggle="yes">MMV_Im2Im</italic> (<ext-link xlink:href="https://scicrunch.org/resolver/RRID:SCR_024630" ext-link-type="uri">RRID:SCR_024630</ext-link>) package. Since our package is designed in a very generic way, it is possible to continuously expand the functionalities when available (ideally with community contributions). For example, diffusion models [<xref rid="bib49" ref-type="bibr">49</xref>] can be thought of as a modern extension of the pix2pix-type framework and therefore are within our horizon to include into <italic toggle="yes">MMV_Im2Im</italic>. For the unsupervised framework, only CycleGAN-type methods are supported. We are planning to extend the unsupervised framework with Imaginaire [<xref rid="bib50" ref-type="bibr">50</xref>], which will greatly extend the applicability of <italic toggle="yes">MMV_Im2Im</italic> (e.g., learning the transformation from one single image to another single image or one set of images to another set of images). Meanwhile, supporting the self-supervised framework will be our next major milestone.</p></sec><sec id="sec3-3"><title>Customized <italic toggle="yes">mmv_im2im.data_modules</italic> for bioimaging applications</title><p>The <italic toggle="yes">data_modules</italic> implements a general module for data handling for all different frameworks mentioned above, from how to load the data to how to set up the dataloader for training and validation. Different people may prefer to organize their training data in different ways, such as using csv to organize input and the corresponding ground truth, or making different folders (e.g., &#x0201c;image&#x0201d; and &#x0201c;ground_truth&#x0201d;) with input and the corresponding ground truth sharing the same file name, and so on. Or some people may prefer to do a random train/validation split, while others like to presplit train and validation into different folders, and so forth. Currently, the data_module in <italic toggle="yes">MMV_Im2Im</italic> supports 4 different ways of data loading, where we try to cover as many common scenarios as possible, so that everyone will feel comfortable using it.</p><p>A big challenge in the dataloader in bioimaging applications is that there could be not only a large amount of files but files of very large sizes. To deal with each individual large image, we used the delayed loading from aicsimageio for efficient image reading. Besides, we adopted the <monospace>PersistentDataloader</monospace> from MONAI to further optimize the efficiency. In specific, after loading a large image and running through all the deterministic operations, like intensity normalization or spatial padding, the <monospace>PersistentDataLoader</monospace> will pickle and save the data in a temporary folder, to avoid repeating the heavy computation on large files in each training iteration. To handle the potentially large number of files, we implemented the data_module with the capability of loading only a certain portion of the data into the memory in each epoch and reloading with a different portion every certain number of epochs. By doing this, we were able to efficiently train an instance segmentation model with more than 125&#x000a0;K images, where each raw image is about 15&#x000a0;MB.</p></sec><sec id="sec3-4"><title>State-of-the-art training with the pytorch-lightning Trainer</title><p>We fully adopted the Trainer from pytorch-lightning, which has been widely used by the machine learning community and widely tested on both R&#x00026;D problems and industrial-scale applications. In a nutshell, simply by specifying the training parameters in the yaml file, users can set up multi-GPU training, half-precision training, automatic learning rate finder, automatic batch size finder, early stopping, stochastic weight averaging, and so on. This allows users to focus on the research problems without worrying about the ML engineering.</p></sec></sec><sec sec-type="discussion" id="sec4"><title>Discussion</title><p>In this work, we presented a new open-source Python package <italic toggle="yes">MMV_Im2Im</italic> package for image-to-image transformations in bioimaging applications. We demonstrated the applicability on more than 10 different problems or datasets to give biomedical researchers a holistic view of the general image-to-image transformation concepts with diverse examples. This package was not a simple collection of existing methods. Instead, we distilled the knowledge from existing methods and created this generic version with state-of-the-art ML engineering techniques, which made the package easy to understand, easy to use, and easy to extend for future. We hope this package can serve as the starting point for other researchers doing AI-based image-to-image transformation research and eventually build a large shared community in the field of image-to-image transformation for bioimaging.</p><sec id="sec4-1"><title>Further works</title><p>One of main directions for extending <italic toggle="yes">MMV_Im2Im</italic> is to prepack common bioimaging datasets as a Dataset module, so that DL researchers can use it for algorithm development and benchmarking, and new users can easily use it for learning microscopy image-to-image transformation. We will continue improving the functionalities of the package, such as supporting more models and methods, such as diffusion-based models [<xref rid="bib49" ref-type="bibr">49</xref>], unsupervised denoising [<xref rid="bib45" ref-type="bibr">45</xref>], or Imaginaire [<xref rid="bib50" ref-type="bibr">50</xref>]. Besides, we also plan to develop 2 auxiliary packages, <italic toggle="yes">MMV_Im2Im_Auto</italic> and <italic toggle="yes">MMV_Im2Im_Active</italic>. In specific, when you have a reasonable amount of training data, <italic toggle="yes">MMV_Im2Im_Auto</italic> will take advantage of the fact that <italic toggle="yes">MMV_Im2Im</italic> is fully configurable with yaml files and automatically generates a set of potentially good configurations, then find the optimal solution for you by cross-validation. On the other hand, when you only have very limited training data, or even with only pseudo ground truth, <italic toggle="yes">MMV_Im2Im_Active</italic> will help to build preliminary models from the limited training data and gradually refine the model with human-in-the-loop by active learning [<xref rid="bib22" ref-type="bibr">21</xref>]. All the packages will also be wrapped into napari plugins [<xref rid="bib51" ref-type="bibr">51</xref>] to allow no-code operation and therefore be more friendly to users without experience in programming.</p><p>The image-to-image transformation frameworks implemented in the current version do not explicitly take temporal information into account. We treat images (2D or 3D) at each time step independently. Thanks to the flexibility of aicsimageio, our package can directly read even multichannel 3D time-lapse data (i.e, 5D) during training or inference, if necessary. But the computation is done at individual time steps. A common method to integrate the temporal context with spatial information is the convolutional recurrent neural network (CRNN) [<xref rid="bib52" ref-type="bibr">52</xref>]. The support of CRNN will be part of our future work.</p><p>Another type of microscopy image analysis problem related to image-to-image transformation is image registration, where we learn how to transform the &#x0201c;floating&#x0201d; image spatially so that it is optimally aligned with the reference image in the physical space. Recent methods are able to transform the floating image into its registered version through deep neural networks [<xref rid="bib53" ref-type="bibr">53</xref>]. This will be another important direction for future extension.</p><p>Beyond <italic toggle="yes">MMV_Im2Im</italic>, we hope to develop a similar package for other problems (without reinventing wheels). For example, as we mentioned in the instance segmentation application, Mask-RCNN type models are also very powerful instance segmentation methods and, in theory, can also be generalized beyond 2D images. However, Mask-RCNN would fit more to a detection framework, instead of image-to-image transformation. It will be supported in our <italic toggle="yes">MMV_NDet</italic> (NDet&#x000a0;=&#x000a0;N-dimensional detection) package, currently under development.</p></sec></sec><sec id="sec5"><title>Code Availability and Requirements</title><list list-type="bullet"><list-item><p>Project name: MMV_Im2Im (Microscopy Machine Vision, Image-to-Image transformation)</p></list-item><list-item><p>Project homepage: [<xref rid="bib1" ref-type="bibr">22</xref>]</p></list-item><list-item><p>Operating system(s): Linux and Windows (when using GPU), also MacOS (when only using CPU)</p></list-item><list-item><p>Programming language: Python</p></list-item><list-item><p>Other requirements: PyTorch 2.0.1 or higher, PyTorch Lightning &#x0003e;2.0.0, and all other additional dependencies are specified as in [<xref rid="bib1" ref-type="bibr">22</xref>]</p></list-item><list-item><p>License: MIT license</p></list-item></list><p>To enhance the accessibility and traceability of our toolbox, we registered it with biotools (bio.tools ID: biotools:mmv_im2im) and workflow hub [<xref rid="bib54" ref-type="bibr">54</xref>].</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>giad120_GIGA-D-23-00068_Original_Submission</label><media xlink:href="giad120_giga-d-23-00068_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup2" position="float" content-type="local-data"><label>giad120_GIGA-D-23-00068_Revision_1</label><media xlink:href="giad120_giga-d-23-00068_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup3" position="float" content-type="local-data"><label>giad120_GIGA-D-23-00068_Revision_2</label><media xlink:href="giad120_giga-d-23-00068_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup4" position="float" content-type="local-data"><label>giad120_GIGA-D-23-00068_Revision_3</label><media xlink:href="giad120_giga-d-23-00068_revision_3.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup5" position="float" content-type="local-data"><label>giad120_Response_to_Reviewer_Comments_Original_Submission</label><media xlink:href="giad120_response_to_reviewer_comments_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup6" position="float" content-type="local-data"><label>giad120_Response_to_Reviewer_Comments_Revision_1</label><media xlink:href="giad120_response_to_reviewer_comments_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup7" position="float" content-type="local-data"><label>giad120_Response_to_Reviewer_Comments_Revision_2</label><media xlink:href="giad120_response_to_reviewer_comments_revision_2.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup8" position="float" content-type="local-data"><label>giad120_Reviewer_1_Report_Original_Submission</label><caption><p>Chris Armit -- 5/14/2023 Reviewed</p></caption><media xlink:href="giad120_reviewer_1_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup9" position="float" content-type="local-data"><label>giad120_Reviewer_1_Report_Revision_1</label><caption><p>Chris Armit -- 10/10/2023 Reviewed</p></caption><media xlink:href="giad120_reviewer_1_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup10" position="float" content-type="local-data"><label>giad120_Reviewer_2_Report_Original_Submission</label><caption><p>Estibaliz G&#x000c3;<sup>3</sup>mez-de-Mariscal -- 5/29/2023 Reviewed</p></caption><media xlink:href="giad120_reviewer_2_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup11" position="float" content-type="local-data"><label>giad120_Reviewer_2_Report_Revision_1</label><caption><p>Estibaliz G&#x000c3;<sup>3</sup>mez-de-Mariscal -- 10/9/2023 Reviewed</p></caption><media xlink:href="giad120_reviewer_2_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgement</title><p>We thank the MONAI team for their support in our process of development and the aicsimageio team for advice on how to integrate aicsimageio into the package..</p></ack><sec sec-type="data-availability" id="sec6"><title>Data Availability</title><p>In general, all data used in this work were from open-accessible public repositories, released with other publications under open-source licenses. All data used in this work were only for research purposes, and we confirm that we did not use these for any other noncommercial purpose or commercial purpose. The scripts we used to download and reorganize the data can be found in the release branch called &#x0201c;paper_version&#x0201d; within our repository [<xref rid="bib1" ref-type="bibr">22</xref>]. Detailed information about each dataset is listed below, in the same order as the Results section. Snapshots of our code and other data further supporting this work are openly available in the <italic toggle="yes">GigaScience</italic> repository, GigaDB [<xref rid="bib55" ref-type="bibr">55</xref>]. In addition, we deposited all the trained models and sample data at Zenodo [<xref rid="bib56" ref-type="bibr">56</xref>] to ensure the reproducibility of our work.</p><sec id="sec6-1"><title>1. Label-free prediction of nuclear structure from 2D/3D brightfield images:</title><p>
<bold>2D:</bold> The data were downloaded from [<xref rid="bib28" ref-type="bibr">28</xref>] and [<xref rid="bib57" ref-type="bibr">57</xref>]. We used all the data from the 2 sources, while 15% of the data were held out for testing. In specific, for data source 1 [<xref rid="bib28" ref-type="bibr">28</xref>], it contains a time-lapse tiff of 240 time steps, each with 5 channels (only channels 3 and 5 were used in this work).</p><list list-type="bullet"><list-item><p>Channel 1: Low Contrast Digital Phase Contrast (DPC)</p></list-item><list-item><p>Channel 2: High Contrast DPC</p></list-item><list-item><p>Channel 3: Brightfield (the input in our study)</p></list-item><list-item><p>Channel 4: EGFP-&#x003b1;-tubulin</p></list-item><list-item><p>Channel 5: mCherry-H2B (the ground truth in our study)</p></list-item></list><p>For data source 2 [<xref rid="bib57" ref-type="bibr">57</xref>], it contains 2 subfolders (train and test), each with snapshots sliced from different time-lapse data. Each snapshot is saved as 6 different tiff files (only the _bf and the second channel of _fluo were used in this work):</p><list list-type="bullet"><list-item><p>_bf: bright field (the input in our study)</p></list-item><list-item><p>_cyto: cytoplasm segmentation mask</p></list-item><list-item><p>_dpc: phase contrast</p></list-item><list-item><p>_fluo: 2 channels, first cytoplasm, second H2B (the H2B channel is the ground truth in our study)</p></list-item><list-item><p>_nuclei: nuclei segmentation mask</p></list-item><list-item><p>_sqrdpc: square-root phase contrast</p></list-item></list><p>
<bold>3D:</bold> The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket [<xref rid="bib58" ref-type="bibr">58</xref>], which was released with the publication [<xref rid="bib29" ref-type="bibr">29</xref>]. Each FOV is a multichannel 3D image, of which the brightfield and the corresponding structure channels were used as input and ground truth, respectively. Experiments were done on 4 different cell lines: fibrillarin (structure_name&#x000a0;=&#x000a0;&#x0201c;FBL&#x0201d;), nucleophosmin (structure_name&#x000a0;=&#x000a0;&#x0201c;NPM1&#x0201d;), lamin b1 (structure_name&#x000a0;=&#x000a0;&#x0201c;LMNB1&#x0201d;), and histone H2B (structure_name&#x000a0;=&#x000a0;&#x0201c;HIST1H2BJ&#x0201d;), with 20% of the data held out for testing.</p></sec><sec id="sec6-2"><title>2. 2D semantic segmentation of tissues from H&#x00026;E images</title><p>These data were originally used for the MICCAI GlaS challenge [<xref rid="bib59" ref-type="bibr">59</xref>] and are also available from a number of other sources [<xref rid="bib60" ref-type="bibr">60</xref>, <xref rid="bib61" ref-type="bibr">61</xref>]. We had 1 training set (85 images) and 2 test sets (60 and 20 images). We kept the same train/test split as in the challenge.</p></sec><sec id="sec6-3"><title>3. Instance segmentation in microscopy images</title><p>
<bold>2D:</bold> The data were downloaded from [<xref rid="bib62" ref-type="bibr">62</xref>] for segmenting <italic toggle="yes">C. elegans</italic> from widefield images [<xref rid="bib63" ref-type="bibr">63</xref>]. We used all images from the dataset, while 5% of the data were held out for testing.</p><p>
<bold>3D:</bold> The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used the lamin b1 cell line (structure_name&#x000a0;=&#x000a0;&#x0201c;LMNB1&#x0201d;) for these experiments. Each raw FOV is a multichannel 3D image (DNA dye channel, membrane dye channel, structure channel, and brightfield channel), with the instance segmentation of all nuclei available. In our 2 experiments, we used the DNA dye channel and the brightfield channel as input, respectively, while using the same 3D instance segmentation ground truth. Twenty percent of the data were held out for testing.</p></sec><sec id="sec6-4"><title>4. Comparing semantic segmentation and instance segmentation of organelles from 3D confocal microscopy images</title><p>The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used the fibrillarin cell line (structure_name&#x000a0;=&#x000a0;&#x0201c;FBL&#x0201d;) for these experiments. Each raw FOV is a multichannel 3D image (DNA dye channel, membrane dye channel, structure channel, and brightfield channel). The input is always the structure channel. Then, we used the FBL_fine workflow in the Allen Cell and Structure Segmenter [<xref rid="bib22" ref-type="bibr">21</xref>] to generate the semantic segmentation ground truth, and we used the cell instance segmentation to group fibrillarin segmentations belonging to the same cell as unique instances (see more details in the Results section), which will be used as the instance segmentation ground truth. The FBL_fine segmentation workflow was optimized for this cell line, which can be considered a good approximation of the real truth. To be conservative, we excluded images where the mean intensity of the structure channel is outside the range of [450,500], so that the results from the FBL_fine workflow can be a better approximation of the real truth. After removing the &#x0201c;outlier&#x0201d; images, we held out 20% of the data for testing.</p></sec><sec id="sec6-5"><title>5. Unsupervised semantic segmentation of intracellular structures from 2D/3D confocal microscopy images</title><p>
<bold>2D:</bold> The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used the tight junction cell line (structure_name&#x000a0;=&#x000a0;&#x0201c;TJP1&#x0201d;) for this experiment. The original image and corresponding structure segmentation were both in 3D. We took the max intensity projection of the raw structure channel and the corresponding structure segmentation for experimenting unsupervised 2D segmentation. The correspondence between images and segmentations was shuffled to simulate unpaired ground truth. Twenty percent of the data were held out for testing.</p><p>
<bold>3D:</bold> The data were also downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used 3 different cell lines for these experiments: Golgi (structure_name&#x000a0;=&#x000a0;&#x0201c;ST6GAL1&#x0201d;), mitochondria (structure_name&#x000a0;=&#x000a0;&#x0201c;TOMM20&#x0201d;), and histone H2B (structure_name&#x000a0;=&#x000a0;&#x0201c;HIST12BJ&#x0201d;). For Golgi and mitochondria, we simply used the corresponding structure segmentation from the dataset. For histone H2B, we took the released nuclear instance segmentation and converted it to binary as semantic segmentation results. The correspondence between images and segmentations was shuffled to simulate unpaired ground truth. Twenty percent of the data were held out for testing.</p></sec><sec id="sec6-6"><title>6. Generating synthetic microscopy images from binary masks</title><p>
<bold>2D:</bold> The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used the nucleophosmin cell line (structure_name&#x000a0;=&#x000a0;&#x0201c;NPM1&#x0201d;) for this experiment. The original image and corresponding structure segmentation were both in 3D. We took the max intensity projection of the raw structure channel and the corresponding structure segmentation for this experiment. The input is binary segmentation, while the ground truth is the raw image.</p><p>
<bold>3D:</bold> The data were downloaded from the hiPSC single-cell image dataset from the Allen Cell Quilt Bucket: [<xref rid="bib58" ref-type="bibr">58</xref>]. We used the histone H2B cell line (structure_name&#x000a0;=&#x000a0;&#x0201c;HIST1H2BJ&#x0201d;) for these experiments. For the experiment with coarse masks, we used the binarized nuclear segmentation as the input, while for the experiment with detailed masks, we used the structure segmentation of H2B as the input. The ground truth is always the raw 3D structure image.</p></sec><sec id="sec6-7"><title>7. Image denoising for microscopy images</title><p>The data were downloaded from [<xref rid="bib64" ref-type="bibr">64</xref>], which was released with the publication [<xref rid="bib46" ref-type="bibr">46</xref>]. We used 2 datasets, &#x0201c;Denoising_Planaria.tar.gz&#x0201d; and &#x0201c;Denoising_Tribolium.tar.gz.&#x0201d; We kept the original train/test splitting in the datasets.</p></sec><sec id="sec6-8"><title>8. Imaging modality transformation from 3D confocal microscopy images to stimulated emission depletion (STED) microscopy images</title><p>The data were downloaded from [<xref rid="bib65" ref-type="bibr">65</xref>], which was released with the publication [<xref rid="bib9" ref-type="bibr">8</xref>]. We used 2 datasets, <italic toggle="yes">Microtubule</italic> and <italic toggle="yes">Nuclear_Pore_complex</italic>, from &#x0201c;Confocal_2_STED.zip.&#x0201d; We kept the original train/test splitting in the datasets.</p></sec><sec id="sec6-9"><title>9. Staining transformation in multiplex experiments</title><p>This dataset was downloaded from [<xref rid="bib66" ref-type="bibr">66</xref>], which was released with the publication [<xref rid="bib6" ref-type="bibr">5</xref>]. We used the dataset &#x0201c;BC-DeepLIIF_Training_Set.zip&#x0201d; and &#x0201c;BC-DeepLIIF_Validation_Set.zip.&#x0201d; In our 3 experiments, we always used the IHC image as the input and standard hematoxylin stain image, mpIF nuclear image, and mpIF LAP2beta image as ground truth, correspondingly.</p></sec><sec id="sec6-10"><title>10. Models and sample data</title><p>To help researchers get started with our tool, we have deposited all models used in the manuscript as well as sample data at [<xref rid="bib56" ref-type="bibr">56</xref>].</p></sec></sec><sec id="sec7"><title>Abbreviations</title><p>AI: artificial intelligence; CRNN: convolutional recurrent neural network; DL: deep FCN: fully convolutional network; GPU: graphics processing unit; hiPSC: human induced pluripotent stem cell; ML: machine learning; mpIF: multiplex immunofluorescence; R&#x00026;D: research and development; STED: stimulated emission depletion.</p></sec><sec sec-type="COI-statement" id="sec8"><title>Competing Interests</title><p>The authors declare that they have no competing interests.</p></sec><sec id="sec9"><title>Funding</title><p>This work is supported by the Federal Ministry of Education and Research (Bundesministerium f&#x000fc;r Bildung und Forschung, BMBF) under the funding reference 161L0272 and by the Ministry of Culture and Science of the State of North Rhine-Westphalia (Ministerium f&#x000fc;r Kultur und Wissenschaft des Landes Nordrhein-Westfalen, MKW NRW).</p></sec><sec id="sec10"><title>Authors&#x02019; Contributions</title><p>J.C. planned the project and implemented most of the software. J.S. tested the software and ran all the experiments. Y.Z. added Docker support to the software, while J.S. and Y.Z. contributed minor fixes to the code. J.C. wrote the paper together with J.S., and Y.Z. contributed to proofreading.</p></sec><ref-list id="ref1"><title>References</title><ref id="bib2"><label>1.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Lim</surname>
<given-names>B</given-names>
</string-name>, <string-name><surname>Son</surname><given-names>S</given-names></string-name>, <string-name><surname>Kim</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Enhanced deep residual networks for single image super-resolution</article-title>. <source>In: 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source>. <publisher-loc>Honolulu, HI, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2017</year>. <pub-id pub-id-type="doi">10.1109/cvprw.2017.151</pub-id>.</mixed-citation></ref><ref id="bib3"><label>2.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Isola</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Zhu</surname><given-names>J-Y</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Image-to-Image translation with conditional adversarial networks</article-title>. <source>In: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Honolulu, HI, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2017</year>. <pub-id pub-id-type="doi">10.1109/cvpr.2017.632</pub-id>.</mixed-citation></ref><ref id="bib4"><label>3.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kirillov</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Panoptic segmentation</article-title>. <source>In: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Long Beach, CA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2019</year>. <pub-id pub-id-type="doi">10.1109/cvpr.2019.00963</pub-id>.</mixed-citation></ref><ref id="bib5"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ounkomol</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Seshamani</surname><given-names>S</given-names></string-name>, <string-name><surname>Maleckar</surname><given-names>MM</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Label-free prediction of three-dimensional fluorescence images from transmitted-light microscopy</article-title>. <source>Nat Methods</source>. <year>2018</year>;<volume>15</volume>:<fpage>917</fpage>&#x02013;<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0111-2</pub-id>.<pub-id pub-id-type="pmid">30224672</pub-id>
</mixed-citation></ref><ref id="bib6"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ghahremani</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Kaufman</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</article-title>. <source>Nat Mach Intell</source>. <year>2022</year>;<volume>4</volume>:<fpage>401</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-022-00471-x</pub-id>.<pub-id pub-id-type="pmid">36118303</pub-id>
</mixed-citation></ref><ref id="bib7"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fang</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Monroe</surname><given-names>F</given-names></string-name>, <string-name><surname>Novak</surname><given-names>SW</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning-based point-scanning super-resolution imaging</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>406</fpage>&#x02013;<lpage>16</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01080-z</pub-id>.<pub-id pub-id-type="pmid">33686300</pub-id>
</mixed-citation></ref><ref id="bib8"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Edlund</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Jackson</surname><given-names>TR</given-names></string-name>, <string-name><surname>Khalid</surname><given-names>N</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>LIVECell&#x02014;a large-scale dataset for label-free live cell segmentation</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>1038</fpage>&#x02013;<lpage>45</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01249-6</pub-id>.<pub-id pub-id-type="pmid">34462594</pub-id>
</mixed-citation></ref><ref id="bib9"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Sasaki</surname><given-names>H</given-names></string-name>, <string-name><surname>Lai</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Three-dimensional residual channel attention networks denoise and sharpen fluorescence microscopy image volumes</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>678</fpage>&#x02013;<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01155-x</pub-id>.<pub-id pub-id-type="pmid">34059829</pub-id>
</mixed-citation></ref><ref id="bib10"><label>9.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Ronneberger</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name></person-group>. <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. <source>In:&#x000a0;Medical Image Computing and Computer-Assisted Intervention (MICCAI),&#x000a0;vol. 9351.</source><publisher-loc>Munich, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2015</year>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>.</mixed-citation></ref><ref id="bib11"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lalit</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Tomancak</surname><given-names>P</given-names></string-name>, <string-name><surname>Jug</surname><given-names>F</given-names></string-name></person-group>. <article-title>EmbedSeg: embedding-based instance segmentation for biomedical microscopy data</article-title>. <source>Med Image Anal</source>. <year>2022</year>;<volume>81</volume>:<fpage>102523</fpage>. <pub-id pub-id-type="doi">10.1016/j.media.2022.102523</pub-id>.<pub-id pub-id-type="pmid">35926335</pub-id>
</mixed-citation></ref><ref id="bib12"><label>11.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Lalit</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Tomancak</surname><given-names>P</given-names></string-name>, <string-name><surname>Jug</surname><given-names>F</given-names></string-name></person-group>. <article-title>Embedding-based instance segmentation in microscopy</article-title>. <source>In: Proceedings of the Fourth Conference on Medical Imaging with Deep Learning</source>. <publisher-loc>L&#x000fc;beck, Germany</publisher-loc>: <publisher-name>PMLR</publisher-name>, <year>2021</year>. <comment><ext-link xlink:href="https://proceedings.mlr.press/v143/lalit21a.html" ext-link-type="uri">https://proceedings.mlr.press/v143/lalit21a.html</ext-link></comment>.</mixed-citation></ref><ref id="bib13"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ihle</surname>
<given-names>SJ</given-names>
</string-name>, <string-name><surname>Reichmuth</surname><given-names>AM</given-names></string-name>, <string-name><surname>Girardin</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Unsupervised data to content transformation with histogram-matching cycle-consistent generative adversarial networks</article-title>. <source>Nat Mach Intell</source>. <year>2019</year>;<volume>1</volume>:<fpage>461</fpage>&#x02013;<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-019-0096-2</pub-id>.</mixed-citation></ref><ref id="bib14"><label>13.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhu</surname>
<given-names>J-Y</given-names>
</string-name>, <string-name><surname>Park</surname><given-names>T</given-names></string-name>, <string-name><surname>Isola</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Unpaired image-to-image translation using cycle-consistent adversarial networks</article-title>. <source>In: 2017 IEEE International Conference on Computer Vision (ICCV)</source>. <publisher-loc>Venice, Italy</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2017</year>. <pub-id pub-id-type="doi">10.1109/iccv.2017.244</pub-id>.</mixed-citation></ref><ref id="bib15"><label>14.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Falcon</surname>
<given-names>W</given-names>
</string-name>, <string-name><surname>Borovec</surname><given-names>J</given-names></string-name>, <string-name><surname>W&#x000e4;lchli</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>PyTorchLightning/pytorch-lightning: 0.7.6 release</article-title>. <source>Zenodo.</source><year>2020</year>. <pub-id pub-id-type="doi">10.5281/zenodo.3828935</pub-id>.</mixed-citation></ref><ref id="bib16"><label>15.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Izmailov</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Podoprikhin</surname><given-names>D</given-names></string-name>, <string-name><surname>Garipov</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Averaging weights leads to wider optima and better generalization</article-title>. <source>In: 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI</source>. <publisher-loc>Monterey, California, USA</publisher-loc>, <year>2018</year>. <comment><ext-link xlink:href="http://auai.org/uai2018/proceedings/papers/313.pdf" ext-link-type="uri">http://auai.org/uai2018/proceedings/papers/313.pdf</ext-link></comment></mixed-citation></ref><ref id="bib17"><label>16.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>OpenMMLab</collab>
</person-group>. <comment><ext-link xlink:href="https://openmmlab.com/" ext-link-type="uri">https://openmmlab.com/</ext-link></comment>.</mixed-citation></ref><ref id="bib18"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<collab>MMSegmentation Contributors</collab>
</person-group>. <article-title>MMSegmentation: openMMLab semantic segmentation toolbox and benchmark</article-title>. <source>GitHub</source>. <year>2023</year>. <comment><ext-link xlink:href="https://github.com/open-mmlab/mmsegmentation" ext-link-type="uri">https://github.com/open-mmlab/mmsegmentation</ext-link></comment>.</mixed-citation></ref><ref id="bib19"><label>18.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Deng</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Dong</surname><given-names>W</given-names></string-name>, <string-name><surname>Socher</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>ImageNet: a large-scale hierarchical image database</article-title>. <source>In: 2009 IEEE Conference on Computer Vision and Pattern Recognition</source>. <publisher-loc>Miami, FL, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2009</year>. <pub-id pub-id-type="doi">10.1109/cvpr.2009.5206848</pub-id>.</mixed-citation></ref><ref id="bib20"><label>19.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>The MONAI Consortium</collab>
</person-group>. <article-title>Project MONAI</article-title>. <source>Zenodo.</source><year>2020.</year>; <pub-id pub-id-type="doi">10.5281/zenodo.4323059</pub-id>.</mixed-citation></ref><ref id="bib21"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Brown</surname>
<given-names>EM</given-names>
</string-name>, <string-name><surname>Toloudis</surname><given-names>D</given-names></string-name>, <string-name><surname>Sherman</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>AICSImageIO: image reading, metadata conversion, and image writing for microscopy images in pure Python</article-title>. <comment>GitHub</comment>. <year>2021</year>. <comment><ext-link xlink:href="https://github.com/AllenCellModeling/aicsimageio" ext-link-type="uri">https://github.com/AllenCellModeling/aicsimageio</ext-link></comment>.</mixed-citation></ref><ref id="bib22"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Ding</surname><given-names>L</given-names></string-name>, <string-name><surname>Viana</surname><given-names>MP</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Allen Cell and Structure Segmenter: a new open source toolkit for segmenting 3D intracellular structures in fluorescence microscopy images</article-title>. <source>Biorxiv.</source><year>2018</year>. <pub-id pub-id-type="doi">10.1101/491035</pub-id>.</mixed-citation></ref><ref id="bib1"><label>22.</label><mixed-citation publication-type="journal">
<article-title>MMV_Im2Im Transformation</article-title>. <source>GitHub</source>. <year>2023</year>. <comment><ext-link xlink:href="https://github.com/mmv-lab/mmv_im2im" ext-link-type="uri">https://github.com/mmv-lab/mmv_im2im</ext-link></comment>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Himmelstein</surname>
<given-names>DS</given-names>
</string-name>, <string-name><surname>Rubinetti</surname><given-names>V</given-names></string-name>, <string-name><surname>Slochower</surname><given-names>DR</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Open collaborative writing with Manubot</article-title>. <source>PLoS Comput Biol</source>. <year>2019</year>;<volume>15</volume>:<fpage>e1007128</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1007128</pub-id>.<pub-id pub-id-type="pmid">31233491</pub-id>
</mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<article-title>MMV_Im2im: an open source toolbox for image-to-image transformation in microscopy images</article-title>. <source>GitHub.</source><year>2023</year>. <comment><ext-link xlink:href="https://github.com/MMV-Lab/im2im-paper" ext-link-type="uri">https://github.com/MMV-Lab/im2im-paper</ext-link></comment></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>LaChance</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Cohen</surname><given-names>DJ</given-names></string-name></person-group>. <article-title>Practical fluorescence reconstruction microscopy for large samples and low-magnification imaging</article-title>. <source>PLoS Comput Biol</source>. <year>2020</year>;<volume>16</volume>:<fpage>e1008443</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008443</pub-id>.<pub-id pub-id-type="pmid">33362219</pub-id>
</mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Reinke</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Tizabi</surname><given-names>MD</given-names></string-name>, <string-name><surname>Baumgartner</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Understanding metric-related pitfalls in image analysis validation</article-title>. <source>arXiv</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arxiv.2302.01790</pub-id>.</mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Viana</surname><given-names>MP</given-names></string-name>, <string-name><surname>Rafelski</surname><given-names>SM</given-names></string-name></person-group>. <article-title>When seeing is not believing: application-appropriate validation matters for quantitative bioimage analysis</article-title>. <source>Nat Methods</source>. <year>2023</year>;<volume>20</volume>:<fpage>968</fpage>&#x02013;<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-023-01881-4</pub-id>.<pub-id pub-id-type="pmid">37433995</pub-id>
</mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Guiet</surname>
<given-names>R</given-names>
</string-name>
</person-group>. <article-title>HeLa &#x0201c;Kyoto&#x0201d; cells under the scope</article-title>. <source>Zenodo.</source><year>2022.</year>; <pub-id pub-id-type="doi">10.5281/zenodo.6139958</pub-id>.</mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Viana</surname>
<given-names>MP</given-names>
</string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name>, <string-name><surname>Knijnenburg</surname><given-names>TA</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Integrated intracellular organization and its variations in human iPS cells</article-title>. <source>Nature</source>. <year>2023</year>;<volume>613</volume>:<fpage>345</fpage>&#x02013;<lpage>54</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-022-05563-7</pub-id>.<pub-id pub-id-type="pmid">36599983</pub-id>
</mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Kerfoot</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Clough</surname><given-names>J</given-names></string-name>, <string-name><surname>Oksuz</surname><given-names>I</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Left-ventricle quantification using residual U-net</article-title>. <source>In: Statistical Atlases and Computational Models of the Heart Atrial Segmentation and LV Quantification Challenges</source>. <publisher-loc>Granada, Spain</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2019</year>. <pub-id pub-id-type="doi">10.1007/978-3-030-12029-0_40</pub-id>.</mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Oktay</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Schlemper</surname><given-names>J</given-names></string-name>, <string-name><surname>Folgoc</surname><given-names>LL</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Attention U-Net: learning where to look for the Pancreas</article-title>. <source>In: Proceedings of Medical Imaging with Deep Learning</source>. <publisher-loc>Amsterdam, NL</publisher-loc>: <publisher-name>OpenReview</publisher-name>, <year>2018</year>. <ext-link xlink:href="https://openreview.net/forum?id=Skft7cijM" ext-link-type="uri">https://openreview.net/forum?id=Skft7cijM</ext-link>.</mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hatamizadeh</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Nath</surname><given-names>V</given-names></string-name>, <string-name><surname>Tang</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images</article-title>. In: <source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries.</source><year>2022</year>. <pub-id pub-id-type="doi">10.1007/978-3-031-08999-2_22</pub-id>.</mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Hatamizadeh</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Tang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Nath</surname><given-names>V</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>UNETR: transformers for 3D medical image segmentation</article-title>. <source>In: 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</source>. <publisher-loc>Waikoloa, HI, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2022</year>. <pub-id pub-id-type="doi">10.1109/wacv51458.2022.00181</pub-id>.</mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sirinukunwattana</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Snead</surname><given-names>DRJ</given-names></string-name>, <string-name><surname>Rajpoot</surname><given-names>NM</given-names></string-name></person-group>. <article-title>A stochastic polygons model for glandular structures in colon histology images</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2015</year>;<volume>34</volume>:<fpage>2366</fpage>&#x02013;<lpage>78</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2015.2433900</pub-id>.<pub-id pub-id-type="pmid">25993703</pub-id>
</mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sirinukunwattana</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Pluim</surname><given-names>JPW</given-names></string-name>, <string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Gland segmentation in colon histology images: the glas challenge contest</article-title>. <source>Med Image Anal</source>. <year>2017</year>;<volume>35</volume>:<fpage>489</fpage>&#x02013;<lpage>502</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2016.08.008</pub-id>.<pub-id pub-id-type="pmid">27614792</pub-id>
</mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Macenko</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Niethammer</surname><given-names>M</given-names></string-name>, <string-name><surname>Marron</surname><given-names>JS</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A method for normalizing histology slides for quantitative analysis</article-title>. <source>In: 2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</source>. <publisher-loc>Boston, Massachusetts, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2009</year>. <pub-id pub-id-type="doi">10.1109/isbi.2009.5193250</pub-id>.</mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Schmidt</surname>
<given-names>U</given-names>
</string-name>, <string-name><surname>Weigert</surname><given-names>M</given-names></string-name>, <string-name><surname>Broaddus</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Cell detection with star-convex polygons</article-title>. <source>In: Medical Image Computing and Computer Assisted Intervention&#x02014;MICCAI</source>. <publisher-loc>Granada, Spain</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2018</year>. <pub-id pub-id-type="doi">10.1007/978-3-030-00934-2_30</pub-id>.</mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Weigert</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Schmidt</surname><given-names>U</given-names></string-name>, <string-name><surname>Haase</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Star-convex polyhedra for 3D object detection and segmentation in microscopy</article-title>. In: <source>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</source>. <publisher-loc>Snowmass Village, CO, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2020</year>. <pub-id pub-id-type="doi">10.1109/wacv45572.2020.9093435</pub-id>.</mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Mandal</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Uhlmann</surname><given-names>V</given-names></string-name></person-group>. <article-title>Splinedist: automated cell segmentation with spline curves</article-title>. <source>In: 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</source>. <publisher-loc>France</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2021</year>. <pub-id pub-id-type="doi">10.1109/isbi48211.2021.9433928</pub-id>.</mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Stringer</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>T</given-names></string-name>, <string-name><surname>Michaelos</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>:<fpage>100</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>.<pub-id pub-id-type="pmid">33318659</pub-id>
</mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Cutler</surname>
<given-names>KJ</given-names>
</string-name>, <string-name><surname>Stringer</surname><given-names>C</given-names></string-name>, <string-name><surname>Lo</surname><given-names>TW</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation</article-title>. <source>Nat Methods</source>. <year>2022</year>;<volume>19</volume>:<fpage>1438</fpage>&#x02013;<lpage>48</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-022-01639-4</pub-id>.<pub-id pub-id-type="pmid">36253643</pub-id>
</mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>He</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Gkioxari</surname><given-names>G</given-names></string-name>, <string-name><surname>Dollar</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Mask R-CNN</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2020</year>;<volume>42</volume>:<fpage>386</fpage>&#x02013;<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2844175</pub-id>.<pub-id pub-id-type="pmid">29994331</pub-id>
</mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Romera</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Alvarez</surname><given-names>JM</given-names></string-name>, <string-name><surname>Bergasa</surname><given-names>LM</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>ERFNet: efficient residual factorized ConvNet for real-time semantic segmentation</article-title>. <source>IEEE Trans Intell Transport Syst</source>. <year>2018</year>;<volume>19</volume>:<fpage>263</fpage>&#x02013;<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1109/TITS.2017.2750080</pub-id>.</mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Moy</surname>
<given-names>TI</given-names>
</string-name>, <string-name><surname>Conery</surname><given-names>AL</given-names></string-name>, <string-name><surname>Larkins-Ford</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>High-throughput screen for novel antimicrobials using a whole animal infection model</article-title>. <source>ACS Chem Biol</source>. <year>2009</year>;<volume>4</volume>:<fpage>527</fpage>&#x02013;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1021/cb900084v</pub-id>.<pub-id pub-id-type="pmid">19572548</pub-id>
</mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Prakash</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Delbracio</surname><given-names>M</given-names></string-name>, <string-name><surname>Milanfar</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Interpretable unsupervised diversity denoising and artefact removal</article-title>. <source>In: Proceedings of the Tenth International Conference on Learning Representations</source>. <publisher-loc>Virtual</publisher-loc>: <publisher-name>OpenReview</publisher-name>, <year>2021</year>. <comment><ext-link xlink:href="https://openreview.net/pdf?id=DfMqlB0PXjM" ext-link-type="uri">https://openreview.net/pdf?id=DfMqlB0PXjM</ext-link></comment>.</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Weigert</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Schmidt</surname><given-names>U</given-names></string-name>, <string-name><surname>Boothe</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title>. <source>Nat Methods</source>. <year>2018</year>;<volume>15</volume>:<fpage>1090</fpage>&#x02013;<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id>.<pub-id pub-id-type="pmid">30478326</pub-id>
</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Richardson</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Weiss</surname><given-names>I</given-names></string-name>, <string-name><surname>Feldman</surname><given-names>Y</given-names></string-name></person-group>. <article-title>Pyrallis&#x02014;simple configuration with dataclasses</article-title>. <source>Github</source>; <year>2023</year>. <comment><ext-link xlink:href="https://github.com/eladrich/pyrallis" ext-link-type="uri">https://github.com/eladrich/pyrallis</ext-link></comment>.</mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Weiss</surname><given-names>K</given-names></string-name>, <string-name><surname>Navab</surname><given-names>N</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>DeStripe: a Self2Self spatio-spectral graph neural network with unfolded Hessian for stripe artifact removal in light-sheet microscopy</article-title>. <source>In:&#x000a0;Medical Image Computing and Computer Assisted Intervention (MICCAI)</source>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2022</year>. <pub-id pub-id-type="doi">10.1007/978-3-031-16440-8_10</pub-id>.</mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Waibel</surname>
<given-names>DJE</given-names>
</string-name>, <string-name><surname>R&#x000f6;ell</surname><given-names>E</given-names></string-name>, <string-name><surname>Rieck</surname><given-names>B</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A diffusion model predicts 3D shapes from 2D microscopy images</article-title>. <source>arXiv</source>; <year>2022</year>. <pub-id pub-id-type="doi">10.48550/arxiv.2208.14125</pub-id>.</mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<collab>Imaginaire Contributors</collab>
</person-group>. <article-title>Imaginaire</article-title>. <source>GitHub</source>; <year>2023</year>. <comment><ext-link xlink:href="https://github.com/NVlabs/imaginaire" ext-link-type="uri">https://github.com/NVlabs/imaginaire</ext-link></comment>.</mixed-citation></ref><ref id="bib51"><label>51.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Ahlers</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Mor&#x000e9;</surname><given-names>DA</given-names></string-name>, <string-name><surname>Amsalem</surname><given-names>O</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>napari: a multi-dimensional image viewer for Python</article-title>. <source>Zenodo</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.3555620</pub-id>.</mixed-citation></ref><ref id="bib52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jing</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Tian</surname><given-names>Y</given-names></string-name></person-group>. <article-title>Self-supervised visual feature learning with deep neural networks: a survey</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2021</year>;<volume>43</volume>:<fpage>4037</fpage>&#x02013;<lpage>58</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2020.2992393</pub-id>.<pub-id pub-id-type="pmid">32386141</pub-id>
</mixed-citation></ref><ref id="bib53"><label>53.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Mok</surname>
<given-names>TCW</given-names>
</string-name>, <string-name><surname>Chung</surname><given-names>ACS</given-names></string-name></person-group>. <article-title>Fast symmetric diffeomorphic image registration with convolutional neural networks</article-title>. <source>In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Seattle, WA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>, <year>2020</year>. <pub-id pub-id-type="doi">10.1109/cvpr42600.2020.00470</pub-id>.</mixed-citation></ref><ref id="bib54"><label>54.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sonneck</surname>
<given-names>J</given-names>
</string-name>
</person-group>. <article-title>MMV_Im2Im</article-title>. <source>WorkflowHub.</source><year>2023</year>. <pub-id pub-id-type="doi">10.48546/workflowhub.workflow.626.1</pub-id>.</mixed-citation></ref><ref id="bib55"><label>55.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sonneck</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name></person-group>. <article-title>Supporting data for &#x0201c;MMV_Im2Im: An Open-Source Microscopy Machine Vision Toolbox for Image-to-Image Transformation.&#x0201d;</article-title>. <source>GigaScience Database.</source><year>2023</year>. <pub-id pub-id-type="doi">10.5524/102477</pub-id>.</mixed-citation></ref><ref id="bib56"><label>56.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Sonneck</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name></person-group>. <article-title>MMV_Im2Im: an open source microscopy machine vision toolbox for image-to-image transformation</article-title>. <source>Zenodo.</source><year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.10034416</pub-id>.</mixed-citation></ref><ref id="bib57"><label>57.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Guiet</surname>
<given-names>R</given-names>
</string-name>
</person-group>. <article-title>Automatic labelling of HeLa &#x0201c;Kyoto&#x0201d; cells using deep learning tools</article-title>. <source>Zenodo.</source><year>2022</year>. <pub-id pub-id-type="doi">10.5281/zenodo.6140063</pub-id>.</mixed-citation></ref><ref id="bib58"><label>58.</label><mixed-citation publication-type="other">
<article-title>The hiPSC single-cell image dataset</article-title>. <comment><ext-link xlink:href="https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset" ext-link-type="uri">https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset</ext-link></comment></mixed-citation></ref><ref id="bib59"><label>59.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sirinukunwattana</surname>
<given-names>K</given-names>
</string-name>, <etal>et al.</etal></person-group>
<article-title>Gland segmentation in colon histology images: The glas challenge contest</article-title>. <source>Med Image Anal</source>. <year>2017</year>;<volume>35</volume>:<fpage>489</fpage>&#x02013;<lpage>502</lpage>.<pub-id pub-id-type="pmid">27614792</pub-id>
</mixed-citation></ref><ref id="bib60"><label>60.</label><mixed-citation publication-type="other">
<article-title>GlaS@MICCAI'2015: Gland segmentation</article-title>. <comment><ext-link xlink:href="https://www.kaggle.com/datasets/sani84/glasmiccai2015-gland-segmentation" ext-link-type="uri">https://www.kaggle.com/datasets/sani84/glasmiccai2015-gland-segmentation</ext-link></comment></mixed-citation></ref><ref id="bib61"><label>61.</label><mixed-citation publication-type="other">
<article-title>Gland segmentation in histology images challenge (GlaS) dataset</article-title>. <ext-link xlink:href="https://github.com/twpkevin06222/Gland-Segmentation" ext-link-type="uri">https://github.com/twpkevin06222/Gland-Segmentation</ext-link>.</mixed-citation></ref><ref id="bib62"><label>62.</label><mixed-citation publication-type="other">
<article-title>Broad Bioimage Benchmark Collection: C. elegangs live/dead assay</article-title>. <comment><ext-link xlink:href="https://bbbc.broadinstitute.org/BBBC010" ext-link-type="uri">https://bbbc.broadinstitute.org/BBBC010</ext-link></comment></mixed-citation></ref><ref id="bib63"><label>63.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ljosa</surname>
<given-names>V</given-names>
</string-name>, <string-name><surname>Sokolnicki</surname><given-names>KL</given-names></string-name>, <string-name><surname>Carpenter</surname><given-names>AE</given-names></string-name></person-group>. <article-title>Annotated high-throughput microscopy image sets for validation</article-title>. <source>Nat Methods</source>. <year>2012</year>;<volume>9</volume>:<fpage>637</fpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2083</pub-id>.<pub-id pub-id-type="pmid">22743765</pub-id>
</mixed-citation></ref><ref id="bib64"><label>64.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Weigert</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Schmidt</surname><given-names>U</given-names></string-name>, <string-name><surname>Boothe</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Content aware image restoration: pushing the limits of fluorescence microscopy&#x02014;supplemental data</article-title>. <comment><ext-link xlink:href="https://publications.mpi-cbg.de/publications-sites/7207/" ext-link-type="uri">https://publications.mpi-cbg.de/publications-sites/7207/</ext-link>.</comment></mixed-citation></ref><ref id="bib65"><label>65.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>J</given-names>
</string-name>
</person-group>. <article-title>3D residual channel attention networks denoise and sharpen fluorescence microscopy image volumes</article-title>. <source>Zenodo.</source><year>2021</year>. <pub-id pub-id-type="doi">10.5281/zenodo.4624364</pub-id>.</mixed-citation></ref><ref id="bib66"><label>66.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Ghahremani</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>Y</given-names></string-name>, <string-name><surname>Kaufman</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</article-title>. <source>Zenodo</source>. <year>2021.</year>; <comment><ext-link xlink:href="https://zenodo.org/record/4751737#.Y9gbv4HMLVZ" ext-link-type="uri">https://zenodo.org/record/4751737#.Y9gbv4HMLVZ</ext-link></comment>.</mixed-citation></ref></ref-list></back></article>