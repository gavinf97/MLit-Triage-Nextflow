<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName journalpublishing.dtd?><?SourceDTD.Version 2.3?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Genet</journal-id><journal-id journal-id-type="iso-abbrev">Front Genet</journal-id><journal-id journal-id-type="publisher-id">Front. Genet.</journal-id><journal-title-group><journal-title>Frontiers in Genetics</journal-title></journal-title-group><issn pub-type="epub">1664-8021</issn><publisher><publisher-name>Frontiers Media S.A.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">6277570</article-id><article-id pub-id-type="doi">10.3389/fgene.2018.00585</article-id><article-categories><subj-group subj-group-type="heading"><subject>Genetics</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Prediction of Drug-Likeness Using Deep Autoencoder Neural Networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Qiwan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/619371/overview"/></contrib><contrib contrib-type="author"><name><surname>Feng</surname><given-names>Mudong</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib><contrib contrib-type="author"><name><surname>Lai</surname><given-names>Luhua</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/119755/overview"/></contrib><contrib contrib-type="author"><name><surname>Pei</surname><given-names>Jianfeng</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="c001"><sup>*</sup></xref><uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/394937/overview"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>Center for Quantitative Biology, Academy for Advanced Interdisciplinary Studies, Peking University</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff><aff id="aff2"><sup>2</sup><institution>BNLMS, State Key Laboratory for Structural Chemistry of Unstable and Stable Species, Peking-Tsinghua Center for Life Sciences at College of Chemistry and Molecular Engineering, Peking University</institution>, <addr-line>Beijing</addr-line>, <country>China</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Ping Gong, Engineer Research and Development Center (ERDC), United States</p></fn><fn fn-type="edited-by"><p>Reviewed by: Yun Tang, East China University of Science and Technology, China; Shengyong Yang, Sichuan University, China</p></fn><corresp id="c001">*Correspondence: Jianfeng Pei, <email>jfpei@pku.edu.cn</email></corresp><fn fn-type="other" id="fn002"><p>This article was submitted to Toxicogenomics, a section of the journal Frontiers in Genetics</p></fn></author-notes><pub-date pub-type="epub"><day>27</day><month>11</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>9</volume><elocation-id>585</elocation-id><history><date date-type="received"><day>31</day><month>8</month><year>2018</year></date><date date-type="accepted"><day>09</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2018 Hu, Feng, Lai and Pei.</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>Hu, Feng, Lai and Pei</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><abstract><p>Due to diverse reasons, most drug candidates cannot eventually become marketed drugs. Developing reliable computational methods for prediction of drug-likeness of candidate compounds is of vital importance to improve the success rate of drug discovery and development. In this study, we used a fully connected neural networks (FNN) to construct drug-likeness classification models with deep autoencoder to initialize model parameters. We collected datasets of drugs (represented by ZINC World Drug), bioactive molecules (represented by MDDR and WDI), and common molecules (represented by ZINC All Purchasable and ACD). Compounds were encoded with MOLD2 two-dimensional structure descriptors. The classification accuracies of drug-like/non-drug-like model are 91.04% on WDI/ACD databases, and 91.20% on MDDR/ZINC, respectively. The performance of the models outperforms previously reported models. In addition, we develop a drug/non-drug-like model (ZINC World Drug vs. ZINC All Purchasable), which distinguishes drugs and common compounds, with a classification accuracy of 96.99%. Our work shows that by using high-latitude molecular descriptors, we can apply deep learning technology to establish state-of-the-art drug-likeness prediction models.</p></abstract><kwd-group><kwd>drug-likeness</kwd><kwd>ZINC</kwd><kwd>MDDR</kwd><kwd>deep learning</kwd><kwd>auto-encoder</kwd></kwd-group><counts><fig-count count="2"/><table-count count="7"/><equation-count count="0"/><ref-count count="42"/><page-count count="8"/><word-count count="0"/></counts></article-meta></front><body><sec><title>Introduction</title><p>Over the past several decades, various novel and effective techniques, such as high-throughput screening(HTS), fragment-based drug discovery (FBDD), single-cell analysis, have been developed and led to remarkable progresses in the field of drug discovery. However, it is noted that the amount of new chemical entities (NCEs) approved by FDA did not grow as rapidly as expected (<xref rid="B13" ref-type="bibr">Darrow and Kesselheim</xref>, <xref rid="B13" ref-type="bibr">2014</xref>). According to statistics, the success rate of candidate compounds found in preclinical detection is about 40%, while the rate of candidate compounds entering the market is only 10% (<xref rid="B27" ref-type="bibr">Lipper, 1999</xref>).</p><p>About 40% of the candidate compounds not being marketed is due to their poor biopharmaceutical properties, also commonly referred to as drug-likeness, which includes poor chemical stability, poor solubility, poor permeability and poor metabolic (<xref rid="B38" ref-type="bibr">Venkatesh and Lipper, 2000</xref>). Drug-likeness, derived from structures and properties of existing drugs and drug candidates, has been widely used to filter out undesirable compounds in early phases of drug discovery. The initial concept of drug-like rules is proposed by Lipinsky, known as the rule-of-five which contains four simple physicochemical parameter definitions (MWT &#x02264; 500, log P &#x02264; 5, H-bond donors &#x02264; 5, H-bond acceptors &#x02264; 10) (<xref rid="B26" ref-type="bibr">Lipinski, 2004</xref>). Using these definitions may predict whether a compound can become an oral drug candidate. In 2012, Hopkins et al. propose the quantitative estimate of drug-likeness (QED) measure, which was a weighted desirability function based on the statistical distribution of eight selected molecular properties for a set of 771 orally absorbed small molecule drugs and applied to molecular target druggability assessment (<xref rid="B5" ref-type="bibr">Bickerton et al., 2012</xref>). Due to the ambiguous definition of molecular properties between the drugs and non-drug and the prediction is not satisfactory with few descriptors, later works tried to combine more comprehensive descriptors and a large amount of compound data to develop drug-likeness prediction models with high accuracies from a quantitative perspective.</p><p>A drug-likeness prediction model introduced by Wagener et al., involved molecular descriptors related to numbers of different atom types and decision trees for discriminating between potential drugs and nondrugs. The model was trained using 10,000 compounds from the ACD and the WDI, and its prediction ACC on an independent validation data set of 177,747 compounds was 82.6% (<xref rid="B40" ref-type="bibr">Wagener and van Geerestein, 2000</xref>). In 2003, Byvatov and co-workers used various different descriptor sets and descriptor combinations to characterize compound and applied SVM and artificial neural network (ANN) systems to solve the drug/nondrug classification problem. Both methods reached 80% correct predictions and their results indicated SVM seemed to be more robust (<xref rid="B6" ref-type="bibr">Byvatov et al., 2003</xref>). A later model reported by Muller was also based on SVM with a careful model selection procedure for improving the prediction results of <xref rid="B6" ref-type="bibr">Byvatov et al. (2003)</xref> (<xref rid="B31" ref-type="bibr">M&#x000fc;ller et al., 2005</xref>). In 2008, Li et al implemented ECFP_4 (Extended Connectivity Fingerprints) for characterizing the molecules and used a probability SVM model to classify drug-like and non-drug-like molecules. The model significantly improved the prediction ACC when compared to previous work on the same data sets, and it is surprising that when using a larger data set of 341,601 compounds the classifier increased the ACC to 92.73% (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>). Schneider et al. applied decision trees to perform a gradual <italic>in silico</italic> screening for drug-like compounds based on SMARTS strings and the molecular weight, XlogP, and the molar refractivity as descriptor space for compounds (<xref rid="B34" ref-type="bibr">Schneider et al., 2008</xref>). In 2012, Tian et al implemented 21 physicochemical properties and the LCFP_6 fingerprint encoding molecules and used the naive Bayesian classification (NBC) and recursive partitioning (RP) to construct drug-like/non-drug-like classifier, which achieved 90.9% ACC (<xref rid="B37" ref-type="bibr">Tian et al., 2012</xref>). These studies showed that machine learning techniques are highly potential for the drug-likeness prediction problem combined with big data sets.</p><p>Deep learning is a new wave of machine learning based on artificial neural networks (ANN) (<xref rid="B3" ref-type="bibr">Bengio, 2009</xref>; <xref rid="B39" ref-type="bibr">Vincent et al., 2010</xref>). Since 2006, DL has been showing superior performances in many fields, such as computer vision (<xref rid="B20" ref-type="bibr">Hinton et al., 2006</xref>; <xref rid="B11" ref-type="bibr">Coates et al., 2011</xref>; <xref rid="B23" ref-type="bibr">Krizhevsky et al., 2012</xref>; <xref rid="B18" ref-type="bibr">He et al., 2016</xref>), natural language processing (<xref rid="B12" ref-type="bibr">Dahl et al., 2012</xref>; <xref rid="B35" ref-type="bibr">Socher et al., 2012</xref>; <xref rid="B16" ref-type="bibr">Graves et al., 2013</xref>; <xref rid="B30" ref-type="bibr">Mikolov et al., 2013</xref>; <xref rid="B2" ref-type="bibr">Bahdanau et al., 2016</xref>), bioinformatics and chemoinformatics (<xref rid="B14" ref-type="bibr">Di Lena et al., 2012</xref>; <xref rid="B28" ref-type="bibr">Lyons et al., 2014</xref>; <xref rid="B19" ref-type="bibr">Heffernan et al., 2015</xref>; <xref rid="B9" ref-type="bibr">Chen et al., 2016</xref>; <xref rid="B42" ref-type="bibr">Zeng et al., 2016</xref>). Compared to traditional machine learning methods, DL with multiple levels of layers can automatically transform raw data into a suitable internal feature representation which is beneficial for detection or classification tasks (<xref rid="B24" ref-type="bibr">LeCun et al., 2015</xref>). In this study we used deep autoencoder neural networks to construct powerful prediction models for drug-likeness and manually built three larger data sets abstracted from MDDR (<xref rid="B29" ref-type="bibr">MACCS-II Drug Data Report [MDDR], 2004</xref>), WDI (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>), ACD (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>) and ZINC (<xref rid="B22" ref-type="bibr">Irwin et al., 2012</xref>; <xref rid="B36" ref-type="bibr">Sterling and Irwin, 2015</xref>). The molecular descriptors of compound were calculated by Mold2 (<xref rid="B21" ref-type="bibr">Hong et al., 2008</xref>) and Padel (<xref rid="B41" ref-type="bibr">Yap, 2011</xref>). The classification accuracies of drug-like/non-drug-like model are 91.04% on WDI / ACD databases, and 91.20% on MDDR /ZINC, respectively. The performance of the models outperforms previously reported models. In addition, we developed a drug/non-drug-like model (ZINC World Drug vs. MDDR), which distinguishes drugs and common compounds, with a classification ACC of 96.99%. Our work shows that by using high-latitude molecular descriptors, we can apply DL technology to establish state-of-the-art drug-likeness prediction models.</p><sec><title>Datasets</title><sec><title>Benchmark Datasets</title><p>In this study, the whole chemical space was divided into drug, drug-like and non-drug-like. Marketed drug molecules were represented by ZINC WORLD DRUG (<xref rid="B36" ref-type="bibr">Sterling and Irwin, 2015</xref>) (version 2015, 2500 molecules) dataset. Drug-like molecules were represented by MDDR (<xref rid="B29" ref-type="bibr">MACCS-II Drug Data Report [MDDR], 2004</xref>) (200 k molecules) dataset and WDI (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>) (version 2002, 40k molecules) dataset. Non-drug-like molecules were represented by ACD (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>) (version 2002, 300 k molecules) and ZINC ALL PURCHASABLE (<xref rid="B22" ref-type="bibr">Irwin et al., 2012</xref>) (version 2012) datasets; the latter was randomly sampled to reduce its size to 200 k. Originally, drug-like datasets contained both marketed and drug-like molecules, and non-drug-like datasets contained the other two datasets. All datasets contained 2D molecular structure information in SDF format. Detailed information of the dataset pairs used in this study can be found in Table <xref rid="T1" ref-type="table">1</xref>.</p><table-wrap id="T1" position="float"><label>Table 1</label><caption><p>Detailed information of the dataset pairs.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Dataset pair</th><th valign="top" align="center" rowspan="1" colspan="1">Number of positive</th><th valign="top" align="center" rowspan="1" colspan="1">Number of negative</th><th valign="top" align="center" rowspan="1" colspan="1">Total</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">WDI/ACD</td><td valign="top" align="center" rowspan="1" colspan="1">38,260</td><td valign="top" align="center" rowspan="1" colspan="1">288,540</td><td valign="top" align="center" rowspan="1" colspan="1">326,800</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MDDR/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">171,850</td><td valign="top" align="center" rowspan="1" colspan="1">199,220</td><td valign="top" align="center" rowspan="1" colspan="1">371,070</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">WORLDDRUG/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">3,380</td><td valign="top" align="center" rowspan="1" colspan="1">199,220</td><td valign="top" align="center" rowspan="1" colspan="1">202,600</td></tr></tbody></table></table-wrap></sec><sec><title>Data Preprocessing</title><p>Data cleaning can be a crucial step in cheminformatics calculation, as expounded by <xref rid="B15" ref-type="bibr">Fourches et al. (2010)</xref>. We used a process (see Table <xref rid="T2" ref-type="table">2</xref>) similar to that of Fourches et al. to preprocess our raw data downloaded, making it less error-prone in descriptor calculation. After descriptor calculations, we also post-processed the resulting descriptor matrix (see Table <xref rid="T2" ref-type="table">2</xref>).</p><table-wrap id="T2" position="float"><label>Table 2</label><caption><p>Data preprocessing and post-processing steps used in this study.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="center" colspan="2" rowspan="1">Data processing<hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Step Name/Software</th><th valign="top" align="left" rowspan="1" colspan="1">Step description</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Element filter/KNIME (<xref rid="B4" ref-type="bibr">Berthold et al., 2009</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Hydrocarbons are removed. Molecules containing elements other than C H O N P S Cl Br I Si are removed.</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Remove Mixture/KNIME (<xref rid="B4" ref-type="bibr">Berthold et al., 2009</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">All records containing more than one molecules are removed.</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Standardize/ChemAxon Standardizer (<xref rid="B8" ref-type="bibr">ChemAxon Standardizer, 2010</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Neutralize, tautomerize, aromatize, and clean 2D</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Remove duplicate/OpenBabel (<xref rid="B33" ref-type="bibr">O&#x02019;Boyle et al., 2011</xref>)</td><td valign="top" align="left" rowspan="1" colspan="1">Two molecules having the same InChI(including stereochemistry) means duplication. If a molecule appears in both drug set and nondrug set, it is removed from nondrug set. As for duplications in the same set, only the one that appears first is kept.</td></tr><tr><td valign="top" align="left" colspan="2" rowspan="1">Data post-processing</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Remove error values/Python</td><td valign="top" align="left" rowspan="1" colspan="1">If a descriptor has the value of N/A or &#x02018;infinity&#x02019;, the molecule it belongs to is removed.</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Remove constant descriptors/Python</td><td valign="top" align="left" rowspan="1" colspan="1">If a descriptor has the same value across all molecules, the descriptor is removed from the descriptor list.</td></tr></tbody></table></table-wrap></sec><sec><title>Descriptor Calculation</title><p>We used 2D descriptors to encode the molecules. Molecules after preprocessing were calculated by MOLD2 (<xref rid="B21" ref-type="bibr">Hong et al., 2008</xref>), resulting a descriptor matrix of &#x0223c;700 descriptors per molecule. Then descriptor matrix was subjected to post-processing described in Table <xref rid="T2" ref-type="table">2</xref>. We also tried the Padel descriptors (<xref rid="B41" ref-type="bibr">Yap, 2011</xref>), which showed inferior performance in this study and was discarded.</p></sec><sec><title>Over-Sampling Algorithms</title><p>Due to the special classification task, the positive and negative samples collected by us were not balanced in this study. Predictive model developed using imbalanced data could be biased and inaccurate. Therefore, we adopted two methods to balance our data sets to make the ratio of positive and negative samples approximately equal. The first method was to copy the minority class making the ratio 1:1, the second one was to use SMOTE (<xref rid="B7" ref-type="bibr">Chawla et al., 2002</xref>; <xref rid="B17" ref-type="bibr">Han et al., 2005</xref>; <xref rid="B32" ref-type="bibr">Nguyen et al., 2011</xref>), which is an improved scheme based on random oversampling algorithm. Here we used imbalanced-learn package downloaded from<sup><xref ref-type="fn" rid="fn01">1</xref></sup> to apply SMOTE. For each task, we used these two oversampling methods to balance the data. For each model, firstly, we randomly split the datasets on the proportion of 9:1 as training set and validation set, secondly, we used the above two methods to balance the training set, so that the number of positive and negative samples during training was equal. The training set was used to train models with 5-CV and the additional validation set was used to evaluate models.</p></sec></sec></sec><sec sec-type="materials|methods" id="s1"><title>Materials and Methods</title><sec><title>Stacked Autoencoder</title><p>An autoencoder was an unsupervised learning algorithm that trains a neural network to reconstruct its input and more capable of catching the intrinsic structures of input data, instead of just memorizing. Intuitively, it attempted to build an encoding-decoding process so that the output <inline-formula><mml:math id="M6"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of the model is approximately similar to the input x. The SAE was a neural network consisting of multiple layers of sparse autoencoders, where the output of each layer was connected to the inputs of the successive layer. A schematic architecture of a SAE was shown in Figure <xref ref-type="fig" rid="F1">1</xref>. We trained the AE model with 2D chemical descriptors to find the intrinsic relationship between descriptors, then used the parameters of the AE model to initialize the classification model.</p><fig id="F1" position="float"><label>FIGURE 1</label><caption><p>A schematic architecture of a stacked autoencoder. Left) the architecture of autoencoder, layer-by-layer can be stacked. Right) a pre-trained autoencoder to initialize a fully connected network with the same structure for classifying.</p></caption><graphic xlink:href="fgene-09-00585-g001"/></fig></sec><sec><title>Defining Models</title><p>According to the partition of chemical space into drug, drug-like and non-drug-like, there can be two kinds of classification models, drug-like/non-drug-like, drug/non-drug-like. The first one matched the traditional definition of drug-likeness. The second one also bore considerable practical value, but no model had been published to address it. In this study, to address drug-like/non-drug-like classification, we proposed two models, MDDRWDI/ZINC (which means MDDR and WDI as positive set, ZINC as negative set) and WDI/ACD. To address drug/non-drug-like classification, we proposed WORLDDRUG/ZINC (which means ZINC WORLD DRUG as positive set, ZINC ALL PURCHASABLE as negative set) model.</p></sec><sec><title>Network Training and Hyperparameter Optimization</title><p>In this study, we used the open-source software library Keras (<xref rid="B10" ref-type="bibr">Chollet, 2015</xref>) based on Tensorflow (<xref rid="B1" ref-type="bibr">Abadi et al., 2016</xref>) to construct SAE model and classification model. Firstly, a single hidden layer AE was trained. The number of hidden layer nodes K, was a hyperparameter needs to be compared across different networks and tuned. During training, we used Truncated-Normal initializer to generates a truncated normal distribution of layer weights. In all case, we applied Bayesian optimization (Hyperas, a python library based on hyperopt<sup><xref ref-type="fn" rid="fn02">2</xref></sup>) to optimize the hyperparameter, such as the number of hidden layer nodes K, the value of L2 weight regularizer, the value of dropout, the type of activation function, the type of optimizer, the value of batch size. The final optimal hyper-parameter settings were listed in Table <xref rid="T3" ref-type="table">3</xref>.</p><table-wrap id="T3" position="float"><label>Table 3</label><caption><p>Hyper-parameter settings of the stacked autoencoder.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Hyperparameter</th><th valign="top" align="center" rowspan="1" colspan="1">Setting</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Initializer</td><td valign="top" align="center" rowspan="1" colspan="1">TruncatedNormal</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Number of hidden layers</td><td valign="top" align="center" rowspan="1" colspan="1">1</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Number of hidden layer nodes</td><td valign="top" align="center" rowspan="1" colspan="1">512</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">L2 Normalization term</td><td valign="top" align="center" rowspan="1" colspan="1">1e-4</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Dropout rate</td><td valign="top" align="center" rowspan="1" colspan="1">0.14</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Activation</td><td valign="top" align="center" rowspan="1" colspan="1">Relu</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Batch size</td><td valign="top" align="center" rowspan="1" colspan="1">128</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Optimizer</td><td valign="top" align="center" rowspan="1" colspan="1">Adam</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Loss</td><td valign="top" align="center" rowspan="1" colspan="1">mse for AE, binary crossentropy for classifier</td></tr></tbody></table></table-wrap><p>Considering that although the data set has been balanced, the model results may be overfitting, so we optimized the weight of the positive and negative sample loss of the logarithmic likelihood loss function as:</p><disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mtext>&#x02009;</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><p>where y<sub>k</sub> represented the k<sup>th</sup> compound label. y<sub>k</sub> = 1 or 0, means k<sup>th</sup> compound was the drug-like or non-drug-like compound, respectively. a<sub>k</sub> = P(y<sub>k</sub> = 1|x<sub>k</sub>) was the probability to be the drug-like compound of k<sup>th</sup> compound calculated by model. w was the weight of the positive sample loss. For different cases, we chose the most suitable w from the range of (0.5&#x0223c;1.0) to avoid overfitting. Then we trained all models with 5-CV and enforced early stopping based on classification ACC on the test set. Finally, each case had 5 trained models and the average value was the final judgement of these models.</p></sec><sec><title>Model Evaluation</title><p>All models were evaluated by five indexes. The ACC, SP, and sensitivity(SE), MCC, area under the receiver operating characteristic curve (AUC), the previous four criteria were defined, respectively, as follow:</p><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></sec></sec><sec><title>Results</title><sec><title>Compare Different Over-Sampling Methods</title><p>After we tried pre-training on validation test with 5-CV, we found that more layers and neuron numbers did not improve the predictive power. In all case, one hidden layer was sufficient for our classification objective. By analyzing the two different over-sampling methods to balance datasets, copy the minority class and SMOTE, we found the latter can achieve better prediction accuracies in Table <xref rid="T4" ref-type="table">4</xref>.</p><table-wrap id="T4" position="float"><label>Table 4</label><caption><p>Performance on the training sets with 5-CV.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Model</th><th valign="top" align="center" colspan="4" rowspan="1">Copy the minority class<hr/></th><th valign="top" align="center" colspan="4" rowspan="1">SMOTE over-sampling<hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">ACC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">SP</th><th valign="top" align="center" rowspan="1" colspan="1">AUC</th><th valign="top" align="center" rowspan="1" colspan="1">ACC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">SP</th><th valign="top" align="center" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">WDI/ACD</td><td valign="top" align="center" rowspan="1" colspan="1">0.8923</td><td valign="top" align="center" rowspan="1" colspan="1">0.8991</td><td valign="top" align="center" rowspan="1" colspan="1">0.8859</td><td valign="top" align="center" rowspan="1" colspan="1">0.9598</td><td valign="top" align="center" rowspan="1" colspan="1">0.9265</td><td valign="top" align="center" rowspan="1" colspan="1">0.9244</td><td valign="top" align="center" rowspan="1" colspan="1">0.9286</td><td valign="top" align="center" rowspan="1" colspan="1">0.9783</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MDDR/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9095</td><td valign="top" align="center" rowspan="1" colspan="1">0.8855</td><td valign="top" align="center" rowspan="1" colspan="1">0.9302</td><td valign="top" align="center" rowspan="1" colspan="1">0.9701</td><td valign="top" align="center" rowspan="1" colspan="1">0.9116</td><td valign="top" align="center" rowspan="1" colspan="1">0.9141</td><td valign="top" align="center" rowspan="1" colspan="1">0.9092</td><td valign="top" align="center" rowspan="1" colspan="1">0.9719</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">WORLD/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9910</td><td valign="top" align="center" rowspan="1" colspan="1">0.9961</td><td valign="top" align="center" rowspan="1" colspan="1">0.9859</td><td valign="top" align="center" rowspan="1" colspan="1">0.9986</td><td valign="top" align="center" rowspan="1" colspan="1">0.9906</td><td valign="top" align="center" rowspan="1" colspan="1">0.9937</td><td valign="top" align="center" rowspan="1" colspan="1">0.9874</td><td valign="top" align="center" rowspan="1" colspan="1">0.9990</td></tr></tbody></table></table-wrap><p>With the same dataset, the ACC of a SVM model built by Li et al was 92.73% (<xref rid="B25" ref-type="bibr">Li et al., 2007</xref>) and our WDI/ACD model achieves an ACC of 92.65%, almost identical to Li&#x02019;s results. Our MDDRWDI/ZINC model classified drug-like/non-drug-like molecules with a satisfactory ACC of 91.16%, making it the state-of-the-art drug-likeness prediction model. These results suggest that autoencoder is a potential machine learning algorithm in drug-likeness prediction. The ACC of our drug/non-drug-like prediction model based on World Drug/ZINC dataset was as high as 99.06%, showing that it is easier to distinguish compounds from drugs or non-drugs. Although it is not excluded that the ACC of the latter models is related to the serious imbalance of the original data set, we believe that such drug/non-drug-like prediction model will likely benefit drug development.</p></sec><sec><title>Optimize the Weights in the Loss Function</title><p>We observed that when using the independent external validation set pre-segmented from the original data to evaluate model, the prediction ACC of the model tended to be slightly lower than that of training, but the sensitivity value was significantly lower and the SP value was higher (Table <xref rid="T5" ref-type="table">5</xref>), indicating that the models have some over-fitting in training.</p><table-wrap id="T5" position="float"><label>Table 5</label><caption><p>Performance of the models on the validation sets.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Model</th><th valign="top" align="center" colspan="5" rowspan="1">Using SMOTE over-sampling<hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="center" rowspan="1" colspan="1">ACC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">SP</th><th valign="top" align="center" rowspan="1" colspan="1">MCC</th><th valign="top" align="center" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">WDI/ACD</td><td valign="top" align="center" rowspan="1" colspan="1">0.9014</td><td valign="top" align="center" rowspan="1" colspan="1">0.7683</td><td valign="top" align="center" rowspan="1" colspan="1">0.9191</td><td valign="top" align="center" rowspan="1" colspan="1">0.6014</td><td valign="top" align="center" rowspan="1" colspan="1">0.9271</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MDDR/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9025</td><td valign="top" align="center" rowspan="1" colspan="1">0.9012</td><td valign="top" align="center" rowspan="1" colspan="1">0.9036</td><td valign="top" align="center" rowspan="1" colspan="1">0.8043</td><td valign="top" align="center" rowspan="1" colspan="1">0.9669</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">WORLD/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9800</td><td valign="top" align="center" rowspan="1" colspan="1">0.7544</td><td valign="top" align="center" rowspan="1" colspan="1">0.9838</td><td valign="top" align="center" rowspan="1" colspan="1">0.5690</td><td valign="top" align="center" rowspan="1" colspan="1">0.9707</td></tr></tbody></table></table-wrap><p>The underlying reason may be that the positive sample ratio in the original data was too low, and we randomly divided the positive and negative samples in the original data set according to 9:1 to build the training set and the validation set. Even if the SMOTE method was used to balance the positive and negative samples in the train set, the new positive sample generated by SMOTE depended on positive sample in the original training set, so the positive sample information of the external verification set was less included.</p><p>In order to overcome the over-fitting on the negative samples, we increased the weight of positive sample loss in the loss function to enhance the learning ability of the model to the positive sample side. We tested the weigh values (details in Formula 1) from 0.5 to 1 with 20 intervals, and record the values of ACC, SE, and SP on the validation set varying with weight, as shown in Figure <xref ref-type="fig" rid="F2">2</xref>.</p><fig id="F2" position="float"><label>FIGURE 2</label><caption><p>Evaluations of different models vary with weight of positive sample loss.</p></caption><graphic xlink:href="fgene-09-00585-g002"/></fig><p>For different models, the intersection point of SE and SP in the curves of Figure <xref ref-type="fig" rid="F2">2</xref> corresponded to a balanced weight value. By fine-tuning, the weights corresponding to the four models are (0.69, 0.55 and 0.9). After using these weights for the loss functions, the ACC of the training set in different models fells slightly and the SE improves. As the model reinforces the prediction of positive samples, the SE and SP of the validation set in different models are close (shown in Tables <xref rid="T6" ref-type="table">6</xref>, <xref rid="T7" ref-type="table">7</xref>).</p><table-wrap id="T6" position="float"><label>Table 6</label><caption><p>Performance on the training set after optimizing the weight of loss function.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Model</th><th valign="top" align="center" colspan="5" rowspan="1">SMOTE over-sampling<hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1">ACC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">SP</th><th valign="top" align="center" rowspan="1" colspan="1">MCC</th><th valign="top" align="center" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">WDI/ACD</td><td valign="top" align="center" rowspan="1" colspan="1">0.9104</td><td valign="top" align="center" rowspan="1" colspan="1">0.9694</td><td valign="top" align="center" rowspan="1" colspan="1">0.8515</td><td valign="top" align="center" rowspan="1" colspan="1">0.8270</td><td valign="top" align="center" rowspan="1" colspan="1">0.9757</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MDDR/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9120</td><td valign="top" align="center" rowspan="1" colspan="1">0.9219</td><td valign="top" align="center" rowspan="1" colspan="1">0.9020</td><td valign="top" align="center" rowspan="1" colspan="1">0.8243</td><td valign="top" align="center" rowspan="1" colspan="1">0.9726</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">WORLD/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9699</td><td valign="top" align="center" rowspan="1" colspan="1">0.9985</td><td valign="top" align="center" rowspan="1" colspan="1">0.9414</td><td valign="top" align="center" rowspan="1" colspan="1">0.9416</td><td valign="top" align="center" rowspan="1" colspan="1">0.9955</td></tr></tbody></table></table-wrap><table-wrap id="T7" position="float"><label>Table 7</label><caption><p>Performance on the validation set after optimizing the weight of loss function.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1">Model</th><th valign="top" align="center" colspan="5" rowspan="1">SMOTE over-sampling<hr/></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1"/><th valign="top" align="left" rowspan="1" colspan="1">ACC</th><th valign="top" align="center" rowspan="1" colspan="1">SE</th><th valign="top" align="center" rowspan="1" colspan="1">SP</th><th valign="top" align="center" rowspan="1" colspan="1">MCC</th><th valign="top" align="center" rowspan="1" colspan="1">AUC</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">WDI/ACD</td><td valign="top" align="center" rowspan="1" colspan="1">0.8458</td><td valign="top" align="center" rowspan="1" colspan="1">0.8524</td><td valign="top" align="center" rowspan="1" colspan="1">0.8449</td><td valign="top" align="center" rowspan="1" colspan="1">0.5286</td><td valign="top" align="center" rowspan="1" colspan="1">0.9253</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">MDDR/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9046</td><td valign="top" align="center" rowspan="1" colspan="1">0.9174</td><td valign="top" align="center" rowspan="1" colspan="1">0.8935</td><td valign="top" align="center" rowspan="1" colspan="1">0.8095</td><td valign="top" align="center" rowspan="1" colspan="1">0.9699</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">WORLD/ZINC</td><td valign="top" align="center" rowspan="1" colspan="1">0.9366</td><td valign="top" align="center" rowspan="1" colspan="1">0.8804</td><td valign="top" align="center" rowspan="1" colspan="1">0.9376</td><td valign="top" align="center" rowspan="1" colspan="1">0.4049</td><td valign="top" align="center" rowspan="1" colspan="1">0.9622</td></tr></tbody></table></table-wrap><p>Although the MCC is generally regarded as a balanced measure, it is seriously affected by the number gap between positive and negative samples of data sets and the confusion matrix calculated by the model. The MCC is satisfactory for the balanced training sets. But in the validation sets, the data set becomes more unbalanced, and the MCC becomes smaller, which was inevitable.</p></sec></sec><sec><title>Discussion</title><p>In image recognition problems, where AE was originated, several layers of AE are often stacked to make a SAE. Though SAE was found to be more powerful than single layer AE there, we found that SAE is flawed here in drug-likeness problems, making multi-layer SAE perform much poorer than single layer AE.</p><p>When a layer of AE is trained, it is expected to give output as close as possible to its input, and the error can be defined as the mean value of output minus input. In this study, when training the model, we found that the ACC of the normalized (z-score) input was much higher than scaling input to [-1,1]. After standardizing the data, the error of AE is 0.8, an order of magnitude higher than typical values in image recognition. Stacking layers of AE will further amplify the error, making the SAE-initialized NN perform poorly in classification.</p><p>We propose that such a flaw of AE stems from how input data in different dimensions are interrelated. In image recognition, each pixel is a dimension; in drug-likeness prediction and related areas, each descriptor is a dimension. The training goal of AE is to learn the relationship among dimensions, to encode input information into hidden layer dimensions. So it is very likely that AE would do worse if the relationship among dimensions is intrinsically more chaotic and irregular. The relationship among pixels is regular in that they are organized as a 2D grid and that neighbor pixels bare some similarity and complementarity. Such good properties are absent in relationship among descriptors, resulting in the failure of AE input reconstruction process. Despite the fact that AE reconstruction error is large, our model still performs well in classification. In our opinion, this is due to the regularization effect of AE pre-training. With unsupervised pre-training, the model is more capable of truly learning data, less prone to simply memorizing data.</p><p>Imbalanced data sets are a common problem. Although there are some methods such as SMOTE, which can generate new data to balance the data set, this method of generating data is much dependent on the distribution of samples. Once the distribution of samples is very sparse, then the new data is likely to deviate from the space where the original data is exited. Developing method to find data mapping spaces based on the distribution of existing data is critical to generating data to balance the data set, such as the current popular deep generation model. Developing new algorithms to train unbalanced data sets is also an important research direction.</p><p>In this study, DL has once again shown its capacity for improving prediction models. Despite the success, we believe that there is still much space for further development. A key aspect is to adapt current DL methods to specific problems. Such adaptations should be based on a better comprehension of current DL methods. That is, knowing which part of the method can be universally applied, and which part should be modified according to the nature of data. For example, in this study, we believe that the regularization effect of AE pre-training is a universal part, while the part of AE input reconstruction should be canceled or modified when input data is irregular.</p></sec><sec><title>Conclusion</title><p>In this study, we manually built two larger data sets, drug-like/non-drug-like and drug/non-drug-like. Then using the AE pre-training method, we developed drug-likeness prediction models. The ACC of classification based on WDI and ACD databases was improved to 91.04%. Our model achieved classification ACC of 91.20% on MDDRWDI/ZINC dataset, making it the state-of-the-art drug-likeness prediction model, showing the predictive power of DL model outperforms traditional machine learning methods. In addition, we developed a drug/non-drug-like model (ZINC World Drug vs. ZINC All Purchasable), which distinguished drugs and common compounds, with a classification ACC of 96.99%. We proposed that AE pre-training served as a better regularization method in this study. The fail of multi-layer SAE reconstruction in this study indicated that due to the specific nature of data, some modifications may be needed when applying DL to different fields. We hope machine learning researchers and chemists collaborate closely to solve such a problem in the future, bringing further comprehension and applications of DL method in chemical problems.</p></sec><sec><title>Author Contributions</title><p>QH and MF wrote the codes and analyzed the data. LL and JP conceived the work. All authors wrote the paper.</p></sec><sec><title>Conflict of Interest Statement</title><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec></body><back><fn-group><fn fn-type="financial-disclosure"><p><bold>Funding.</bold> This work has been supported in part by the National Natural Science Foundation of China (21673010 and 21633001) and the Ministry of Science and Technology of China (2016YFA0502303).</p></fn></fn-group><fn-group><fn id="fn01"><label>1</label><p><ext-link ext-link-type="uri" xlink:href="http://contrib.scikit-learn.org/imbalanced-learn/stable/install.html">http://contrib.scikit-learn.org/imbalanced-learn/stable/install.html</ext-link></p></fn><fn id="fn02"><label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/maxpumperla/hyperas">https://github.com/maxpumperla/hyperas</ext-link></p></fn></fn-group><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M.</given-names></name><name><surname>Barham</surname><given-names>P.</given-names></name><name><surname>Chen</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>Z.</given-names></name><name><surname>Davis</surname><given-names>A.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>&#x0201c;Tensorflow: a system for large-scale machine learning,&#x0201d; in</article-title>
<source><italic>Proceedings of the12th USENIX Symposium on Operating Systems Design and Implementation (OSDI &#x02019;16)</italic></source>, Vol. 16 (Savannah, GA: USENIX), <fpage>265</fpage>&#x02013;<lpage>283</lpage>.</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D.</given-names></name><name><surname>Chorowski</surname><given-names>J.</given-names></name><name><surname>Serdyuk</surname><given-names>D.</given-names></name><name><surname>Brakel</surname><given-names>P.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;End-to-end attention-based large vocabulary speech recognition,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic></source> (Lujiazui: IEEE), <fpage>4945</fpage>&#x02013;<lpage>4949</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2016.7472618</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group> (<year>2009</year>). <article-title>&#x0201c;Learning deep architectures for AI,&#x0201d; in</article-title>
<source><italic>Foundations and Trends<sup>&#x000ae;</sup></italic> in Machine Learning</source>
<volume>Vol. 2</volume>
<role>ed.</role>
<person-group person-group-type="editor"><name><surname>Jordan</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>Hanover, MA</publisher-loc>: <publisher-name>ACM Digital Library</publisher-name>), <fpage>1</fpage>&#x02013;<lpage>127</lpage>.</mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthold</surname><given-names>M. R.</given-names></name><name><surname>Cebron</surname><given-names>N.</given-names></name><name><surname>Dill</surname><given-names>F.</given-names></name><name><surname>Gabriel</surname><given-names>T. R.</given-names></name><name><surname>K&#x000f6;tter</surname><given-names>T.</given-names></name><name><surname>Meinl</surname><given-names>T.</given-names></name><etal/></person-group> (<year>2009</year>). <article-title>KNIME-the Konstanz information miner: version 2.0 and beyond.</article-title>
<source><italic>ACM SIGKDD Explor. Newslett.</italic></source>
<volume>11</volume>
<fpage>26</fpage>&#x02013;<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1145/1656274.1656280</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bickerton</surname><given-names>G. R.</given-names></name><name><surname>Paolini</surname><given-names>G. V.</given-names></name><name><surname>Besnard</surname><given-names>J.</given-names></name><name><surname>Muresan</surname><given-names>S.</given-names></name><name><surname>Hopkins</surname><given-names>A. L.</given-names></name></person-group> (<year>2012</year>). <article-title>Quantifying the chemical beauty of drugs.</article-title>
<source><italic>Nat. chem.</italic></source>
<volume>4</volume>:<issue>90</issue>. <pub-id pub-id-type="doi">10.1038/nchem.1243</pub-id>
<?supplied-pmid 22270643?><pub-id pub-id-type="pmid">22270643</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byvatov</surname><given-names>E.</given-names></name><name><surname>Fechner</surname><given-names>U.</given-names></name><name><surname>Sadowski</surname><given-names>J.</given-names></name><name><surname>Schneider</surname><given-names>G.</given-names></name></person-group> (<year>2003</year>). <article-title>Comparison of support vector machine and artificial neural network systems for drug/nondrug classification.</article-title>
<source><italic>J. Chem. Inf. Comput. Sci.</italic></source>
<volume>43</volume>
<fpage>1882</fpage>&#x02013;<lpage>1889</lpage>. <pub-id pub-id-type="doi">10.1021/ci0341161</pub-id>
<?supplied-pmid 14632437?><pub-id pub-id-type="pmid">14632437</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname><given-names>N. V.</given-names></name><name><surname>Bowyer</surname><given-names>K. W.</given-names></name><name><surname>Hall</surname><given-names>L. O.</given-names></name><name><surname>Kegelmeyer</surname><given-names>W. P.</given-names></name></person-group> (<year>2002</year>). <article-title>SMOTE: synthetic minority over-sampling technique.</article-title>
<source><italic>J. Artif. Intell. Res.</italic></source>
<volume>16</volume>
<fpage>321</fpage>&#x02013;<lpage>357</lpage>. <pub-id pub-id-type="doi">10.1613/jair.953</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal">ChemAxon Standardizer. (<year>2010</year>). <source><italic>ChemAxon Standardizer Version 5.4.4.1.</italic></source> Budapest: ChemAxon.</mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Narayan</surname><given-names>R.</given-names></name><name><surname>Subramanian</surname><given-names>A.</given-names></name><name><surname>Xie</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>). <article-title>Gene expression inference with deep learning.</article-title>
<source><italic>Bioinformatics</italic></source>
<volume>32</volume>
<fpage>1832</fpage>&#x02013;<lpage>1839</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw074</pub-id>
<?supplied-pmid 26873929?><pub-id pub-id-type="pmid">26873929</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F.</given-names></name></person-group> (<year>2015</year>). <source><italic>Keras. GitHub.</italic></source> Available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</ext-link>.</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coates</surname><given-names>A.</given-names></name><name><surname>Ng</surname><given-names>A.</given-names></name><name><surname>Lee</surname><given-names>H.</given-names></name></person-group> (<year>2011</year>). <article-title>&#x0201c;An analysis of single-layer networks in unsupervised feature learning,&#x0201d; in</article-title>
<source><italic>Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</italic></source> (Fort Lauderdale, FL: PMLR), <fpage>215</fpage>&#x02013;<lpage>223</lpage>.</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahl</surname><given-names>G. E.</given-names></name><name><surname>Yu</surname><given-names>D.</given-names></name><name><surname>Deng</surname><given-names>L.</given-names></name><name><surname>Acero</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.</article-title>
<source><italic>IEEE Trans. Audio Speech Lang. Process.</italic></source>
<volume>20</volume>
<fpage>30</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1109/TASL.2011.2134090</pub-id></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darrow</surname><given-names>J. J.</given-names></name><name><surname>Kesselheim</surname><given-names>A. S.</given-names></name></person-group> (<year>2014</year>). <article-title>Drug development and FDA approval, 1938&#x02013;2013.</article-title>
<source><italic>N. Engl. J. Med.</italic></source>
<volume>370</volume>:<issue>e39</issue>. <pub-id pub-id-type="doi">10.1056/NEJMp1402114</pub-id>
<?supplied-pmid 24963591?><pub-id pub-id-type="pmid">24963591</pub-id></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Lena</surname><given-names>P.</given-names></name><name><surname>Nagata</surname><given-names>K.</given-names></name><name><surname>Baldi</surname><given-names>P.</given-names></name></person-group> (<year>2012</year>). <article-title>Deep architectures for protein contact map prediction.</article-title>
<source><italic>Bioinformatics</italic></source>
<volume>28</volume>
<fpage>2449</fpage>&#x02013;<lpage>2457</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bts475</pub-id>
<?supplied-pmid 22847931?><pub-id pub-id-type="pmid">22847931</pub-id></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fourches</surname><given-names>D.</given-names></name><name><surname>Muratov</surname><given-names>E.</given-names></name><name><surname>Tropsha</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Trust, but verify: on the importance of chemical structure curation in cheminformatics and QSAR modeling research.</article-title>
<source><italic>J. Chem. Inf. Comput. Sci.</italic></source>
<volume>50</volume>
<fpage>1189</fpage>&#x02013;<lpage>1204</lpage>. <pub-id pub-id-type="doi">10.1021/ci100176x</pub-id>
<?supplied-pmid 20572635?><pub-id pub-id-type="pmid">20572635</pub-id></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>A.</given-names></name><name><surname>Mohamed</surname><given-names>A. R.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2013</year>). <article-title>&#x0201c;Speech recognition with deep recurrent neural networks,&#x0201d; in</article-title>
<source><italic>Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</italic></source> (Vancouver: IEEE), <fpage>6645</fpage>&#x02013;<lpage>6649</lpage>. <pub-id pub-id-type="doi">10.1109/ICASSP.2013.6638947</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>W. Y.</given-names></name><name><surname>Mao</surname><given-names>B. H.</given-names></name></person-group> (<year>2005</year>). &#x0201c;<article-title>Borderline-SMOTE: a new over-sampling method in imbalanced data sets learning</article-title>,&#x0201d; in <source><italic>Proceedings of the</italic></source>
<source><italic>International Conference on Intelligent Computing</italic></source> (Berlin: Springer), <fpage>878</fpage>&#x02013;<lpage>887</lpage>. <pub-id pub-id-type="doi">10.1007/11538059_91</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2016</year>). <article-title>&#x0201c;Deep residual learning for image recognition,&#x0201d; in</article-title>
<source><italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic></source> (Seattle, WA: IEEE), <fpage>770</fpage>&#x02013;<lpage>778</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffernan</surname><given-names>R.</given-names></name><name><surname>Paliwal</surname><given-names>K.</given-names></name><name><surname>Lyons</surname><given-names>J.</given-names></name><name><surname>Dehzangi</surname><given-names>A.</given-names></name><name><surname>Sharma</surname><given-names>A.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>Improving prediction of secondary structure, local backbone angles, and solvent accessible surface area of proteins by iterative deep learning.</article-title>
<source><italic>Sci. Rep.</italic></source>
<volume>5</volume>:<issue>11476</issue>. <pub-id pub-id-type="doi">10.1038/srep11476</pub-id>
<?supplied-pmid 26098304?><pub-id pub-id-type="pmid">26098304</pub-id></mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G. E.</given-names></name><name><surname>Osindero</surname><given-names>S.</given-names></name><name><surname>Teh</surname><given-names>Y. W.</given-names></name></person-group> (<year>2006</year>). <article-title>A fast learning algorithm for deep belief nets.</article-title>
<source><italic>Neural Comput.</italic></source>
<volume>18</volume>
<fpage>1527</fpage>&#x02013;<lpage>1554</lpage>. <pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1527</pub-id>
<?supplied-pmid 16764513?><pub-id pub-id-type="pmid">16764513</pub-id></mixed-citation></ref><ref id="B21"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>H.</given-names></name><name><surname>Xie</surname><given-names>Q.</given-names></name><name><surname>Ge</surname><given-names>W.</given-names></name><name><surname>Qian</surname><given-names>F.</given-names></name><name><surname>Fang</surname><given-names>H.</given-names></name><name><surname>Shi</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2008</year>). <article-title>Mold2, molecular descriptors from 2D structures for chemoinformatics and toxicoinformatics.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>48</volume>
<fpage>1337</fpage>&#x02013;<lpage>1344</lpage>. <pub-id pub-id-type="doi">10.1021/ci800038f</pub-id>
<?supplied-pmid 18564836?><pub-id pub-id-type="pmid">18564836</pub-id></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irwin</surname><given-names>J. J.</given-names></name><name><surname>Sterling</surname><given-names>T.</given-names></name><name><surname>Mysinger</surname><given-names>M. M.</given-names></name><name><surname>Bolstad</surname><given-names>E. S.</given-names></name><name><surname>Coleman</surname><given-names>R. G.</given-names></name></person-group> (<year>2012</year>). <article-title>ZINC: a free tool to discover chemistry for biology.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>52</volume>
<fpage>1757</fpage>&#x02013;<lpage>1768</lpage>. <pub-id pub-id-type="doi">10.1021/ci3001277</pub-id>
<?supplied-pmid 22587354?><pub-id pub-id-type="pmid">22587354</pub-id></mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Imagenet classification with deep convolutional neural networks,&#x0201d; in</article-title>
<source><italic>Proceedings of the Conference on Advances in Neural Information Processing Systems</italic></source> (Lake Tahoe, NV: ACM Digital Library), <fpage>1097</fpage>&#x02013;<lpage>1105</lpage>.</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning.</article-title>
<source><italic>Nature</italic></source>
<volume>521</volume>:<issue>436</issue>. <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
<?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q.</given-names></name><name><surname>Bender</surname><given-names>A.</given-names></name><name><surname>Pei</surname><given-names>J.</given-names></name><name><surname>Lai</surname><given-names>L.</given-names></name></person-group> (<year>2007</year>). <article-title>A large descriptor set and a probabilistic kernel-based classifier significantly improve druglikeness classification.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>47</volume>
<fpage>1776</fpage>&#x02013;<lpage>1786</lpage>. <pub-id pub-id-type="doi">10.1021/ci700107y</pub-id>
<?supplied-pmid 17718552?><pub-id pub-id-type="pmid">17718552</pub-id></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipinski</surname><given-names>C. A.</given-names></name></person-group> (<year>2004</year>). <article-title>Lead-and drug-like compounds: the rule-of-five revolution.</article-title>
<source><italic>Drug Discov. Today Technol.</italic></source>
<volume>1</volume>
<fpage>337</fpage>&#x02013;<lpage>341</lpage>. <pub-id pub-id-type="doi">10.1016/j.ddtec.2004.11.007</pub-id>
<?supplied-pmid 24981612?><pub-id pub-id-type="pmid">24981612</pub-id></mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lipper</surname><given-names>R. A.</given-names></name></person-group> (<year>1999</year>). <article-title>How can we optimize selection of drug development candidates from many compounds at the discovery stage?</article-title>
<source><italic>Mod. Drug Discov.</italic></source>
<volume>2</volume>
<fpage>55</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyons</surname><given-names>J.</given-names></name><name><surname>Dehzangi</surname><given-names>A.</given-names></name><name><surname>Heffernan</surname><given-names>R.</given-names></name><name><surname>Sharma</surname><given-names>A.</given-names></name><name><surname>Paliwal</surname><given-names>K.</given-names></name><name><surname>Sattar</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Predicting backbone C&#x003b1; angles and dihedrals from protein sequences by stacked sparse auto-encoder deep neural network.</article-title>
<source><italic>J. Comput. Chem.</italic></source>
<volume>35</volume>
<fpage>2040</fpage>&#x02013;<lpage>2046</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.23718</pub-id>
<?supplied-pmid 25212657?><pub-id pub-id-type="pmid">25212657</pub-id></mixed-citation></ref><ref id="B29"><mixed-citation publication-type="journal">MACCS-II Drug Data Report [MDDR] (<year>2004</year>). <source><italic>MACCS-II Drug Data Report [MDDR].</italic></source> San Leandro, CA: Molecular Design Limited.</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>T.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Corrado</surname><given-names>G. S.</given-names></name><name><surname>Dean</surname><given-names>J.</given-names></name></person-group> (<year>2013</year>). <article-title>&#x0201c;Distributed representations of words and phrases and their compositionality,&#x0201d; in</article-title>
<source><italic>Proceedings of the conference on Advances in Neural Information Processing Systems</italic></source> (Lake Tahoe, NV: ACM Digital Library), <fpage>3111</fpage>&#x02013;<lpage>3119</lpage>.</mixed-citation></ref><ref id="B31"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>M&#x000fc;ller</surname><given-names>K. R.</given-names></name><name><surname>R&#x000e4;tsch</surname><given-names>G.</given-names></name><name><surname>Sonnenburg</surname><given-names>S.</given-names></name><name><surname>Mika</surname><given-names>S.</given-names></name><name><surname>Grimm</surname><given-names>M.</given-names></name><name><surname>Heinrich</surname><given-names>N.</given-names></name></person-group> (<year>2005</year>). <article-title>Classifying &#x02018;drug-likeness&#x02019; with kernel-based learning methods.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>45</volume>
<fpage>249</fpage>&#x02013;<lpage>253</lpage>. <pub-id pub-id-type="doi">10.1021/ci049737o</pub-id>
<?supplied-pmid 15807485?><pub-id pub-id-type="pmid">15807485</pub-id></mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>H. M.</given-names></name><name><surname>Cooper</surname><given-names>E. W.</given-names></name><name><surname>Kamei</surname><given-names>K.</given-names></name></person-group> (<year>2011</year>). <article-title>Borderline over-sampling for imbalanced data classification.</article-title>
<source><italic>Int. J. Knowl. Eng. Soft Data Paradig.</italic></source>
<volume>3</volume>
<fpage>4</fpage>&#x02013;<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1016/j.jbi.2017.03.002</pub-id>
<?supplied-pmid 28286029?><pub-id pub-id-type="pmid">28286029</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O&#x02019;Boyle</surname><given-names>N. M.</given-names></name><name><surname>Banck</surname><given-names>M.</given-names></name><name><surname>James</surname><given-names>C. A.</given-names></name><name><surname>Morley</surname><given-names>C.</given-names></name><name><surname>Vandermeersch</surname><given-names>T.</given-names></name><name><surname>Hutchison</surname><given-names>G. R.</given-names></name></person-group> (<year>2011</year>). <article-title>Open Babel: an open chemical toolbox.</article-title>
<source><italic>J. Cheminform.</italic></source>
<volume>3</volume>:<issue>33</issue>. <pub-id pub-id-type="doi">10.1186/1758-2946-3-33</pub-id>
<?supplied-pmid 21982300?><pub-id pub-id-type="pmid">21982300</pub-id></mixed-citation></ref><ref id="B34"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>N.</given-names></name><name><surname>J&#x000e4;ckels</surname><given-names>C.</given-names></name><name><surname>Andres</surname><given-names>C.</given-names></name><name><surname>Hutter</surname><given-names>M. C.</given-names></name></person-group> (<year>2008</year>). <article-title>Gradual in silico filtering for druglike substances.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>48</volume>
<fpage>613</fpage>&#x02013;<lpage>628</lpage>. <pub-id pub-id-type="doi">10.1021/ci700351y</pub-id>
<?supplied-pmid 18269264?><pub-id pub-id-type="pmid">18269264</pub-id></mixed-citation></ref><ref id="B35"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Manning</surname><given-names>C. D.</given-names></name></person-group> (<year>2012</year>). <article-title>&#x0201c;Deep learning for NLP (without magic),&#x0201d; in</article-title>
<source><italic>Tutorial Abstracts of ACL 2012</italic></source>, <role>ed.</role>
<person-group person-group-type="editor"><name><surname>Strube</surname><given-names>M.</given-names></name></person-group> (<publisher-loc>Stroudsburg, PA</publisher-loc>: <publisher-name>Association for Computational Linguistics</publisher-name>), <fpage>12</fpage>&#x02013;<lpage>14</lpage>.</mixed-citation></ref><ref id="B36"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sterling</surname><given-names>T.</given-names></name><name><surname>Irwin</surname><given-names>J. J.</given-names></name></person-group> (<year>2015</year>). <article-title>ZINC 15&#x02013;ligand discovery for everyone.</article-title>
<source><italic>J. Chem. Inf. Model.</italic></source>
<volume>55</volume>
<fpage>2324</fpage>&#x02013;<lpage>2337</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00559</pub-id>
<?supplied-pmid 26479676?><pub-id pub-id-type="pmid">26479676</pub-id></mixed-citation></ref><ref id="B37"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>S.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Xu</surname><given-names>X.</given-names></name><name><surname>Hou</surname><given-names>T.</given-names></name></person-group> (<year>2012</year>). <article-title>Drug-likeness analysis of traditional chinese medicines: prediction of drug-likeness using machine learning approaches.</article-title>
<source><italic>Mol. Pharm.</italic></source>
<volume>9</volume>
<fpage>2875</fpage>&#x02013;<lpage>2886</lpage>. <pub-id pub-id-type="doi">10.1021/mp300198d</pub-id>
<?supplied-pmid 22738405?><pub-id pub-id-type="pmid">22738405</pub-id></mixed-citation></ref><ref id="B38"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkatesh</surname><given-names>S.</given-names></name><name><surname>Lipper</surname><given-names>R. A.</given-names></name></person-group> (<year>2000</year>). <article-title>Role of the development scientist in compound lead selection and optimization.</article-title>
<source><italic>J. Pharm. Sci.</italic></source>
<volume>89</volume>
<fpage>145</fpage>&#x02013;<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1002/(SICI)1520-6017(200002)89:2&#x0003c;145::AID-JPS2&#x0003e;3.0.CO;2-6</pub-id>
<?supplied-pmid 10688744?><pub-id pub-id-type="pmid">10688744</pub-id></mixed-citation></ref><ref id="B39"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>P.</given-names></name><name><surname>Larochelle</surname><given-names>H.</given-names></name><name><surname>Lajoie</surname><given-names>I.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Manzagol</surname><given-names>P. A.</given-names></name></person-group> (<year>2010</year>). <article-title>Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion.</article-title>
<source><italic>J. Mach. Learn. Res.</italic></source>
<volume>11</volume>
<fpage>3371</fpage>&#x02013;<lpage>3408</lpage>.</mixed-citation></ref><ref id="B40"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagener</surname><given-names>M.</given-names></name><name><surname>van Geerestein</surname><given-names>V. J.</given-names></name></person-group> (<year>2000</year>). <article-title>Potential drugs and nondrugs: prediction and identification of important structural features.</article-title>
<source><italic>J. Chem. Inf. Comput. Sci.</italic></source>
<volume>40</volume>
<fpage>280</fpage>&#x02013;<lpage>292</lpage>. <pub-id pub-id-type="doi">10.1021/ci990266t</pub-id>
<?supplied-pmid 10761129?><pub-id pub-id-type="pmid">10761129</pub-id></mixed-citation></ref><ref id="B41"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yap</surname><given-names>C. W.</given-names></name></person-group> (<year>2011</year>). <article-title>PaDEL-descriptor: an open source software to calculate molecular descriptors and fingerprints.</article-title>
<source><italic>J. Comput. Chem.</italic></source>
<volume>32</volume>
<fpage>1466</fpage>&#x02013;<lpage>1474</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.21707</pub-id>
<?supplied-pmid 21425294?><pub-id pub-id-type="pmid">21425294</pub-id></mixed-citation></ref><ref id="B42"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H.</given-names></name><name><surname>Edwards</surname><given-names>M. D.</given-names></name><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Gifford</surname><given-names>D. K.</given-names></name></person-group> (<year>2016</year>). <article-title>Convolutional neural network architectures for predicting DNA&#x02013;protein binding.</article-title>
<source><italic>Bioinformatics</italic></source>
<volume>32</volume>
<fpage>i121</fpage>&#x02013;<lpage>i127</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btw255</pub-id>
<?supplied-pmid 27307608?><pub-id pub-id-type="pmid">27307608</pub-id></mixed-citation></ref></ref-list><glossary><title>Abbreviations</title><def-list id="DL1"><def-item><term>5-CV</term><def><p>5-fold cross-validation</p></def></def-item><def-item><term>ACC</term><def><p>accuracy</p></def></def-item><def-item><term>AE</term><def><p>autoencoder</p></def></def-item><def-item><term>AUC</term><def><p>areas under the receiver operating characteristic curve</p></def></def-item><def-item><term>DL</term><def><p>deep learning</p></def></def-item><def-item><term>DNN</term><def><p>deep neural network</p></def></def-item><def-item><term>FNN</term><def><p>fully connected neural network</p></def></def-item><def-item><term>MCC</term><def><p>Matthews correlation coefficient</p></def></def-item><def-item><term>SAE</term><def><p>stacked autoencoder</p></def></def-item><def-item><term>SE</term><def><p>sensitivity</p></def></def-item><def-item><term>SP</term><def><p>specificity</p></def></def-item><def-item><term>SVM</term><def><p>support vector machine.</p></def></def-item></def-list></glossary></back></article>