<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-title>BMC Bioinformatics</journal-title><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">2275242</article-id><article-id pub-id-type="publisher-id">1471-2105-9-57</article-id><article-id pub-id-type="pmid">18221567</article-id><article-id pub-id-type="doi">10.1186/1471-2105-9-57</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Gene function prediction using labeled and unlabeled data</article-title></title-group><contrib-group><contrib id="A1" contrib-type="author"><name><surname>Zhao</surname><given-names>Xing-Ming</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><xref ref-type="aff" rid="I3">3</xref><email>xmzhao@aihara.jst.go.jp</email></contrib><contrib id="A2" contrib-type="author"><name><surname>Wang</surname><given-names>Yong</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>ywang@amss.ac.cn</email></contrib><contrib id="A3" corresp="yes" contrib-type="author"><name><surname>Chen</surname><given-names>Luonan</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><xref ref-type="aff" rid="I4">4</xref><xref ref-type="aff" rid="I5">5</xref><email>chen@eic.osaka-sandai.ac.jp</email></contrib><contrib id="A4" corresp="yes" contrib-type="author"><name><surname>Aihara</surname><given-names>Kazuyuki</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><email>aihara@sat.t.u-tokyo.ac.jp</email></contrib></contrib-group><aff id="I1"><label>1</label>ERATO Aihara Complexity Modelling Project, JST, 4-6-1 Komaba, Meguro, Tokyo, Japan</aff><aff id="I2"><label>2</label>Intelligent Computing Lab, Hefei Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, Anhui, 230031, China</aff><aff id="I3"><label>3</label>Institute of Industrial Science, The University of Tokyo, Tokyo, Japan</aff><aff id="I4"><label>4</label>Department of Electrical Engineering and Electronics, Osaka Sangyo University, Osaka, Japan</aff><aff id="I5"><label>5</label>Institute of system biology, Shanghai University, Shanghai, China</aff><pub-date pub-type="collection"><year>2008</year></pub-date><pub-date pub-type="epub"><day>28</day><month>1</month><year>2008</year></pub-date><volume>9</volume><fpage>57</fpage><lpage>57</lpage><ext-link ext-link-type="uri" xlink:href="http://www.biomedcentral.com/1471-2105/9/57"/><history><date date-type="received"><day>5</day><month>6</month><year>2007</year></date><date date-type="accepted"><day>28</day><month>1</month><year>2008</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2008 Zhao et al; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2008</copyright-year><copyright-holder>Zhao et al; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0"/>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</p><!--<rdf xmlns="http://web.resource.org/cc/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:dc="http://purl.org/dc/elements/1.1" xmlns:dcterms="http://purl.org/dc/terms"><Work xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" rdf:about=""><license rdf:resource="http://creativecommons.org/licenses/by/2.0"/><dc:type rdf:resource="http://purl.org/dc/dcmitype/Text"/><dc:author>
               Zhao
               Xing-Ming
               
               
               
               xmzhao@aihara.jst.go.jp
            </dc:author><dc:title>
            Gene function prediction using labeled and unlabeled data
         </dc:title><dc:date>2008</dc:date><dcterms:bibliographicCitation>BMC Bioinformatics 9(1): 57-. (2008)</dcterms:bibliographicCitation><dc:identifier type="sici">1471-2105(2008)9:1&#x0003c;57&#x0003e;</dc:identifier><dcterms:isPartOf>urn:ISSN:1471-2105</dcterms:isPartOf><License rdf:about="http://creativecommons.org/licenses/by/2.0"><permits rdf:resource="http://web.resource.org/cc/Reproduction" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/Distribution" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Notice" xmlns=""/><requires rdf:resource="http://web.resource.org/cc/Attribution" xmlns=""/><permits rdf:resource="http://web.resource.org/cc/DerivativeWorks" xmlns=""/></License></Work></rdf>--></license></permissions><abstract><sec><title>Background</title><p>In general, gene function prediction can be formalized as a classification problem based on machine learning technique. Usually, both labeled positive and negative samples are needed to train the classifier. For the problem of gene function prediction, however, the available information is only about positive samples. In other words, we know which genes have the function of interested, while it is generally unclear which genes do not have the function, i.e. the negative samples. If all the genes outside of the target functional family are seen as negative samples, the imbalanced problem will arise because there are only a relatively small number of genes annotated in each family. Furthermore, the classifier may be degraded by the false negatives in the heuristically generated negative samples.</p></sec><sec><title>Results</title><p>In this paper, we present a new technique, namely Annotating Genes with Positive Samples (AGPS), for defining negative samples in gene function prediction. With the defined negative samples, it is straightforward to predict the functions of unknown genes. In addition, the AGPS algorithm is able to integrate various kinds of data sources to predict gene functions in a reliable and accurate manner. With the one-class and two-class Support Vector Machines as the core learning algorithm, the AGPS algorithm shows good performances for function prediction on yeast genes.</p></sec><sec><title>Conclusion</title><p>We proposed a new method for defining negative samples in gene function prediction. Experimental results on yeast genes show that AGPS yields good performances on both training and test sets. In addition, the overlapping between prediction results and GO annotations on unknown genes also demonstrates the effectiveness of the proposed method.</p></sec></abstract></article-meta></front><body><sec><title>Background</title><p>One of the main goals in post-genomic era is to predict the biological functions of genes. Recently, with the rapid advance in high-throughput biotechnologies, such as yeast two-hybrid systems [<xref ref-type="bibr" rid="B1">1</xref>], protein complex [<xref ref-type="bibr" rid="B2">2</xref>,<xref ref-type="bibr" rid="B3">3</xref>] and microarray expression profiles [<xref ref-type="bibr" rid="B4">4</xref>], a large amount of biological data have been generated. These data are rich sources for deducing and understanding gene functions. For example, protein-protein interaction data are widely exploited for inferring functions of genes with the assumption that interacting proteins have the same or similar functions, i.e. "guilty by association" rule [<xref ref-type="bibr" rid="B5">5</xref>-<xref ref-type="bibr" rid="B10">10</xref>]. In addition, gene expression data have been widely used for gene function prediction, where genes with similar expression patterns are assumed to have similar functions [<xref ref-type="bibr" rid="B11">11</xref>]. In the literature, it has been shown that integration of different kinds of data sources can considerably improve prediction results [<xref ref-type="bibr" rid="B12">12</xref>-<xref ref-type="bibr" rid="B15">15</xref>]. With various kinds of high-throughput data, the machine learning techniques, especially Support Vector Machines (SVMs), have been used for predicting gene functions and shown promising results [<xref ref-type="bibr" rid="B16">16</xref>,<xref ref-type="bibr" rid="B17">17</xref>].</p><p>Despite the good performance of the machine learning techniques, there are some limitations with existing methods, where gene function prediction is formalized as a classification problem. Generally, to construct a classifier for gene function prediction, one needs a number of labeled training samples. In this case, it is relatively easy to find positive samples (i.e. genes annotated with the function of interested) that have been annotated by human experts. However, it is hard to find the representative negative samples because the available information in the annotation databases, such as Gene Ontology (GO) [<xref ref-type="bibr" rid="B18">18</xref>] and the Munich Information Center for Protein Sequences (MIPS) [<xref ref-type="bibr" rid="B19">19</xref>], is only about positive samples, i.e. we know which gene belongs to which functional class but we are not sure which gene does not belong to the class. Hence, the available information to us is a set of genes that have the target function, whereas it is unclear whether the other genes have the function or not. Furthermore, since one gene may be annotated by more than one function, it is inappropriate to use all the other genes outside of the target functional class as negative samples (e.g. some of them may actually have such a function). In addition, the imbalanced problem will arise if all the genes outside of the target functional family are seen as negative samples because usually there are only a relatively small number of genes annotated with the function, while the number of negative samples may be hundreds even thousands times the one of positive samples. Therefore, the classifier may be degraded by the false negative samples or imbalanced data [<xref ref-type="bibr" rid="B20">20</xref>].</p><p>In this paper, a new technique, namely Annotating Genes with Positive Samples (AGPS), is presented for defining negative samples in gene function prediction. In particular, a functional linkage graph is constructed to integrate heterogeneous information sources and the singular value decomposition (SVD) technique is employed to reduce dimensionality and remove noise from the data. Then, the AGPS algorithm is presented to define negative samples and predict functions of unknown genes. In this work, genes annotated with the target function are denoted as labeled data, while those annotated with other functions instead of the target annotation are denoted as unlabeled data. The unlabeled data are defined here because we assume that the genes without target annotation may have the function even though they are currently not annotated with the function. The goal of the AGPS algorithm is to automatically generate negative samples from unlabeled data in the learning procedure. Unlike the conventional single-class learning algorithm that is trained only on positive samples, e.g. one-class SVMs [<xref ref-type="bibr" rid="B21">21</xref>], the AGPS algorithm defines negative samples from unlabeled data automatically in the learning procedure, and therefore is expected to have a superior performance. With SVMs as the core learning algorithm, the AGPS algorithm can predict the functions of unknown genes effectively. Recently, the basic idea has also been employed by other researchers [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B26">26</xref>], where promising performances have been demonstrated. The major difference between AGPS and the existing algorithms [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B26">26</xref>] is that instead of recognizing the positive samples from the unlabeled data directly, the AGPS algorithm aims to define the representative negative samples from unlabeled data. With the negative samples available, it is straightforward to predict the functions of unknown genes by utilizing both the positive samples and the defined negative samples. In addition, to exploit all available information, the AGPS algorithm can also integrate various kinds of data sources from high-throughput technologies so as to predict gene functions in a reliable manner.</p><p>To demonstrate the effectiveness, the proposed method is applied to predict functions of <italic>S. cerevisiae </italic>genes in the following procedures. Firstly, the data from protein interaction network, gene expression profiles and protein complex data are integrated into a functional linkage graph. Secondly, SVD is used to reduce the dimensionality and remove noise by extracting the dominant structure of the functional linkage graph. Finally, the AGPS algorithm is employed to predict the gene functions based on the refined data.</p></sec><sec><title>Results and Discussions</title><sec><title>Data sources and processing</title><p>In this study, three kinds of data were integrated into a functional linkage graph for function prediction of <italic>S. cerevisiae </italic>genes. The three data sources include protein-protein interaction, gene expression profiles and protein complex data.</p><sec><title>Functional annotation</title><p>The functional annotation data of <italic>S. cerevisiae </italic>genes used here were obtained from the FunCat 2.0 [<xref ref-type="bibr" rid="B27">27</xref>] functional classification scheme in 2006, which can be downloaded from the Comprehensive Yeast Genome Database (CYGD) of MIPS [<xref ref-type="bibr" rid="B19">19</xref>]. The annotation data in FunCat are organized as a hierarchical and tree like structure with up to six levels of increasing specificity. In total, the FunCat includes 1307 functional categories. A protein annotated by one function in the functional tree is also annotated by all the parents of the functional node. In this work, 13 general functional classes were selected, and consequently 4049 genes have been annotated in total. Table <xref ref-type="table" rid="T2">2</xref> shows the selected functional classes and the corresponding number of genes.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>The functional categories and genes used in this paper</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Functional categories</td><td align="center">Number of genes</td></tr></thead><tbody><tr><td align="left">01 metabolism</td><td align="center">967</td></tr><tr><td align="left">02 energy</td><td align="center">241</td></tr><tr><td align="left">10 cell cycle and DNA processing</td><td align="center">727</td></tr><tr><td align="left">11 transcription</td><td align="center">829</td></tr><tr><td align="left">12 protein synthesis</td><td align="center">364</td></tr><tr><td align="left">14 protein fate</td><td align="center">680</td></tr><tr><td align="left">20 cellular transport</td><td align="center">726</td></tr><tr><td align="left">30 cellular communication</td><td align="center">86</td></tr><tr><td align="left">32 cell rescue, defense and virulence</td><td align="center">307</td></tr><tr><td align="left">34 interaction with the environment</td><td align="center">332</td></tr><tr><td align="left">40 cell fate</td><td align="center">201</td></tr><tr><td align="left">42 biogenesis of cellular components</td><td align="center">471</td></tr><tr><td align="left">43 cell type differentiation</td><td align="center">354</td></tr></tbody></table></table-wrap></sec><sec><title>Protein interaction data</title><p>The protein interaction data used here were obtained from the BioGRID database [<xref ref-type="bibr" rid="B28">28</xref>]. The 2.0.20 version of BioGRID for yeast was used in this work. The dataset contains 82,633 pairs of interactions among 5,299 yeast genes, of which 4049 genes are annotated by the 13 functional classes. The protein-protein interaction can be represented as a network, where the vertices are genes and the edges are interactions among genes.</p></sec><sec><title>Gene expression profiles</title><p>The gene expression dataset used in this work was downloaded from the Stanford Gene expression Database (SMD), which contains the results from [<xref ref-type="bibr" rid="B29">29</xref>-<xref ref-type="bibr" rid="B33">33</xref>]. The missing values in the gene expression profiles were estimated by the <italic>KNNimpute </italic>algorithm [<xref ref-type="bibr" rid="B34">34</xref>], where <italic>k </italic>was set at 15. The dataset contains 6012 common genes, where 5,132 genes are among the 5,299 genes in the protein interaction dataset. Consequently, the dataset used in this work contains 5,132 genes with 278 real value features for gene expression data.</p></sec><sec><title>Protein complexes</title><p>The protein complex data were obtained from the MIPS database in 2006, including the data from [<xref ref-type="bibr" rid="B2">2</xref>] and [<xref ref-type="bibr" rid="B3">3</xref>]. The protein complex data were used here because genes occurring in the same complex are assumed to have the same or similar functions. Although we cannot infer direct interaction relationship among genes from protein complex data, the genes occurring in the same complex are generally considered to have functional correlations. Hence, we assigned functional relationships to genes occurring in the same complex, where an edge was constructed for a pair of genes occurring in the same complex. Finally, 62,042 functional edges were assigned to our dataset.</p></sec><sec><title>Data preprocessing</title><p>The relationship between any pair of genes in the protein interaction and complex datasets generated above was denoted as binary relationship here, because the relationship is only expressed by "yes" or "no", i.e. "yes" if two genes interact or occur in the same complex (where an edge was constructed), otherwise "no". The network with the binary relationship was denoted as a binary network. Unlike existing methods that utilize the binary network for gene function prediction, we used the binary network to estimate the functional similarities among genes. Specifically, to evaluate the functional similarity between a pair of genes, the Czekanowski-Dice distance (CD-distance) [<xref ref-type="bibr" rid="B35">35</xref>] was employed in this work. The CD-distance between genes <italic>g</italic>1 and <italic>g</italic>2 is defined as:</p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M1" name="1471-2105-9-57-i1" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>&#x00394;</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x0222a;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <italic>N</italic><sub><italic>g </italic></sub>means the set containing gene <italic>g </italic>and its interacting partners, <italic>N</italic><sub><italic>g</italic>1 </sub>&#x0222a; <italic>N</italic><sub><italic>g</italic>2 </sub>means the union of <italic>N</italic><sub><italic>g</italic>1 </sub>and <italic>N</italic><sub><italic>g</italic>2</sub>, <italic>N</italic><sub><italic>g</italic>1 </sub>&#x02229; <italic>N</italic><sub><italic>g</italic>2 </sub>means the intersection of <italic>N</italic><sub><italic>g</italic>1 </sub>and <italic>N</italic><sub><italic>g</italic>2</sub>, and <italic>N</italic><sub><italic>g</italic>1</sub>&#x00394;<italic>N</italic><sub><italic>g</italic>2 </sub>means the symmetric difference between two datasets <italic>N</italic><sub><italic>g</italic>1 </sub>and <italic>N</italic><sub><italic>g</italic>2</sub>. The network with edges accompanied by functional similarity was denoted as the functional network in this paper.</p><p>For the protein interaction network, it is straightforward to apply the CD-distance. After that, the functional similarity between any pair of genes was represented as a real value between 0 and 1. The smaller the value, the more likely the pair of genes have the similar function. The functional network obtained from protein interaction data was denoted as <italic>G</italic>1.</p><p>For the gene expression profiles, the Pearson correlation coefficients were first calculated, and a binary network was then constructed for the dataset, where an edge was added if the absolute value of the correlation coefficient between corresponding pair of genes is larger than 0.7. In such a way, one binary network can be generated. Subsequently, the CD-distance was applied to the binary network just as described above. The obtained functional network from gene expression data was denoted as <italic>G</italic>2.</p><p>For the protein complex data, the CD-distance was applied directly to the binary network, and the resulted functional network was denoted as <italic>G</italic>3. Finally, we got three functional networks for yeast, and the functional similarity between any pair of genes in all of the three networks was measured by CD-distance. Furthermore, the three functional networks obtained in this way were merged into one integrated functional network <italic>G </italic>= <italic>&#x003b1;G</italic>1 + <italic>&#x003b2;G</italic>2 + <italic>&#x003b3;G</italic>3, where <italic>G </italic>is an <italic>M </italic>&#x000d7; <italic>M </italic>matrix and <italic>M </italic>is the number of genes. A simple rule <italic>&#x003b1; </italic>: <italic>&#x003b2; </italic>: <italic>&#x003b3; </italic>= 1 : 1 : 1 was employed in this work. The idea behind the simple rule is that all the three data sources contain functional relationships among genes, and integrating them into one functional network can complement each other. The resulted functional network was denoted as the functional linkage graph in this paper.</p></sec></sec><sec><title>Results of 10-fold cross-validation on training data</title><p>With the functional linkage graph generated above, the AGPS algorithm was applied to predict functions of <italic>S. cerevisiae </italic>genes. Here, genes annotated with the target function <bold>F </bold>were regarded as positive samples or labeled samples, while those annotated with other functions instead of <bold>F </bold>were seen as unlabeled samples. Our aim is to define the negative samples and then predict the potential genes that may be annotated with <bold>F </bold>from the unknown genes. In the following experiments, gene function prediction was formalized as a multi-class classification problem, which was then reduced to a set of binary classification problems ("one vs the other" here). All the experiments were conducted by utilizing the LIBSVM [<xref ref-type="bibr" rid="B36">36</xref>].</p><p>First, we evaluated the proposed method on the training set [see Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>]. In this paper, the training set consists of genes annotated in the MIPS annotation of March 2004, where 3663 genes have been annotated by the selected 13 functional classes (i.e. Table <xref ref-type="table" rid="T2">2</xref>). To see the effectiveness of the defined negative samples on function prediction, AGPS was compared against four other methods including conventional two-class SVMs, one-class SVMs, PSoL [<xref ref-type="bibr" rid="B23">23</xref>], and kernel integration that is a simplified version of the one described in [<xref ref-type="bibr" rid="B16">16</xref>]. Furthermore, dimensionality reduction was performed to reduce computation cost and complexity. The SVD technique was employed to reduce the dimensionality and remove noise in this work, where 10-fold cross-validation was adopted to determine the number of components that should be kept in dimensionality reduction. For fair comparison, dimensionality reduction was performed for all the methods but kernel integration to find the informative components. No dimensionality reduction was performed for kernel integration method due to its specific data structure. The Radial Basis Function (RBF) kernel was employed for all the methods used in this work. The number of features selected for different methods can be found in Table <xref ref-type="table" rid="T3">3</xref>.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>The number of features used for each class in the paper</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Functional categories</td><td align="center" colspan="5">Number of features</td></tr><tr><td></td><td colspan="5"><hr></hr></td></tr><tr><td></td><td align="center">AGPS</td><td align="center">PSoL</td><td align="center">one-class SVMs</td><td align="center">two-class SVMs</td><td align="center">two-class SVMs_balanced</td></tr></thead><tbody><tr><td align="left">01 metabolism</td><td align="center">295</td><td align="center">110</td><td align="center">10</td><td align="center">10</td><td align="center">10</td></tr><tr><td align="left">02 energy</td><td align="center">115</td><td align="center">210</td><td align="center">10</td><td align="center">60</td><td align="center">145</td></tr><tr><td align="left">10 cell cycle and DNA processing</td><td align="center">175</td><td align="center">160</td><td align="center">10</td><td align="center">10</td><td align="center">10</td></tr><tr><td align="left">11 transcription</td><td align="center">280</td><td align="center">210</td><td align="center">10</td><td align="center">260</td><td align="center">10</td></tr><tr><td align="left">12 protein synthesis</td><td align="center">25</td><td align="center">160</td><td align="center">260</td><td align="center">260</td><td align="center">10</td></tr><tr><td align="left">14 protein fate</td><td align="center">160</td><td align="center">160</td><td align="center">10</td><td align="center">10</td><td align="center">295</td></tr><tr><td align="left">20 cellular transport</td><td align="center">190</td><td align="center">260</td><td align="center">10</td><td align="center">60</td><td align="center">10</td></tr><tr><td align="left">30 cellular communication</td><td align="center">70</td><td align="center">160</td><td align="center">10</td><td align="center">110</td><td align="center">70</td></tr><tr><td align="left">32 cell rescue, defense and virulence</td><td align="center">295</td><td align="center">160</td><td align="center">10</td><td align="center">110</td><td align="center">250</td></tr><tr><td align="left">34 interaction with the environment</td><td align="center">85</td><td align="center">210</td><td align="center">10</td><td align="center">10</td><td align="center">295</td></tr><tr><td align="left">40 cell fate</td><td align="center">55</td><td align="center">260</td><td align="center">10</td><td align="center">10</td><td align="center">100</td></tr><tr><td align="left">42 biogenesis of cellular components</td><td align="center">25</td><td align="center">260</td><td align="center">10</td><td align="center">10</td><td align="center">190</td></tr><tr><td align="left">43 cell type differentiation</td><td align="center">40</td><td align="center">210</td><td align="center">10</td><td align="center">10</td><td align="center">40</td></tr></tbody></table></table-wrap><p>For AGPS, the 10-fold cross-validation was employed to find the optimal parameters for kernel function, i.e. the positive set was randomly divided into 10 groups, where 9 of 10 subsets from the positive set were used as the positive training set, while the rest one was seen as a validation set. Both the validation genes and those outside of the target functional family were seen as unlabeled data. The learning procedure of AGPS in each trial of 10-fold cross-validation can be found in Table <xref ref-type="table" rid="T1">1</xref>. In each trial of 10-fold cross-validation, the best classifier and the corresponding negative set were returned. Consequently, the negative samples occurring most often in the returned negative sets were taken as the representative negative samples, and the size of the final negative samples was controlled to nearly equal to that of the positive set. For example, after the 10-fold cross-validation, 10 negative sets were returned and the samples were ranked according to the times that they occurred in the 10 negative sets. Finally, the samples with the highest frequency were selected as the representative negative set. By selecting the negative samples in this way, the false negatives can be reduced. The final 10-fold cross-validation results were obtained based on the positive set and the defined negative set with the parameters determined above, which works in the same way as the conventional two-class SVMs does.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Annotating Genes with Positive Samples (AGPS)</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left"><bold>Input:</bold></td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;- positive training data <bold>P1</bold></td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;- validation set <bold>P2</bold></td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;- unlabeled data <bold>Ku</bold></td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;- unknown gene <bold>Ug</bold></td></tr><tr><td align="left"><bold>Output:</bold></td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;- Prediction results</td></tr><tr><td align="left"><bold>Stage 1: </bold>Learning</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;<bold>U </bold>= <bold>Ku </bold>+ <bold>P2</bold>;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Stage 1.1: Initial negative set generation</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Construct classifier <italic>f</italic><sub>1 </sub>based on <bold>P1 </bold>and <bold>U </bold>with one-class SVMs;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Classify <bold>U </bold>using <italic>f</italic><sub>1</sub>. The predicted negative set <bold>N</bold><sub><bold>1 </bold></sub>is used as the initial negative training set in Stage 1.2;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- <bold>U </bold>= <bold>U </bold>- <bold>N</bold><sub><bold>1</bold></sub>.</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Stage 1.2: Negative set expansion</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Classifier set <italic>FC </italic>= [ ], negative set <italic>NS </italic>= [ ], <italic>i </italic>= 1.</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- repeat</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- <italic>i </italic>= <italic>i </italic>+ 1;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Construct classifier <italic>f</italic><sub><italic>i </italic></sub>based on <bold>P1 </bold>and <bold>N</bold><sub><bold>1 </bold></sub>with two-class SVMs;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- <italic>FC</italic>(<italic>i </italic>- 1) = <italic>f</italic><sub><italic>i</italic></sub>, <italic>NS</italic>(<italic>i </italic>- 1) = <bold>N1</bold>;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Classify <bold>U </bold>by <italic>f</italic><sub><italic>i</italic></sub>, <bold>N</bold><sub><bold>2 </bold></sub>is the predicted negative set, where |<bold>N</bold><sub><bold>2</bold></sub>| &#x02264; <italic>k</italic>|<bold>P1</bold>|;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- <bold>N</bold><sub><bold>1 </bold></sub>= [<bold>N</bold><sub><bold>2</bold></sub>; <bold>N</bold><sub><bold>SV</bold></sub>], where <bold>N</bold><sub><bold>SV </bold></sub>is the negative SVs of <italic>f</italic><sub><italic>i </italic></sub>in the previous step;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- <bold>U </bold>= <bold>U </bold>- <bold>N2</bold>.</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- until |<bold>U</bold>| &#x0003c;<italic>k</italic>|<bold>P1</bold>|</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Stage 1.3: Classifier and negative set selection</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Classify <bold>U </bold>with classifiers from <italic>FC</italic>, and select the classifier <italic>FC</italic>(<italic>i</italic>) with the best prediction accuracy;</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;- Return negative set <bold>TN </bold>&#x02190; <italic>NS</italic>(<italic>i</italic>).</td></tr><tr><td align="left"><bold>Stage 2: </bold>classification</td></tr><tr><td align="left">&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;&#x000a0;Classify <bold>Ug </bold>with <bold>P </bold>and <bold>TN</bold>, where <bold>P </bold>= <bold>P1 </bold>+ <bold>P2</bold>.</td></tr></tbody></table></table-wrap><p>For PSoL, the 10-fold cross-validation was employed to find the optimal parameters for kernel function. In PSoL, the unlabeled data were defined to include those genes outside of the target functional class, unknown genes and validation genes. The learning of PSoL was implemented in the similar way as AGPS except that PSoL has only the learning stage in Table <xref ref-type="table" rid="T1">1</xref> and returns possible positive samples at the end of learning and does not select classifier. After the cross-validation, the best parameters were determined for PSoL and the best results were recorded. The details of PSoL can be found in [<xref ref-type="bibr" rid="B23">23</xref>].</p><p>For one-class SVMs, the genes outside of the target functional class were seen as negative samples and the classifier was trained only on the positive training set. The 10-fold cross-validation was utilized to find the optimal parameters for kernel functions, where 9/10 of the positive set was used as the positive training set and the rest was seen as positive validation set. Furthermore, one randomly-selected negative subset was used as the negative validation set, where the randomly-selected negative subset has nearly the same size as the positive validation set (i.e. 1/10 of the positive set). With the cross-validation, the best parameters for one-class SVMs were determined and the best results were recorded.</p><p>For two-class SVMs, the negative samples consist of genes outside of the target functional class. The 10-fold cross-validation was utilized to find the parameters with which they can best separate the positive samples from the negative samples. Furthermore, a balanced training set was generated for the two-class SVMs, where the negative samples with the same size as the positive samples were randomly selected from the genes outside of the functional class, and this technique has been used to define negative samples in the literature [<xref ref-type="bibr" rid="B37">37</xref>].</p><p>For the kernel integration method, the diffusion kernel was applied to binary networks generated by protein-protein interaction and complexes while the RBF kernel was applied to gene expression profiles. The parameters of the kernels were determined by 10-fold cross-validation. Instead of utilizing the semi-definite programming to find the optimal wight coefficients as descried in [<xref ref-type="bibr" rid="B16">16</xref>], the kernel matrices obtained were normalized and simply added together to form a new kernel. Furthermore, a balanced training set was generated, where the negative samples with the same size as the positive samples were randomly selected from the genes outside of the functional class.</p><p>Note that we compared different methods here to investigate the influence of different negative samples on gene function prediction, in particular to see the effectiveness of the defined negative samples by the proposed method on gene function prediction. In other words, the purpose of the comparison is not to demonstrate the superiority of the proposed method over existing methods but to show the effectiveness of the AGPS on defining negative samples for gene function prediction.</p><p>The results of 10-fold cross-validation by the five methods were shown in Table <xref ref-type="table" rid="T4">4</xref>, which includes the results by two-class SVMs and kernel integration on balanced data. It can be seen from Table <xref ref-type="table" rid="T4">4</xref> that the AGPS algorithm performs comparably well with other methods due to the defined negative samples. Furthermore, all the methods utilizing the negative samples outperform the one-class SVMs that was trained only on positive samples. The poor performance of the one-class SVMs is due to the relatively fewer positive training samples. For the two-class SVMs and kernel integration, it can be clearly seen that with balanced data, the performance of both classifiers can be considerably improved. In other words, the imbalanced data can indeed degrade the performance of the classifiers. Furthermore, the results on imbalanced and balanced data demonstrate the importance of selecting negative samples in gene function prediction. Compared with PSoL, the AGPS algorithm can get a higher recall, i.e. it can recognize more positive samples hidden in the unknown data, because it defines better representative negative samples in the learning procedure.</p><table-wrap position="float" id="T4"><label>Table 4</label><caption><p>The results of 10-fold cross-validation by the five methods averaged over 13 classes</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">Methods</td><td align="center"><italic>precision</italic>(%)</td><td align="center"><italic>recall</italic>(%)</td><td align="center"><italic>F</italic>1(%)</td></tr></thead><tbody><tr><td align="left">AGPS</td><td align="center">68</td><td align="center">61</td><td align="center">61</td></tr><tr><td align="left">PSoL</td><td align="center">68</td><td align="center">37</td><td align="center">47</td></tr><tr><td align="left">two-class SVMs</td><td align="center">45</td><td align="center">24</td><td align="center">33</td></tr><tr><td align="left">two-class SVMs_balanced</td><td align="center">61</td><td align="center">70</td><td align="center">69</td></tr><tr><td align="left">one-class SVMs</td><td align="center">50</td><td align="center">21</td><td align="center">31</td></tr><tr><td align="left">kernel integration</td><td align="center">58</td><td align="center">28</td><td align="center">37</td></tr><tr><td align="left">kernel integration_balanced</td><td align="center">64</td><td align="center">47</td><td align="center">52</td></tr></tbody></table></table-wrap></sec><sec><title>Results on old data</title><p>Since March 2004, 386 previous unknown yeast genes have been annotated by the selected 13 functional classes (in 2006). Hence, the 386 genes were not involved in the training procedure in the previous section [see Additional file <xref ref-type="supplementary-material" rid="S2">2</xref>]. To validate AGPS and other methods, these 386 genes were regarded as test set and used to validate the models trained in the previous section. For the test data, the AGPS algorithm works as a conventional two-class SVMs here with parameters and negative set defined above. For PSoL, the unlabeled data were defined to include unknown genes and those genes outside of the target functional class. Note that the test data were included in unknown genes in PSoL. With the best parameters determined in the training procedure and all positive samples, PSoL was applied to find out putative positive samples from unknown genes. For the other three methods, the classifiers trained above were just utilized to predict the functions of unknown genes. Specifically, the two-class SVMs and kernel integration trained on imbalanced and balanced data were separately applied to predict gene functions. Furthermore, the <italic>ROC </italic>score, i.e. the area under the ROC curve, was also utilized to evaluate the overall performance of the classifiers. The <italic>ROC </italic>score was not used in the previous section because there are not negative samples available for single class methods. The results are shown in Table <xref ref-type="table" rid="T5">5</xref>.</p><table-wrap position="float" id="T5"><label>Table 5</label><caption><p>The prediction results by the five methods averaged over 13 classes</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="center">Methods</td><td align="center"><italic>precision </italic>(%)</td><td align="center"><italic>recall </italic>(%)</td><td align="center"><italic>F</italic>1 (%)</td><td align="center"><italic>ROC </italic>score</td><td align="center">coverage <sup><italic>a</italic></sup></td></tr></thead><tbody><tr><td align="center">AGPS</td><td align="center">15</td><td align="center">66</td><td align="center">22</td><td align="center">0.61</td><td align="center">13 (13)</td></tr><tr><td align="center">PSoL</td><td align="center">20</td><td align="center">18</td><td align="center">19</td><td align="center">0.55</td><td align="center">12 (13)</td></tr><tr><td align="center">two-class SVMs</td><td align="center">28</td><td align="center">10</td><td align="center">16</td><td align="center">0.53</td><td align="center">11 (13)</td></tr><tr><td align="center">two-class SVMs_balanced</td><td align="center">18</td><td align="center">36</td><td align="center">29</td><td align="center">0.57</td><td align="center">10(13)</td></tr><tr><td align="center">one-class SVMs</td><td align="center">10</td><td align="center">42</td><td align="center">15</td><td align="center">0.53</td><td align="center">13 (13)</td></tr><tr><td align="center">kernel integration</td><td align="center">39</td><td align="center">16</td><td align="center">23</td><td align="center">0.56</td><td align="center">11(13)</td></tr><tr><td align="center">kernel integration_balanced</td><td align="center">11</td><td align="center">32</td><td align="center">24</td><td align="center">0.59</td><td align="center">6(13)</td></tr></tbody></table><table-wrap-foot><p><sup><italic>a </italic></sup>The digit in the parenthesis is the true number of functional classes whereas the number outside is the number of classes that can be predicted by the corresponding method.</p></table-wrap-foot></table-wrap><p>It can be seen from Table <xref ref-type="table" rid="T5">5</xref> that the AGPS algorithm outperforms all the other methods with respect to the overall performance, i.e. <italic>ROC </italic>scores. The poor performance of one-class SVMs is caused by the relatively small number of positive training samples, which result in underfitting. The PSoL algorithm performs well because it defines the negative samples like the AGPS algorithm. However, the selected negative samples and predicted positive samples by PSoL may not be true. On the other hand, the AGPS algorithm defines the representative negative samples that can best recognize the positive samples from the unlabeled data instead of predicting positive samples from the unlabeled data directly, which is also the reason why the AGPS algorithm can achieve a much higher recall rate. Working in this way, the AGPS algorithm is able to reduce false negatives. The results on balanced and imbalanced data demonstrate again that the two-class SVMs and kernel integration can be degraded by the imbalance problem. On the other hand, the difference between results on balanced and imbalanced data also shows the importance of selecting negative samples in gene function prediction. Although the two-class SVMs and kernel integration have higher <italic>F</italic>1 scores, they have lower recall rates compared against AGPS. However, a higher recall is much important because the biologists are mainly interested in which genes have the target function instead of which genes not. Although the imbalanced problem is avoided, both two-class SVMs and kernel integration trained on balanced data do not perform as well as the AGPS algorithm because the randomly selected negative training samples cannot capture the true distribution of negative samples very well.</p><p>To see the ability of the five methods to recover positive genes from unknown data, we compared the number of genes that the five methods can predict correctly from unknown genes on each functional class, where the results by both two-class SVMs and kernel integration trained on balanced data were also included. Figure <xref ref-type="fig" rid="F1">1</xref> shows the results obtained by the five methods, where two-class SVMs_balanced means the results by two-class SVMs trained on balanced data and the same for kernel integration. It can be easily observed from Figure <xref ref-type="fig" rid="F1">1</xref> that the AGPS algorithm can recover most of the unknown genes for nearly each functional class.</p><fig position="float" id="F1"><label>Figure 1</label><caption><p><bold>The number of genes predicted correctly for the 13 functional classes</bold>. The prediction results obtained by the five methods: AGPS, PSoL, two-class SVMs, one-class SVMs and kernel integration methods, where two-class SVMs_balanced means the results by two-class SVMs trained on balanced data and the same for kernel integration. The height of the bar in the figure means the number of genes that the five methods can recover correctly from unlabeled genes for each functional class, respectively.</p></caption><graphic xlink:href="1471-2105-9-57-1"/></fig><p>Furthermore, we compared our method against other methods class by class. The number of classes versus one <italic>ROC </italic>score threshold was countered, and a higher curve means a better result. Figure <xref ref-type="fig" rid="F2">2</xref> shows the results by the five methods, where two-class SVMs_balanced means the results by two-class SVMs trained on balanced data and the same for kernel integration. It can be seen from the results that the proposed method outperforms all the other methods in this case, which confirms the effectiveness of the proposed method.</p><fig position="float" id="F2"><label>Figure 2</label><caption><p><bold>Comparison of the five methods class by class</bold>. Comparison of the performance among the five methods, where two-class SVMs_balanced means the results by two-class SVMs trained on balanced data and the same for kernel integration. The number of classes versus one <italic>ROC </italic>score threshold is countered, and a higher curve means a better result.</p></caption><graphic xlink:href="1471-2105-9-57-2"/></fig><p>In addition, we compared the performance of the best two single-class methods, i.e. AGPS and PSoL, class by class. Figure <xref ref-type="fig" rid="F3">3</xref> shows the comparison of the <italic>ROC </italic>scores by the two methods on each functional class. It can be seen from Figure <xref ref-type="fig" rid="F3">3</xref> that the AGPS algorithm outperforms PSoL on nearly each class, which also verifies the effectiveness of the proposed AGPS algorithm.</p><fig position="float" id="F3"><label>Figure 3</label><caption><p><bold>Comparison of AGPS and PSoL class by class</bold>. Comparison of the performance between the two single-class methods, i.e. AGPS and PSoL, class by class. The <italic>ROC </italic>scores obtained by the two methods for each functional class are compared.</p></caption><graphic xlink:href="1471-2105-9-57-3"/></fig></sec><sec><title>Predicting functions of unknown genes</title><p>For the MIPS annotation data in 2006, there are 802 genes marked as not annotated in our dataset. With all the annotated genes in each functional class as the positive training set and the defined negative genes, the AGPS algorithm was applied to predict the functions of the 802 unknown genes. Although these genes are not annotated in the MIPS database, some of them may be annotated in the GO database with different functional terms. The GO annotations in 2006 were downloaded from GO database, and the GO terms assigned to the unknown genes were mapped to the MIPS annotation terms using the "go2mips" mapping table from GO. Finally, 457 genes annotated by GO can be mapped to MIPS, among which 21 genes have been annotated by 8 of the selected 13 functional classes [see Additional file <xref ref-type="supplementary-material" rid="S3">3</xref>].</p><p>To validate the predictions, the predicted results of AGPS were compared against the GO annotations for the unknown genes. Table <xref ref-type="table" rid="T6">6</xref> shows the results of the predicted terms versus GO terms, where only the predicted annotations that match GO terms with the same MIPS annotation are shown. It can be seen from Table <xref ref-type="table" rid="T6">6</xref> that the functions of nearly 71% (15/21) of the annotated genes from GO can be successfully recovered by AGPS, which confirms again the efficiency and effectiveness of the proposed algorithm.</p><table-wrap position="float" id="T6"><label>Table 6</label><caption><p>Predicted annotations by AGPS algorithm versus annotations from GO</p></caption><table frame="hsides" rules="groups"><thead><tr><td align="left">MIPS functional categories</td><td align="left">Gene ontology</td><td align="left">genes annotated by GO</td><td align="left">genes predicted by AGPS that match GO annotation</td></tr></thead><tbody><tr><td align="left">01 metabolism</td><td align="left">GO:0008152</td><td align="left">YEL044W YHL029C YGL185C YMR010W</td><td align="left">YHL029C YGL185C YMR010W</td></tr><tr><td align="left">10 cell cycle and dna processing</td><td align="left">GO:0007067 GO:0006260 GO:0006281</td><td align="left">YDR106W YGL168W YER038C</td><td align="left">YDR168W YER038C</td></tr><tr><td align="left">11 transcription</td><td align="left">GO:0006364 GO:0006396</td><td align="left">YLR196W YLR204W</td><td align="left">YLR196W</td></tr><tr><td align="left">12 protein synthesis</td><td align="left">GO:0043037</td><td align="left">YFR032C YLR287C</td><td align="left">YFR032C YLR287C</td></tr><tr><td align="left">14 protein fate</td><td align="left">GO:0006457</td><td align="left">YNL310C</td><td align="left">YNL310C</td></tr><tr><td align="left">20 cellular transport</td><td align="left">GO:0006888</td><td align="left">YDL099W</td><td align="left">YDL099W</td></tr><tr><td align="left">32 cell rescue, defense and virulence</td><td align="left">GO:0006974 GO:0006950 GO:0006979</td><td align="left">YOL063C YMR251W YDR346C</td><td align="left">YOL063C YMR251W YDR346C</td></tr><tr><td align="left">42 biogenesis of cellular components</td><td align="left">GO:0019898 GO:0007005 GO:0007047</td><td align="left">YPL005W YDR339C YNL149C YKR100C YNL310C YOR060C</td><td align="left">YKR100C YOR060C</td></tr></tbody></table><table-wrap-foot><p>The predicted terms versus GO terms, where only the predicted annotations that match GO terms with the corresponding MIPS annotations are shown.</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Conclusion</title><p>Annotating genes with biological functions is one of the main goals in post-genomic era. An alternative way to this problem is to formalize gene function prediction as a classification problem. In this paper, we proposed a new algorithm, namely AGPS, to define negative samples in gene function prediction. The AGPS algorithm is different from existing methods, which have inappropriate assumptions about those genes that have no target annotation. Specifically, we do not simply regard those genes without target annotation as negative samples because one gene generally has multiple functions and it may indeed have the function even though it is not annotated with the target function currently. Unlike conventional binary classifier which needs both definite positive and negative samples, the new technique presented in this paper annotates genes by requiring only positive samples which are available in the public database. With explicit positive samples, the AGPS algorithm can define representative negative samples automatically in the learning procedure. Utilizing the defined negative samples, the AGPS algorithm performs comparably well with the existing methods in terms of prediction accuracy. In addition, the proposed method is able to integrate various kinds of data sources to infer gene functions in a reliable and accurate manner. In particular, by integrating the heterogeneous data from protein interaction, microarray profiles and protein complexes into a functional linkage graph, the AGPS algorithm was applied to predict functions of yeast genes in this paper. Furthermore, the SVD technique was utilized to reduce the dimensionality and remove noise, thereby significantly improving computational efficiency. Experiment results on yeast genes show that the prediction results on novel genes considerably overlap with GO, which confirms the effectiveness and the efficiency of proposed method.</p><p>What worth mentioning is that other kinds of data, e.g. protein subcellular localization and protein domains, can also be easily integrated for gene function prediction in our method. Furthermore, the AGPS algorithm can be expected to be applied to other fields in bioinformatics except gene function prediction. In this paper, we only applied the AGPS algorithm to the general functional classes. In the future, we will apply AGPS to more specific functional classes. In addition, the functional classes are considered independently here. However, generally there are correlations among these biological functions. If one gene has function <italic>f</italic>1, it is possible that the gene also has function <italic>f</italic>2 because function <italic>f</italic>1 correlates with function <italic>f</italic>2. Therefore, the performance will be improved if the correlation among genes is taken into account. The correlation among functions will be considered in the future work.</p></sec><sec sec-type="methods"><title>Methods</title><p>In this section, we present a new method for defining negative samples in gene function prediction. Figure <xref ref-type="fig" rid="F4">4</xref> gives an overview of the proposed method: (1) First, the protein interaction data, gene expression profiles and protein complex data for yeast genes are integrated into one functional linkage graph; (2) Then, the SVD technique is utilized to project the gene vectors into low-dimensional feature space by uncovering the dominant structure of the functional linkage graph; (3) Finally, the AGPS algorithm is utilized to define negative samples and to predict the functions of genes. The detailed procedure of the proposed method is given in the following subsections.</p><fig position="float" id="F4"><label>Figure 4</label><caption><p><bold>Schematic flow chart of the proposed method</bold>. Schematic flow chart of the proposed method. First, the protein interaction data, gene expression profiles and protein complex data for yeast genes are integrated into one functional linkage graph; Then, the SVD technique is utilized to project the gene vectors into low-dimensional feature space by uncovering the dominant structure of the functional linkage graph; Finally, the AGPS algorithm is utilized to predict the functions of genes.</p></caption><graphic xlink:href="1471-2105-9-57-4"/></fig><sec><title>Singular Value Decomposition</title><p>With the functional linkage graph generated previously, a total of 5132 features are generated for each gene. Compared to the number of features, the number of samples in each class lies in the range of (76, 909). Generally, too many features may significantly increase computation cost or make the classification problem much hard. Hence, it is necessary to extract the informative features and discard the effect of the noise. However, without any information on negative samples, the common feature selection methods cannot be used here. In this paper, the Singular Value Decomposition (SVD) technique is employed to uncover the dominant structure of the functional linkage graph. Recently, the SVD method has also been successfully applied to find functional clusters in large network [<xref ref-type="bibr" rid="B38">38</xref>,<xref ref-type="bibr" rid="B39">39</xref>], where it is shown that the SVD technique is effective to find the dominant structure of the network.</p><p>In SVD, given a matrix <bold>A </bold>of size <italic>M </italic>&#x000d7; <italic>N</italic>, <bold>A </bold>can be decomposed into three matrices:</p><p><disp-formula id="bmcM2"><label>(2)</label><bold>A </bold>= <bold>S&#x003a3;V</bold><sup><italic>T</italic></sup></disp-formula></p><p>where <bold>S </bold>is the left singular matrix of size <italic>M </italic>&#x000d7; <italic>K </italic>(<italic>K </italic>is the rank of matrix <bold>A</bold>), <bold>V </bold>is the right singular matrix of size <italic>N </italic>&#x000d7; <italic>K</italic>, and <bold>&#x003a3; </bold>is the diagonal matrix of size <italic>K </italic>&#x000d7; <italic>K </italic>with non-negative eigenvalues <italic>&#x003bb;</italic><sub>1 </sub>&#x0003e; <italic>&#x003bb;</italic><sub>2 </sub>&#x0003e;,...,&#x0003e; <italic>&#x003bb;</italic><sub><italic>K </italic></sub>= 0. In this paper, <italic>A </italic>= <bold>G </bold>(i.e. the adjacent matrix of the functional linkage graph), and <italic>M </italic>= <italic>N</italic>.</p><p>After applying SVD to the matrix <bold>G</bold>, one can express <bold>G </bold>as follows:</p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M2" name="1471-2105-9-57-i2" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>&#x003bb;</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>s</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mi>r</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <bold>s</bold><sub><italic>r </italic></sub>and <bold>v</bold><sub><italic>r </italic></sub>respectively represent the <italic>r</italic><sup><italic>th </italic></sup>column of <bold>S </bold>and <bold>V</bold>, corresponding to the <italic>r</italic><sup><italic>th </italic></sup>eigenvalue. It can be seen from Equation (3) that the larger the eigenvalue the more it contributes to matrix <bold>G</bold>. Hence, to reduce the dimensionality, one can simply discard the smaller values in the diagonal matrix <bold>&#x003a3; </bold>and keep the top <italic>R </italic>eigenvalues. Accordingly, by using the first <italic>R </italic>columns of <bold>S </bold>and <bold>V</bold>, the sizes of the three matrices <bold>S</bold>, <bold>&#x003a3;</bold>, <bold>V </bold>are reduced to <italic>M </italic>&#x000d7; <italic>R</italic>, <italic>R </italic>&#x000d7; <italic>R </italic>and <italic>M </italic>&#x000d7; <italic>R</italic>, respectively. Therefore, the number of features is reduced to <italic>R</italic>. Note that <bold>S&#x003a3; </bold>gives coordinates of rows of <bold>G </bold>in the space of <italic>R </italic>principle components, and rows of <bold>V</bold><sup><italic>T </italic></sup>are eigenvectors of <bold>G</bold><sup><italic>T</italic></sup><bold>G</bold>.</p><p>Given a new gene vector <italic>G</italic><sub><italic>i </italic></sub>= [<italic>G</italic><sub><italic>i</italic>1</sub>,...,<italic>G</italic><sub><italic>iM</italic></sub>] (i.e. the vector for the <italic>i</italic><sup><italic>th </italic></sup>gene), we can project <italic>G</italic><sub><italic>i </italic></sub>into the <italic>R </italic>dimensional subspace as follows:</p><p><disp-formula id="bmcM4"><label>(4)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M3" name="1471-2105-9-57-i3" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:msup><mml:mi>G</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <inline-formula><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M4" name="1471-2105-9-57-i4" overflow="scroll"><mml:semantics><mml:mrow><mml:msub><mml:msup><mml:mi>G</mml:mi><mml:mo>&#x02032;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> is the new vector with dimensionality <italic>R </italic>(<italic>R </italic>&#x0226a; <italic>M</italic>).</p></sec><sec><title>Annotating genes with positive samples</title><p>In general, one needs both positive and negative samples to train the classifier, which is subsequently used to make classification. However, there are usually no negative samples in practice for gene function prediction because the available information to us is only about positive samples as described in previous sections. On the other hand, it is inappropriate to use genes currently not annotated with the target function as negative samples because some of them may actually have the function. Furthermore, the number of negative samples generated in this way is much larger than the one of positive samples, which will cause the imbalanced problem and degrade the performance of the classifier [<xref ref-type="bibr" rid="B20">20</xref>].</p><p>In the literature, some single class learning techniques have been proposed [<xref ref-type="bibr" rid="B22">22</xref>-<xref ref-type="bibr" rid="B26">26</xref>], e.g. one-class SVMs [<xref ref-type="bibr" rid="B21">21</xref>], to distinguish one class of data from the others in the feature space. The one-class SVMs can avoid the imbalanced problem by learning only from the positive set, where it draws a decision boundary to cover most of the positive samples in the feature space. However, there are only a small number of genes annotated in each class here. In other words, there are only a few positive samples available. Therefore, without a negative set, one-class SVMs trained only on a few positive samples tends to overfit easily.</p><p>To overcome these problems, we propose a new algorithm, namely Annotating Genes with Positive Samples (AGPS), in this paper. Here, genes with target annotation are denoted as the positive set <bold>P</bold>, genes without target annotation (i.e. annotated with other functions) are denoted as the unlabeled data <bold>Ku</bold>, and unknown genes (i.e. without any annotation) are denoted as <bold>Ug</bold>. In our study, for a specific biological function, the genes without target annotation (<bold>Ku </bold>in this case) are regarded as unlabeled data instead of negative samples because genes are generally annotated with multiple functions and it is inappropriate to simply define those genes not annotated with the target function currently as negative samples. Our goal is to predict the functions of unknown genes based on <bold>P </bold>and <bold>Ku</bold>, where the AGPS algorithm is able to define the negative set automatically in the learning procedure given positive and unlabeled data. The idea behind the AGPS algorithm is to find a subset of negative samples from unlabeled data, where the defined negative set can best recover the positive samples hidden in the unlabeled data. Furthermore, the defined negative set may be a small part of the true negative set but can represent the whole negative set well and avoid the imbalanced problem because the defined negative set can best recover the positive samples from unlabeled data and has a reasonable size. To approach this goal, the positive set is divided into positive training set <bold>P1 </bold>and validation set <bold>P2</bold>, where <bold>P2 </bold>is put into <bold>Ku </bold>to form a new unlabeled data <bold>U </bold>(i.e. <bold>U </bold>= <bold>P2 </bold>+ <bold>Ku</bold>). Although <bold>U </bold>is unlabeled data, the label of <bold>P2 </bold>is known. Therefore, we can select those samples in <bold>U </bold>to best recover <bold>P2 </bold>from <bold>U</bold>. In this procedure, we define a set of samples in <bold>U </bold>as representative negative samples <bold>TN</bold>, with which we get the best prediction results on validation set <bold>P2</bold>. The AGPS algorithm consists of two stages: (1) Learning; (2) Classification. The flowchart of AGPS algorithm is shown in Table <xref ref-type="table" rid="T1">1</xref>. In the first stage, one-class SVMs is first utilized to draw an initial decision boundary to cover most of the whole dataset including positive and unlabeled data. The data points not covered by the generated decision boundary are regarded as negative data points because these data points are far from the positive set in the feature space. With the generated initial negative dataset, the two-class SVMs is then employed to expand and refine the negative set from unlabeled data, where each classifier is trained on the positive training set and the negative set generated in the previous iteration, and the trained classifier is subsequently used to classify the remaining unlabeled data. The classifier and negative set generated in each step are recorded. This procedure continues until the stopping criteria are satisfied.</p><p>It can be seen that the error in the previous step will affect the current step. To reduce the accumulative error and to avoid the imbalanced problem, the size of the extracted negative samples in the current step is set to |<bold>N</bold><sub><bold>cur</bold></sub>| &#x02264; <italic>k</italic>|<bold>P</bold>|, where <bold>N</bold><sub><bold>cur </bold></sub>is the predicted negative genes corresponding to the top <italic>n </italic>(<italic>n </italic>&#x02264; <italic>k</italic>|<bold>P</bold>|) smallest decision values by SVMs and <italic>k </italic>is set to 3 in this work so that the false negatives can be reduced to some extent. The negative training set is then set to <bold>N </bold>= [<bold>N</bold><sub><bold>cur</bold></sub>; <bold>N</bold><sub><bold>sv</bold></sub>], where <bold>N</bold><sub><bold>sv </bold></sub>denotes the set of negative support vectors (SVs) and <bold>N</bold><sub><bold>cur </bold></sub>is <bold>N</bold><sub><bold>2 </bold></sub>in Table <xref ref-type="table" rid="T1">1</xref>. The idea behind this is that the negative SVs represent well the previous negative training set, and it is not necessary to merge the previous negative set into the current extracted negative set. Thus, the size of negative set is controlled. After obtaining the negative sets and the trained classifiers, one needs to find out the best classifier that can recover the largest number of positive samples lied in the unlabeled data because the classifiers trained above have different discriminative power on the left unlabeled data. The discrimination ability of the trained classifiers is evaluated with <italic>F</italic>1 defined in the next section. Accordingly, the negative set corresponding to the best classifier is returned as the representative negative samples, and the selection of negative samples in this way can help reduce the false negatives to some extent. With the positive and negative samples available, it is straightforward to predict the function of unknown genes in the same way as the conventional two-class SVMs does.</p><p>It can be seen from above that there are some similarities between AGPS and the existing methods, i.e. PSoL [<xref ref-type="bibr" rid="B23">23</xref>] and SVMC [<xref ref-type="bibr" rid="B22">22</xref>]. For AGPS and SVMC, both algorithms use one-class SVMs to construct the initial negative set and two-class SVMs to refine the negative set. On the other hand, for AGPS and PSoL, both algorithms use two-class SVMs to refine the negative set during negative set expansion. However, there are many differences among these three algorithms. The goal of AGPS is to refine a subset of negative samples from unlabeled data, where the defined negative set may be a small part of the true negative set but can represent the whole negative set well and also avoid the imbalanced problem because the defined negative set best recovers the positive samples from unlabeled data and has a reasonable size. With the defined negative set and existing positive set, AGPS employs two-class SVMs to classify unknown genes. On the other hand, the PSoL and SVMC algorithms aim to find possible positive samples from unlabeled data. However, since there are much more negative samples than positive ones in the unlabeled data, the putative positive samples may contain false positives. Furthermore, both PSoL and SVMC do not select the best classifier trained in the learning procedure and the best discriminative negative samples as AGPS does. Therefore, the negative samples generated in PSoL and SVMC may contain more false negatives compared against AGPS. The details of PSoL and SVMC can be found in [<xref ref-type="bibr" rid="B23">23</xref>] and [<xref ref-type="bibr" rid="B22">22</xref>], respectively.</p></sec><sec><title>Evaluation measures of performance</title><p>In this study, the <italic>precision</italic>, <italic>recall </italic>and <italic>F</italic>1 measures are used to evaluate the performance of the classifiers, and defined as follows:</p><p><disp-formula id="bmcM5"><label>(5)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M5" name="1471-2105-9-57-i5" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM6"><label>(6)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M6" name="1471-2105-9-57-i6" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p><disp-formula id="bmcM7"><label>(7)</label><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="M7" name="1471-2105-9-57-i7" overflow="scroll"><mml:semantics><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#x02217;</mml:mo><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>&#x02217;</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac><mml:mo>&#x000d7;</mml:mo><mml:mn>100</mml:mn><mml:mi>%</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:semantics></mml:math></disp-formula></p><p>where <italic>TP </italic>is the number of genes having function <bold>F </bold>and predicted correctly, <italic>FP </italic>is the number of genes predicted to have function <bold>F </bold>but actually not, and <italic>RP </italic>is the real number of genes that have function <bold>F</bold>.</p></sec></sec><sec><title>Authors' contributions</title><p>XZ and LC conceptualized and designed the method. XZ and YW developed the software. LC and KA analyzed and interpreted the data on its biological content. The manuscript was written by XZ and YW, and LC and KA revised it critically. All authors read and approved the final manuscript.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="S1"><caption><title>Additional file 1</title><p>The functional classes and corresponding genes used in 10-fold cross validation. The names of genes in each class are listed, and these genes have been annotated in MIPS until 2004.</p></caption><media xlink:href="1471-2105-9-57-S1.txt" mimetype="text" mime-subtype="plain"><caption><p>Click here for file</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="S2"><caption><title>Additional file 2</title><p>The functional classes and corresponding genes used in test stage. The names of genes in each class are listed, and these genes are not annotated in 2004 but annotated in MIPS in 2006.</p></caption><media xlink:href="1471-2105-9-57-S2.txt" mimetype="text" mime-subtype="plain"><caption><p>Click here for file</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="S3"><caption><title>Additional file 3</title><p>The predicted functions of those genes that are not annotated until 2006. These unknown gene are annotated with only the selected 13 functional classes by AGPS.</p></caption><media xlink:href="1471-2105-9-57-S3.txt" mimetype="text" mime-subtype="plain"><caption><p>Click here for file</p></caption></media></supplementary-material></sec></body><back><ack><sec><title>Acknowledgements</title><p>The authors would like to thank the anonymous reviewers for their constructive suggestions to improve the work. This work was partly supported by the National High Technology Research and Development Program of China (2006AA02Z309).</p></sec></ack><ref-list><ref id="B1"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chien</surname><given-names>C</given-names></name><name><surname>Bartel</surname><given-names>P</given-names></name><name><surname>Sternglanz</surname><given-names>R</given-names></name><name><surname>Fields</surname><given-names>S</given-names></name></person-group><article-title>The Two-Hybrid System: A Method to Identify and Clone Genes for Proteins that Interact with a Protein of Interest</article-title><source>Proc Natl Acad Sci USA</source><year>1991</year><volume>88</volume><fpage>9578</fpage><lpage>9582</lpage><pub-id pub-id-type="pmid">1946372</pub-id><pub-id pub-id-type="doi">10.1073/pnas.88.21.9578</pub-id></citation></ref><ref id="B2"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gavin</surname><given-names>AC</given-names></name><name><surname>B&#x000f6;sche</surname><given-names>M</given-names></name><name><surname>Krause</surname><given-names>R</given-names></name><name><surname>Grandi</surname><given-names>P</given-names></name><name><surname>Marzioch</surname><given-names>M</given-names></name><name><surname>Bauer</surname><given-names>A</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Rick</surname><given-names>JM</given-names></name><name><surname>Michon</surname><given-names>AM</given-names></name><name><surname>Cruciat</surname><given-names>CM</given-names></name><name><surname>Remor</surname><given-names>M</given-names></name><name><surname>H&#x000f6;fert</surname><given-names>C</given-names></name><name><surname>Schelder</surname><given-names>M</given-names></name><name><surname>Brajenovic</surname><given-names>M</given-names></name><name><surname>Ruffner</surname><given-names>H</given-names></name><name><surname>Merino</surname><given-names>A</given-names></name><name><surname>Klein</surname><given-names>K</given-names></name><name><surname>Hudak</surname><given-names>M</given-names></name><name><surname>Dickson</surname><given-names>D</given-names></name><name><surname>Rudi</surname><given-names>T</given-names></name><name><surname>Gnau</surname><given-names>V</given-names></name><name><surname>Bauch</surname><given-names>A</given-names></name><name><surname>Bastuck</surname><given-names>S</given-names></name><name><surname>Huhse</surname><given-names>B</given-names></name><name><surname>Leutwein</surname><given-names>C</given-names></name><name><surname>Heurtier</surname><given-names>MA</given-names></name><name><surname>Copley</surname><given-names>RR</given-names></name><name><surname>Edelmann</surname><given-names>A</given-names></name><name><surname>Querfurth</surname><given-names>E</given-names></name><name><surname>Rybin</surname><given-names>V</given-names></name><name><surname>Drewes</surname><given-names>G</given-names></name><name><surname>Raida</surname><given-names>M</given-names></name><name><surname>Bouwmeester</surname><given-names>T</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Seraphin</surname><given-names>B</given-names></name><name><surname>Kuster</surname><given-names>B</given-names></name><name><surname>Neubauer</surname><given-names>G</given-names></name><name><surname>Superti-Furga</surname><given-names>G</given-names></name></person-group><article-title>Functional organization of the yeast proteome by systematic analysis of protein complexes</article-title><source>Nature</source><year>2002</year><volume>415</volume><fpage>141</fpage><lpage>147</lpage><pub-id pub-id-type="pmid">11805826</pub-id><pub-id pub-id-type="doi">10.1038/415141a</pub-id></citation></ref><ref id="B3"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Y</given-names></name><name><surname>Gruhler</surname><given-names>A</given-names></name><name><surname>Heilbut</surname><given-names>A</given-names></name><name><surname>Bader</surname><given-names>GD</given-names></name><name><surname>Moore</surname><given-names>L</given-names></name><name><surname>Adams</surname><given-names>SL</given-names></name><name><surname>Millar</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>P</given-names></name><name><surname>Bennett</surname><given-names>K</given-names></name><name><surname>Boutilier</surname><given-names>K</given-names></name><name><surname>Yang</surname><given-names>L</given-names></name><name><surname>Wolting</surname><given-names>C</given-names></name><name><surname>Donaldson</surname><given-names>I</given-names></name><name><surname>Schandorff</surname><given-names>S</given-names></name><name><surname>Shewnarane</surname><given-names>J</given-names></name><name><surname>Vo</surname><given-names>M</given-names></name><name><surname>Taggart</surname><given-names>J</given-names></name><name><surname>Goudreault</surname><given-names>M</given-names></name><name><surname>Muskat</surname><given-names>B</given-names></name><name><surname>Alfarano</surname><given-names>C</given-names></name><name><surname>Dewar</surname><given-names>D</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Michalickova</surname><given-names>K</given-names></name><name><surname>Willems</surname><given-names>AR</given-names></name><name><surname>Sassi</surname><given-names>H</given-names></name><name><surname>Nielsen</surname><given-names>PA</given-names></name><name><surname>Rasmussen</surname><given-names>KJ</given-names></name><name><surname>Andersen</surname><given-names>JR</given-names></name><name><surname>Johansen</surname><given-names>LE</given-names></name><name><surname>Hansen</surname><given-names>LH</given-names></name><name><surname>Jespersen</surname><given-names>H</given-names></name><name><surname>Podtelejnikov</surname><given-names>A</given-names></name><name><surname>Nielsen</surname><given-names>E</given-names></name><name><surname>Crawford</surname><given-names>J</given-names></name><name><surname>Poulsen</surname><given-names>V</given-names></name><name><surname>S&#x000f8;rensen</surname><given-names>BD</given-names></name><name><surname>Matthiesen</surname><given-names>J</given-names></name><name><surname>Hendrickson</surname><given-names>RC</given-names></name><name><surname>Gleeson</surname><given-names>F</given-names></name><name><surname>Pawson</surname><given-names>T</given-names></name><name><surname>Moran</surname><given-names>MF</given-names></name><name><surname>Durocher</surname><given-names>D</given-names></name><name><surname>Mann</surname><given-names>M</given-names></name><name><surname>Hogue</surname><given-names>CW</given-names></name><name><surname>Figeys</surname><given-names>D</given-names></name><name><surname>Tyers</surname><given-names>M</given-names></name></person-group><article-title>Systematic identification of protein complexes in Saccharomyces cerevisiae by mass spectrometry</article-title><source>Nature</source><year>2002</year><volume>415</volume><fpage>180</fpage><lpage>183</lpage><pub-id pub-id-type="pmid">11805837</pub-id><pub-id pub-id-type="doi">10.1038/415180a</pub-id></citation></ref><ref id="B4"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Spellman</surname><given-names>PT</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name></person-group><article-title>Cluster analysis and display of genome-wide expression patterns</article-title><source>Proc Natl Acad Sci USA</source><year>1998</year><volume>95</volume><fpage>14863</fpage><lpage>14868</lpage><pub-id pub-id-type="pmid">9843981</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.25.14863</pub-id></citation></ref><ref id="B5"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chua</surname><given-names>HN</given-names></name><name><surname>Sung</surname><given-names>WK</given-names></name><name><surname>Wong</surname><given-names>L</given-names></name></person-group><article-title>Exploiting indirect neighbours and topological weight to predict protein function from protein-protein interactions</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>1623</fpage><lpage>1630</lpage><pub-id pub-id-type="pmid">16632496</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btl145</pub-id></citation></ref><ref id="B6"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schwikowski</surname><given-names>B</given-names></name><name><surname>Uetz</surname><given-names>P</given-names></name><name><surname>Fields</surname><given-names>S</given-names></name></person-group><article-title>A network of protein-protein interactions in yeast</article-title><source>Nat Biotechnol</source><year>2000</year><volume>18</volume><fpage>1257</fpage><lpage>1261</lpage><pub-id pub-id-type="pmid">11101803</pub-id><pub-id pub-id-type="doi">10.1038/82360</pub-id></citation></ref><ref id="B7"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hishigaki</surname><given-names>H</given-names></name><name><surname>Nakai</surname><given-names>K</given-names></name><name><surname>Ono</surname><given-names>T</given-names></name><name><surname>Tanigami</surname><given-names>A</given-names></name><name><surname>Takagi</surname><given-names>T</given-names></name></person-group><article-title>Assessment of prediction accuracy of protein function from protein-protein interaction data</article-title><source>Yeast</source><year>2001</year><volume>18</volume><pub-id pub-id-type="pmid">11284008</pub-id></citation></ref><ref id="B8"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Samanta</surname><given-names>MP</given-names></name><name><surname>Liang</surname><given-names>S</given-names></name></person-group><article-title>Predicting protein functions from redundancies in large-scale protein interaction networks</article-title><source>Proc Natl Acad Sci USA</source><year>2003</year><volume>100</volume><fpage>12579</fpage><lpage>12583</lpage><pub-id pub-id-type="pmid">14566057</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2132527100</pub-id></citation></ref><ref id="B9"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brun</surname><given-names>C</given-names></name><name><surname>Chevenet</surname><given-names>F</given-names></name><name><surname>Martin</surname><given-names>D</given-names></name><name><surname>Wojcik</surname><given-names>J</given-names></name><name><surname>Guenoche</surname><given-names>A</given-names></name><name><surname>Jacq</surname><given-names>B</given-names></name></person-group><article-title>Functional classification of proteins for the prediction of cellular function from a protein-protein interaction network</article-title><source>Genome Biology</source><year>2003</year><volume>5</volume><fpage>R6</fpage><pub-id pub-id-type="pmid">14709178</pub-id><pub-id pub-id-type="doi">10.1186/gb-2003-5-1-r6</pub-id></citation></ref><ref id="B10"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Vazquez</surname><given-names>A</given-names></name><name><surname>Flammini</surname><given-names>A</given-names></name><name><surname>Maritan</surname><given-names>A</given-names></name><name><surname>Vespignani</surname><given-names>A</given-names></name></person-group><article-title>Global protein function prediction from protein-protein interaction networks</article-title><source>Nat Biotechnol</source><year>2003</year><volume>21</volume><fpage>697</fpage><lpage>700</lpage><pub-id pub-id-type="pmid">12740586</pub-id><pub-id pub-id-type="doi">10.1038/nbt825</pub-id></citation></ref><ref id="B11"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Kao</surname><given-names>MCJ</given-names></name><name><surname>Wong</surname><given-names>WH</given-names></name></person-group><article-title>From the Cover: Transitive functional annotation by shortest-path analysis of gene expression data</article-title><source>Proc Natl Acad Sci USA</source><year>2002</year><volume>99</volume><fpage>12783</fpage><lpage>12788</lpage><pub-id pub-id-type="pmid">12196633</pub-id><pub-id pub-id-type="doi">10.1073/pnas.192159399</pub-id></citation></ref><ref id="B12"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>M</given-names></name><name><surname>Tu</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>T</given-names></name></person-group><article-title>Mapping gene ontology to proteins based on protein-protein interaction data</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><fpage>895</fpage><lpage>902</lpage><pub-id pub-id-type="pmid">14751964</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btg500</pub-id></citation></ref><ref id="B13"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name></person-group><article-title>Global protein function annotation through mining genome-scale data in yeast Saccharomyces cerevisiae</article-title><source>Nucl Acids Res</source><year>2004</year><volume>32</volume><fpage>6414</fpage><lpage>6424</lpage><pub-id pub-id-type="pmid">15585665</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkh978</pub-id></citation></ref><ref id="B14"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Kazuyuki</surname><given-names>A</given-names></name></person-group><article-title>Protein function prediction with the shortest path in functional linkage graph and boosting</article-title><source>J Bioinformatics Research and Application</source></citation></ref><ref id="B15"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><article-title>Protein domain annotation with integration of heterogeneous information sources</article-title><source>Proteins</source></citation></ref><ref id="B16"><citation citation-type="book"><person-group person-group-type="author"><name><surname>Lanckriet</surname><given-names>GR</given-names></name><name><surname>Deng</surname><given-names>M</given-names></name><name><surname>Cristianini</surname><given-names>N</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Noble</surname><given-names>WS</given-names></name></person-group><article-title>Kernel-based data fusion and its application to protein function prediction in yeast</article-title><source>Pac Symp Biocomput</source><year>2004</year><publisher-name>Division of Electrical Engineering, University of California, Berkeley, USA</publisher-name><fpage>300</fpage><lpage>311</lpage><pub-id pub-id-type="pmid">14992512</pub-id></citation></ref><ref id="B17"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Barutcuoglu</surname><given-names>Z</given-names></name><name><surname>Schapire</surname><given-names>RE</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title>Hierarchical multi-label prediction of gene function</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>830</fpage><lpage>836</lpage><pub-id pub-id-type="pmid">16410319</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btk048</pub-id></citation></ref><ref id="B18"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>M</given-names></name><name><surname>Ball</surname><given-names>CA</given-names></name><name><surname>Blake</surname><given-names>JA</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Cherry</surname><given-names>JM</given-names></name><name><surname>Davis</surname><given-names>AP</given-names></name><name><surname>Dolinski</surname><given-names>K</given-names></name><name><surname>Dwight</surname><given-names>SS</given-names></name><name><surname>Eppig</surname><given-names>JT</given-names></name><name><surname>Harris</surname><given-names>MA</given-names></name><name><surname>Hill</surname><given-names>DP</given-names></name><name><surname>Issel-Tarver</surname><given-names>L</given-names></name><name><surname>Kasarskis</surname><given-names>A</given-names></name><name><surname>Lewis</surname><given-names>S</given-names></name><name><surname>Matese</surname><given-names>JC</given-names></name><name><surname>Richardson</surname><given-names>JE</given-names></name><name><surname>Ringwald</surname><given-names>M</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Sherlock</surname><given-names>G</given-names></name></person-group><article-title>Gene ontology: tool for the unification of biology. The Gene Ontology Consortium</article-title><source>Nat Genet</source><year>2000</year><volume>25</volume><fpage>25</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">10802651</pub-id><pub-id pub-id-type="doi">10.1038/75556</pub-id></citation></ref><ref id="B19"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mewes</surname><given-names>HW</given-names></name><name><surname>Frishman</surname><given-names>D</given-names></name><name><surname>Guldener</surname><given-names>U</given-names></name><name><surname>Mannhaupt</surname><given-names>G</given-names></name><name><surname>Mayer</surname><given-names>K</given-names></name><name><surname>Mokrejs</surname><given-names>M</given-names></name><name><surname>Morgenstern</surname><given-names>B</given-names></name><name><surname>Munsterkotter</surname><given-names>M</given-names></name><name><surname>Rudd</surname><given-names>S</given-names></name><name><surname>Weil</surname><given-names>B</given-names></name></person-group><article-title>MIPS: a database for genomes and protein sequences</article-title><source>Nucl Acids Res</source><year>2002</year><volume>30</volume><fpage>31</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">11752246</pub-id><pub-id pub-id-type="doi">10.1093/nar/30.1.31</pub-id></citation></ref><ref id="B20"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Kazuyuki</surname><given-names>A</given-names></name></person-group><article-title>Protein classification with imbalanced data</article-title><source>Proteins</source><year>2008</year><volume>70</volume><fpage>1125</fpage><lpage>1132</lpage><pub-id pub-id-type="pmid">18076026</pub-id><pub-id pub-id-type="doi">10.1002/prot.21870</pub-id></citation></ref><ref id="B21"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000f6;lkopf</surname><given-names>B</given-names></name><name><surname>Platt</surname><given-names>JC</given-names></name><name><surname>Shawe-Taylor</surname><given-names>JC</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name><name><surname>Williamson</surname><given-names>RC</given-names></name></person-group><article-title>Estimating the support of a high-dimensional distribution</article-title><source>Neural Computation</source><year>2001</year><volume>13</volume><fpage>1443</fpage><lpage>1471</lpage><pub-id pub-id-type="pmid">11440593</pub-id><pub-id pub-id-type="doi">10.1162/089976601750264965</pub-id></citation></ref><ref id="B22"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name></person-group><article-title>Single-Class Classification with Mapping Convergence</article-title><source>Mach Learn</source><year>2005</year><volume>61</volume><fpage>49</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1007/s10994-005-1122-7</pub-id></citation></ref><ref id="B23"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Ding</surname><given-names>C</given-names></name><name><surname>Meraz</surname><given-names>RF</given-names></name><name><surname>Holbrook</surname><given-names>SR</given-names></name></person-group><article-title>PSoL: a positive sample only learning algorithm for finding non-coding RNA genes</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>2590</fpage><lpage>2596</lpage><pub-id pub-id-type="pmid">16945945</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btl441</pub-id></citation></ref><ref id="B24"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>WS</given-names></name><name><surname>Yu</surname><given-names>PS</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Partially Supervised Classification of Text Documents</article-title><source>ICML '02: Proceedings of the Nineteenth International Conference on Machine Learning</source><year>2002</year><fpage>387</fpage><lpage>394</lpage></citation></ref><ref id="B25"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>B</given-names></name></person-group><article-title>Learning to classify text using positive and unlabeled data</article-title><source>Proceedings of Eighteenth International Joint Conference on Artificial Intelligence</source><year>2003</year><fpage>587</fpage><lpage>594</lpage></citation></ref><ref id="B26"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>H</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>C-C</surname><given-names>K</given-names></name></person-group><article-title>PEBL: Positive Example-Based Learning for Web Page Classification Using SVM</article-title><source>Proc ACM SIGKDD Int'l Conf Knowledge Discovery in Databases (KDD02)</source><year>2002</year><fpage>239</fpage><lpage>248</lpage></citation></ref><ref id="B27"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ruepp</surname><given-names>A</given-names></name><name><surname>Zollner</surname><given-names>A</given-names></name><name><surname>Maier</surname><given-names>D</given-names></name><name><surname>Albermann</surname><given-names>K</given-names></name><name><surname>Hani</surname><given-names>J</given-names></name><name><surname>Mokrejs</surname><given-names>M</given-names></name><name><surname>Tetko</surname><given-names>I</given-names></name><name><surname>Guldener</surname><given-names>U</given-names></name><name><surname>Mannhaupt</surname><given-names>G</given-names></name><name><surname>Munsterkotter</surname><given-names>M</given-names></name><name><surname>Mewes</surname><given-names>HW</given-names></name></person-group><article-title>The FunCat, a functional annotation scheme for systematic classification of proteins from whole genomes</article-title><source>Nucl Acids Res</source><year>2004</year><volume>32</volume><fpage>5539</fpage><lpage>5545</lpage><pub-id pub-id-type="pmid">15486203</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkh894</pub-id></citation></ref><ref id="B28"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>C</given-names></name><name><surname>Breitkreutz</surname><given-names>BJ</given-names></name><name><surname>Reguly</surname><given-names>T</given-names></name><name><surname>Boucher</surname><given-names>L</given-names></name><name><surname>Breitkreutz</surname><given-names>A</given-names></name><name><surname>Tyers</surname><given-names>M</given-names></name></person-group><article-title>BioGRID: a general repository for interaction datasets</article-title><source>Nucl Acids Res</source><year>2006</year><volume>34</volume><fpage>D535</fpage><lpage>539</lpage><pub-id pub-id-type="pmid">16381927</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkj109</pub-id></citation></ref><ref id="B29"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Spellman</surname><given-names>PT</given-names></name><name><surname>Sherlock</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>MQ</given-names></name><name><surname>Iyer</surname><given-names>VR</given-names></name><name><surname>Anders</surname><given-names>K</given-names></name><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Futcher</surname><given-names>B</given-names></name></person-group><article-title>Comprehensive Identification of Cell Cycle-regulated Genes of the Yeast Saccharomyces cerevisiae by Microarray Hybridization</article-title><source>Mol Biol Cell</source><year>1998</year><volume>9</volume><fpage>3273</fpage><lpage>3297</lpage><pub-id pub-id-type="pmid">9843569</pub-id></citation></ref><ref id="B30"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gasch</surname><given-names>AP</given-names></name><name><surname>Spellman</surname><given-names>PT</given-names></name><name><surname>Kao</surname><given-names>CM</given-names></name><name><surname>Carmel-Harel</surname><given-names>O</given-names></name><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Storz</surname><given-names>G</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>Genomic Expression Programs in the Response of Yeast Cells to Environmental Changes</article-title><source>Mol Biol Cell</source><year>2000</year><volume>11</volume><fpage>4241</fpage><lpage>4257</lpage><pub-id pub-id-type="pmid">11102521</pub-id></citation></ref><ref id="B31"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Gasch</surname><given-names>AP</given-names></name><name><surname>Huang</surname><given-names>M</given-names></name><name><surname>Metzner</surname><given-names>S</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Elledge</surname><given-names>SJ</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>Genomic Expression Responses to DNA-damaging Agents and the Regulatory Role of the Yeast ATR Homolog Mec1p</article-title><source>Mol Biol Cell</source><year>2001</year><volume>12</volume><fpage>2987</fpage><lpage>3003</lpage><pub-id pub-id-type="pmid">11598186</pub-id></citation></ref><ref id="B32"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Yoshimoto</surname><given-names>H</given-names></name><name><surname>Saltsman</surname><given-names>K</given-names></name><name><surname>Gasch</surname><given-names>AP</given-names></name><name><surname>Li</surname><given-names>HX</given-names></name><name><surname>Ogawa</surname><given-names>N</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Cyert</surname><given-names>MS</given-names></name></person-group><article-title>Genome-wide Analysis of Gene Expression Regulated by the Calcineurin/Crz1p Signaling Pathway in Saccharomyces cerevisiae</article-title><source>J Biol Chem</source><year>2002</year><volume>277</volume><fpage>31079</fpage><lpage>31088</lpage><pub-id pub-id-type="pmid">12058033</pub-id><pub-id pub-id-type="doi">10.1074/jbc.M202718200</pub-id></citation></ref><ref id="B33"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ogawa</surname><given-names>N</given-names></name><name><surname>DeRisi</surname><given-names>J</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name></person-group><article-title>New Components of a System for Phosphate Accumulation and Polyphosphate Metabolism in Saccharomyces cerevisiae Revealed by Genomic Expression Analysis</article-title><source>Mol Biol Cell</source><year>2000</year><volume>11</volume><fpage>4309</fpage><lpage>4321</lpage><pub-id pub-id-type="pmid">11102525</pub-id></citation></ref><ref id="B34"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Troyanskaya</surname><given-names>O</given-names></name><name><surname>Cantor</surname><given-names>M</given-names></name><name><surname>Sherlock</surname><given-names>G</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name><name><surname>Altman</surname><given-names>RB</given-names></name></person-group><article-title>Missing value estimation methods for DNA microarrays</article-title><source>Bioinformatics</source><year>2001</year><volume>17</volume><fpage>520</fpage><lpage>525</lpage><pub-id pub-id-type="pmid">11395428</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/17.6.520</pub-id></citation></ref><ref id="B35"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brun</surname><given-names>C</given-names></name><name><surname>Chevenet</surname><given-names>F</given-names></name><name><surname>Martin</surname><given-names>D</given-names></name><name><surname>Wojcik</surname><given-names>J</given-names></name><name><surname>Guenoche</surname><given-names>A</given-names></name><name><surname>Jacq</surname><given-names>B</given-names></name></person-group><article-title>Functional classification of proteins for the prediction of cellular function from a protein-protein interaction network</article-title><source>Genome Biology</source><year>2003</year><volume>5</volume><fpage>R6</fpage><pub-id pub-id-type="pmid">14709178</pub-id><pub-id pub-id-type="doi">10.1186/gb-2003-5-1-r6</pub-id></citation></ref><ref id="B36"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>CC</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><source>LIBSVM: a library for support vector machines</source><ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm"/></citation></ref><ref id="B37"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>RJ</given-names></name><name><surname>Dubchak</surname><given-names>I</given-names></name><name><surname>Holbrook</surname><given-names>SR</given-names></name></person-group><article-title>A computational approach to identify genes for functional RNAs in genomic sequences</article-title><source>Nucl Acids Res</source><year>2001</year><volume>29</volume><fpage>3928</fpage><lpage>3938</lpage><pub-id pub-id-type="pmid">11574674</pub-id></citation></ref><ref id="B38"><citation citation-type="other"><person-group person-group-type="author"><name><surname>Drineas</surname><given-names>P</given-names></name><name><surname>Frieze</surname><given-names>A</given-names></name><name><surname>Kannan</surname><given-names>R</given-names></name><name><surname>Vempala</surname><given-names>S</given-names></name><name><surname>Vinay</surname><given-names>V</given-names></name></person-group><article-title>Clustering Large Graphs via the Singular Value Decomposition</article-title><source>Mach Learn</source><volume>56</volume><fpage>9</fpage><lpage>33</lpage></citation></ref><ref id="B39"><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sen</surname><given-names>T</given-names></name><name><surname>Kloczkowski</surname><given-names>A</given-names></name><name><surname>Jernigan</surname><given-names>R</given-names></name></person-group><article-title>Functional clustering of yeast proteins from the protein-protein interaction network</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><fpage>355</fpage><pub-id pub-id-type="pmid">16863590</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-7-355</pub-id></citation></ref></ref-list></back></article>