<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">5910428</article-id><article-id pub-id-type="publisher-id">24760</article-id><article-id pub-id-type="doi">10.1038/s41598-018-24760-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Computational Protein Design with Deep Learning Neural Networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Jingxue</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Cao</surname><given-names>Huali</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>John Z. H.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Qi</surname><given-names>Yifei</given-names></name><address><email>yfqi@chem.ecnu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0369 6365</institution-id><institution-id institution-id-type="GRID">grid.22069.3f</institution-id><institution>Shanghai Engineering Research Center of Molecular Therapeutics and New Drug Development, </institution><institution>School of Chemistry and Molecular Engineering, East China Normal University, </institution></institution-wrap>Shanghai, 200062 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.449457.f</institution-id><institution>NYU-ECNU Center for Computational Chemistry at NYU Shanghai, </institution></institution-wrap>Shanghai, 200062 China </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8753</institution-id><institution-id institution-id-type="GRID">grid.137628.9</institution-id><institution>Department of Chemistry, </institution><institution>New York University, NY, </institution></institution-wrap>NY, 10003 USA </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1760 2008</institution-id><institution-id institution-id-type="GRID">grid.163032.5</institution-id><institution>Collaborative Innovation Center of Extreme Optics, </institution><institution>Shanxi University, </institution></institution-wrap>Taiyuan, Shanxi 030006 China </aff></contrib-group><pub-date pub-type="epub"><day>20</day><month>4</month><year>2018</year></pub-date><pub-date pub-type="pmc-release"><day>20</day><month>4</month><year>2018</year></pub-date><pub-date pub-type="collection"><year>2018</year></pub-date><volume>8</volume><elocation-id>6349</elocation-id><history><date date-type="received"><day>12</day><month>10</month><year>2017</year></date><date date-type="accepted"><day>10</day><month>4</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2018</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article&#x02019;s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article&#x02019;s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Computational protein design has a wide variety of applications. Despite its remarkable success, designing a protein for a given structure and function is still a challenging task. On the other hand, the number of solved protein structures is rapidly increasing while the number of unique protein folds has reached a steady number, suggesting more structural information is being accumulated on each fold. Deep learning neural network is a powerful method to learn such big data set and has shown superior performance in many machine learning fields. In this study, we applied the deep learning neural network approach to computational protein design for predicting the probability of 20 natural amino acids on each residue in a protein. A large set of protein structures was collected and a multi-layer neural network was constructed. A number of structural properties were extracted as input features and the best network achieved an accuracy of 38.3%. Using the network output as residue type restraints improves the average sequence identity in designing three natural proteins using Rosetta. Moreover, the predictions from our network show ~3% higher sequence identity than a previous method. Results from this study may benefit further development of computational protein design methods.</p></abstract><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2018</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Proteins perform a vast number of functions in cells including signal transduction, DNA replication, catalyzing reactions, etc. Engineering and designing proteins for specific structure and function not only deepen our understanding of the protein sequence-structure relationship, but also have wide applications in chemistry, biology and medicine<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Over the past three decades, remarkable successes have been achieved in protein design, in which some of the designs were guided by computational methods. Examples of some recent successful computational protein designs include novel folds<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, novel enzymes<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, vaccines<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, antibodies<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>, novel protein assemblies<sup><xref ref-type="bibr" rid="CR9">9</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref></sup>, ligand-binding proteins<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>, and membrane proteins<sup><xref ref-type="bibr" rid="CR16">16</xref>&#x02013;<xref ref-type="bibr" rid="CR18">18</xref></sup>. Comprehensive coverage of the protein designs until 2014 is provided by Samish<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, and more recent ones are reviewed elsewhere<sup><xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR23">23</xref></sup>. In general, the input of computational protein design is the backbone structure of a target protein (or part of a target protein). Through computational sampling and optimization, sequences that are likely&#x000a0;to fold into the desired structure are generated for experimental verification. The scoring function usually contains physics-based terms such as van der Waals and electrostatic energy as well as knowledge-based terms such as sidechain rotamer<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> and backbone dihedral preference obtained from statistics of protein structures<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup>. In many cases, the sequences from computational design are subject to further filtering by considering various factors such as shape complementarity<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> and <italic>in silico</italic> folding free energy landscape<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>, where human experience and familiarity with the designed protein play an important role, indicating that there is a gap between current design methods and fully automatic designs.</p><p id="Par3">On the other hand, while the number of known protein structures is increasing rapidly, the number of unique protein folds is saturating. As of July 2017, there are ~132,000 structures in the protein data bank (PDB)<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> with a yearly increase of ~10,000, but the number of unique folds has not changed in the past few years, suggesting more data are accumulated on each fold, and therefore statistical learning and utilizing the existing structures are likely able to improve the design methods<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. Recently, two statistical potentials for protein design have been developed<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>, and the ABACUS potential<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> has been successfully used in designing proteins<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>. While these statistical potentials have a physical basis, machine learning especially deep-learning neural network has recently become a popular method to analyze big data sets, extract complex features, and make accurate predictions<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>.</p><p id="Par4">Deep-learning neural network, as a machine learning technique, is becoming increasingly powerful with the development of new algorithms and computer hardware, and has been applied to learning massive data sets in a variety of fields such as image recognition<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, language processing<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, and game playing<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Particularly in computational biology/chemistry, it has been used in protein-ligand scoring<sup><xref ref-type="bibr" rid="CR40">40</xref>&#x02013;<xref ref-type="bibr" rid="CR42">42</xref></sup>, protein-protein interaction prediction<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>, protein secondary structure prediction<sup><xref ref-type="bibr" rid="CR44">44</xref>&#x02013;<xref ref-type="bibr" rid="CR49">49</xref></sup>, protein contact map prediction<sup><xref ref-type="bibr" rid="CR50">50</xref>&#x02013;<xref ref-type="bibr" rid="CR52">52</xref></sup>, and compound toxicity<sup><xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup> and liver injury prediction<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, among others<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. In many cases it shows better performance than other machine learning methods. The advantage of using deep neural network is that it can learn high-order features from simple input data such as atom coordinates and types. The technical details such as network architecture, data representations vary from application to application, but the fundamental requirement of applying deep neural network is the availability of a large amount of data. With the aforementioned rich protein structure data available, it is promising to apply deep neural network in computational protein design. Zhou and coworkers have used the neural network approach to tackle this problem and developed the SPIN method to predict the sequence profile of a protein given the backbone structure<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. The input features of SPIN include &#x003c6;, &#x003c8; dihedrals of the target residue, sequence profile of 5-residue fragment derived from similar structures (the target residue and four subsequent residues), and a rotamer-based energy profile of the target residue using the DFIRE potential<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. SPIN was trained on 1532 non-redundant proteins and reaches a sequence identity of 30.3% on a test set containing 500 proteins. This sequence identity is at the lower boundary of homologous protein<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> and is not sufficient to improve protein design significantly.</p><p id="Par5">In this study, we applied deep-learning neural networks in computational protein design using new structural features, new network architecture, and a larger protein structure data set, with the aim of improving the accuracy in protein design. Instead of taking the whole input structure into account, we use a sliding widow method that has been used in protein secondary structure prediction, and predict the residue identity of each position one by one. We consider the target residue and its neighboring residues in three-dimensional spaces, with the assumption that the identity of the target residue should be compatible with its surrounding residues. We collected a large set of high-resolution protein structures and extracted the coordinates of each residue and its environment. The performance of the neural network on different input setups was compared, and application of the network outputs in protein design was investigated.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Network architecture, input, and training</title><p id="Par6">The input of the computational protein design problem is the backbone structure of a protein (or part of a protein). Instead of predicting the residue types of all positions in the input protein simultaneously, we consider each target residue and its neighbor residues (for simplicity, non-protein residues are not considered). In the simplest case, we consider a target position and its closest neighboring residue determined by C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance, and feed their input features to a neural network that consists of an input layer, several hidden layers and a softmax layer as output. The output dimension of the softmax layer is set to 20 so that the 20 output numbers that sum to one can be interpreted as the probabilities of 20 residue types of the target residue. This network is named residue probability network hereinafter (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1A</xref>). Such a simple network that considers only one neighbor residue obviously cannot make satisfactory predictions. In this study, we take into account the target residue and its 10&#x02013;30 neighbor residues by repeatedly using the residue probability network that shares the same parameters. This setup is similar to the application of convolution layer in image recognition where the same convolution network is applied to different regions of the input image. One drawback of this setup is that the output of each target-neighbor residue pair is equally weighted. Apparently, some neighbor residues have larger impacts on the identity of the target residue than others. To overcome this, we construct another network that takes the same input as the residue probability network but outputs a single number as the weight (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1B</xref>). The output of the residue probability network is multiplied by this weight and then concatenated. Several fully-connected layers are then constructed on top of the weighted residue probabilities and a 20-dimentional softmax layer is used as the final output (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1C</xref>), which can be interpreted as the probabilities of 20 residue types of the target residue.<fig id="Fig1"><label>Figure 1</label><caption><p>Architecture of the neural networks. (<bold>A</bold>) The residue probability network, (<bold>B</bold>) Weight network, and (<bold>C</bold>) The full network. The residue probability and weight networks are used as subnetworks that share the same set of network parameters for different inputs. Each <italic>input</italic> consists of the features from the target residue and one of its neighbor residues.</p></caption><graphic xlink:href="41598_2018_24760_Fig1_HTML" id="d29e488"/></fig></p><p id="Par7">The input for the residue probability and weight network consists of features from the target residue and one of its neighbor residues (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1C</xref>). The features include basic geometric and structural properties of the residues such as C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance, <italic>cos</italic> and <italic>sin</italic> values of backbone dihedrals &#x003c6;, &#x003c8; and &#x003c9;, relative location of the neighbor residue to the target residue determined by a unit vector from the C<sub>&#x003b1;</sub> atom of the target residue to the C<sub>&#x003b1;</sub> atom of the neighbor residue, three-type secondary structures, number of backbone-backbone hydrogen-bonds, and solvent accessible surface area of backbone atoms (see Methods). To train the neural network, we collected high-resolution protein structures from PDB using filtering conditions including structure determination method, resolution, chain length, and sequence identity (see Methods). Briefly, three data sets are prepared based on three sequence identity cutoffs (30%, 50%, and 90%, referred to as SI30, SI50, and SI90) to remove homologous proteins. For each of these data sets, each residue and its <italic>N</italic> (<italic>N</italic>&#x02009;=&#x02009;10, 15, 20, 25, 30) closest neighbor residues based on C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance are extracted as a cluster. These clusters are randomly split into five sets for five-fold cross-validation (Table&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>). Hereinafter, we will use SI30N10 to refer to the dataset from 30% sequence identity cutoff with 10 neighbor residues. Similar naming rules apply to other datasets as well. The number of layers and nodes in each fully-connected layer were determined by training and test on the smallest data set SI30N10. The neural network training was performed for 1000 epochs to ensure convergence.</p></sec><sec id="Sec4"><title>Overall and amino acid specific accuracy</title><p id="Par8">Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref> shows the overall accuracy (percent of residues that are correctly predicted) and standard deviations on different datasets from five-fold cross-validation. As expected, datasets with higher protein identity cutoffs show better accuracy due to more data samples and higher similarities between samples. However, considering that the number of data samples almost doubled from SI30 to SI90 dataset (Table&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>), the improvement in accuracy is not significant. Furthermore, in each protein identity cutoff, including 15 neighbor residues shows best accuracy. Including fewer neighboring residues likely under-represents the environment of the target residue, whereas including too many neighbor residues will generate noises in the inputs and thus require more data samples for training. An alternative way of extracting the neighbor residues is to use a certain distance cutoff. However, this strategy requires that the input size of the neutral network to be flexible, which will be investigated in future studies.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Accuracy from five-fold cross-validation of the neural network on different datasets with different number of neighbor residues.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Identity cutoff</th><th><italic>N</italic>&#x02009;=&#x02009;10</th><th><italic>N</italic>&#x02009;=&#x02009;15</th><th><italic>N</italic>&#x02009;=&#x02009;20</th><th><italic>N</italic>&#x02009;=&#x02009;25</th><th><italic>N</italic>&#x02009;=&#x02009;30</th></tr></thead><tbody><tr><td>30%</td><td>0.329<break/>(0.001)<sup>*</sup></td><td>
<bold>0.340</bold>
<break/>(<bold>0.005)</bold></td><td>0.333<break/>(0.009)</td><td>0.331<break/>(0.006)</td><td>0.321<break/>(0.015)</td></tr><tr><td>50%</td><td>0.353<break/>(0.003)</td><td>
<bold>0.364</bold>
<break/>(<bold>0.005)</bold></td><td>0.358<break/>(0.005)</td><td>0.359<break/>(0.006)</td><td>0.342<break/>(0.007)</td></tr><tr><td>90%</td><td>0.367<break/>(0.001)</td><td>
<bold>0.383</bold>
<break/>(<bold>0.004)</bold></td><td>0.382<break/>(0.006)</td><td>0.379<break/>(0.007)</td><td>0.352<break/>(0.013)</td></tr></tbody></table><table-wrap-foot><p><sup>*</sup>Numbers in parentheses are standard deviations.</p></table-wrap-foot></table-wrap></p><p id="Par9">We next exam the amino-acid specific accuracy using the results from the SI90N15 dataset that has the best overall accuracy. To this end, we define the recall and precision for each amino acid. Recall is the percent of native residues that are correctly predicted (recovered), and precision is the percent of predictions that are correct. Pro and Gly have higher recall and precision than other residues with Pro achieves 92.1% recall and 62.7% precision (Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>). This is because Pro has an exceptional conformational rigidity and Gly is highly flexible in terms of backbone dihedrals. A neural network can easily learn these distinct structural properties. The amino acids that have lower recall/precision generally have lower abundance in the training set, for example Met, Gln and His (Figure&#x000a0;<xref rid="MOESM1" ref-type="media">S1</xref>), although we already applied bias to these low-abundance amino acids in training. To further characterize the amino-acid specific accuracy, we calculated the probability of each native amino acid being predicted as 20 amino acids, and plot it in a 2D native <italic>vs</italic> predicted heat map (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>). The amino acids in x- and y-axis are ordered by their properties and similarities with each other. The diagonal grids show higher probabilities, as expected. Interestingly, there are several groups along the diagonal including RK, DN, VI, and FYW, indicating that the neural network frequently predicts one amino acid as another within each group. Considering the similarities of amino acids within each group, replacing one amino acid with another from the same group probably does not disrupt the protein structure, which suggests that the neural network may mispredict the native amino acid, but still provide a reasonable answer.<fig id="Fig2"><label>Figure 2</label><caption><p>Recall and precision of different amino acids of the network trained on the SI90N15 dataset. Recall is the percent of native residues that are correctly predicted (recovered), and precision is the percent of predictions that are correct.</p></caption><graphic xlink:href="41598_2018_24760_Fig2_HTML" id="d29e716"/></fig><fig id="Fig3"><label>Figure 3</label><caption><p>Probability of each amino acid being predicted as 20 amino acids.</p></caption><graphic xlink:href="41598_2018_24760_Fig3_HTML" id="d29e725"/></fig></p></sec><sec id="Sec5"><title>Top-<italic>K</italic> accuracy and its application in protein design</title><p id="Par10">Because the output of the neural network is the probability of 20 amino acids at a target position, in addition to the&#x000a0;overall accuracy mentioned above, it is also possible to calculate the top<italic>-K</italic> accuracy: if the native amino acid is within the top-<italic>K</italic> predictions (<italic>K</italic> amino acids that have the highest probabilities), the prediction is considered correct. The top-2, 3, 5, and 10 accuracy of the network trained on the SI90N15 dataset reaches 54.3%, 64.0%, 76.3%, and 91.7% respectively, suggesting the native amino acids are enriched in the first half of the predictions (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). A simple application of such information is to restrain the available amino acid types at a target position during protein design. As an illustrative example, we applied the top-3, 5, and 10 predictions as residue-type restraints in designing three proteins including an all-&#x003b1; protein (PDB ID 2B8I<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>), an all-&#x003b2; protein (PDB ID 1HOE<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>), and a mixed &#x003b1;&#x003b2; protein (PDB ID 2IGD, Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>). None of these proteins are included in our training set. The crystal structures of these proteins were used as inputs for the neural network trained on SI90N15 dataset. The top-3, 5, and 10 amino acids for each position were used as restraints in the fixed-backbone design program <italic>fixbb</italic> in Rosetta<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. As a control, we listed the top one accuracy of the neural network on these proteins, and also performed fixed-backbone design without any residue-type restraints (all 20 natural amino acids are allowed at each position). As <italic>fixbb</italic> uses a stochastic design algorithm, we generated 500 sequences for each protein and calculated the average sequence identity to the native proteins (Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>). In the three proteins, using information from the neural network predictions improves the average sequence identity, but the best <italic>K</italic> value is system dependent, and in some cases the results are worse than those in restraints-free designs (e.g., top-1 in 1HOE).<fig id="Fig4"><label>Figure 4</label><caption><p>Top-<italic>K</italic> accuracy of the neural network trained on the SI90N15 dataset.</p></caption><graphic xlink:href="41598_2018_24760_Fig4_HTML" id="d29e786"/></fig><fig id="Fig5"><label>Figure 5</label><caption><p>Structures of the proteins used in protein design with residue-type restraints.</p></caption><graphic xlink:href="41598_2018_24760_Fig5_HTML" id="d29e795"/></fig><table-wrap id="Tab2"><label>Table 2</label><caption><p>Average sequence identity of Rosetta fixed-backbone design on three proteins with/without residue-type restraints.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Protein</th><th>No-restrain<sup>*</sup></th><th>Top 1</th><th>Top 3<sup>*</sup></th><th>Top 5<sup>*</sup></th><th>Top 10<sup>*</sup></th></tr></thead><tbody><tr><td>2B8I</td><td>0.276&#x02009;&#x000b1;&#x02009;0.033</td><td>0.337</td><td>0.306&#x02009;&#x000b1;&#x02009;0.017 (0.558)</td><td><bold>0.354&#x02009;</bold>&#x000b1;<bold>&#x02009;0.021</bold>
<bold>(0.688)</bold></td><td>0.293&#x02009;&#x000b1;&#x02009;0.037 (0.883)</td></tr><tr><td>1HOE</td><td>0.408&#x02009;&#x000b1;&#x02009;0.026</td><td>0.338</td><td><bold>0.473&#x02009;</bold>&#x000b1;<bold>&#x02009;0.018</bold>
<bold>(0.635)</bold></td><td>0.441&#x02009;&#x000b1;&#x02009;0.018 (0.689)</td><td>0.416&#x02009;&#x000b1;&#x02009;0.028 (0.851)</td></tr><tr><td>2IGD</td><td>0.409&#x02009;&#x000b1;&#x02009;0.034</td><td>
<bold>0.475</bold>
</td><td>0.473&#x02009;&#x000b1;&#x02009;0.023 (0.705)</td><td>0.401&#x02009;&#x000b1;&#x02009;0.028 (0.754)</td><td>0.408&#x02009;&#x000b1;&#x02009;0.032 (0.967)</td></tr></tbody></table><table-wrap-foot><p><sup>*</sup>Sequence identities are presented as average&#x02009;&#x000b1;&#x02009;standard deviation from 500 designs. Numbers in parentheses are maximal possible identities given the residue-type restraints.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec6"><title>Comparison with SPIN</title><p id="Par11">Finally, we compare the performance of our network with SPIN developed by Zhou and coworkers<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. SPIN was trained on 1532 non-redundant proteins and reaches a sequence identity of 30.3% on a test set containing 500 proteins. The training and test set were collected using a sequence identity cutoff of 30%. SPIN was also evaluated on 50 proteins from the 500 test proteins for comparison with Rosetta<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. As the structure ID of these 50 proteins are known (Table&#x000a0;<xref rid="MOESM1" ref-type="media">S2</xref>), we set out to compare out network with SPIN on these 50 proteins. For a fair comparison, we re-trained our network on the SI30N15 dataset without the 50 proteins. Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> lists the average sequence identity from both methods when top 1 to 10 predictions are considered. Our network shows ~3% higher identity than SPIN. The number of data samples almost tripled in our study (~1.5 million training residues for our network and ~0.45 million residues for SPIN assuming each protein has 300 residues) but the improvement of accuracy is not significant, indicating certain limitation in learning sequence information from protein structures.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Average sequence identity of SPIN and our network on 50 test proteins.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Top 1</th><th>Top 2</th><th>Top 3</th><th>Top 5</th><th>Top 10</th></tr></thead><tbody><tr><td>SPIN</td><td>0.302</td><td>0.453</td><td>0.552</td><td>0.677</td><td>0.868</td></tr><tr><td>This study<sup>*</sup></td><td>0.330<break/>(0.002)</td><td>0.487<break/>(0.005)</td><td>0.585<break/>(0.002)</td><td>0.717<break/>(0.001)</td><td>0.896<break/>(0.002)</td></tr></tbody></table><table-wrap-foot><p><sup>*</sup>Numbers in parentheses are standard deviations from 5 networks trained on the same dataset with different random number seeds.</p></table-wrap-foot></table-wrap></p><p id="Par12">Nonetheless, the networks trained on the larger data set in this study could still be beneficial to computational protein design. As in real applications, amino acid probability learned on larger data set could be more useful, as long as it is not biased. As an example, we tested both methods on the <italic>de novo</italic> designed protein Top7 (PDB ID 1QYS<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, not included in our training set). The top 1 prediction from SPIN shows an identity of 0.250, while the top 1 predictions from the SI30N15, SI50N15, and SI90N15 network have identities of 0.283, 0.304, and 0.402.</p><p id="Par13">In addition to comparing sequence identities, we also compared out predictions with the position-specific scoring matrix (PSSM) from PSI-BLAST<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. The PSSMs of the 50 test proteins were obtained by running PSI-BLAST against the non-redundant protein sequences database available at ftp://ftp.ncbi.nlm.nih.gov/blast/db/, and converted to pseudo probability matrixes of 20 amino acids at each residue. The root mean square error (RMSE) of the matrixes to those predicted by our network and SPIN were calculated. Our network and SPIN show very similar RMSE values (0.139 for our network and 0.141 for SPIN). It should be noted that SPIN was trained on PSSMs from PSI-BLAST for predicting sequence profiles whereas our network was trained on protein sequences only.</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussions</title><p id="Par14">In this study, we have developed deep-learning neural networks for computational protein design. The networks achieve an accuracy of 38.3% on the dataset with 90% sequence identity cutoff when 15 neighboring residues are included. This accuracy is limited not only by our neural network approach but also by the nature of protein structures. It is known that two proteins with a&#x000a0;low sequence identity (~30%) can fold into similar structures<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. In a DNA repair enzyme 3-methyladenine DNA glycosylase, the probability that a random mutation can inactivate the protein was found to be 34%&#x02009;&#x000b1;&#x02009;6%, indicating a large proportion of mutations can be tolerated in this protein<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>. Moreover, residues at active sites are subject to functional restraints and are not necessary the most stable ones<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. Our neural network approach is similar to a structural comparison method that extracts and integrates similar local structures from known structures. Therefore, its accuracy is limited by the average number of tolerable amino acids at each residue in the training set. Fortunately, the native amino acid is concentrated in the top predictions (top-5 and 10 accuracies are 76.3% and 91.7%). By integration the network output with molecular-mechanics scoring functions, it should be possible to identify the correct amino acid from the top predictions and further improve the accuracy. Particularly, the network preforms well on Gly and Pro, due to its ability to learn distinct structural features, but less satisfying on hydrophobic residues that are likely more important for the correct folding of a protein. Including solvation energy in the molecular mechanics scoring functions is probably a promising way for future development.</p><p id="Par15">In our approach, the environment of a target residue is simply considered using the <italic>N</italic> closest residues based on C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distances. This method may exclude some residues that have important interactions with the target residue. To quantitatively characterize this, we calculated the distance rank (<italic>M</italic>, the rank of C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance of a neighbor residue among all residues surrounding the target residue) of neighbor residues that have contacts (heavy atom distance &#x0003c;4.5&#x02009;&#x000c5;) with the target residue in our dataset, and found that 96.2% contacting residues have <italic>M</italic>&#x02009;&#x02264;&#x02009;20, and 98.9% contacting residues have <italic>M</italic>&#x02009;&#x02264;&#x02009;30, which means 3.8% and 1.1% of the contacting residues are not included in the environment if <italic>N</italic>&#x02009;=&#x02009;20 and 30, respectively. Moreover, for terminal residues that are highly exposed, 20 neighbors may contain residues that do not have contacts with the target residue, which will generate noises in the inputs. Using distance cutoff instead of residue number cutoff may solve this problem. However, the distance cutoff method requires the input size to be highly flexible from several residues to tens of residues, which should be carefully considered during network construction.</p><p id="Par16">Knowing the possible amino acids with good confidence at the designing positions may reduce the search space significantly and increase the chance to make a successful design. Our test of Rosetta design on three proteins shows that it is possible to improve the sequence identity by using the output from our neural network as residue-type restraints. However, the optimal number of amino acids to be used as restraints is system dependent. More importantly, in our neural network, the prediction on each residue is independent from each other. For real designs, it is important to simultaneously consider the identities of the neighbor residues by using molecular-mechanics-based or statistical scoring functions like the ones in Rosetta. In this regard, the predicted probability of each amino acid should be explicitly taken into account. As the prediction of a trained neural network on a protein structure only takes several seconds, we expect our approach to pave the way for further development of computational protein design methods.</p></sec><sec id="Sec8" sec-type="materials|methods"><title>Methods</title><sec id="Sec9"><title>Datasets and input features</title><p id="Par17">The training set was collected from PDB<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> using the following criteria: (1) the structure is determined with x-ray crystallography, (2) the resolution is better than 2&#x02009;&#x000c5;, (3) the chain length is longer than 50, and (4) the structure does not have any DNA/RNA molecules. To investigate the effects of sequence homology on prediction accuracy, the structures that satisfy these conditions were retrieved with 30%, 50%, and 90% sequence identities. The resulting entries were cross-referenced with the OPM database<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> to remove membrane proteins. Structures that have D-amino acids were also discarded. The resulting structure dataset consists of 10173 (30% sequence identity), 14064 (50% sequence identity), and 17607 structures (90% sequence identity). To remove the bias from non-biological interface in the crystal asymmetric unit, the biological assembly provided by PDB was used. If multiple biological assemblies exist for one structure, the first one&#x000a0;was used. For each of these structures, non-protein residues such as water, ion, and ligand were removed, and each protein residue and its <italic>N</italic> closest (<italic>N</italic>&#x02009;=&#x02009;10, 15, 20, 25, 30, ranked based on C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance) neighboring residues were extracted as a structural cluster. Clusters that have any atoms with an occupancy &#x0003c;1 or missing backbone atoms were discarded. Protein oligomeric state was also considered during cluster extraction so that if a structure contains several identical subunits, only one of the subunits was used. Each cluster was then translated and orientated so that the C<sub>&#x003b1;</sub>, N, and C atoms of the target residue are located at the origin, the &#x02013;<italic>x</italic> axis, and the <italic>z</italic>&#x02009;=&#x02009;0 plane, respectively.</p><p id="Par18">The input features for the neural networks are (1) for the central residues: <italic>cos</italic> and <italic>sin</italic> values of backbone dihedrals &#x003c6;, &#x003c8; and &#x003c9;, total solvent accessible surface area (SASA) of backbone atoms (C<sub>&#x003b1;</sub>, N, C, and O), and three-type (helix, sheet, loop) secondary structure; (2) for the neighbor residues: <italic>cos</italic> and <italic>sin</italic> values of backbone dihedrals &#x003c6;, &#x003c8;, and &#x003c9;, total SASA of backbone atoms, C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance to the central residue, unit vector from the C<sub>&#x003b1;</sub> atom of the central residue to the C<sub>&#x003b1;</sub> atom of the neighbor residue, C<sub>&#x003b1;</sub>-N unit vector of the neighbor residue, C<sub>&#x003b1;</sub>-C unit vector of the neighbor residue, three-type secondary structure, and number of backbone-backbone hydrogen bonds between the central residue and the neighbor residue. The C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub> distance, C<sub>&#x003b1;</sub>-C<sub>&#x003b1;</sub>, C<sub>&#x003b1;</sub>-N, and C<sub>&#x003b1;</sub>-C unit vectors were used to define the exact position and orientation of the neighbor residue with respect to the central residue. <italic>cos</italic> and <italic>sin</italic> values of the dihedrals were used because the dihedrals that range from &#x02212;180 to 180 are not continuous at &#x02212;180 and 180. The SASA value was calculated using the Naccess program<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> on the whole protein structure (not on a structural cluster) with the sidechain atom removed, because during protein design, the identity of a residue and thus its sidechain atoms are unknown. Secondary structure was assigned with Stride<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>. All other features were calculated with an in-house program.</p></sec><sec id="Sec10"><title>Deep neural-network learning</title><p id="Par19">The neural network was constructed using the Keras library (<ext-link ext-link-type="uri" xlink:href="http://keras.io">http://keras.io</ext-link>) with rectified linear unit (ReLU) as the activation function for all layers. Training was performed using the categorical cross entropy as the loss function and the stochastic gradient descent method for optimization with a learning rate of 0.01, a Nesterov momentum of 0.9, and a batch size of 40,000. To account for the different abundance of each residue type in the training set, the training samples were weighted as: <italic>W</italic><sub><italic>i</italic></sub>&#x02009;=&#x02009;<italic>N</italic><sub><italic>max</italic></sub>/<italic>N</italic><sub><italic>i</italic></sub>, where <italic>N</italic><sub><italic>max</italic></sub> is the maximal number of samples of all 20 residue types, and <italic>N</italic><sub><italic>i</italic></sub> is the number of samples of residue type <italic>i</italic>. This bias would force the neural network to learn more from the residue types that are underrepresented in the training set. The output of the neural-network is the probability of 20 amino acids for the central residue of a cluster.</p></sec><sec id="Sec11"><title>Rosetta design</title><p id="Par20">Rosetta design was carried out with the <italic>fixbb</italic> program and talaris2014 score in Rosetta 3.7<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. The crystal structures of the design targets were used as inputs without any prior minimization. 500 designs were performed for each protein with and without residue-type restraints, which were incorporated using the &#x0201c;<italic>-resfile</italic>&#x0201d; option.</p></sec><sec id="Sec12"><title>Data availability</title><p id="Par21">The datasets generated and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></sec></sec><sec sec-type="supplementary-material"><title>Electronic supplementary material</title><sec id="Sec13"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="41598_2018_24760_MOESM1_ESM.docx"><caption><p>Supporting Information</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Electronic supplementary material</bold></p><p><bold>Supplementary information</bold> accompanies this paper at 10.1038/s41598-018-24760-x.</p></fn><fn><p><bold>Publisher's note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was supported by the National Natural Science Foundation of China (Grant no. 31700646) to Y.Q. and (Grant no. 21433004) J.Z., Ministry of Science and Technology of China (Grant no. 2016YFA0501700), NYU Global Seed Grant, and Shanghai Putuo District (Grant 2014-A-02) to J.Z. We thank the Supercomputer Center of East China Normal University for providing us computer time.</p></ack><notes notes-type="author-contribution"><title>Author Contributions</title><p>Y.Q. and J.Z. designed the study and experiments. Y.Q., J.W. and H.C. performed the experiments. Y.Q. and J.Z. prepared manuscript.</p></notes><notes notes-type="COI-statement"><sec id="FPar1"><title>Competing Interests</title><p id="Par22">The authors declare no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sandhya</surname><given-names>S</given-names></name><name><surname>Mudgal</surname><given-names>R</given-names></name><name><surname>Kumar</surname><given-names>G</given-names></name><name><surname>Sowdhamini</surname><given-names>R</given-names></name><name><surname>Srinivasan</surname><given-names>N</given-names></name></person-group><article-title>Protein sequence design and its applications</article-title><source>Curr Opin Struct Biol</source><year>2016</year><volume>37</volume><fpage>71</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2015.12.004</pub-id><?supplied-pmid 26773478?><pub-id pub-id-type="pmid">26773478</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhlman</surname><given-names>B</given-names></name><etal/></person-group><article-title>Design of a novel globular protein fold with atomic-level accuracy</article-title><source>Science</source><year>2003</year><volume>302</volume><fpage>1364</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1126/science.1089427</pub-id><?supplied-pmid 14631033?><pub-id pub-id-type="pmid">14631033</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>L</given-names></name><etal/></person-group><article-title>De novo computational design of retro-aldol enzymes</article-title><source>Science</source><year>2008</year><volume>319</volume><fpage>1387</fpage><lpage>1391</lpage><pub-id pub-id-type="doi">10.1126/science.1152692</pub-id><?supplied-pmid 18323453?><pub-id pub-id-type="pmid">18323453</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothlisberger</surname><given-names>D</given-names></name><etal/></person-group><article-title>Kemp elimination catalysts by computational enzyme design</article-title><source>Nature</source><year>2008</year><volume>453</volume><fpage>190</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1038/nature06879</pub-id><?supplied-pmid 18354394?><pub-id pub-id-type="pmid">18354394</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correia</surname><given-names>BE</given-names></name><etal/></person-group><article-title>Computational design of epitope-scaffolds allows induction of antibodies specific for a poorly immunogenic HIV vaccine epitope</article-title><source>Structure</source><year>2010</year><volume>18</volume><fpage>1116</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1016/j.str.2010.06.010</pub-id><?supplied-pmid 20826338?><pub-id pub-id-type="pmid">20826338</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Correia</surname><given-names>BE</given-names></name><etal/></person-group><article-title>Proof of principle for epitope-focused vaccine design</article-title><source>Nature</source><year>2014</year><volume>507</volume><fpage>201</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1038/nature12966</pub-id><?supplied-pmid 24499818?><pub-id pub-id-type="pmid">24499818</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver-Fay</surname><given-names>A</given-names></name><etal/></person-group><article-title>Computationally Designed Bispecific Antibodies using Negative State Repertoires</article-title><source>Structure</source><year>2016</year><volume>24</volume><fpage>641</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1016/j.str.2016.02.013</pub-id><?supplied-pmid 26996964?><pub-id pub-id-type="pmid">26996964</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>SM</given-names></name><etal/></person-group><article-title>Generation of bispecific IgG antibodies by structure-based design of an orthogonal Fab interface</article-title><source>Nat Biotechnol</source><year>2014</year><volume>32</volume><fpage>191</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1038/nbt.2797</pub-id><?supplied-pmid 24463572?><pub-id pub-id-type="pmid">24463572</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bale</surname><given-names>JB</given-names></name><etal/></person-group><article-title>Accurate design of megadalton-scale two-component icosahedral protein complexes</article-title><source>Science</source><year>2016</year><volume>353</volume><fpage>389</fpage><lpage>394</lpage><pub-id pub-id-type="doi">10.1126/science.aaf8818</pub-id><?supplied-pmid 27463675?><pub-id pub-id-type="pmid">27463675</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonen</surname><given-names>S</given-names></name><name><surname>DiMaio</surname><given-names>F</given-names></name><name><surname>Gonen</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><article-title>Design of ordered two-dimensional arrays mediated by noncovalent protein-protein interfaces</article-title><source>Science</source><year>2015</year><volume>348</volume><fpage>1365</fpage><lpage>1368</lpage><pub-id pub-id-type="doi">10.1126/science.aaa9897</pub-id><?supplied-pmid 26089516?><pub-id pub-id-type="pmid">26089516</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsia</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Design of a hyperstable 60-subunit protein dodecahedron</article-title><source>Nature</source><year>2016</year><volume>535</volume><fpage>136</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1038/nature18010</pub-id><?supplied-pmid 27309817?><pub-id pub-id-type="pmid">27309817</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>NP</given-names></name><etal/></person-group><article-title>Accurate design of co-assembling multi-component protein nanomaterials</article-title><source>Nature</source><year>2014</year><volume>510</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/nature13404</pub-id><?supplied-pmid 24870237?><pub-id pub-id-type="pmid">24870237</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>NP</given-names></name><etal/></person-group><article-title>Computational design of self-assembling protein nanomaterials with atomic level accuracy</article-title><source>Science</source><year>2012</year><volume>336</volume><fpage>1171</fpage><lpage>1174</lpage><pub-id pub-id-type="doi">10.1126/science.1219364</pub-id><?supplied-pmid 22654060?><pub-id pub-id-type="pmid">22654060</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinberg</surname><given-names>CE</given-names></name><etal/></person-group><article-title>Computational design of ligand-binding proteins with high affinity and selectivity</article-title><source>Nature</source><year>2013</year><volume>501</volume><fpage>212</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/nature12443</pub-id><?supplied-pmid 24005320?><pub-id pub-id-type="pmid">24005320</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L</given-names></name><etal/></person-group><article-title>A protein engineered to bind uranyl selectively and with femtomolar affinity</article-title><source>Nat Chem</source><year>2014</year><volume>6</volume><fpage>236</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1038/nchem.1856</pub-id><?supplied-pmid 24557139?><pub-id pub-id-type="pmid">24557139</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Computational design and experimental characterization of peptides intended for pH-dependent membrane insertion and pore formation</article-title><source>ACS Chem Biol</source><year>2015</year><volume>10</volume><fpage>1082</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1021/cb500759p</pub-id><?supplied-pmid 25630033?><pub-id pub-id-type="pmid">25630033</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korendovych</surname><given-names>IV</given-names></name><etal/></person-group><article-title>De novo design and molecular assembly of a transmembrane diporphyrin-binding protein complex</article-title><source>J Am Chem Soc</source><year>2010</year><volume>132</volume><fpage>15516</fpage><lpage>15518</lpage><pub-id pub-id-type="doi">10.1021/ja107487b</pub-id><?supplied-pmid 20945900?><pub-id pub-id-type="pmid">20945900</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joh</surname><given-names>NH</given-names></name><etal/></person-group><article-title>De novo design of a transmembrane Zn(2)(+)-transporting four-helix bundle</article-title><source>Science</source><year>2014</year><volume>346</volume><fpage>1520</fpage><lpage>1524</lpage><pub-id pub-id-type="doi">10.1126/science.1261172</pub-id><?supplied-pmid 25525248?><pub-id pub-id-type="pmid">25525248</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Samish, I. in <italic>Computational protein design</italic> (ed Ilan Samish) Ch. 2, 21&#x02013;94 (Humana Press, 2016).</mixed-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>PS</given-names></name><name><surname>Boyken</surname><given-names>SE</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name></person-group><article-title>The coming of age of de novo protein design</article-title><source>Nature</source><year>2016</year><volume>537</volume><fpage>320</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1038/nature19946</pub-id><?supplied-pmid 27629638?><pub-id pub-id-type="pmid">27629638</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Lai</surname><given-names>L</given-names></name></person-group><article-title>Computational design of ligand-binding proteins</article-title><source>Curr Opin Struct Biol</source><year>2016</year><volume>45</volume><fpage>67</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2016.11.021</pub-id><?supplied-pmid 27951448?><pub-id pub-id-type="pmid">27951448</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norn</surname><given-names>CH</given-names></name><name><surname>Andre</surname><given-names>I</given-names></name></person-group><article-title>Computational design of protein self-assembly</article-title><source>Curr Opin Struct Biol</source><year>2016</year><volume>39</volume><fpage>39</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2016.04.002</pub-id><?supplied-pmid 27127996?><pub-id pub-id-type="pmid">27127996</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name></person-group><article-title>Computational protein design for given backbone: recent progresses in general method-related aspects</article-title><source>Curr Opin Struct Biol</source><year>2016</year><volume>39</volume><fpage>89</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2016.06.013</pub-id><?supplied-pmid 27348345?><pub-id pub-id-type="pmid">27348345</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapovalov</surname><given-names>MV</given-names></name><name><surname>Dunbrack</surname><given-names>RL</given-names><suffix>Jr.</suffix></name></person-group><article-title>A smoothed backbone-dependent rotamer library for proteins derived from adaptive kernel density estimates and regressions</article-title><source>Structure</source><year>2011</year><volume>19</volume><fpage>844</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.str.2011.03.019</pub-id><?supplied-pmid 21645855?><pub-id pub-id-type="pmid">21645855</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Dai</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title>Energy functions in de novo protein design: current challenges and future prospects</article-title><source>Annu Rev Biophys</source><year>2013</year><volume>42</volume><fpage>315</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1146/annurev-biophys-083012-130315</pub-id><?supplied-pmid 23451890?><pub-id pub-id-type="pmid">23451890</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boas</surname><given-names>FE</given-names></name><name><surname>Harbury</surname><given-names>PB</given-names></name></person-group><article-title>Potential energy functions for protein design</article-title><source>Curr Opin Struct Biol</source><year>2007</year><volume>17</volume><fpage>199</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2007.03.006</pub-id><?supplied-pmid 17387014?><pub-id pub-id-type="pmid">17387014</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doyle</surname><given-names>L</given-names></name><etal/></person-group><article-title>Rational design of alpha-helical tandem repeat proteins with closed architectures</article-title><source>Nature</source><year>2015</year><volume>528</volume><fpage>585</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1038/nature16191</pub-id><?supplied-pmid 26675735?><pub-id pub-id-type="pmid">26675735</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhardwaj</surname><given-names>G</given-names></name><etal/></person-group><article-title>Accurate de novo design of hyperstable constrained peptides</article-title><source>Nature</source><year>2016</year><volume>538</volume><fpage>329</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1038/nature19791</pub-id><?supplied-pmid 27626386?><pub-id pub-id-type="pmid">27626386</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berman</surname><given-names>HM</given-names></name><etal/></person-group><article-title>The Protein Data Bank</article-title><source>Nucleic Acids Res</source><year>2000</year><volume>28</volume><fpage>235</fpage><lpage>242</lpage><pub-id pub-id-type="doi">10.1093/nar/28.1.235</pub-id><?supplied-pmid 10592235?><pub-id pub-id-type="pmid">10592235</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broom</surname><given-names>A</given-names></name><name><surname>Trainor</surname><given-names>K</given-names></name><name><surname>MacKenzie</surname><given-names>DW</given-names></name><name><surname>Meiering</surname><given-names>EM</given-names></name></person-group><article-title>Using natural sequences and modularity to design common and novel protein topologies</article-title><source>Curr Opin Struct Biol</source><year>2016</year><volume>38</volume><fpage>26</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.sbi.2016.05.007</pub-id><?supplied-pmid 27270240?><pub-id pub-id-type="pmid">27270240</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khersonsky</surname><given-names>O</given-names></name><name><surname>Fleishman</surname><given-names>SJ</given-names></name></person-group><article-title>Why reinvent the wheel? Building new proteins based on ready-made parts</article-title><source>Protein Sci</source><year>2016</year><volume>25</volume><fpage>1179</fpage><lpage>1187</lpage><pub-id pub-id-type="doi">10.1002/pro.2892</pub-id><?supplied-pmid 26821641?><pub-id pub-id-type="pmid">26821641</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Topham</surname><given-names>CM</given-names></name><name><surname>Barbe</surname><given-names>S</given-names></name><name><surname>Andre</surname><given-names>I</given-names></name></person-group><article-title>An Atomistic Statistically Effective Energy Function for Computational Protein Design</article-title><source>J Chem Theory Comput</source><year>2016</year><volume>12</volume><fpage>4146</fpage><lpage>4168</lpage><pub-id pub-id-type="doi">10.1021/acs.jctc.6b00090</pub-id><?supplied-pmid 27341125?><pub-id pub-id-type="pmid">27341125</pub-id></element-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>P</given-names></name><etal/></person-group><article-title>Protein design with a comprehensive statistical energy function and boosted by experimental selection for foldability</article-title><source>Nat Commun</source><year>2014</year><volume>5</volume><fpage>5330</fpage><pub-id pub-id-type="doi">10.1038/ncomms6330</pub-id><?supplied-pmid 25345468?><pub-id pub-id-type="pmid">25345468</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name></person-group><article-title>Computational Protein Design Under a Given Backbone Structure with the ABACUS Statistical Energy Function</article-title><source>Methods Mol Biol</source><year>2017</year><volume>1529</volume><fpage>217</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1007/978-1-4939-6637-0_10</pub-id><?supplied-pmid 27914053?><pub-id pub-id-type="pmid">27914053</pub-id></element-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>X</given-names></name><etal/></person-group><article-title>Proteins of well-defined structures can be designed without backbone readjustment by a statistical model</article-title><source>J Struct Biol</source><year>2016</year><volume>196</volume><fpage>350</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1016/j.jsb.2016.08.002</pub-id><?supplied-pmid 27522946?><pub-id pub-id-type="pmid">27522946</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><?supplied-pmid 26017442?><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Simonyan, K. &#x00026; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. <italic>ArXiv e-print</italic>s <bold>1409</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2014arXiv1409.1556S">http://adsabs.harvard.edu/abs/2014arXiv1409.1556S</ext-link> (2014).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Collobert, R. &#x00026; Weston, J. A unified architecture for natural language processing: deep neural networks with multitask learning. <italic>Proceedings of the 25th international conference on Machine learning</italic>, 160&#x02013;167 (2008).</mixed-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><etal/></person-group><article-title>Mastering the game of Go with deep neural networks and tree search</article-title><source>Nature</source><year>2016</year><volume>529</volume><fpage>484</fpage><lpage>489</lpage><pub-id pub-id-type="doi">10.1038/nature16961</pub-id><?supplied-pmid 26819042?><pub-id pub-id-type="pmid">26819042</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Gomes, J., Ramsundar, B., Feinberg, E. N. &#x00026; Pande, V. S. Atomic Convolutional Networks for Predicting Protein-Ligand Binding Affinity. <italic>ArXiv e-prints</italic><bold>1703</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2017arXiv170310603G">http://adsabs.harvard.edu/abs/2017arXiv170310603G</ext-link> (2017).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Wallach, I., Dzamba, M. &#x00026; Heifets, A. AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery. <italic>ArXiv e-print</italic>s <bold>1510</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2015arXiv151002855W">http://adsabs.harvard.edu/abs/2015arXiv151002855W</ext-link> (2015).</mixed-citation></ref><ref id="CR42"><label>42.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ragoza</surname><given-names>M</given-names></name><name><surname>Hochuli</surname><given-names>J</given-names></name><name><surname>Idrobo</surname><given-names>E</given-names></name><name><surname>Sunseri</surname><given-names>J</given-names></name><name><surname>Koes</surname><given-names>DR</given-names></name></person-group><article-title>Protein-Ligand Scoring with Convolutional Neural Networks</article-title><source>J Chem Inf Model</source><year>2017</year><volume>57</volume><fpage>942</fpage><lpage>957</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.6b00740</pub-id><?supplied-pmid 28368587?><pub-id pub-id-type="pmid">28368587</pub-id></element-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Sun, T. L., Zhou, B., Lai, L. H. &#x00026; Pei, J. F. Sequence-based prediction of protein protein interaction using a deep-learning algorithm. <italic>Bmc Bioinformatics</italic><bold>18</bold> (2017).</mixed-citation></ref><ref id="CR44"><label>44.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heffernan</surname><given-names>R</given-names></name><etal/></person-group><article-title>Improving prediction of secondary structure, local backbone angles, and solvent accessible surface area of proteins by iterative deep learning</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><fpage>11476</fpage><pub-id pub-id-type="doi">10.1038/srep11476</pub-id><?supplied-pmid 26098304?><pub-id pub-id-type="pmid">26098304</pub-id></element-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Li, Z. &#x00026; Yu, Y. Protein Secondary Structure Prediction Using Cascaded Convolutional and RecurrentNeural Networks. <italic>ArXiv e-print</italic>s <bold>1604</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2016arXiv160407176L">http://adsabs.harvard.edu/abs/2016arXiv160407176L</ext-link> (2016).</mixed-citation></ref><ref id="CR46"><label>46.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Peng</surname><given-names>J</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><article-title>Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields</article-title><source>Sci Rep</source><year>2016</year><volume>6</volume><fpage>18962</fpage><pub-id pub-id-type="doi">10.1038/srep18962</pub-id><?supplied-pmid 26752681?><pub-id pub-id-type="pmid">26752681</pub-id></element-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Busia, A., Collins, J. &#x00026; Jaitly, N. Protein Secondary Structure Prediction Using Deep Multi-scale Convolutional Neural Networks and Next-Step Conditioning. <italic>ArXiv e-prints</italic><bold>1611</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2016arXiv161101503B">http://adsabs.harvard.edu/abs/2016arXiv161101503B</ext-link> (2016).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Kaae S&#x000f8;nderby, S. &#x00026; Winther, O. Protein Secondary Structure Prediction with Long Short Term MemoryNetworks. <italic>ArXiv e-print</italic>s <bold>1412</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2014arXiv1412.7828K">http://adsabs.harvard.edu/abs/2014arXiv1412.7828K</ext-link> (2014).</mixed-citation></ref><ref id="CR49"><label>49.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faraggi</surname><given-names>E</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Kurgan</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title>SPINE X: improving protein secondary structure prediction by multistep learning coupled with prediction of solvent accessible surface area and backbone torsion angles</article-title><source>J Comput Chem</source><year>2012</year><volume>33</volume><fpage>259</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1002/jcc.21968</pub-id><?supplied-pmid 22045506?><pub-id pub-id-type="pmid">22045506</pub-id></element-citation></ref><ref id="CR50"><label>50.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><article-title>Accurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><fpage>e1005324</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id><?supplied-pmid 28056090?><pub-id pub-id-type="pmid">28056090</pub-id></element-citation></ref><ref id="CR51"><label>51.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Lena</surname><given-names>P</given-names></name><name><surname>Nagata</surname><given-names>K</given-names></name><name><surname>Baldi</surname><given-names>P</given-names></name></person-group><article-title>Deep architectures for protein contact map prediction</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>2449</fpage><lpage>2457</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts475</pub-id><?supplied-pmid 22847931?><pub-id pub-id-type="pmid">22847931</pub-id></element-citation></ref><ref id="CR52"><label>52.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickholt</surname><given-names>J</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name></person-group><article-title>Predicting protein residue-residue contacts using deep networks and boosting</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>3066</fpage><lpage>3072</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts598</pub-id><?supplied-pmid 23047561?><pub-id pub-id-type="pmid">23047561</pub-id></element-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Mayr, A., Klambauer, G., Unterthiner, T. &#x00026; Hochreiter, S. DeepTox: Toxicity Prediction using Deep Learning. <italic>Frontiers in Environmental Science</italic><bold>3</bold> (2016).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Unterthiner, T., Mayr, A., Klambauer, G. &#x00026; Hochreiter, S. Toxicity Prediction using Deep Learning. <italic>ArXiv e-print</italic>s <bold>1503</bold>, <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2015arXiv150301445U">http://adsabs.harvard.edu/abs/2015arXiv150301445U</ext-link> (2015).</mixed-citation></ref><ref id="CR55"><label>55.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Deep Learning for Drug-Induced Liver Injury</article-title><source>J Chem Inf Model</source><year>2015</year><volume>55</volume><fpage>2085</fpage><lpage>2093</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.5b00238</pub-id><?supplied-pmid 26437739?><pub-id pub-id-type="pmid">26437739</pub-id></element-citation></ref><ref id="CR56"><label>56.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goh</surname><given-names>GB</given-names></name><name><surname>Hodas</surname><given-names>NO</given-names></name><name><surname>Vishnu</surname><given-names>A</given-names></name></person-group><article-title>Deep learning for computational chemistry</article-title><source>J Comput Chem</source><year>2017</year><volume>38</volume><fpage>1291</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1002/jcc.24764</pub-id><?supplied-pmid 28272810?><pub-id pub-id-type="pmid">28272810</pub-id></element-citation></ref><ref id="CR57"><label>57.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Faraggi</surname><given-names>E</given-names></name><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title>Direct prediction of profiles of sequences compatible with a protein structure by neural networks with fragment-based local and energy-based nonlocal profiles</article-title><source>Proteins</source><year>2014</year><volume>82</volume><fpage>2565</fpage><lpage>2573</lpage><pub-id pub-id-type="doi">10.1002/prot.24620</pub-id><?supplied-pmid 24898915?><pub-id pub-id-type="pmid">24898915</pub-id></element-citation></ref><ref id="CR58"><label>58.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title>Distance-scaled, finite ideal-gas reference state improves structure-derived potentials of mean force for structure selection and stability prediction</article-title><source>Protein Sci</source><year>2002</year><volume>11</volume><fpage>2714</fpage><lpage>2726</lpage><pub-id pub-id-type="doi">10.1110/ps.0217002</pub-id><?supplied-pmid 12381853?><pub-id pub-id-type="pmid">12381853</pub-id></element-citation></ref><ref id="CR59"><label>59.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rost</surname><given-names>B</given-names></name></person-group><article-title>Twilight zone of protein sequence alignments</article-title><source>Protein Eng</source><year>1999</year><volume>12</volume><fpage>85</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1093/protein/12.2.85</pub-id><?supplied-pmid 10195279?><pub-id pub-id-type="pmid">10195279</pub-id></element-citation></ref><ref id="CR60"><label>60.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Crystal structure and functional studies reveal that PAS factor from Vibrio vulnificus is a novel member of the saposin-fold family</article-title><source>J Mol Biol</source><year>2006</year><volume>355</volume><fpage>491</fpage><lpage>500</lpage><pub-id pub-id-type="doi">10.1016/j.jmb.2005.10.074</pub-id><?supplied-pmid 16318855?><pub-id pub-id-type="pmid">16318855</pub-id></element-citation></ref><ref id="CR61"><label>61.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pflugrath</surname><given-names>JW</given-names></name><name><surname>Wiegand</surname><given-names>G</given-names></name><name><surname>Huber</surname><given-names>R</given-names></name><name><surname>Vertesy</surname><given-names>L</given-names></name></person-group><article-title>Crystal structure determination, refinement and the molecular model of the alpha-amylase inhibitor Hoe-467A</article-title><source>J Mol Biol</source><year>1986</year><volume>189</volume><fpage>383</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1016/0022-2836(86)90520-6</pub-id><?supplied-pmid 3489104?><pub-id pub-id-type="pmid">3489104</pub-id></element-citation></ref><ref id="CR62"><label>62.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaver-Fay</surname><given-names>A</given-names></name><etal/></person-group><article-title>ROSETTA3: an object-oriented software suite for the simulation and design of macromolecules</article-title><source>Methods Enzymol</source><year>2011</year><volume>487</volume><fpage>545</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-381270-4.00019-6</pub-id><?supplied-pmid 21187238?><pub-id pub-id-type="pmid">21187238</pub-id></element-citation></ref><ref id="CR63"><label>63.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altschul</surname><given-names>SF</given-names></name><etal/></person-group><article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title><source>Nucleic Acids Res</source><year>1997</year><volume>25</volume><fpage>3389</fpage><lpage>3402</lpage><pub-id pub-id-type="doi">10.1093/nar/25.17.3389</pub-id><?supplied-pmid 9254694?><pub-id pub-id-type="pmid">9254694</pub-id></element-citation></ref><ref id="CR64"><label>64.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>HH</given-names></name><name><surname>Choe</surname><given-names>J</given-names></name><name><surname>Loeb</surname><given-names>LA</given-names></name></person-group><article-title>Protein tolerance to random amino acid change</article-title><source>Proc Natl Acad Sci USA</source><year>2004</year><volume>101</volume><fpage>9205</fpage><lpage>9210</lpage><pub-id pub-id-type="doi">10.1073/pnas.0403255101</pub-id><?supplied-pmid 15197260?><pub-id pub-id-type="pmid">15197260</pub-id></element-citation></ref><ref id="CR65"><label>65.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tokuriki</surname><given-names>N</given-names></name><name><surname>Stricher</surname><given-names>F</given-names></name><name><surname>Serrano</surname><given-names>L</given-names></name><name><surname>Tawfik</surname><given-names>DS</given-names></name></person-group><article-title>How protein stability and new functions trade off</article-title><source>PLoS Comput Biol</source><year>2008</year><volume>4</volume><fpage>e1000002</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000002</pub-id><?supplied-pmid 18463696?><pub-id pub-id-type="pmid">18463696</pub-id></element-citation></ref><ref id="CR66"><label>66.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lomize</surname><given-names>MA</given-names></name><name><surname>Lomize</surname><given-names>AL</given-names></name><name><surname>Pogozheva</surname><given-names>ID</given-names></name><name><surname>Mosberg</surname><given-names>HI</given-names></name></person-group><article-title>OPM: orientations of proteins in membranes database</article-title><source>Bioinformatics</source><year>2006</year><volume>22</volume><fpage>623</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btk023</pub-id><?supplied-pmid 16397007?><pub-id pub-id-type="pmid">16397007</pub-id></element-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">&#x02018;NACCESS&#x02019;, Computer Program (Department of Biochemistry and Molecular Biology, University College London., 1993).</mixed-citation></ref><ref id="CR68"><label>68.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frishman</surname><given-names>D</given-names></name><name><surname>Argos</surname><given-names>P</given-names></name></person-group><article-title>Knowledge-based protein secondary structure assignment</article-title><source>Proteins</source><year>1995</year><volume>23</volume><fpage>566</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1002/prot.340230412</pub-id><?supplied-pmid 8749853?><pub-id pub-id-type="pmid">8749853</pub-id></element-citation></ref></ref-list></back></article>