<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN" "JATS-archivearticle1-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 1?><front><journal-meta><journal-id journal-id-type="nlm-ta">J Cheminform</journal-id><journal-id journal-id-type="iso-abbrev">J Cheminform</journal-id><journal-title-group><journal-title>Journal of Cheminformatics</journal-title></journal-title-group><issn pub-type="epub">1758-2946</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">7035778</article-id><article-id pub-id-type="publisher-id">414</article-id><article-id pub-id-type="doi">10.1186/s13321-020-0414-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>A self-attention based message passing neural network for predicting molecular lipophilicity and aqueous solubility</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tang</surname><given-names>Bowen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kramer</surname><given-names>Skyler T.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Meijuan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Qiu</surname><given-names>Yingkun</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Wu</surname><given-names>Zhen</given-names></name><address><email>wuzhen@xmu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4809-0514</contrib-id><name><surname>Xu</surname><given-names>Dong</given-names></name><address><email>xudong@missouri.edu</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.12955.3a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2264 7233</institution-id><institution>Fujian Provincial Key Laboratory of Innovative Drug Target Research, School of Pharmaceutical Sciences, </institution><institution>Xiamen University, </institution></institution-wrap>Xiamen, 361000 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.134936.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2162 3504</institution-id><institution>Department of Electrical Engineering and Computer Science, Informatics Institute, and Christopher S. Bond Life Sciences Center, </institution><institution>University of Missouri, </institution></institution-wrap>Columbia, MO 65211 USA </aff></contrib-group><pub-date pub-type="epub"><day>21</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="pmc-release"><day>21</day><month>2</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>12</volume><elocation-id>15</elocation-id><history><date date-type="received"><day>4</day><month>10</month><year>2019</year></date><date date-type="accepted"><day>27</day><month>1</month><year>2020</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2020</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p></license></permissions><abstract id="Abs1"><p id="Par1">Efficient and accurate prediction of molecular properties, such as lipophilicity and solubility, is highly desirable for rational compound design in chemical and pharmaceutical industries. To this end, we build and apply a graph-neural-network framework called self-attention-based message-passing neural network (SAMPN) to study the relationship between chemical properties and structures in an interpretable way. The main advantages of SAMPN are that it directly uses chemical graphs and breaks the black-box mold of many machine/deep learning methods. Specifically, its attention mechanism indicates the degree to which each atom of the molecule contributes to the property of interest, and these results are easily visualized. Further, SAMPN outperforms random forests and the deep learning framework MPN from Deepchem. In addition, another formulation of SAMPN (Multi-SAMPN) can simultaneously predict multiple chemical properties with higher accuracy and efficiency than other models that predict one specific chemical property. Moreover, SAMPN can generate chemically visible and interpretable results, which can help researchers discover new pharmaceuticals and materials. The source code of the SAMPN prediction pipeline is freely available at Github (<ext-link ext-link-type="uri" xlink:href="https://github.com/tbwxmu/SAMPN">https://github.com/tbwxmu/SAMPN</ext-link>).</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Message passing network</kwd><kwd>Attention mechanism</kwd><kwd>Deep learning</kwd><kwd>Lipophilicity</kwd><kwd>Aqueous solubility</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000057</institution-id><institution>National Institute of General Medical Sciences</institution></institution-wrap></funding-source><award-id>GM126985</award-id><principal-award-recipient><name><surname>Xu</surname><given-names>Dong</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>US National Institutes of Health BD2K Training</institution></funding-source><award-id>T32LM012410</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2020</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Accurate and reliable prediction of molecular properties is an important ingredient in drug discovery and chemical material projects [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR3">3</xref>]. Characterizing quantitative structure-bioactivity/structure&#x02013;property relationships (QSAR/QSPR) of compounds has always been a hot topic in medicinal and material chemistry [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>], but such relationships are usually difficult to elucidate with heuristic rules or empirical measurements. Machine learning (ML) methods, such as random forests (RF) and support vector machines (SVM), have aided the discovery process of new chemical drugs and materials [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>]. For example, random forests models with atom pair descriptors have been used by many pharmaceutical companies to construct QSAR models [<xref ref-type="bibr" rid="CR7">7</xref>], and Bayesian optimization models have been used to design nanostructures for phonon transport [<xref ref-type="bibr" rid="CR8">8</xref>]. More recently, however, neural-network-based methods have greatly accelerated this field and will be briefly discussed below.</p><p id="Par3">Many ML methods first convert chemical molecules into a computer-interpretable representation, utilizing physicochemical properties from experimental/computational measurements [<xref ref-type="bibr" rid="CR9">9</xref>] or by using molecular fingerprints. Physiochemical properties include mass, charge, refractivity, and many other physical features of the molecules. The most widely used molecular conversion, however, is the molecular fingerprint, which encodes a molecular structure into a series of binary digits (a bit vector) [<xref ref-type="bibr" rid="CR10">10</xref>] based on substructures that may or may not be pre-defined, depending on the class of fingerprints being used. For example, extended-connectivity fingerprints (ECFP) can split one molecule into many substructures (not pre-defined) and encode all of them into just one bit vector with different identifiers [<xref ref-type="bibr" rid="CR11">11</xref>]. ENREF_10 Alternatively, bit vectors may be extended into count vectors that indicate the number of each substructure found in the molecule, not just its presence/absence.</p><p id="Par4">Compared to the previously-mentioned traditional methods, artificial neural networks (ANNs) have become increasingly popular in predicting molecular properties. For example, a three-layered ANN with E-state indices was used to predict aqueous solubility of organic molecules [<xref ref-type="bibr" rid="CR15">15</xref>]. More recently, graph-based networks were applied to predict lipophilicity and solubility [<xref ref-type="bibr" rid="CR16">16</xref>]. These network-based models have shown impressive results and made good contributions for developing new methods.</p><p id="Par5">Fixed fingerprint feature extraction rules of molecules are useful to accurately reflect underlying chemical substructures, though these may not be the best-suited representation for all tasks. Hence, researchers have to spend much time and effort to carefully determine which features are most relevant to their models. This is especially problematic with the utilization of physical features, which may require advanced variable selection techniques or a high-level of empirical knowledge. In contrast, some deep learning networks based on the simplified molecular input line entry system (SMILES) [<xref ref-type="bibr" rid="CR12">12</xref>] codes can automatically learn the molecular features [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. However, this may cause the model to focus on the SMILES grammar and not the implicated molecular structure. This limitation of the SMILES-based deep learning models is hard to avoid as the SMILES representation is not designed to capture molecular similarity. Generally, molecules with similar chemical structures can be encoded into very different SMILES strings. Even for the same molecular structure, there are often non-unique SMILES strings as Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>A displays. Though the process of generating canonical SMILES is well known, the process is inconsistent among chemical toolkits. For example, the &#x02018;canonical&#x02019; SMILES code for caffeine is CN1C=NC2=C1C(=O)N(C)C(=O)N2C according to RDKit, Cn1cnc2c1c(=O)n(C)c(=O)n2C according to Obabel, and CN1C=NC2=C1C(=O)N(C(=O)N2C)C according to PubChem.<fig id="Fig1"><label>Fig.&#x000a0;1</label><caption><p>Conversion of a chemical structure into a mathematical graph. <bold>a</bold> A chemical structure usually has a unique graph but multiple SMILES strings. <bold>b</bold> Relationship list between node indices and edge indices, which are converted from the chemical graph. <bold>c</bold> The lists of Node2Edge, Edge2Node, Edge2Revedge and Node2NeiNode, derived from (<bold>b</bold>)</p></caption><graphic xlink:href="13321_2020_414_Fig1_HTML" id="MO1"/></fig></p><p id="Par6">Using the natural chemical graph instead of the SMILES representation may be more suitable for chemical property predictions. Briefly, a graph consists of nodes and edges that connect two or more nodes to one another. Analogously, a chemical graph considers atoms as nodes and bonds as the edges connecting atoms to one another. Our formulation considers these edges as bidirectional, meaning that the bond connecting atom A to atom B is the same as the bond connecting atom B to atom A. An example chemical graph can be seen in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a.</p><p id="Par7">Essential chemical properties such as molecular validity are more easily represented in two-dimensional chemical graphs than linear SMILES. Unlike SMILES codes chemical graphs are invariant to molecule permutations, i.e., one molecular structure has one graph but multiple SMILES representations. Recently, graph-based deep learning models are reported in QSAR and QSPR studies [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR17">17</xref>&#x02013;<xref ref-type="bibr" rid="CR21">21</xref>]. However, according to these references, predictions are difficult to interpret, since most neural networks act as black boxes [<xref ref-type="bibr" rid="CR22">22</xref>].</p><p id="Par8">In this paper, we describe a self-attention-based message-passing neural network (SAMPN) model, which is a modification of Deepchem&#x02019;s MPN [<xref ref-type="bibr" rid="CR16">16</xref>] and is state-of-the-art in deep learning. It directly learns the most relevant features of each QSAR/QSAPR task in the learning process and assigns the degree of importance for substructures to improve the interpretability of prediction. Our SAMPN graph network utilizes the chemical graph structure described above, where each edge is derived from the chemical bond and each atom is the node. Both our message passing neural network (MPN) and SAMPN model can be used as multi-target models (Multi-MPN or Multi-SAMPN), which can learn not only the relationship between chemical structures and properties, but also the relationship between intrinsic attributes of molecules. To demonstrate our computational methods, we chose lipophilicity and aqueous solubility as the target properties as they were very important chemical descriptors that pervade every aspect of bioactivity, drug metabolism and pharmacokinetic (DMPK) profiles [<xref ref-type="bibr" rid="CR23">23</xref>].</p><p id="Par9">To our knowledge, this is the first time that a model like SAMPN has been used to predict chemical properties from experimental data for QSPR studies. The results from our experiments demonstrate that our SAMPN network yields superior performance relative to traditional ML-based models and previous deep-learning models (i.e., Deepchem&#x02019;s MPN [<xref ref-type="bibr" rid="CR16">16</xref>]). Furthermore, the predictions of SAMPN are easily understood and visualized, since the integrated attention mechanism can color the atoms of the molecule based on their contributions to the property of interest.</p></sec><sec id="Sec2"><title>Methods and materials</title><sec id="Sec3"><title>Datasets and data process</title><p id="Par10">Datasets of molecular lipophilicity and aqueous solubility were used for developing and testing our method. Lipophilicity is usually quantified by the <italic>n</italic>-octanol/water partition coefficient P and preferentially displayed in a logarithmic form as logP. The raw lipophilicity data was downloaded from CHEMBL3301361 deposited by AstraZeneca [<xref ref-type="bibr" rid="CR24">24</xref>] and includes 4200 molecules. Aqueous solubility is the saturated concentration of the chemical in the aqueous phase, which is usually displayed with unit log(mol/L) and is represented as logS. This dataset was downloaded from the online chemical database and modeling environment (OCHEM) [<xref ref-type="bibr" rid="CR25">25</xref>] and includes 1311 experimental records. The dataset distributions are plotted in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S1.</p><p id="Par11">As both datasets are small relative to the typical size requirements of deep learning models, we use tenfold stratified cross-validation [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR35">35</xref>], where each dataset was randomly split into a training and validation set (80% and 10%, respectively) for parameter selection and a test dataset (10%) for model comparisons. Then, we repeated all experiments three times with different random seeds. This process ensures that the model does not simply memorize the training and is capable of generalizing to new molecules.</p><p id="Par12">For the initial data preprocessing, duplicate molecules were removed so that each chemical structure in the data was unique, while the maximum one of the related properties was kept. Molecules unrecognized by RDkit (version 2019.3) [<xref ref-type="bibr" rid="CR26">26</xref>], a cheminformatics toolkit implemented in Python, were also deleted. Only two columns (&#x02018;smiles&#x02019; and &#x02018;experimental value&#x02019;) were kept as the input data to our models. Each downloaded SMILES representation was then converted into a directed graph before training the SAMPN model using the MPN encoder, which was adapted from Deepchem and Chemprop [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. The directed graphs were mainly composed of index lists of nodes and edges shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c. Take the substructure of N&#x02013;C as an example: a chemical bond between the N and C atoms can derive two edges (C:0&#x02009;&#x02192;&#x02009;N:1 and N:0&#x02009;&#x02192;&#x02009;C:1). The number of nodes is equal to the number of atoms and the number of edges is always double the number of bonds, since we consider edges to be bidirectional.</p></sec><sec id="Sec4"><title>Message passing network encoder</title><p id="Par13">Instead of manually selected features, using molecular graph structures directly was first reported in 1994 [<xref ref-type="bibr" rid="CR29">29</xref>]. In recent years, graph-based methods have been used to analyze various aspects of chemical systems [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR30">30</xref>] and compare with fingerprints [<xref ref-type="bibr" rid="CR31">31</xref>]. Graph-based models provide a natural way to describe chemical molecules, where atoms in the molecule are equivalent to nodes and chemical bonds to the edges in a graph. The message-passing network is a variant of the graph-theoretical approaches, which gradually merges information from distant atoms by extending radially through bonds as displayed in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. Those passing messages were used to encode all substructures of a molecule by an adaptive learning approach, which extracts useful representations of molecules suited to the target predictions.<fig id="Fig2"><label>Fig.&#x000a0;2</label><caption><p>Representation of SAMPN architecture. The main part of the MPN encoder converts the neighbor features to a molecule matrix, then followed by a self-attention layer and fully connected networks to make a final prediction</p></caption><graphic xlink:href="13321_2020_414_Fig2_HTML" id="MO2"/></fig></p><p id="Par14">The message passing network encoder works as follows in Eqs.&#x000a0;(<xref rid="Equ1" ref-type="">1</xref>&#x02013;<xref rid="Equ3" ref-type="">3</xref>). The passing message <italic>M</italic> from atom <italic>x</italic> to atom <italic>y</italic> in the <italic>d</italic>-th iteration (message passing depth) is calculated as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_{xy}^{d = 1} = Re\left( {W_{inp} \cdot f_{x} f_{y} } \right)$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">xy</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">inp</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13321_2020_414_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_{xy}^{d &#x0003e; 1} = Re\left( {W_{inp} \cdot f_{x} f_{y} + W_{h} \sum\limits_{z \in N\left( x \right)\backslash y} {M_{zx}^{d - 1} } } \right)$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">xy</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">inp</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mrow><mml:mo stretchy="true">\</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">zx</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13321_2020_414_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par15">Here, <italic>Re</italic> is the activation function (Relu). <inline-formula id="IEq1"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{inp}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">inp</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{h}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>W</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq2.gif"/></alternatives></inline-formula> are the learned weight matrices. As we use the edge-dependent neural network to pass a message, node feature <italic>f</italic><sub><italic>x</italic></sub> is concatenated with edge feature <italic>f</italic><sub><italic>xy</italic></sub> to form the merged node-edge feature <italic>f</italic><sub><italic>x</italic></sub><italic>f</italic><sub><italic>xy</italic></sub>. Node feature <italic>f</italic><sub><italic>x</italic></sub>, is derived by atom type, formal charge, valence, and aromaticity. Similarly, edge feature <italic>f</italic><sub><italic>xy</italic></sub> is derived from bond order, ring status and direction connection. The definitions of node <italic>f</italic><sub><italic>x</italic></sub> features and edge <italic>f</italic><sub><italic>xy</italic></sub> features are displayed in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>. The initial message <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$M_{xy}^{d = 1}$$\end{document}</tex-math><mml:math id="M10"><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">xy</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq3.gif"/></alternatives></inline-formula>, which <italic>x</italic> sends to <italic>y</italic>, is generated from the merged node-edge feature <italic>f</italic><sub><italic>x</italic></sub><italic>f</italic><sub><italic>xy</italic></sub> by a neural network as described in Eq.&#x000a0;(<xref rid="Equ1" ref-type="">1</xref>).<table-wrap id="Tab1"><label>Table&#x000a0;1</label><caption><p>Descriptions of node and edge features</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Attribute</th><th align="left">Description</th><th align="left">Dimension</th></tr></thead><tbody><tr><td align="left" colspan="3">Node</td></tr><tr><td align="left">&#x000a0;Atom type</td><td align="left">All currently known chemical elements</td><td char="." align="char">118</td></tr><tr><td align="left">&#x000a0;Degree</td><td align="left">Number of heavy atom neighbors</td><td char="." align="char">6</td></tr><tr><td align="left">&#x000a0;Formal charge</td><td align="left">Charge assigned to an atom (&#x02212;&#x02009;2, &#x02212;&#x02009;1, 0, 1, 2)</td><td char="." align="char">5</td></tr><tr><td align="left">&#x000a0;Chirality label</td><td align="left">R, S, unspecified and unrecognized type of chirality</td><td char="." align="char">4</td></tr><tr><td align="left">&#x000a0;Hybridization</td><td align="left">sp, sp<sup>2</sup>, sp<sup>3</sup>, sp<sup>3</sup>d, or sp<sup>3</sup>d<sup>2</sup></td><td char="." align="char">5</td></tr><tr><td align="left">&#x000a0;Aromaticity</td><td align="left">Aromatic atom or not</td><td char="." align="char">1</td></tr><tr><td align="left" colspan="3">Edge</td></tr><tr><td align="left">&#x000a0;Bond type</td><td align="left">Single, double, triple, or aromatic</td><td char="." align="char">4</td></tr><tr><td align="left">&#x000a0;Ring</td><td align="left">Whether the bond is in a ring</td><td char="." align="char">1</td></tr><tr><td align="left">&#x000a0;Bond stereo</td><td align="left">Nature of the bond&#x02019;s stereochemistry (none, any, Z, E, cis, or trans)</td><td char="." align="char">6</td></tr></tbody></table></table-wrap></p><p id="Par16">In a chemical graph, atoms denote the node set <italic>x&#x02208;V,</italic> and bonds denote the edge set <italic>(x,y)&#x02208;E.</italic> Each edge has its own direction in the SAMPN model. <italic>N(x)</italic> or <italic>N(y)</italic> stands for the group of neighbor nodes of <italic>x</italic> or <italic>y</italic>, respectively. <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z \in N\left( x \right)\backslash y$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mrow><mml:mo stretchy="true">\</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq4.gif"/></alternatives></inline-formula> means the neighbors of <italic>x</italic> do not contain <italic>y</italic>. Node <italic>x</italic> is allowed to send a message to a neighbor node <italic>y</italic> only after node <italic>x</italic> has received messages from all neighbor nodes except <italic>y</italic>. We use the skip connection in the message passing steps as in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> (displayed in between neighbor features and self-features). This skip connection allows the message to pass a very long distance without vanishing gradient problem when using backpropagation. The generated messages exchange and update based on the merged node-edge feature and the previous message passing step as Eq.&#x000a0;(<xref rid="Equ2" ref-type="">2</xref>) defined.</p><p id="Par17">The latent vector <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{y}$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>h</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq5.gif"/></alternatives></inline-formula> of each node, take Node 2&#x02019;s latent vector <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{2}$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>h</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq6.gif"/></alternatives></inline-formula> as an example in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>, is obtained by aggregating its neighbor messages in Eq.&#x000a0;(<xref rid="Equ3" ref-type="">3</xref>) after the message-passing process:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{y} = Re\left( {W_{o} \left( {W_{ah} \cdot f_{y} + \sum\limits_{z \in N\left( y \right)} {M_{zy}^{d} } } \right)} \right)$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">ah</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo movablelimits="false">&#x02211;</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:mi>y</mml:mi></mml:mfenced></mml:mrow></mml:munder><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:mi mathvariant="italic">zy</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13321_2020_414_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where, <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{y}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>h</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq7.gif"/></alternatives></inline-formula> captures the local chemical structure features based on the passing depth, and <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{o}$$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mi>W</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{ah}$$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">ah</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13321_2020_414_Article_IEq9.gif"/></alternatives></inline-formula> are the learned weight matrices. More detailed information of SAMPN algorithm can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Table S1 in Supporting Materials. Applying the above Eqs.&#x000a0;(<xref rid="Equ1" ref-type="">1</xref>&#x02013;<xref rid="Equ3" ref-type="">3</xref>) on a chemical graph generates the final graph representation <italic>G</italic>&#x02009;=&#x02009;{<italic>h</italic><sub><italic>1</italic></sub> &#x02026; <italic>h</italic><sub><italic>i</italic></sub> &#x02026; <italic>h</italic><sub><italic>n</italic></sub>}, which combines with the self-attention mechanism and fully-connected neural networks to make the final prediction.</p></sec><sec id="Sec5"><title>Self-attention mechanism</title><p id="Par18">All hidden states of a node are directly combined into a single vector, which may not make the difference among the learned features explainable [<xref ref-type="bibr" rid="CR32">32</xref>]. A better way is to apply the attention mechanism to obtain a context vector for the target node by focusing on its neighbors and local environment. Take Node 2 as an example (the blue node in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>), after several message passing steps, Node 2 has hidden state h<sub>2</sub>, which represents the substructure centered at Atom 2. Meanwhile, all the rest nodes have the same process and h<sub>n</sub> represents the substructure centered at Atom n. Since different substructures have different contribution to the molecular property, we can use the attention mechanism to capture the different influences of substructures in contributing to the target molecular property.</p><p id="Par19">A self-attention layer is then added to identify the relationship between the substructure contribution to the target property of a molecule. A dot-product attention algorithm was implemented to take the whole molecular graph representation G as the input. The self-attentive weighted molecule graph embedding can be formed as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_{att} = softmax\left( {G \cdot G^{T} } \right)$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">att</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>G</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mi>G</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="13321_2020_414_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E_{G} = W_{att} \cdot G$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi mathvariant="italic">att</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:math><graphic xlink:href="13321_2020_414_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>W</italic><sub><italic>att</italic></sub> is the self-attention score that implicitly indicates the contribution of local chemical graph to the target property. As <italic>G</italic>&#x02009;=&#x02009;{<italic>h</italic><sub><italic>1</italic></sub> &#x02026; <italic>h</italic><sub><italic>i</italic></sub> &#x02026; <italic>h</italic><sub><italic>n</italic></sub>}, each row of <italic>W</italic><sub><italic>att</italic></sub> is the attention weight between the <italic>i</italic>-th atom and the rest atoms in the molecule. <italic>E</italic><sub><italic>G</italic></sub> is the attentive embedding matrix, where each row corresponds to the attention weighted hidden vector of the node. Then, the global average pooling is used on the sum of <italic>G</italic> and <italic>E</italic><sub><italic>G</italic></sub> to get the molecule latent vector as Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows in the purple rectangle. Finally, the latent vector is combined with several layers of fully connected networks for the target property prediction.</p></sec><sec id="Sec6"><title>Model training and hyperparameter optimization</title><p id="Par20">The code for the MPN encoder was mainly adapted from Deepchem and Chemprop [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. Both the MPN encoder and self-attention mechanism were implemented with Python and Pytorch version 1.0, an open-source framework for deep learning [<xref ref-type="bibr" rid="CR33">33</xref>]. MPN, Multi-MPN, SAMPN and Multi-SAMPN models were trained with the Adam optimizer using the same learning rate schedule in [<xref ref-type="bibr" rid="CR34">34</xref>].</p><p id="Par21">Multiple metrics were used to evaluate the performance of our models: mean absolute error (MAE), root mean squared error (RMSE), mean squared error (MSE), coefficient of determination (R<sup>2</sup>) and Pearson correlation coefficient (PC). Lower values of MAE, MSE, and RMSE indicate better predictive performance. Conversely, higher values for PC and R<sup>2</sup> indicate better models or better fits for the data. While some of these metrics tell the same story, the inclusion of all of these values may provide a rich benchmark for future studies.</p><p id="Par22">A grid search algorithm was used to adjust the hyperparameters with Hyperopt package version 0.1.2 [<xref ref-type="bibr" rid="CR35">35</xref>]. Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref> shows the hyperparameters to be optimized and the search space. We chose RMSE on the validation set as the metric to find the most suitable combination of the hyperparameters within the search space. In the lipophilicity-QSPR task, one of the best combinations of hyperparameters was {&#x02018;activation&#x02019;: &#x02018;ReLU&#x02019;; &#x02018;depth&#x02019;: 4; &#x02018;dropout&#x02019;: 0.25; &#x02018;layers of fully connected networks&#x02019;: 2; &#x02018;hidden size&#x02019;: 384}. All the message passing neural network models (MPN, SAMPN, Multi-MPN and Multi-SAMPN) utilized the above hyperparameters to test the final performance with using the tenfold stratified cross-validation on the whole dataset.<table-wrap id="Tab2"><label>Table&#x000a0;2</label><caption><p>Hyperparameters optimization for MPN and SAMPN</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">MPN and SAMPN</th><th align="left">Hyperparameters</th><th align="left">Range (interval)</th></tr></thead><tbody><tr><td align="left"/><td align="left">Activation function</td><td align="left">Tanh, ELU, LeakyReLU ReLU, PReLU, SELU</td></tr><tr><td align="left"/><td align="left">Steps of message passing</td><td align="left">2&#x02013;6 (1)</td></tr><tr><td align="left"/><td align="left">Graph embedding size</td><td align="left">32&#x02013;512 (32)</td></tr><tr><td align="left"/><td align="left">Dropout rate</td><td align="left">0.0&#x02013;0.4 (0.05)</td></tr><tr><td align="left"/><td align="left">Layers of fully connected network</td><td align="left">1&#x02013;3 (1)</td></tr></tbody></table></table-wrap></p><p id="Par23">In addition to using the published results from Deepchem&#x02019;s MPN, we also built a pure MPN model to establish a baseline without the self-attention and all the rest configurations were kept the same to SAMPN. To compare the single-task and multi-target based deep learning network, we built the multi-MPN and multi-SAMPN. The multi-target-based model used a merged molecule dataset from &#x02018;Lipophilicity&#x02019; and &#x02018;Water Solubility&#x02019; as described in Supporting Materials. All the used parameters were kept the same between MPN and SAMPN.</p></sec><sec id="Sec7"><title>Random forest</title><p id="Par24">To compare our SAMPN method with the traditional machine learning methods, we chose a random forest model as the baseline. Random forest (RF) [<xref ref-type="bibr" rid="CR36">36</xref>] is a supervised learning algorithm with an ensemble of decision trees generated from a bootstrapped (bagged) sampling of compounds and features. It is widely used in the traditional structure&#x02013;property relation research [<xref ref-type="bibr" rid="CR37">37</xref>], and was considered as a &#x0201c;gold standard&#x0201d; according to its robustness, easy usage and high prediction accuracy in structure&#x02013;property relationship research [<xref ref-type="bibr" rid="CR38">38</xref>]. Here, the ECFP with a fixed length of 1024 [<xref ref-type="bibr" rid="CR12">12</xref>] was used with the RF model, which was implemented in Python 3.6.3 [<xref ref-type="bibr" rid="CR39">39</xref>] with the package Scikit-learn, version 0.21.2 [<xref ref-type="bibr" rid="CR40">40</xref>]. For the RF model, more trees generally increase performance and make predictions more stable, but it also slows down the computation heavily. We set 500 trees for a good balance point as suggested in [<xref ref-type="bibr" rid="CR16">16</xref>] for most QSPR studies.</p></sec></sec><sec id="Sec8"><title>Results and discussion</title><sec id="Sec9"><title>Lipophilicity and solubility prediction</title><p id="Par25">In each QSPR task, we built RF, MPN, SAMPN, multi-MPN and multi-SAMPN models to explore the relationship between the target property and the molecular structure. For the lipophilicity prediction, both single-target based and multiple-target based model have good performance according to RMSE (SAMPN: 0.579&#x02009;&#x000b1;&#x02009;0.036; Multi-SAMPN: 0.571&#x02009;&#x000b1;&#x02009;0.032). Without the self-attention mechanism, the performance of MPN decreased as Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> and Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a show. Nevertheless, the result of our new formulation of the MPN or Multi-MPN was still much better than the one from the MPN version of Deepchem (0.719&#x02009;&#x000b1;&#x02009;0.031) [<xref ref-type="bibr" rid="CR16">16</xref>], and performance increased even more with the inclusion of the attention mechanism. Our MPN model is different from Deepchem in that we did not use any recurrent neural networks (RNN) in our network architecture, which improved the speed of our MPN model in training.<table-wrap id="Tab3"><label>Table&#x000a0;3</label><caption><p>Models&#x02019; performance (root-mean-square error) on lipophilicity database</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset (size)</th><th align="left">Model</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left" rowspan="7">Lipophilicity (4200)</td><td align="left">RF</td><td char="&#x000b1;" align="char">0.824&#x02009;&#x000b1;&#x02009;0.041</td></tr><tr><td align="left">MPN (Deepchem)<sup>a</sup></td><td char="&#x000b1;" align="char">0.630&#x02009;&#x000b1;&#x02009;0.059</td></tr><tr><td align="left">MPN (Deepchem)<sup>b</sup></td><td char="&#x000b1;" align="char">0.652&#x02009;&#x000b1;&#x02009;0.061</td></tr><tr><td align="left">MPN</td><td char="&#x000b1;" align="char">0.630&#x02009;&#x000b1;&#x02009;0.059</td></tr><tr><td align="left">SAMPN</td><td char="&#x000b1;" align="char">0.579&#x02009;&#x000b1;&#x02009;0.036</td></tr><tr><td align="left">Multi-MPN</td><td char="&#x000b1;" align="char">0.594&#x02009;&#x000b1;&#x02009;0.039</td></tr><tr><td align="left"><italic>Multi-SAMPN</italic></td><td char="&#x000b1;" align="char"><italic>0.571&#x02009;&#x000b1;&#x02009;0.032</italic></td></tr><tr><td align="left" rowspan="7">Water solubility (1311)</td><td align="left">RF</td><td char="&#x000b1;" align="char">1.096&#x02009;&#x000b1;&#x02009;0.092</td></tr><tr><td align="left">MPN (Deepchem-1128)<sup>a</sup></td><td char="&#x000b1;" align="char">0.580&#x02009;&#x000b1;&#x02009;0.030</td></tr><tr><td align="left">MPN (Deepchem)<sup>b</sup></td><td char="&#x000b1;" align="char">0.676&#x02009;&#x000b1;&#x02009;0.022</td></tr><tr><td align="left">MPN</td><td char="&#x000b1;" align="char">0.694&#x02009;&#x000b1;&#x02009;0.050</td></tr><tr><td align="left">SAMPN</td><td char="&#x000b1;" align="char">0.688&#x02009;&#x000b1;&#x02009;0.057</td></tr><tr><td align="left">Multi-MPN</td><td char="&#x000b1;" align="char">0.674&#x02009;&#x000b1;&#x02009;0.074</td></tr><tr><td align="left"><italic>Multi-SAMPN</italic></td><td char="&#x000b1;" align="char"><italic>0.661&#x02009;&#x000b1;&#x02009;0.063</italic></td></tr></tbody></table><table-wrap-foot><p>Italics represents the best performance in the results</p><p><sup>a</sup>Values were reported in [<xref ref-type="bibr" rid="CR16">16</xref>]. In the lipophilicity prediction, we use the same dataset with Deepchem. In the water solubility prediction, our used dataset is larger than Deepchem used (1128 molecules)</p><p><sup>b</sup>Values were calculated from the same data and the same stratified cross-validation protocol in our work</p></table-wrap-foot></table-wrap><fig id="Fig3"><label>Fig.&#x000a0;3</label><caption><p>Models&#x02019; performance on lipophilicity (<bold>a</bold>, <bold>c</bold>) and aqueous solubility (<bold>b</bold>, <bold>d</bold>) with the same tenfold stratified cross-validation. Error bars represent standard deviations</p></caption><graphic xlink:href="13321_2020_414_Fig3_HTML" id="MO8"/></fig></p><p id="Par26">For the solubility prediction, message passing based networks also greatly improved the performance over the traditional model (RF) as displayed in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref> and Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b. The MPN from Deepchem also displayed a good performance (0.580&#x02009;&#x000b1;&#x02009;0.030 RMSD) on the water solubility prediction [<xref ref-type="bibr" rid="CR16">16</xref>]; however, they used a small water solubility dataset (1128 molecules). For comparison purposes, we used their default setting MPN (Deepchem) on our water solubility data (1311 molecules) with our stratified cross-validation protocol. The scripts of the detail calculation process were available in our Github repository. After that, MPN (Deepchem) shows similar performance (0.676&#x02009;&#x000b1;&#x02009;0.022) with our MPN (0.694&#x02009;&#x000b1;&#x02009;0.050). Based on our model results (performance: SAMPN&#x02009;&#x0003e;&#x02009;MPN; Multi-SAMPN&#x02009;&#x0003e;&#x02009;Multi-MPN), the self-attention mechanism can improve the performance of message passing neural networks in both lipophilicity and solubility prediction. And multi-target models have better performance than single task-based model (performance: Multi-SAMPN&#x02009;&#x0003e;&#x02009;SAMPN; Multi-MPN&#x02009;&#x0003e;&#x02009;MPN). In our study, no matter choosing which metric as Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref> displayed, we can get the same conclusion as we motioned above. The better predictive performance of the multi-target model is probably from the benefit that Multi-SAMPN or Multi-MPN can use the learned feature from lipophilicity to help the solubility prediction, and vice versa. It is worth mentioning that Multi-SAMPN or Multi-MPN can predict the lipophilicity and aqueous solubility simultaneously rather than step by step prediction like SAMPN or MPN. Although our case indicates that a multi-target model performs better than the single-target model, it requires more studies to show whether this is general, since our case only used only one lipophilicity and water solubility dataset.</p></sec><sec id="Sec10"><title>Visualize the attention</title><p id="Par27">While higher prediction accuracy is always desirable, the ability to interpret a QSPR model is also important. Model comparison and interpretation can be facilitated by a visualization technique, making it possible to identify the learned features that drive compound property predictions. In the SAMPN model, we can obtain the attention weight scores from the self-attention mechanism. For a specific molecule, we obtain the difference between each atom&#x02019;s weight score and the average attention weight of the molecule. We define the above difference as the attention coefficient of each atom and those attention weight coefficients are very useful to gain insight into which parts of a molecule increase the target molecular property and which decrease it.</p><p id="Par28">By using heat map coloring on each molecule (such as in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>a&#x02013;f), it is easy to see which parts of molecule play a more important role in the lipophilicity or water solubility of molecule. The lipophilicity and solubility heat maps are helpful for chemists to optimize the lipophilicity and solubility of a particular molecule. Consider Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>b, a depiction of 1H-indazole after using our model. This molecule has a relatively high lipophilicity, as it has a large &#x003c0;-electron-conjugated system in its fused aromatic ring. However, the nitrogen-containing section of the molecule displays strong anti-lipophilic properties relative to the rest of the molecule. This may, in part, be due to nitrogen&#x02019;s contribution (as &#x02018;N&#x02019; or &#x02018;NH&#x02019;) to a hydrogen bonding-network with its surroundings. Thus, altering 1H-indazole to disrupt that potential network may increase the molecule&#x02019;s lipophilicity. To test this hypothesis, we used SAMPN to predict the lipophilicity of benzo[<italic>d</italic>]isothiazole (Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Fig. S2), the molecule made by exchanging the &#x02018;NH&#x02019; of 1H-indazole with &#x02018;S&#x02019; (sulfur). As expected, this change did increase the molecule&#x02019;s lipophilicity. Another example is the primary amine group in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>f, which can easily form hydrogen bonds with water molecules. This is reflected in red for a predicted soluble feature.<fig id="Fig4"><label>Fig.&#x000a0;4</label><caption><p>Heat map molecule coloring on lipophilicity (<bold>a</bold>&#x02013;<bold>c</bold>) and solubility (<bold>d</bold>&#x02013;<bold>f</bold>). <bold>a</bold>&#x02013;<bold>c</bold> Red indicates a predicted anti-lipophilic feature and blue indicates a predicted lipophilic feature. <bold>d</bold>&#x02013;<bold>f</bold> Red indicates a predicted soluble feature and blue indicates a predicted anti-soluble feature</p></caption><graphic xlink:href="13321_2020_414_Fig4_HTML" id="MO9"/></fig></p></sec></sec><sec id="Sec11"><title>Conclusions</title><p id="Par29">In this work, we have proposed a self-attention-based message passing neural network for identifying the relationship between molecular lipophilicity/solubility and structure. Our SAMPN model outperforms the conventional random forests and the previous graph neural network-based model. By applying the attention mechanism, SAMPN can provide some insights on the atomic sources of lipophilicity/solubility, which is different from black box approaches that most machine learning and deep learning methods used. The results from SAMPN are easy to understand by coloring the attention scores directly on the molecular graph, which is useful as a guide to adjust the lipophilicity or solubility of one molecule. In addition, our message-passing neural networks can be easily trained as a multi-target model, which makes it better in computational efficiency and predictive performance. With this approach and our case study, we believe our method can be applied to other quantitative structure&#x02013;property relationship studies and help chemists optimize molecular properties directly from the chemical structures.</p></sec><sec sec-type="supplementary-material"><title>Supplementary information</title><sec id="Sec12"><p>
<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="13321_2020_414_MOESM1_ESM.docx"><caption><p><bold>Additional file 1.</bold> The Supporting Information can be found in the supporting documents. The source code and the prepared datasets are available in the SAMPN Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/tbwxmu/SAMPN">https://github.com/tbwxmu/SAMPN</ext-link>).</p></caption></media></supplementary-material>
</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher's Note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s13321-020-0414-z.</p></sec><ack><title>Acknowledgements</title><p>The authors acknowledge the group of the State Key Laboratory of Physical Chemistry of Solid Surfaces at Xiamen University, for the use of their high-performance computing resources.</p></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>BT and DX designed the study. BT implemented the models and wrote the manuscript. All authors contributed to the interpretation of results. All authors reviewed and edited the manuscript. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This work was funded in part by the program of China Scholarships Council No. 201806310017 and the US National Institutes of Health Grant R35-GM126985.&#x000a0;SK&#x02019;s effort was funded by&#x000a0;US National Institutes of Health BD2K&#x000a0;Training&#x000a0;Grant&#x000a0;T32LM012410.</p></notes><notes notes-type="data-availability"><title>Availability of data and materials</title><p>All data sets and code are available at GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/tbwxmu/SAMPN">https://github.com/tbwxmu/SAMPN</ext-link>.</p></notes><notes notes-type="COI-statement"><title>Competing interests</title><p id="Par30">The authors declare that they have no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>K</given-names></name><name><surname>Biegler</surname><given-names>F</given-names></name><name><surname>Ramakrishnan</surname><given-names>R</given-names></name><name><surname>Pronobis</surname><given-names>W</given-names></name><name><surname>Von Lilienfeld</surname><given-names>OA</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>K-R</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name></person-group><article-title>Machine learning predictions of molecular properties: accurate many-body potentials and non-locality in chemical space</article-title><source>J Phys Chem Lett</source><year>2015</year><volume>6</volume><fpage>2326</fpage><lpage>2331</lpage><pub-id pub-id-type="doi">10.1021/acs.jpclett.5b00831</pub-id><pub-id pub-id-type="pmid">26113956</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherkasov</surname><given-names>A</given-names></name><name><surname>Muratov</surname><given-names>EN</given-names></name><name><surname>Fourches</surname><given-names>D</given-names></name><name><surname>Varnek</surname><given-names>A</given-names></name><name><surname>Baskin</surname><given-names>II</given-names></name><name><surname>Cronin</surname><given-names>M</given-names></name><name><surname>Dearden</surname><given-names>J</given-names></name><name><surname>Gramatica</surname><given-names>P</given-names></name><name><surname>Martin</surname><given-names>YC</given-names></name><name><surname>Todeschini</surname><given-names>R</given-names></name></person-group><article-title>Qsar modeling: where have you been? Where are you going to?</article-title><source>J Med Chem</source><year>2014</year><volume>57</volume><fpage>4977</fpage><lpage>5010</lpage><pub-id pub-id-type="doi">10.1021/jm4004285</pub-id><pub-id pub-id-type="pmid">24351051</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>H</given-names></name><name><surname>Engkvist</surname><given-names>O</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Olivecrona</surname><given-names>M</given-names></name><name><surname>Blaschke</surname><given-names>T</given-names></name></person-group><article-title>The rise of deep learning in drug discovery</article-title><source>Drug Discov Today</source><year>2018</year><volume>23</volume><fpage>1241</fpage><lpage>1250</lpage><?supplied-pmid 29366762?><pub-id pub-id-type="pmid">29366762</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>T</given-names></name><name><surname>Epa</surname><given-names>VC</given-names></name><name><surname>Burden</surname><given-names>FR</given-names></name><name><surname>Winkler</surname><given-names>DA</given-names></name></person-group><article-title>Quantitative structure-property relationship modeling of diverse materials properties</article-title><source>Chem Rev</source><year>2012</year><volume>112</volume><fpage>2889</fpage><lpage>2919</lpage><pub-id pub-id-type="doi">10.1021/cr200066h</pub-id><pub-id pub-id-type="pmid">22251444</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>G&#x000f3;mez-Bombarelli</surname><given-names>R</given-names></name><name><surname>Aguilera-Iparraguirre</surname><given-names>J</given-names></name><name><surname>Hirzel</surname><given-names>TD</given-names></name><name><surname>Duvenaud</surname><given-names>D</given-names></name><name><surname>Maclaurin</surname><given-names>D</given-names></name><name><surname>Blood-Forsythe</surname><given-names>MA</given-names></name><name><surname>Chae</surname><given-names>HS</given-names></name><name><surname>Einzinger</surname><given-names>M</given-names></name><name><surname>Ha</surname><given-names>D-G</given-names></name><name><surname>Wu</surname><given-names>T</given-names></name></person-group><article-title>Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach</article-title><source>Nat Mater</source><year>2016</year><volume>15</volume><fpage>1120</fpage><pub-id pub-id-type="doi">10.1038/nmat4717</pub-id><pub-id pub-id-type="pmid">27500805</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mannodi-Kanakkithodi</surname><given-names>A</given-names></name><name><surname>Pilania</surname><given-names>G</given-names></name><name><surname>Huan</surname><given-names>TD</given-names></name><name><surname>Lookman</surname><given-names>T</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title>Machine learning strategy for accelerated design of polymer dielectrics</article-title><source>Sci Rep</source><year>2016</year><volume>6</volume><fpage>20952</fpage><pub-id pub-id-type="doi">10.1038/srep20952</pub-id><pub-id pub-id-type="pmid">26876223</pub-id></element-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Feinberg EN, Sheridan R, Joshi E, Pande VS, Cheng AC (2019) Step change improvement in Admet prediction with Potentialnet deep Featurization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/190311789">arXiv:190311789</ext-link></mixed-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>S</given-names></name><name><surname>Shiga</surname><given-names>T</given-names></name><name><surname>Feng</surname><given-names>L</given-names></name><name><surname>Hou</surname><given-names>Z</given-names></name><name><surname>Tsuda</surname><given-names>K</given-names></name><name><surname>Shiomi</surname><given-names>J</given-names></name></person-group><article-title>Designing nanostructures for phonon transport via bayesian optimization</article-title><source>Phys Rev X</source><year>2017</year><volume>7</volume><fpage>021024</fpage></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansch</surname><given-names>C</given-names></name><name><surname>Maloney</surname><given-names>PP</given-names></name><name><surname>Fujita</surname><given-names>T</given-names></name><name><surname>Muir</surname><given-names>RM</given-names></name></person-group><article-title>Correlation of biological activity of phenoxyacetic acids with hammett substituent constants and partition coefficients</article-title><source>Nature</source><year>1962</year><volume>194</volume><fpage>178</fpage><pub-id pub-id-type="doi">10.1038/194178b0</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riniker</surname><given-names>S</given-names></name><name><surname>Landrum</surname><given-names>GA</given-names></name></person-group><article-title>Open-source platform to benchmark fingerprints for ligand-based virtual screening</article-title><source>J Cheminform</source><year>2013</year><volume>5</volume><fpage>26</fpage><pub-id pub-id-type="doi">10.1186/1758-2946-5-26</pub-id><pub-id pub-id-type="pmid">23721588</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>M</given-names></name></person-group><article-title>Extended-connectivity fingerprints</article-title><source>J Chem Inf Model</source><year>2010</year><volume>50</volume><fpage>742</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1021/ci100050t</pub-id><pub-id pub-id-type="pmid">20426451</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weininger</surname><given-names>D</given-names></name></person-group><article-title>Smiles, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title><source>J Chem Inf Comput Sci.</source><year>1988</year><volume>28</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olivecrona</surname><given-names>M</given-names></name><name><surname>Blaschke</surname><given-names>T</given-names></name><name><surname>Engkvist</surname><given-names>O</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name></person-group><article-title>Molecular de-novo design through deep reinforcement learning</article-title><source>Journal of cheminformatics</source><year>2017</year><volume>9</volume><fpage>48</fpage><pub-id pub-id-type="doi">10.1186/s13321-017-0235-x</pub-id><pub-id pub-id-type="pmid">29086083</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yan</surname><given-names>X</given-names></name><name><surname>Gu</surname><given-names>Q</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>D</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name></person-group><article-title>Deepchemstable: chemical stability prediction with an attention-based graph convolution network</article-title><source>J Chem Inf Model</source><year>2019</year><volume>14</volume><fpage>1044</fpage><lpage>1049</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.8b00672</pub-id></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tetko</surname><given-names>IV</given-names></name><name><surname>Tanchuk</surname><given-names>VY</given-names></name><name><surname>Kasheva</surname><given-names>TN</given-names></name><name><surname>Villa</surname><given-names>AE</given-names></name></person-group><article-title>Estimation of aqueous solubility of chemical compounds using E-state indices</article-title><source>J Chem Inf Comput Sci</source><year>2001</year><volume>41</volume><fpage>1488</fpage><lpage>1493</lpage><pub-id pub-id-type="doi">10.1021/ci000392t</pub-id><pub-id pub-id-type="pmid">11749573</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Ramsundar</surname><given-names>B</given-names></name><name><surname>Feinberg</surname><given-names>EN</given-names></name><name><surname>Gomes</surname><given-names>J</given-names></name><name><surname>Geniesse</surname><given-names>C</given-names></name><name><surname>Pappu</surname><given-names>AS</given-names></name><name><surname>Leswing</surname><given-names>K</given-names></name><name><surname>Pande</surname><given-names>V</given-names></name></person-group><article-title>Moleculenet: a benchmark for molecular machine learning</article-title><source>Chem Sci</source><year>2018</year><volume>9</volume><fpage>513</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id><pub-id pub-id-type="pmid">29629118</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>R&#x000e9;ti</surname><given-names>T</given-names></name><name><surname>Sharafdini</surname><given-names>R</given-names></name><name><surname>Dregelyi-Kiss</surname><given-names>A</given-names></name><name><surname>Haghbin</surname><given-names>H</given-names></name></person-group><article-title>Graph irregularity indices used as molecular descriptors in qspr studies</article-title><source>MATCH Commun Math Comput Chem</source><year>2018</year><volume>79</volume><fpage>509</fpage><lpage>524</lpage></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Sarkar D, Sharma S, Mukhopadhyay S, Bothra AK (2016) Qsar Studies of Fabh inhibitors using graph theoretical &#x00026; quantum chemical descriptors. Pharmacophore 7</mixed-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>Z</given-names></name><name><surname>Hirayama</surname><given-names>Y</given-names></name><name><surname>Yamanishi</surname><given-names>Y</given-names></name><name><surname>Saigo</surname><given-names>H</given-names></name></person-group><article-title>Mining discriminative patterns from graph data with multiple labels and its application to quantitative structure&#x02013;activity relationship (Qsar) models</article-title><source>J Chem Inf Model</source><year>2015</year><volume>55</volume><fpage>2519</fpage><lpage>2527</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.5b00376</pub-id><pub-id pub-id-type="pmid">26549421</pub-id></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Jiang</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name></person-group><article-title>Molecule property prediction based on spatial graph embedding</article-title><source>J Chem Inf Model</source><year>2019</year><volume>59</volume><fpage>3817</fpage><lpage>3828</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00410</pub-id><pub-id pub-id-type="pmid">31438677</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Jia</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Xing</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Boulnois</surname><given-names>F</given-names></name><name><surname>Fan</surname><given-names>J</given-names></name></person-group><article-title>Chemi-Net: a molecular graph convolutional network for accurate drug property prediction</article-title><source>Int J Mol Sci</source><year>2019</year><volume>20</volume><fpage>3389</fpage><pub-id pub-id-type="doi">10.3390/ijms20143389</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goulon</surname><given-names>A</given-names></name><name><surname>Picot</surname><given-names>T</given-names></name><name><surname>Duprat</surname><given-names>A</given-names></name><name><surname>Dreyfus</surname><given-names>G</given-names></name></person-group><article-title>Predicting activities without computing descriptors: graph machines for Qsar</article-title><source>SAR QSAR Environ Res</source><year>2007</year><volume>18</volume><fpage>141</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1080/10629360601054313</pub-id><pub-id pub-id-type="pmid">17365965</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnott</surname><given-names>JA</given-names></name><name><surname>Planey</surname><given-names>SL</given-names></name></person-group><article-title>The influence of lipophilicity in drug discovery and design</article-title><source>Expert Opin Drug Discov</source><year>2012</year><volume>7</volume><fpage>863</fpage><lpage>875</lpage><pub-id pub-id-type="doi">10.1517/17460441.2012.714363</pub-id><pub-id pub-id-type="pmid">22992175</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">AstraZeneca. Experimental in vitro Dmpk and physicochemical data on a set of publicly disclosed compounds (2016) 10.6019/Chembl3301361</mixed-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sushko</surname><given-names>I</given-names></name><name><surname>Novotarskyi</surname><given-names>S</given-names></name><name><surname>K&#x000f6;rner</surname><given-names>R</given-names></name><name><surname>Pandey</surname><given-names>AK</given-names></name><name><surname>Rupp</surname><given-names>M</given-names></name><name><surname>Teetz</surname><given-names>W</given-names></name><name><surname>Brandmaier</surname><given-names>S</given-names></name><name><surname>Abdelaziz</surname><given-names>A</given-names></name><name><surname>Prokopenko</surname><given-names>VV</given-names></name><name><surname>Tanchuk</surname><given-names>VY</given-names></name><etal/></person-group><article-title>Online chemical modeling environment (Ochem): web platform for data storage, model development and publishing of chemical information</article-title><source>J Comput Aided Mol Des</source><year>2011</year><volume>25</volume><fpage>533</fpage><lpage>554</lpage><pub-id pub-id-type="doi">10.1007/s10822-011-9440-2</pub-id><pub-id pub-id-type="pmid">21660515</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Landrum G. Rdkit: open-source cheminformatics (2006)</mixed-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ramsundar</surname><given-names>B</given-names></name><name><surname>Eastman</surname><given-names>P</given-names></name><name><surname>Walters</surname><given-names>P</given-names></name><name><surname>Pande</surname><given-names>V</given-names></name></person-group><source>Deep Learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more</source><year>2019</year><publisher-loc>Newton</publisher-loc><publisher-name>O&#x02019;Reilly Media, Inc.</publisher-name></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Swanson</surname><given-names>K</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Coley</surname><given-names>CW</given-names></name><name><surname>Eiden</surname><given-names>P</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Guzman-Perez</surname><given-names>A</given-names></name><name><surname>Hopper</surname><given-names>T</given-names></name><name><surname>Kelley</surname><given-names>B</given-names></name><name><surname>Mathea</surname><given-names>M</given-names></name></person-group><article-title>Analyzing learned molecular representations for property prediction</article-title><source>J Chem Inf Model.</source><year>2019</year><volume>59</volume><fpage>3370</fpage><lpage>3388</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id><pub-id pub-id-type="pmid">31361484</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kireev</surname><given-names>DB</given-names></name></person-group><article-title>Chemnet: a novel neural network based method for graph/property mapping</article-title><source>J Chem Inf Comput Sci</source><year>1995</year><volume>35</volume><fpage>175</fpage><lpage>180</lpage><pub-id pub-id-type="doi">10.1021/ci00024a001</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coley</surname><given-names>CW</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Rogers</surname><given-names>L</given-names></name><name><surname>Jamison</surname><given-names>TF</given-names></name><name><surname>Jaakkola</surname><given-names>TS</given-names></name><name><surname>Green</surname><given-names>WH</given-names></name><name><surname>Barzilay</surname><given-names>R</given-names></name><name><surname>Jensen</surname><given-names>KF</given-names></name></person-group><article-title>A graph-convolutional neural network model for the prediction of chemical reactivity</article-title><source>Chem Sci</source><year>2019</year><volume>10</volume><fpage>370</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1039/C8SC04228D</pub-id><pub-id pub-id-type="pmid">30746086</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kearnes</surname><given-names>S</given-names></name><name><surname>McCloskey</surname><given-names>K</given-names></name><name><surname>Berndl</surname><given-names>M</given-names></name><name><surname>Pande</surname><given-names>V</given-names></name><name><surname>Riley</surname><given-names>P</given-names></name></person-group><article-title>Molecular graph convolutions: moving beyond fingerprints</article-title><source>J Comput Aided Mol Des</source><year>2016</year><volume>30</volume><fpage>595</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id><pub-id pub-id-type="pmid">27558503</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A, Adams RP (2015) Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems. pp 2224&#x02013;2232.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Paszke A, Gross S, Chintala S, Chanan G (2017) Pytorch: tensors and dynamic neural networks in python with strong Gpu acceleration. PyTorch: tensors and dynamic neural networks in python with strong GPU acceleration. 6</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser &#x00141;, Polosukhin I (2017) Attention is all you need. In: advances in neural information processing systems. pp 5998&#x02013;6008.</mixed-citation></ref><ref id="CR35"><label>35.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergstra</surname><given-names>J</given-names></name><name><surname>Komer</surname><given-names>B</given-names></name><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Hyperopt: a python library for model selection and hyperparameter optimization</article-title><source>Comput Sci Discov</source><year>2015</year><volume>8</volume><fpage>014008</fpage><pub-id pub-id-type="doi">10.1088/1749-4699/8/1/014008</pub-id></element-citation></ref><ref id="CR36"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>Mach Learn</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="CR37"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polishchuk</surname><given-names>P</given-names></name></person-group><article-title>Interpretation of quantitative structure-activity relationship models: past, present, and future</article-title><source>J Chem Inf Model</source><year>2017</year><volume>57</volume><fpage>2618</fpage><lpage>2639</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.7b00274</pub-id><pub-id pub-id-type="pmid">28949520</pub-id></element-citation></ref><ref id="CR38"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Sheridan</surname><given-names>RP</given-names></name><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Dahl</surname><given-names>GE</given-names></name><name><surname>Svetnik</surname><given-names>V</given-names></name></person-group><article-title>Deep neural nets as a method for quantitative structure&#x02013;activity relationships</article-title><source>J Chem Inf Model</source><year>2015</year><volume>55</volume><fpage>263</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1021/ci500747n</pub-id><pub-id pub-id-type="pmid">25635324</pub-id></element-citation></ref><ref id="CR39"><label>39.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oliphant</surname><given-names>TE</given-names></name></person-group><article-title>Python for Scientific Computing</article-title><source>Comput Sci Eng</source><year>2007</year><volume>9</volume><fpage>10</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id></element-citation></ref><ref id="CR40"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><article-title>Scikit-learn: machine learning in Python</article-title><source>J Mach Learn Res</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref></ref-list></back></article>