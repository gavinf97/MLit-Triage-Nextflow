<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">10603766</article-id><article-id pub-id-type="doi">10.1093/gigascience/giad082</article-id><article-id pub-id-type="publisher-id">giad082</article-id><article-categories><subj-group subj-group-type="heading"><subject>Technical Note</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject><subject>AcademicSubjects/SCI02254</subject></subj-group></article-categories><title-group><article-title>SpheroScan: a user-friendly deep learning tool for spheroid image analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3186-7478</contrib-id><name><surname>Akshay</surname><given-names>Akshay</given-names></name><aff>
<institution>Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern</institution>, <addr-line>3008 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Graduate School for Cellular and Biomedical Sciences, University of Bern</institution>, <addr-line>3012 Bern</addr-line>, <country country="CH">Switzerland</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9248-6348</contrib-id><name><surname>Katoch</surname><given-names>Mitali</given-names></name><aff>
<institution>Institute of Neuropathology, Universit&#x000e4;tsklinikum Erlangen, Friedrich-Alexander-Universit&#x000e4;t Erlangen-N&#x000fc;rnberg (FAU)</institution>, <addr-line>91054 Erlangen</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3986-4028</contrib-id><name><surname>Abedi</surname><given-names>Masoud</given-names></name><aff>
<institution>Department of Medical Data Science, Leipzig University Medical Centre</institution>, <addr-line>04107 Leipzig</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5750-7801</contrib-id><name><surname>Shekarchizadeh</surname><given-names>Navid</given-names></name><aff>
<institution>Department of Medical Data Science, Leipzig University Medical Centre</institution>, <addr-line>04107 Leipzig</addr-line>, <country country="DE">Germany</country></aff><aff>
<institution>Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig</institution>, <addr-line>04105 Leipzig</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0002-9512-9837</contrib-id><name><surname>Besic</surname><given-names>Mustafa</given-names></name><aff>
<institution>Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern</institution>, <addr-line>3008 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Department of Urology, Inselspital University Hospital</institution>, <addr-line>3010 Bern</addr-line>, <country country="CH">Switzerland</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8271-014X</contrib-id><name><surname>Burkhard</surname><given-names>Fiona C</given-names></name><aff>
<institution>Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern</institution>, <addr-line>3008 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Department of Urology, Inselspital University Hospital</institution>, <addr-line>3010 Bern</addr-line>, <country country="CH">Switzerland</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6914-9135</contrib-id><name><surname>Bigger-Allen</surname><given-names>Alex</given-names></name><aff>
<institution>Biological &#x00026; Biomedical Sciences Program, Division of Medical Sciences, Harvard Medical School</institution>, <addr-line>02115 Boston, MA, USA</addr-line></aff><aff>
<institution>Urological Diseases Research Center, Boston Children's Hospital</institution>, <addr-line>Boston, MA</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Department of Surgery, Harvard Medical School, Boston, MA</institution>, <addr-line>02115</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Broad Institute of MIT and Harvard</institution>, <addr-line>Cambridge, MA, 02142</addr-line>, <country country="US">USA</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0943-6236</contrib-id><name><surname>Adam</surname><given-names>Rosalyn M</given-names></name><aff>
<institution>Urological Diseases Research Center, Boston Children's Hospital</institution>, <addr-line>Boston, MA</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Department of Surgery, Harvard Medical School, Boston, MA</institution>, <addr-line>02115</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Broad Institute of MIT and Harvard</institution>, <addr-line>Cambridge, MA, 02142</addr-line>, <country country="US">USA</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2042-1139</contrib-id><name><surname>Monastyrskaya</surname><given-names>Katia</given-names></name><aff>
<institution>Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern</institution>, <addr-line>3008 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Department of Urology, Inselspital University Hospital</institution>, <addr-line>3010 Bern</addr-line>, <country country="CH">Switzerland</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9625-6259</contrib-id><name><surname>Gheinani</surname><given-names>Ali Hashemi</given-names></name><!--Ali.HashemiGheinani@childrens.harvard.edu--><aff>
<institution>Functional Urology Research Group, Department for BioMedical Research DBMR, University of Bern</institution>, <addr-line>3008 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Department of Urology, Inselspital University Hospital</institution>, <addr-line>3010 Bern</addr-line>, <country country="CH">Switzerland</country></aff><aff>
<institution>Urological Diseases Research Center, Boston Children's Hospital</institution>, <addr-line>Boston, MA</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Department of Surgery, Harvard Medical School, Boston, MA</institution>, <addr-line>02115</addr-line>, <country country="US">USA</country></aff><aff>
<institution>Broad Institute of MIT and Harvard</institution>, <addr-line>Cambridge, MA, 02142</addr-line>, <country country="US">USA</country></aff><xref rid="cor1" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">Correspondence address. Ali Hashemi Gheinani, Urological Diseases Research Center, Boston Children's Hospital, Harvard Medical School, Boston, 02115, MA, USA. E-mail: <email>Ali.HashemiGheinani@childrens.harvard.edu</email></corresp></author-notes><pub-date pub-type="epub" iso-8601-date="2023-10-27"><day>27</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><pub-date pub-type="pmc-release"><day>27</day><month>10</month><year>2023</year></pub-date><volume>12</volume><elocation-id>giad082</elocation-id><history><date date-type="received"><day>17</day><month>5</month><year>2023</year></date><date date-type="rev-recd"><day>07</day><month>8</month><year>2023</year></date><date date-type="accepted"><day>14</day><month>9</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2023. Published by Oxford University Press GigaScience.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giad082.pdf"/><abstract><title>Abstract</title><sec id="abs1"><title>Background</title><p>In recent years, 3-dimensional (3D) spheroid models have become increasingly popular in scientific research as they provide a more physiologically relevant microenvironment that mimics <italic toggle="yes">in vivo</italic> conditions. The use of 3D spheroid assays has proven to be advantageous as it offers a better understanding of the cellular behavior, drug efficacy, and toxicity as compared to traditional 2-dimensional cell culture methods. However, the use of 3D spheroid assays is impeded by the absence of automated and user-friendly tools for spheroid image analysis, which adversely affects the reproducibility and throughput of these assays.</p></sec><sec id="abs2"><title>Results</title><p>To address these issues, we have developed a fully automated, web-based tool called SpheroScan, which uses the deep learning framework called Mask Regions with Convolutional Neural Networks (R-CNN) for image detection and segmentation. To develop a deep learning model that could be applied to spheroid images from a range of experimental conditions, we trained the model using spheroid images captured using IncuCyte Live-Cell Analysis System and a conventional microscope. Performance evaluation of the trained model using validation and test datasets shows promising results.</p></sec><sec id="abs3"><title>Conclusion</title><p>SpheroScan allows for easy analysis of large numbers of images and provides interactive visualization features for a more in-depth understanding of the data. Our tool represents a significant advancement in the analysis of spheroid images and will facilitate the widespread adoption of 3D spheroid models in scientific research. The source code and a detailed tutorial for SpheroScan are available at <ext-link xlink:href="https://github.com/FunctionalUrology/SpheroScan" ext-link-type="uri">https://github.com/FunctionalUrology/SpheroScan</ext-link>.</p></sec></abstract><kwd-group kwd-group-type="keywords"><kwd>3D spheroids</kwd><kwd>deep learning</kwd><kwd>image segmentation</kwd><kwd>high-throughput screening</kwd><kwd>Image analysis</kwd><kwd>Mask R-CNN</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Science Foundation</institution><institution-id institution-id-type="DOI">10.13039/100000001</institution-id></institution-wrap>
</funding-source><award-id>310030</award-id><award-id>175773</award-id><award-id>212298</award-id></award-group></funding-group><counts><page-count count="9"/></counts></article-meta></front><body><boxed-text position="float"><label>Key points</label><list list-type="bullet"><list-item><p>A deep learning model was trained to detect and segment spheroids in images from microscopes and IncuCytes.</p></list-item><list-item><p>The model performed well on both types of images, with the total loss decreasing significantly during the training process.</p></list-item><list-item><p>A web tool called SpheroScan was developed to facilitate the analysis of spheroid images, which includes prediction and visualization modules.</p></list-item><list-item><p>SpheroScan is efficient and scalable, making it possible to handle large datasets with ease.</p></list-item><list-item><p>SpheroScan is user-friendly and accessible to researchers, making it a valuable resource for the analysis of spheroid image data.</p></list-item></list></boxed-text><sec sec-type="intro" id="sec2"><title>Introduction</title><p>Two-dimensional (2D) cell culture models have long been a key component of biomedical research, but they often do not accurately replicate the <italic toggle="yes">in vivo</italic> environment [<xref rid="bib1" ref-type="bibr">1</xref>]. In recent years, there has been an increasing realization that 3-dimensional (3D) cell cultures, such as 3D spheroid models, are better able to mimic the <italic toggle="yes">in vivo</italic> environment. Moreover, the 3D cell cultures provide more clinically relevant insights into cellular behavior and responses [<xref rid="bib2" ref-type="bibr">2</xref>, <xref rid="bib3" ref-type="bibr">3</xref>]. The 3D spheroid models, in particular, have become increasingly popular due to their ability to re-create the complex microenvironment found <italic toggle="yes">in vivo</italic>. This has made them a valuable tool for studying a variety of biological processes and diseases.</p><p>Tumor spheroids are widely used for testing anticancer medications [<xref rid="bib4" ref-type="bibr">4</xref>]. They present a compromise between the cell accessibility of adherent cultures and the 3-dimensionality of animal models. Spheroids retain more biological tumor features and reproduce the intratumor environment, which is an important feature when selecting an effective treatment strategy. Most of the spheroid-based assays use the overall size and/or cell survival as a readout [<xref rid="bib5" ref-type="bibr">5</xref>]. Thereby, a quick and easy tool for spheroid size estimation would be advantageous for such applications.</p><p>Another important area of research that is dependent on the spheroid size evaluation is the collagen gel contraction assay (CGCA) method [<xref rid="bib6" ref-type="bibr">6</xref>]. CGCA is a widely used <italic toggle="yes">in vitro</italic> model for studying the interactions between cells and 3D extracellular matrices. These assays help understand matrix remodeling during fibrosis and wound healing. CGCA is a competent tool to evaluate the contractility of myofibroblasts harvested from fibrotic tissues. The advent of aqueous 2-phase printing of cell-containing contractile collagen microgels has further advanced the CGCA technology [<xref rid="bib7" ref-type="bibr">7</xref>]. Recently, the printing of the microscale cell-laden collagen gels has been combined with live-cell imaging and automated image analysis to study the kinetics of cell-mediated contraction of the collagen matrix [<xref rid="bib8" ref-type="bibr">8</xref>]. The image analysis method utilizes a plugin for FIJI, built around Waikato Environment for Knowledge Analysis (WEKA) segmentation.</p><p>Despite the advantages of 3D spheroid models over 2D cell cultures, the lack of fully automated and user-friendly tools for analyzing spheroid images has been a major challenge, hindering widespread adoption and making high-throughput analysis difficult. Spheroid detection in an image is a crucial and challenging part of 3D spheroid assays. Several tools [<xref rid="bib9" ref-type="bibr">9&#x02013;15</xref>] have been previously developed for spheroid image analysis that utilize traditional object detection methods, such as thresholding (using algorithms like watershed [<xref rid="bib16" ref-type="bibr">16</xref>], Otsu [<xref rid="bib17" ref-type="bibr">17</xref>], Yen [<xref rid="bib18" ref-type="bibr">18</xref>]), which involve setting a threshold value for the intensity of pixels and identifying all pixels above that value as part of a spheroid. Other techniques include shape-based detections (using circular/ellipse Hough transform algorithms [<xref rid="bib19" ref-type="bibr">19</xref>], active contours models [<xref rid="bib20" ref-type="bibr">20</xref>]) that identify spheroids based on their shape.</p><p>Unfortunately, these methods prove ineffective in adapting to a wide range of experimental conditions (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S6</xref>). The reason behind this limitation lies in the inherent variability observed in the images of spheroids captured during the assay. This variability arises from several factors, including lighting conditions, the composition of the medium, the quantity of cells utilized, treatment type, presence of debris, and variations in plate shapes, among others. Therefore, these methods require extensive fine-tuning to analyze images from each experiment and sometimes even for each specific image, which is a tedious and time-consuming task.</p><p>In recent years, the use of deep learning techniques for object detection and segmentation has significantly increased [<xref rid="bib21" ref-type="bibr">21&#x02013;25</xref>]. This rise is attributed to their ability to effectively learn from limited-size datasets and adapt to diverse imaging conditions without the need for excessive fine-tuning. Following the trend, several tools and workflows [<xref rid="bib26" ref-type="bibr">26&#x02013;31</xref>] have been developed that utilize deep learning for automatic spheroid detection in images. However, all of them require a moderate to advanced level of computational and programming skills to use. Consequently, many researchers with domain expertise are unable to utilize them easily. Additionally, none of these tools provide visualization features to allow for efficient downstream analysis of spheroid data (<xref rid="sup8" ref-type="supplementary-material">Supplementary Table S1</xref>). This is a significant drawback, as visualizing data can greatly aid in the interpretation and understanding of results.</p><p>To address these challenges, we have developed a fully automated, user-friendly web-based tool called SpheroScan for spheroid detection and interactive visualization of spheroid data using multiple publication-ready plots. Our tool is designed to be accessible to researchers regardless of their computational skills and aims to make the process of analyzing spheroid images as simple and straightforward as possible. We have employed a state-of-the-art deep learning model called Mask R-CNN (Region-based Convolutional Neural Network) for image detection and segmentation. This model has proven to be highly effective in image analysis tasks and allows our tool to accurately detect and segment spheroids in images. With our tool, researchers can easily and quickly analyze large numbers of spheroid images and can use the interactive visualization features to gain a deeper understanding of their data (Fig.&#x000a0;<xref rid="fig1" ref-type="fig">1</xref>).</p><fig position="float" id="fig1"><label>Figure 1:</label><caption><p>Graphical abstract. (A) Data acquisition. We used IncuCyte and microscope platforms to generate spheroid images for the training and evaluation of deep learning model. (B) Deep learning (DL) pipeline. Two models were trained using IncuCyte and microscope image datasets. These models were then evaluated on validation and test datasets. (C) SpheroScan consists of 2 submodules: prediction and visualization. The prediction module applies the trained deep learning models to mask the input spheroid images, producing a CSV file with the area and intensity of each detected spheroid as output. The visualization module enables the user to analyze the output from the prediction module by providing various plots and statistical analyses.</p></caption><graphic xlink:href="giad082fig1" position="float"/></fig></sec><sec sec-type="discussion|results" id="sec3"><title>Results and Discussion</title><sec id="sec3-1"><title>Training and evaluating the performance of deep learning model</title><p>Figure&#x000a0;<xref rid="fig2" ref-type="fig">2</xref> presents the performance of the trained deep learning (DL) model on the training, validation, and testing datasets for microscope and IncuCyte images. The results show that the DL model was able to effectively learn and improve its performance over the course of training for both types of images. In particular, for IncuCyte images, the total loss at baseline was 1.6 for the training data and 1.3 for the validation data. However, in the last epoch, the total loss reached its minimum values of 0.09 and 0.13 for the training and validation data, respectively (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2A</xref>). This represents a significant improvement in performance. Similarly, the bounding box and mask loss started at relatively high values of 0.3 and 0.7, respectively, but decreased to their minimum values of 0.03 and 0.04 in the last epoch (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2B</xref>). The model also performed well on the training and validation datasets for microscope images, with the total loss decreasing from 1.8 and 1.4 to 0.09 and 0.16 at the last epoch, respectively (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2D</xref>). The bounding box and mask losses for the microscope dataset were also low, 0.036 and 0.045, respectively, at the last epoch (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2E</xref>). Overall, these results demonstrate the robustness and effectiveness of the DL model in accurately detecting and segmenting spheroids in images from both microscopes and IncuCytes.</p><fig position="float" id="fig2"><label>Figure 2:</label><caption><p>Results of the deep learning model's performance. The total loss for both training and validation datasets of IncuCyte (A) and microscope (D) images. The bounding box loss and mask loss for the training dataset of IncuCyte (B) and microscope (E) images. The APbbox<sub>@[0.5:0.95]</sub> and APmmask<sub>@[0.5:0.95]</sub> for the validation and test datasets of IncuCyte (C) and microscope (F) images. The APbbox<sub>@[0.5:0.95]</sub> represents the average precision for bounding boxes, and the APmmask<sub>@[0.5:0.95]</sub> represents the average precision for segmentation masks in the range of 0.5 to 0.95.</p></caption><graphic xlink:href="giad082fig2" position="float"/></fig><p>To evaluate the performance of the trained model in segmenting spheroids, we calculated the average precision (AP) metric for bounding boxes and segmentation masks in the range of 0.5 to 0.95. Throughout the text, APbbox<sub>@[0.5:0.95]</sub> represents the AP for bounding boxes, and APmmask<sub>@[0.5:0.95]</sub> represents the AP for segmentation masks. In general, the trained models showed similar performance on the test and validation datasets. The values for APbbox<sub>@[0.5:0.95]</sub> and APmmask<sub>@[0.5:0.95]</sub> were 0.937 and 0.972, respectively, for the validation data and 0.927 and 0.97, respectively, for the test data of IncuCyte images (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2C</xref>). The model's performance on the validation and test datasets for microscopic images was also strong, with scores of 0.89 and 0.944 for APbbox<sub>@[0.5:0.95]</sub> and APmmask<sub>@[0.5:0.95]</sub>, respectively, on the validation data and scores of 0.899 and 0.977, respectively, on the test data (Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2F</xref>).</p><p>Furthermore, we assessed the applicability of SpheroScan in analyzing spheroid images generated by external users using different imaging platforms, diverse cell types, growth mediums, and various lighting conditions. To achieve this objective, we employed SpheroScan to mask spheroids in multiple image datasets obtained from previous studies (<xref rid="sup8" ref-type="supplementary-material">Supplementary Table S2</xref>). In total, we utilized 6 distinct datasets [<xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib27" ref-type="bibr">27</xref>, <xref rid="bib32" ref-type="bibr">32&#x02013;34</xref>], including 4 fluorescence microscopy datasets (including multichannel) and 2 brightfield microscopy datasets (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S7</xref>). The results indicate that SpheroScan effectively detected spheroids in all images from the tested datasets, affirming its adaptability and applicability to external datasets (<xref rid="sup8" ref-type="supplementary-material">Supplementary Table S2</xref>). Nevertheless, the N&#x000fc;rnberg, Elina et al. dataset posed a challenge for SpheroScan as it struggled to identify spheroids in 8 out of 48 images. The difficulty arose from a limited number of cells stained with anti-KI67, resulting in the formation of hollow spheroid-like structure.</p></sec><sec id="sec3-2"><title>SpheroScan characteristics</title><p>We have developed an open-source web tool called SpheroScan to facilitate the analysis of spheroid images. This user-friendly, interactive tool is designed to streamline the process of spheroid segmentation, area calculation, and downstream analysis of spheroid image data. Furthermore, it helps to standardize and accelerate the analysis of spheroid assay results. SpheroScan consists of 2 main modules: prediction and visualization. The prediction module uses previously trained DL models to detect the spheroid in the input images; accordingly, a CSV file is generated with the area, circularity, and intensity of each detected spheroid (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S1A</xref>). The visualization module allows the user to analyze the results of the prediction module through various types of plots and statistical analyses (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S1B</xref>). The plots generated by the visualization module are ready for publication and can be saved as high-quality images in PNG format. Overall, SpheroScan is a powerful and user-friendly tool that greatly simplifies and enhances the analysis of spheroid image data (<xref rid="sup8" ref-type="supplementary-material">Supplementary Figs. S2&#x02013;S4</xref>).</p><p>The runtime complexity of the prediction module is linear, meaning that it scales in proportion to the size of the input data. This is an important property because it means that the prediction module will be efficient and scalable, even when processing large datasets. To confirm the linear runtime complexity of the prediction module, we tested it on 4 different image datasets with various numbers of images. The results of these tests showed that the prediction module consistently had a linear runtime, taking less than 1 second to mask a single image (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3D</xref>). This demonstrates that the prediction module is highly efficient and capable of handling large datasets with ease. We evaluated the runtime performance on a Red Hat server with 16 central processing unit cores and 64 GB of RAM.</p><fig position="float" id="fig3"><label>Figure 3:</label><caption><p>(A) Datasets. Training, validation, and test datasets size for IncuCyte and microscope models. (B) Intersection over union (IoU) metric. The IoU metric is a measure of the overlap between 2 bounding boxes or masks. It is calculated by dividing the overlap area between the predicted and ground-truth regions by the total area of both regions combined. (C) Spheroid intensity calculation. To determine the intensity of the spheroid image, a new image with the same shape and number of pixels as the original is created, but with all pixels set to zero intensity. The predicted contour boundary of the spheroid or spheroids is applied to this new image, and all pixels inside the boundary are set to 255 intensity. The x and y coordinates of each pixel in the new image with a value of 255 are then extracted. The average pixel intensity value for all points within the contour boundary is then calculated using Python's OpenCV module. (D) Runtime analysis. The runtime complexity of the prediction module was analyzed using 4 different image datasets of varying sizes. The results showed that it takes less than a second to mask an image, and the runtime complexity of the prediction module is linear. This means that the time required to process an image increases in proportion to the number of images being processed.</p></caption><graphic xlink:href="giad082fig3" position="float"/></fig></sec></sec><sec id="sec4"><title>Limitations and Considerations</title><p>As with any technology, there are limitations and considerations to keep in mind when using the SpheroScan system. First, it is important to note that this developed tool is primarily designed for use with the spheroid images from IncuCyte and microscope platforms. Additionally, when analyzing images that contain more than 1 spheroid, the performance of the SpheroScan system may decrease. Therefore, it is important to carefully consider the experimental design and imaging conditions to ensure optimal performance and accurate results. The authors aim to expand the training dataset with a diverse range of external images from various experimental environments and platforms in the future to improve and advance the utility of SpheroScan.</p><p>Furthermore, in the current version, the tool provides a limited set of parameters&#x02014;namely, the area, circularity, and brightfield average intensity, to describe the spheroids. Although these parameters are informative and relevant for certain assays, additional parameters, such as volume estimation and cell count estimation, may be required for a more comprehensive characterization. As we continue to enhance the tool, we are actively considering incorporating derived parameters to enhance its applicability across a broader range of experimental scenarios.</p><p>Besides that, we encountered several instances where SpheroScan faced difficulties in accurately masking spheroids in images (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S8</xref>). For example, in <xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S8A</xref>, there was a spheroid with a hollow, spheroid-like structure formed from a limited number of labeled cells, but unfortunately, SpheroScan failed to identify it. Moreover, we noticed challenges with masking spheroid images containing debris and irregular shapes. In such situations, SpheroScan occasionally misidentified some debris as spheroids (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S8B, S8D, and S8E</xref>). However, we found that most of these challenges could be mitigated by adjusting the prediction threshold (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S8C and S8F</xref>). These challenging scenarios indicate that SpheroScan's performance may be influenced by specific image characteristics, such as the complexity of spheroid structures and the presence of debris. Generally, while the SpheroScan system offers many advantages for high-throughput spheroid analysis, it is important to be aware of its limitations and take steps to address them as needed.</p></sec><sec sec-type="conclusions" id="sec5"><title>Conclusion</title><p>The development of the web-based tool SpheroScan represents a significant advancement in the analysis of 3D spheroid images. Using the state-of-the-art DL techniques, our tool accurately detects and segments spheroids in images, making it easy for researchers to analyze large numbers of spheroid images. Additionally, our tool is user-friendly and accessible to researchers regardless of their computational skills, making it a valuable resource for the scientific community. The interactive visualization features provided by our tool also allow for a more in-depth understanding of spheroid data, which will further facilitate the widespread adoption of 3D spheroid models in research. Overall, SpheroScan represents a valuable tool for researchers working with 3D spheroid models and will help to advance the use of these models in scientific research.</p></sec><sec sec-type="materials|methods" id="sec6"><title>Materials and Methods</title><sec id="sec6-1"><title>Implementation</title><p>SpheroScan (<ext-link xlink:href="https://scicrunch.org/resolver/RRID:SCR_023886" ext-link-type="uri">RRID:SCR_023886</ext-link>) was developed using the Plotly Dash [<xref rid="bib35" ref-type="bibr">35</xref>] library in Python (version 3.10.6), and all the plots were made using Plotly. Pandas library [<xref rid="bib36" ref-type="bibr">36</xref>, <xref rid="bib37" ref-type="bibr">37</xref>] was used to store and process the data.</p></sec><sec id="sec6-2"><title>Spheroid image acquisition</title><p>In this study, our goal was to create a generalized DL model that can be used for spheroid images from various experimental setups or laboratory environments. To this end, we applied the aqueous 2-phase solution method to embed the cells of interest into collagen matrix spheroids. To estimate the cell-driven contraction of the collagen matrix, we collected spheroid images from different treatment conditions and time points, using both bladder smooth muscle cells (SMCs) and human embryonic kidney (HEK) cells. SMC cells were chosen for this study since they have the ability to contract, which we expected to lead to the creation of spheroids in a wide range of sizes. HEK cells, on the other hand, do not contract and were used as a negative control to ensure the accuracy of our results. The spheroids were treated with various concentrations of histamine and fetal bovine serum (FBS) and were observed at regular intervals to track their response to these treatments.</p><p>To generate the image datasets needed for a DL model, we performed a spheroid gel contraction assay using 5,000 SMC or HEK cells per collagen spheroid. After the collagen droplet polymerized, the medium was changed and plates were transferred to an IncuCyte Live-Cell Analysis System, which acquired images of the spheroids every hour for 24 hours. Alternatively, we used a ZEISS Axio Vert.A1 Inverted Microscope and manually acquired images of the spheroids at selected time points. By using both methods, we were able to capture a wide range of spheroid images and to create a robust dataset for our DL model.</p><p>A total of 480 images were obtained from the IncuCyte system, and these were randomly divided into a training dataset of 336 images (70%) and a validation dataset of 144 images (30%). An additional test dataset of 50 images was used to evaluate the performance of the trained model. To create a model specifically for microscopic images, we gathered spheroid images from the microscope and divided them into 3 datasets: training, validation, and test. The training dataset included 265 images, the validation dataset included 117 images, and the test dataset included 50 images (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3A</xref>). To test the robustness of the trained model, the spheroids in the test dataset were treated differently from those in the training and validation datasets. The medium used here was smooth muscle cell medium and Dulbecco's Modified Eagle Medium with 0.5% and 1% FBS.</p><p>In the following step, an experienced researcher in the spheroid assay manually annotated the images from IncuCyte and microscopes using the VGG Image Annotator [<xref rid="bib38" ref-type="bibr">38</xref>].</p></sec><sec id="sec6-3"><title>Deep learning framework</title><p>For spheroid detection and segmentation, we used a state-of-the-art DL model called Mask R-CNN and an open-source Python [<xref rid="bib39" ref-type="bibr">39</xref>] library called Detectron2 [<xref rid="bib40" ref-type="bibr">40</xref>]. Mask R-CNN is a method for solving the problem of instance segmentation, which involves both object detection and semantic segmentation. Object detection is the process of identifying and classifying multiple objects within an image, while semantic segmentation involves understanding the image at the pixel level to distinguish individual objects within the image. In order to perform these tasks, Mask R-CNN first uses a deep convolutional neural network (CNN) to process the input image and to generate a set of feature maps. These feature maps are then used as input for the next step in the process.</p><p>Mask R-CNN performs object detection in 2 stages. First, it uses a region proposal network (RPN) module to identify regions of interest (ROIs) within the image. ROIs are defined as bounding boxes with a high probability of containing objects. In the second stage, Mask R-CNN uses an ROI classifier and bounding box regressor module to classify the objects within the ROIs and to determine their bounding boxes. Both the RPN and ROI classifier and bounding box regressor modules are implemented as CNNs.</p><p>For semantic segmentation, Mask R-CNN uses a fully convolutional network (FCN) called the mask segmentation module to predict masks for each ROI determined in the object detection phase. This allows Mask R-CNN to accurately identify and distinguish individual objects within the image and segment them from the background. Overall, the combination of object detection and semantic segmentation allows Mask R-CNN to achieve highly accurate and detailed instance segmentation results (<xref rid="sup8" ref-type="supplementary-material">Supplementary Fig. S5</xref>).</p><p>In this study, we used the Mask R-CNN model for instance segmentation and tuned several of its parameters to fit the specific problem and the dataset we were working with. The backbone of the model was a ResNet-50 feature pyramid network, and we initialized the model with weights from a pretrained COCO instance segmentation model. The batch size for training was set to 4, and the base learning rate was set to 0.00025. The RoIHead batch size was 256, and we used a single output class (for spheroids). We trained the model for a total of 1,000 iterations. In addition to these specified parameters, we used the default values for all other parameters of the Mask R-CNN model.</p></sec><sec id="sec6-4"><title>Evaluation metrics</title><p>To evaluate the performance of the trained models on spheroid segmentation, we used the AP or mean average precision (mAP) metric. mAP is a commonly used evaluation metric in computer vision for measuring the accuracy of instance segmentation and object detection models. Many of the state-of-the-art object detection algorithms, such as Faster R-CNN [<xref rid="bib41" ref-type="bibr">41</xref>], Mask R-CNN [<xref rid="bib42" ref-type="bibr">42</xref>], MobileNet SSD [<xref rid="bib43" ref-type="bibr">43</xref>], and YOLO [<xref rid="bib44" ref-type="bibr">44</xref>], as well as benchmark challenges such as PASCAL VOC [<xref rid="bib45" ref-type="bibr">45</xref>], use AP to evaluate their models. Calculation of AP is dependent on the following metrics:</p><p>
<bold>Precision:</bold> It is defined as the fraction of true instances among all predicted instances and is calculated using the following formula:</p><disp-formula>
<tex-math id="TM0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
{\mathrm{Precision}} = \ \frac{{TP}}{{TP + FP}}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>
<bold>Recall</bold>: It is a metric that represents the fraction of retrieved instances among all relevant instances and is calculated as follows:</p><disp-formula>
<tex-math id="TM0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
{\mathrm{Recall}} = \ \frac{{TP}}{{TP + FN}}
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>
<bold>IoU:</bold> The IoU is a metric that measures the overlap between 2 bounding boxes or masks. It is commonly used to evaluate the accuracy of object detection and instance segmentation models. The IoU value ranges from 0 to 1, with a value of 1 indicating a completely accurate prediction. To calculate the IoU, the overlap between the predicted and ground-truth regions is first determined and divided by the total area of both regions. The IoU is a useful metric because it allows for comparing predictions with different shapes and sizes, as it considers the area of both the predicted and ground truth regions (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3B</xref>).</p><p>
<bold>AP:</bold> The AP is a metric used to evaluate the performance of object detection and instance segmentation models. It is calculated as the area under the precision&#x02013;recall curve, which plots the precision (the proportion of true-positive detections among all positive detections) against the recall (the proportion of true-positive detections among all ground-truth objects) of a model. AP ranges from 0 to 1, with a higher value indicating better performance. A higher AP value indicates that the model can achieve both high precision and high recall, making it a useful metric for evaluating the overall performance of a model. AP can be calculated for a specific IoU threshold as follows:</p><disp-formula>
<tex-math id="TM0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
{\mathrm{AP}} = \mathop \smallint \limits_0^1 {\mathrm{Precision\ d}}\left( {{\mathrm{Recall}}} \right)
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>Often, AP is used as the average over multiple IoU thresholds, and it is calculated as follows:</p><disp-formula>
<tex-math id="TM0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
{\mathrm{mAP}} = \ \frac{1}{n}\mathop \sum \limits_{k = 1}^{k = n} A{P}_k
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>where,</p><disp-formula>
<tex-math id="TM0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{eqnarray*}
A{P}_k{\mathrm{\ }} &#x00026;=&#x00026; {\mathrm{\ AP\ at\ }}k^{\mathrm{th}} {\mathrm{IoU\ threshold}};\\
{\mathrm{\ }}n &#x00026;=&#x00026; {\mathrm{\ Number\ of\ IoU\ thresholds\ under\ consideration}}.
\end{eqnarray*}\end{document}</tex-math>
</disp-formula><p>In the following, AP<sub>@0.75</sub> represents AP at IoU threshold 0.75 and AP<sub>@[0.5:0.95]</sub> represents the average AP over 10 IoU thresholds (from 0.5 to 0.95 with a step size of 0.05).</p></sec><sec id="sec6-5"><title>Area and intensity calculation</title><p>After performing object detection and instance segmentation on an image, we can use the predicted contour boundary of each spheroid to calculate its area, circularity, and intensity. To calculate the area of a spheroid, we use Python's OpenCV library to count the number of pixels within the contour boundary. This gives us the total area of the spheroid in pixels. To calculate the intensity of the spheroid, we follow a similar process. First, we create a new image with the same shape and number of pixels as the original, but with a default intensity of zero. This image is then masked with the predicted contour boundary of the spheroid, setting all pixels within the boundary to a value of 255. We then extract the x and y coordinates of all pixels with a value of 255, which correspond to the pixels within the contour boundary of the spheroid in the original image. Finally, we use OpenCV to calculate the average intensity of these pixels, which gives us the intensity value for the spheroid. This process allows us to accurately measure the area and intensity of each spheroid in an image (Fig.&#x000a0;<xref rid="fig3" ref-type="fig">3C</xref>).</p></sec></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="sup1" position="float" content-type="local-data"><label>giad082_GIGA-D-23-00131_Original_Submission</label><media xlink:href="giad082_giga-d-23-00131_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup2" position="float" content-type="local-data"><label>giad082_GIGA-D-23-00131_Revision_1</label><media xlink:href="giad082_giga-d-23-00131_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup3" position="float" content-type="local-data"><label>giad082_Response_to_Reviewer_Comments_Original_Submission</label><media xlink:href="giad082_response_to_reviewer_comments_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup4" position="float" content-type="local-data"><label>giad082_Reviewer_1_Report_Original_Submission</label><caption><p>Kevin Tr&#x000c3;&#x000b6;ndle -- 6/16/2023 Reviewed</p></caption><media xlink:href="giad082_reviewer_1_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup5" position="float" content-type="local-data"><label>giad082_Reviewer_1_Report_Revision_1</label><caption><p>Kevin Tr&#x000c3;&#x000b6;ndle -- 8/18/2023 Reviewed</p></caption><media xlink:href="giad082_reviewer_1_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup6" position="float" content-type="local-data"><label>giad082_Reviewer_2_Report_Original_Submission</label><caption><p>Francesco Pampaloni -- 7/16/2023 Reviewed</p></caption><media xlink:href="giad082_reviewer_2_report_original_submission.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup7" position="float" content-type="local-data"><label>giad082_Reviewer_2_Report_Revision_1</label><caption><p>Francesco Pampaloni -- 8/16/2023 Reviewed</p></caption><media xlink:href="giad082_reviewer_2_report_revision_1.pdf"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material><supplementary-material id="sup8" position="float" content-type="local-data"><label>giad082_Supplemental_Files</label><media xlink:href="giad082_supplemental_files.zip"><caption><p>Click here for additional data file.</p></caption></media></supplementary-material></sec></body><back><ack id="ack1"><title>Acknowledgement</title><p>We thank Ankush Sharma for his guidance and Niharika Jakhar for testing SpheroScan on various operating systems.</p></ack><sec sec-type="data-availability" id="sec7"><title>Data Availability</title><p>The source code, example input data, and a detailed tutorial for SpheroScan are available at GitHub [<xref rid="bib46" ref-type="bibr">46</xref>]. All supporting data, which include images used for training, validation, and testing [<xref rid="bib47" ref-type="bibr">47</xref>], as well as the trained model weights [<xref rid="bib48" ref-type="bibr">48</xref>], are available at Zenodo. Additionally, spheroid images from the external datasets that have been used to evaluate the applicability of SpheroScan, along with the corresponding masked images, are also available at Zenodo [<xref rid="bib49" ref-type="bibr">49</xref>]. An archival copy of the SpheroScan code is available via the <italic toggle="yes">GigaScience</italic> database GigaDB [<xref rid="bib50" ref-type="bibr">50</xref>].</p></sec><sec id="sec8"><title>Additional Files</title><p>
<bold>Supplementary Table S1</bold>. Comparison of features between SpheroScan and other similar deep learning&#x02013;based tools for automatic spheroid detection. *No information provided in the manuscript. GUI = Graphical User Interface.</p><p>
<bold>Supplementary Table S2</bold>. List of external datasets used to evaluate the performance of SpheroScan on an unseen dataset obtained from various experiments, studies, and conditions.</p><p>
<bold>Supplementary Fig. S1</bold>. SpheroScan Graphical User Interface. (A) Prediction module. The prediction module applies trained DL models to identify and mask spheroid images. It requires a zipped folder of images, platform type, and prediction threshold as input and generates masked images and a CSV file containing the area and intensity data of the identified spheroids as output. (B) Visualization module. The visualization module creates plots and performs statistical analysis using the output file from the prediction module and a metadata file that contains information about the study design. It offers various types of plots and allows users to customize the plot options, such as plot type and color palette. Users can export plots in high-resolution PNG format.</p><p>
<bold>Supplementary Fig. S2</bold>. SpheroScan plot gallery. (A) Bar plot. (B) Bar plot with significance level. A bar plot with significance level is a visual representation of data where the level of significance is indicated by stars. Three asterisks (***) indicate a <italic toggle="yes">P</italic> value of less than 0.001, while "ns" represents a <italic toggle="yes">P</italic> value of 0.05 or greater. The less asterisks, the lower the significance level.</p><p>
<bold>Supplementary Fig. S3</bold>. SpheroScan plot gallery. (A) Bubble plot. A bubble plot is a type of scatterplot where the size of the bubbles represents the mean spheroid area for a certain group. The y-axis displays the relative area or contraction of the spheroid, calculated with respect to a baseline group. (B) Line plot.</p><p>
<bold>Supplementary Fig. S4</bold>. SpheroScan plot gallery. (A) Treemap. A treemap is a method of displaying hierarchical data in which nested rectangles are used to represent different groups. The outer rectangles represent the top-level groups, while the inner rectangles represent subgroups. The size and color of each rectangle in the treemap indicate the mean spheroid areas or intensity of the corresponding group. (B) Scatterplot.</p><p>
<bold>Supplementary Fig. S5</bold>. Mask R-CNN architecture. The Mask R-CNN model consists of 4 main modules: feature extraction, region proposal network (RPN), region of interest (ROI) classifier and bounding box regressor, and mask segmentation. The feature extraction module takes images as input and produces feature maps. The RPN module then runs on the feature maps and uses a sliding window to identify bounding boxes with a high likelihood of containing objects (ROIs). For each ROI, the ROI classifier and bounding box regressor module is used to determine the class label of the object. For semantic segmentation, the Mask R-CNN model uses a fully convolutional network (FCN) in the mask segmentation module to predict a mask for each ROI identified in the object detection phase.</p><p>
<bold>Supplementary Fig. S6</bold>. A comparison between masking methods: thresholding approach versus SpheroScan. (A&#x02013;D) Images masked using the thresholding approach in ImageJ. However, this method proves ineffective in accurately masking the spheroid due to significant contrast variations within the image. (E, F) Corresponding images masked using SpheroScan, demonstrating more accurate results.</p><p>
<bold>Supplementary Fig. S7</bold>. Sample of spheroid images from external datasets. (A&#x02013;C) Fluorescence microscopy images. (D) Fluorescence (multichannel) microscopy image. (E, F) Brightfield microscopy images.</p><p>
<bold>Supplementary Fig. S8</bold>. Challenging scenarios encountered by SpheroScan. Image (A) contains a spheroid formed with a limited number of labeled cells exhibiting a hollow, spheroid-like structure, which was not identified by SpheroScan in this image. Images (B), (D), and (E) represent spheroid images with debris and irregular shapes, where SpheroScan mistakenly identified debris as spheroids at a prediction threshold of 0.8. To address this issue, the threshold was adjusted to 0.95 for image (B) and 0.9 for image (E), leading to correct masking, as shown in images (C) and (F), respectively. However, even after increasing the threshold, SpheroScan still failed to correctly mask the spheroid in image (D).</p></sec><sec id="h1content1696806592254"><title>Abbreviations</title><p>AP: average precision; CGCA: collagen gel contraction assay; CNN: convolutional neural network; FBS: fetal bovine serum; FCN: fully convolutional network; HEK: human embryonic kidney; mAP: mean average precision; R-CNN: Region-based Convolutional Neural Network; ROI: region of interest; RPN: region proposal network; SMC: smooth muscle cell; WEKA: Waikato Environment for Knowledge Analysis.</p></sec><sec id="sec9"><title>Availability of Supporting Source Code and Requirements</title><p>Project name: SpheroScan</p><p>Project homepage: <ext-link xlink:href="https://github.com/FunctionalUrology/SpheroScan" ext-link-type="uri">https://github.com/FunctionalUrology/SpheroScan</ext-link></p><p>BioTool ID: spheroscan</p><p>SciCrunch ID: SpheroScan (<ext-link xlink:href="https://scicrunch.org/resolver/RRID:SCR_023886" ext-link-type="uri">RRID:SCR_023886</ext-link>)</p><p>Operating system(s): Linux or Mac</p><p>Programming language: Python 3.10.6</p><p>Other requirements: Docker, Python, Anaconda, Git</p><p>License: GNU GPL</p></sec><sec id="sec10"><title>Authors&#x02019; Contributions</title><p>K.M., A.H.G., and A.A. conceived the idea for the manuscript. M.B. generated all the data. A.A. developed the deep learning pipeline. A.A. and M.K. developed the code for SpheroScan. K.M., F.C.B., and A.H.G. tested the SpheroScan and provided scientific inputs throughout the development phase. F.C.B., R.M.A., and A.B.A. provided the feedback on biological application of the tool. N.S. and M.A. provided the mathematical support and did the testing and debugging. All authors contributed to writing, proofreading, and correcting the manuscript.</p></sec><sec id="sec11"><title>Funding</title><p>We gratefully acknowledge the financial support of the Swiss National Science Foundation (SNF Grant 310030_175773 to F.C.B. and K.M., 212298 to F.C.B. and A.H.G.) and the Wings for Life Spinal Cord Research Foundation (WFL-AT-06/19 to K.M.). A.H.G. and R.M.A. are supported by R01 DK 077195 and R01 DK127673. M.K. is supported by the Else Kr&#x000f6;ner-Fresenius-Stiftung (EKFS 2021_EKeA.33). The authors acknowledge the financial support from the Federal Ministry of Education and Research of Germany and by the S&#x000e4;chsische Staatsministerium f&#x000fc;r Wissenschaft Kultur und Tourismus in the program Center of Excellence for AI-research &#x0201c;Center for Scalable Data Analytics and Artificial Intelligence Dresden/Leipzig&#x0201d; (project identification number: ScaDS.AI).</p></sec><sec sec-type="COI-statement" id="sec12"><title>Competing Interests</title><p>The authors have declared no competing interests.</p></sec><ref-list id="ref1"><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Br&#x000fc;ningk</surname>
<given-names>SC</given-names>
</string-name>, <string-name><surname>Rivens</surname><given-names>I</given-names></string-name>, <string-name><surname>Box</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>3D Tumour spheroids for the prediction of the effects of radiation and hyperthermia treatments</article-title>. <source>Sci Rep</source>. <year>2020</year>;<volume>10</volume>(<issue>1</issue>):<fpage>1653</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-020-58569-4</pub-id>.<pub-id pub-id-type="pmid">32015396</pub-id></mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Mehta</surname>
<given-names>G</given-names>
</string-name>, <string-name><surname>Hsiao</surname><given-names>AY</given-names></string-name>, <string-name><surname>Ingram</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Opportunities and challenges for use of tumor spheroids as models to test drug delivery and efficacy</article-title>. <source>J Controlled Release</source>. <year>2012</year>;<volume>164</volume>(<issue>2</issue>):<fpage>192</fpage>&#x02013;<lpage>204</lpage>. <pub-id pub-id-type="doi">10.1016/j.jconrel.2012.04.045</pub-id>.</mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Carragher</surname>
<given-names>N</given-names>
</string-name>, <string-name><surname>Piccinini</surname><given-names>F</given-names></string-name>, <string-name><surname>Tesei</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Concerns, challenges and promises of high-content analysis of 3D cellular models</article-title>. <source>Nat Rev Drug Discov</source>. <year>2018</year>;<volume>17</volume>(<issue>8</issue>):<fpage>606</fpage>. <pub-id pub-id-type="doi">10.1038/nrd.2018.99</pub-id>.<pub-id pub-id-type="pmid">29977053</pub-id></mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Smalley</surname>
<given-names>KS</given-names>
</string-name>, <string-name><surname>Lioni</surname><given-names>M</given-names></string-name>, <string-name><surname>Noma</surname><given-names>K</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>In vitro three-dimensional tumor microenvironment models for anticancer drug discovery</article-title>. <source>Expert Opin Drug Discovery</source>. <year>2008</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>&#x02013;<lpage>10</lpage>. <pub-id pub-id-type="doi">10.1517/17460441.3.1.1</pub-id>.</mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Spoerri</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Gunasingh</surname><given-names>G</given-names></string-name>, <string-name><surname>Haass</surname><given-names>NK.</given-names></string-name></person-group>
<article-title>Fluorescence-based quantitative and spatial analysis of tumour spheroids: a proposed tool to predict patient-specific therapy response</article-title>. <source>Frontiers in Digital Health</source>. <year>2021</year>;<volume>3</volume>:<fpage>668390</fpage>. <pub-id pub-id-type="doi">10.3389/fdgth.2021.668390</pub-id>.<pub-id pub-id-type="pmid">34713141</pub-id></mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname>
<given-names>Q</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>P</given-names></string-name>, <string-name><surname>Fang</surname><given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Collagen gel contraction assays: from modelling wound healing to quantifying cellular interactions with three-dimensional extracellular matrices</article-title>. <source>Eur J Cell Biol</source>. <year>2022</year>;<volume>101</volume>(<issue>3</issue>):<fpage>151253</fpage>. <pub-id pub-id-type="doi">10.1016/j.ejcb.2022.151253</pub-id>.<pub-id pub-id-type="pmid">35785635</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Moraes</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Simon</surname><given-names>AB</given-names></string-name>, <string-name><surname>Putnam</surname><given-names>AJ</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Aqueous two-phase printing of cell-containing contractile collagen microgels</article-title>. <source>Biomaterials</source>. <year>2013</year>;<volume>34</volume>(<issue>37</issue>):<fpage>9623</fpage>&#x02013;<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1016/j.biomaterials.2013.08.046</pub-id>.<pub-id pub-id-type="pmid">24034500</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Yamanishi</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Parigoris</surname><given-names>E</given-names></string-name>, <string-name><surname>Takayama</surname><given-names>S</given-names></string-name></person-group>. <article-title>Kinetic analysis of label-free microscale collagen gel contraction using machine learning-aided image analysis</article-title>. <source>Front Bioeng Biotechnol</source>. <year>2020</year>;<volume>8</volume>. <pub-id pub-id-type="doi">10.3389/fbioe.2020.582602</pub-id>.</mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hoque</surname>
<given-names>MT</given-names>
</string-name>, <string-name><surname>Windus</surname><given-names>LCE</given-names></string-name>, <string-name><surname>Lovitt</surname><given-names>CJ</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>PCaAnalyser: a 2D-image analysis based module for effective determination of prostate cancer progression in 3D culture</article-title>. <source>PLoS One</source>. <year>2013</year>;<volume>8</volume>(<issue>11</issue>):<fpage>e79865</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0079865</pub-id>.<pub-id pub-id-type="pmid">24278197</pub-id></mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ivanov</surname>
<given-names>DP</given-names>
</string-name>, <string-name><surname>Parker</surname><given-names>TL</given-names></string-name>, <string-name><surname>Walker</surname><given-names>DA</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Multiplexing spheroid volume, resazurin and acid phosphatase viability assays for high-throughput screening of tumour spheroids and stem cell neurospheres</article-title>. <source>PLoS One</source>. <year>2014</year>;<volume>9</volume>(<issue>8</issue>):<fpage>e103817</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0103817</pub-id>.<pub-id pub-id-type="pmid">25119185</pub-id></mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>W</given-names>
</string-name>, <string-name><surname>Wong</surname><given-names>C</given-names></string-name>, <string-name><surname>Vosburgh</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>High-throughput image analysis of tumor spheroids: a user-friendly software application to measure the size of spheroids automatically and accurately</article-title>. <source>J Visualized Experiments</source>. <year>2014</year>;<volume>89</volume>:<fpage>e51639</fpage>. <pub-id pub-id-type="doi">10.3791/51639</pub-id>.</mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Piccinini</surname>
<given-names>F.</given-names>
</string-name>
</person-group>
<article-title>AnaSP: a software suite for automatic image analysis of multicellular spheroids</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2015</year>;<volume>119</volume>(<issue>1</issue>):<fpage>43</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2015.02.006</pub-id>.<pub-id pub-id-type="pmid">25737369</pub-id></mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Monjaret</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Fernandes</surname><given-names>M</given-names></string-name>, <string-name><surname>Duchemin-Pelletier</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Fully automated one-step production of functional 3D tumor spheroids for high-content screening</article-title>. <source>SLAS Technology</source>. <year>2016</year>;<volume>21</volume>(<issue>2</issue>):<fpage>268</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1177/2211068215607058</pub-id>.</mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rueden</surname>
<given-names>CT</given-names>
</string-name>, <string-name><surname>Schindelin</surname><given-names>J</given-names></string-name>, <string-name><surname>Hiner</surname><given-names>MC</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>ImageJ2: imageJ for the next generation of scientific image data</article-title>. <source>BMC Bioinf</source>. <year>2017</year>;<volume>18</volume>(<issue>1</issue>):<fpage>529</fpage>. <pub-id pub-id-type="doi">10.1186/s12859-017-1934-z</pub-id>.</mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Moriconi</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Palmieri</surname><given-names>V</given-names></string-name>, <string-name><surname>Di&#x000a0;Santo</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>INSIDIA: a FIJI macro delivering high-throughput and high-content spheroid invasion analysis</article-title>. <source>Biotechnol J</source>. <year>2017</year>;<volume>12</volume>(<issue>10</issue>):<fpage>1700140</fpage>. <pub-id pub-id-type="doi">10.1002/biot.201700140</pub-id>.</mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Roerdink</surname>
<given-names>JBTM</given-names>
</string-name>, <string-name><surname>Meijster</surname><given-names>A.</given-names></string-name></person-group>
<article-title>The watershed Transform: definitions, algorithms and parallelization strategies</article-title>. <source>Fundam Inf</source>. <year>2000</year>;<volume>41</volume>(<issue>1&#x02013;2</issue>):<fpage>187</fpage>&#x02013;<lpage>228</lpage>.. <pub-id pub-id-type="doi">10.3233/FI-2000-411207</pub-id>.</mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Otsu</surname>
<given-names>N.</given-names>
</string-name>
</person-group>
<article-title>A threshold selection method from gray-level histograms</article-title>. <source>IEEE Trans Syst Man Cybern</source>. <year>1979</year>;<volume>9</volume>(<issue>1</issue>):<fpage>62</fpage>&#x02013;<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id>.</mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Yen</surname>
<given-names>J-C</given-names>
</string-name>, <string-name><surname>Chang</surname><given-names>F-J</given-names></string-name>, <string-name><surname>Chang</surname><given-names>S.</given-names></string-name></person-group>
<article-title>A new criterion for automatic multilevel thresholding</article-title>. <source>IEEE Trans Image Process</source>. <year>1995</year>;<volume>4</volume>(<issue>3</issue>):<fpage>370</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/83.366472</pub-id>.<pub-id pub-id-type="pmid">18289986</pub-id></mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Duda</surname>
<given-names>RO</given-names>
</string-name>, <string-name><surname>Hart</surname><given-names>PE.</given-names></string-name></person-group>
<article-title>Use of the Hough transformation to detect lines and curves in pictures</article-title>. <source>Commun ACM</source>. <year>1972</year>;<volume>15</volume>(<issue>1</issue>):<fpage>11</fpage>&#x02013;<lpage>15</lpage>. <pub-id pub-id-type="doi">10.1145/361237.361242</pub-id>.</mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Caselles</surname>
<given-names>V</given-names>
</string-name>, <string-name><surname>Kimmel</surname><given-names>R</given-names></string-name>, <string-name><surname>Sapiro</surname><given-names>G</given-names></string-name></person-group>. <article-title>Geodesic active contours</article-title>. <source>Int J Comput Vision</source>. <year>1997</year>;<volume>22</volume>(<issue>1</issue>):<fpage>61</fpage>&#x02013;<lpage>79</lpage>. <pub-id pub-id-type="doi">10.1023/A:1007979827043</pub-id>.</mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Salau</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Krieter</surname><given-names>J.</given-names></string-name></person-group>
<article-title>Instance segmentation with mask R-CNN applied to loose-housed dairy cows in a multi-camera setting</article-title>. <source>Animals</source>. <year>2020</year>;<volume>10</volume>(<issue>12</issue>):<fpage>2402</fpage>. <pub-id pub-id-type="doi">10.3390/ani10122402</pub-id>.<pub-id pub-id-type="pmid">33333993</pub-id></mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hong</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Han</surname><given-names>H-J</given-names></string-name>, <string-name><surname>Lee</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning method for comet segmentation and comet assay image analysis</article-title>. <source>Sci Rep</source>. <year>2020</year>;<volume>10</volume>(<issue>1</issue>):<fpage>18915</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-020-75592-7</pub-id>.<pub-id pub-id-type="pmid">33144610</pub-id></mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sun</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>T&#x000e1;rnok</surname><given-names>A</given-names></string-name>, <string-name><surname>Su</surname><given-names>X</given-names></string-name></person-group>. <article-title>Deep learning-based single-cell optical image studies</article-title>. <source>Cytometry Part A</source>. <year>2020</year>;<volume>97</volume>(<issue>3</issue>):<fpage>226</fpage>&#x02013;<lpage>40</lpage>. <pub-id pub-id-type="doi">10.1002/cyto.a.23973</pub-id>.</mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fudickar</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Nustede</surname><given-names>EJ</given-names></string-name>, <string-name><surname>Dreyer</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Elegans detection with a DIY microscope</article-title>. <source>Biosensors</source>. <year>2021</year>;<volume>11</volume>(<issue>8</issue>):<fpage>257</fpage>. <pub-id pub-id-type="doi">10.3390/bios11080257</pub-id>.<pub-id pub-id-type="pmid">34436059</pub-id></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Beleon</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Pignatta</surname><given-names>S</given-names></string-name>, <string-name><surname>Arienti</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>CometAnalyser: a user-friendly, open-source deep-learning microscopy tool for quantitative comet assay analysis</article-title>. <source>Comput Struct Biotechnol J</source>. <year>2022</year>;<volume>20</volume>:<fpage>4122</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.csbj.2022.07.053</pub-id>.<pub-id pub-id-type="pmid">36016714</pub-id></mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Abdul</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Rajasekar</surname><given-names>S</given-names></string-name>, <string-name><surname>Lin</surname><given-names>DSY</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep-LUMEN assay&#x02014;human lung epithelial spheroid classification from brightfield images using deep learning</article-title>. <source>Lab Chip</source>. <year>2020</year>;<volume>20</volume>(<issue>24</issue>):<fpage>4623</fpage>&#x02013;<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1039/D0LC01010C</pub-id>.<pub-id pub-id-type="pmid">33151236</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lacalle</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Castro-Abril</surname><given-names>HA</given-names></string-name>, <string-name><surname>Randelovic</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>SpheroidJ: an open-source set of tools for spheroid segmentation</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2021</year>;<volume>200</volume>:<fpage>105837</fpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105837</pub-id>.<pub-id pub-id-type="pmid">33221056</pub-id></mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Grexa</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Diosdi</surname><given-names>A</given-names></string-name>, <string-name><surname>Harmati</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>SpheroidPicker for automated 3D cell culture manipulation using deep learning</article-title>. <source>Sci Rep</source>. <year>2021</year>;<volume>11</volume>(<issue>1</issue>):<fpage>14813</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-94217-1</pub-id>.<pub-id pub-id-type="pmid">34285291</pub-id></mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>Z</given-names>
</string-name>, <string-name><surname>Ma</surname><given-names>N</given-names></string-name>, <string-name><surname>Sun</surname><given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Automated evaluation of tumor spheroid behavior in 3D culture using deep learning-based recognition</article-title>. <source>Biomaterials</source>. <year>2021</year>;<volume>272</volume>:<fpage>120770</fpage>. <pub-id pub-id-type="doi">10.1016/j.biomaterials.2021.120770</pub-id>.<pub-id pub-id-type="pmid">33798957</pub-id></mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Trossbach</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>&#x000c5;kerlund</surname><given-names>E</given-names></string-name>, <string-name><surname>Langer</surname><given-names>K</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>High-throughput cell spheroid production and assembly analysis by microfluidics and deep learning</article-title>. <source>SLAS Technology</source>. <year>2023</year>; <pub-id pub-id-type="doi">10.1016/j.slast.2023.03.003</pub-id>.</mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Piccinini</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Peirsman</surname><given-names>A</given-names></string-name>, <string-name><surname>Stellato</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deep learning-based tool for morphotypic analysis of 3D multicellular spheroids</article-title>. <source>J Mech Med Biol</source>. <year>2023</year>;<volume>23</volume>:<fpage>2340034</fpage>. <pub-id pub-id-type="doi">10.1142/S0219519423400341</pub-id>.</mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Peirsman</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Blondeel</surname><given-names>E</given-names></string-name>, <string-name><surname>Ahmed</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>MISpheroID: a knowledgebase and transparency tool for minimum information in spheroid identity</article-title>. <source>Nat Methods</source>. <year>2021</year>;<volume>18</volume>(<issue>11</issue>):<fpage>1294</fpage>&#x02013;<lpage>303</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01291-4</pub-id>.<pub-id pub-id-type="pmid">34725485</pub-id></mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Diosdi</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Hirling</surname><given-names>D</given-names></string-name>, <string-name><surname>Kovacs</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Cell lines and clearing approaches: a single-cell level 3D light-sheet fluorescence microscopy dataset of multicellular spheroids</article-title>. <source>Data Brief</source>. <year>2021</year>;<volume>36</volume>:<fpage>107090</fpage>. <pub-id pub-id-type="doi">10.1016/j.dib.2021.107090</pub-id>.<pub-id pub-id-type="pmid">34026984</pub-id></mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>N&#x000fc;rnberg</surname>
<given-names>E</given-names>
</string-name>, <string-name><surname>Vitacolonna</surname><given-names>M</given-names></string-name>, <string-name><surname>Klicks</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Routine optical clearing of 3D-cell cultures: simplicity forward</article-title>. <source>Front Mol Biosci</source>. <year>2020</year>;<volume>7</volume>:<fpage>20</fpage>. <pub-id pub-id-type="doi">10.3389/fmolb.2020.00020</pub-id>.<pub-id pub-id-type="pmid">32154265</pub-id></mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Hossain</surname>
<given-names>S</given-names>
</string-name>
</person-group>. <article-title>Visualization of bioinformatics data with Dash Bio</article-title>. In: <source>Proceedings of the 18th Python in Science Conference</source>. <person-group person-group-type="editor"><string-name><surname>Calloway</surname><given-names>C</given-names></string-name>, <string-name><surname>Lippa</surname><given-names>D</given-names></string-name>, <string-name><surname>Niederhut</surname><given-names>D</given-names></string-name>, <string-name><surname>Shupe</surname><given-names>D</given-names></string-name></person-group>, eds. <year>2019</year>; <fpage>126</fpage>&#x02013;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.25080/Majora-7ddc1dd1-012</pub-id></mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<collab>The Pandas Development Team</collab>
</person-group>. <article-title>Pandas-Dev/Pandas: Pandas</article-title>. <source>Zenodo</source>. <year>2020</year>. <pub-id pub-id-type="doi">10.5281/zenodo.3509134</pub-id>.</mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>McKinney</surname>
<given-names>W.</given-names>
</string-name>
</person-group>
<article-title>Data structures for statistical computing in Python</article-title>. In: <source>Proceedings of the 9th Python in Science Conference</source>. <person-group person-group-type="editor"><string-name><surname>van&#x000a0;der&#x000a0;Walt</surname><given-names>S</given-names></string-name>, <string-name><surname>Millman</surname><given-names>J</given-names></string-name></person-group>, eds. <year>2010</year>; <fpage>56</fpage>&#x02013;<lpage>61</lpage>. <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id></mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Dutta</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group>
<article-title>The VIA Annotation software for images, audio and video</article-title>. In: <source>Proceedings of the 27th ACM International Conference on Multimedia</source>; <comment>MM &#x02019;19</comment>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>; <year>2019</year>:<fpage>2276</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1145/3343031.3350535</pub-id>.</mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>van&#x000a0;Rossum</surname>
<given-names>G.</given-names>
</string-name>
</person-group>
<article-title>Python Reference Manual</article-title>. <publisher-name>Centre for Mathematics and Computer Science</publisher-name>, <publisher-loc>Amsterdam, Netherlands</publisher-loc>. <year>1995</year>.</mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Wu</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Kirillov</surname><given-names>A</given-names></string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name>, <etal>et al.</etal></person-group>
<comment>Detectron2. <italic toggle="yes">GitHub</italic></comment>. <year>2019</year>. <comment><ext-link xlink:href="https://github.com/facebookresearch/detectron2" ext-link-type="uri">https://github.com/facebookresearch/detectron2</ext-link></comment>.</mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ren</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>He</surname><given-names>K</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1137</fpage>&#x02013;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id>.<pub-id pub-id-type="pmid">27295650</pub-id></mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>He</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Gkioxari</surname><given-names>G</given-names></string-name>, <string-name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Mask R-CNN</article-title>. In: <source>2017 IEEE International Conference on Computer Vision (ICCV)</source>; <year>2017</year>; <fpage>2980</fpage>&#x02013;<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2017.322</pub-id></mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Howard</surname>
<given-names>AG</given-names>
</string-name>, <string-name><surname>Zhu</surname><given-names>M</given-names></string-name>, <string-name><surname>Chen</surname><given-names>B</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>MobileNets: efficient convolutional neural networks for mobile vision applications</article-title>. <comment>arXiv. April 16, 2017. </comment><pub-id pub-id-type="doi">10.48550/arXiv.1704.04861</pub-id>.</mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Redmon</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Divvala</surname><given-names>S</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>You only look once: unified, real-time object detection</article-title>. In: <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>; <publisher-loc>Las Vegas, NV, USA</publisher-loc>. <year>2016</year>; <fpage>779</fpage>&#x02013;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id></mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Everingham</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Van&#x000a0;Gool</surname><given-names>L</given-names></string-name>, <string-name><surname>Williams</surname><given-names>CKI</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Pascal visual object classes (VOC) Challenge</article-title>. <source>Int J Comput Vis</source>. <year>2010</year>;<volume>88</volume>(<issue>2</issue>):<fpage>303</fpage>&#x02013;<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id>.</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="other">
<comment>SpheroScan software repository. <italic toggle="yes">GitHub</italic></comment>. <year>2023</year>. <comment><ext-link xlink:href="https://github.com/FunctionalUrology/SpheroScan" ext-link-type="uri">https://github.com/FunctionalUrology/SpheroScan</ext-link></comment>.</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Akshay</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Katoch</surname><given-names>M</given-names></string-name>, <string-name><surname>Abedi</surname><given-names>M</given-names></string-name><etal>et al.</etal></person-group>, <article-title>Supporting data for &#x0201c;SpheroScan: a user-friendly deep learning tool for spheroid image analysis</article-title>.&#x0201d; <source>Zenodo</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.7555467</pub-id></mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Akshay</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Katoch</surname><given-names>M</given-names></string-name>, <string-name><surname>Abedi</surname><given-names>M</given-names></string-name><etal>et al.</etal></person-group>, <article-title>Trained model weights for &#x0201c;SpheroScan: a user-friendly deep learning tool for spheroid image analysis</article-title>.&#x0201d; <source>Zenodo</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.7552508</pub-id>.</mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Akshay</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Katoch</surname><given-names>M</given-names></string-name>, <string-name><surname>Abedi</surname><given-names>M</given-names></string-name><etal>et al.</etal></person-group>, <article-title>External test datasets for &#x0201c;SpheroScan: a user-friendly deep learning tool for spheroid image analysis</article-title>.&#x0201d; <source>Zenodo</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.5281/zenodo.8211845</pub-id>.</mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Akshay</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Katoch</surname><given-names>M</given-names></string-name>, <string-name><surname>Abedi</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Supporting data for &#x0201c;SpheroScan: A User-Friendly Deep Learning Tool for Spheroid Image Analysis.&#x0201d; GigaScience Database</article-title>. <year>2023</year>. <pub-id pub-id-type="doi">10.5524/102444</pub-id>.</mixed-citation></ref></ref-list></back></article>