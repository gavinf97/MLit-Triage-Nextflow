<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 1.2?><?ConverterInfo.XSLTName jats2jats3.xsl?><?ConverterInfo.Version 1?><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Gigascience</journal-id><journal-id journal-id-type="iso-abbrev">Gigascience</journal-id><journal-id journal-id-type="publisher-id">gigascience</journal-id><journal-title-group><journal-title>GigaScience</journal-title></journal-title-group><issn pub-type="epub">2047-217X</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta>
<article-meta><article-id pub-id-type="pmcid">11629979</article-id><article-id pub-id-type="doi">10.1093/gigascience/giae095</article-id><article-id pub-id-type="publisher-id">giae095</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI00960</subject><subject>AcademicSubjects/SCI02254</subject></subj-group></article-categories><title-group><article-title>Stratum corneum nanotexture feature detection using deep learning and spatial analysis: a noninvasive tool for skin barrier assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9214-5837</contrib-id><name><surname>Wang</surname><given-names>Jen-Hung</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="lead">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="lead">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="lead">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software" degree-contribution="lead">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="lead">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization" degree-contribution="lead">Visualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft" degree-contribution="lead">Writing - original draft</role><aff>
<institution>Department of Health Technology, Technical University of Denmark</institution>, <addr-line>Kongens Lyngby 2800</addr-line>, <country country="DK">Denmark</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4802-0591</contrib-id><name><surname>Pereda</surname><given-names>Jorge</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology" degree-contribution="supporting">Methodology</role><aff>
<institution>Department of Health Technology, Technical University of Denmark</institution>, <addr-line>Kongens Lyngby 2800</addr-line>, <country country="DK">Denmark</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0006-5517-1845</contrib-id><name><surname>Du</surname><given-names>Ching-Wen</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation" degree-contribution="equal">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><aff>
<institution>Department of Health Technology, Technical University of Denmark</institution>, <addr-line>Kongens Lyngby 2800</addr-line>, <country country="DK">Denmark</country></aff><aff>
<institution>Department of Dermatology, National Taiwan University Hospital and National Taiwan University College of Medicine</institution>, <addr-line>Taipei 100225</addr-line>, <country country="TW">Taiwan</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9370-3279</contrib-id><name><surname>Chu</surname><given-names>Chia-Yu</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="supporting">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="supporting">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="supporting">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><!--chiayu@ntu.edu.tw--><aff>
<institution>Department of Dermatology, National Taiwan University Hospital and National Taiwan University College of Medicine</institution>, <addr-line>Taipei 100225</addr-line>, <country country="TW">Taiwan</country></aff><xref rid="cor1" ref-type="corresp"/></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-7177-1054</contrib-id><name><surname>Christensen</surname><given-names>Maria Oberl&#x000e4;nder</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><aff>
<institution>Department of Dermatology, Bispebjerg and Frederiksberg Hospital (BFH), University Hospitals of Copenhagen</institution>, <addr-line>Copenhagen 2400</addr-line>, <country country="DK">Denmark</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-1063-4547</contrib-id><name><surname>Kezic</surname><given-names>Sanja</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><aff>
<institution>Department of Public and Occupational Health, Amsterdam Public Health Research Institute, Amsterdam University Medical Center</institution>, <addr-line>Amsterdam 1105</addr-line>, <country country="NL">The Netherlands</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-7961-4069</contrib-id><name><surname>Jakasa</surname><given-names>Ivone</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><aff>
<institution>Laboratory for Analytical Chemistry, Department of Chemistry and Biochemistry, Faculty of Food Technology and Biotechnology, University of Zagreb</institution>, <addr-line>Zagreb 10000</addr-line>, <country country="HR">Croatia</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3770-1743</contrib-id><name><surname>Thyssen</surname><given-names>Jacob P</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="supporting">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><aff>
<institution>Department of Dermatology, Bispebjerg and Frederiksberg Hospital (BFH), University Hospitals of Copenhagen</institution>, <addr-line>Copenhagen 2400</addr-line>, <country country="DK">Denmark</country></aff></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0009-0002-4239-2578</contrib-id><name><surname>Satheesh</surname><given-names>Sreeja</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation" degree-contribution="supporting">Validation</role><aff>
<institution>Institute of Solid State Physics, Leibniz University Hannover</institution>, <addr-line>Hannover 30167</addr-line>, <country country="DE">Germany</country></aff></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5971-4978</contrib-id><name><surname>Hwu</surname><given-names>Edwin En-Te</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization" degree-contribution="lead">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition" degree-contribution="lead">Funding acquisition</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation" degree-contribution="supporting">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration" degree-contribution="lead">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Resources" vocab-term-identifier="https://credit.niso.org/contributor-roles/resources" degree-contribution="lead">Resources</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision" degree-contribution="lead">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org" vocab-term="Writing - review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing" degree-contribution="supporting">Writing - review &#x00026; editing</role><!--etehw@dtu.dk--><aff>
<institution>Department of Health Technology, Technical University of Denmark</institution>, <addr-line>Kongens Lyngby 2800</addr-line>, <country country="DK">Denmark</country></aff><xref rid="cor2" ref-type="corresp"/></contrib></contrib-group><author-notes><corresp id="cor1">Correspondence address: Chia-Yu Chu, Department of Dermatology, National Taiwan University Hospital and National Taiwan University College of Medicine, 7 Chung San South Road, Taipei 100225, Taiwan. E-mail: <email>chiayu@ntu.edu.tw</email></corresp><corresp id="cor2">Correspondence address: Edwin En-Te Hwu, Department of Health Technology, Technical University of Denmark, &#x000d8;rsteds Plads, Building 345C, Kongens Lyngby 2800, Denmark. E-mail: <email>etehw@dtu.dk</email></corresp></author-notes><pub-date pub-type="collection"><year>2024</year></pub-date><pub-date pub-type="epub" iso-8601-date="2024-12-04"><day>04</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="pmc-release"><day>04</day><month>12</month><year>2024</year></pub-date><volume>13</volume><elocation-id>giae095</elocation-id><history><date date-type="received"><day>27</day><month>3</month><year>2024</year></date><date date-type="rev-recd"><day>23</day><month>9</month><year>2024</year></date><date date-type="accepted"><day>03</day><month>11</month><year>2024</year></date><date date-type="corrected-typeset"><day>04</day><month>12</month><year>2024</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2024. Published by Oxford University Press GigaScience.</copyright-statement><copyright-year>2024</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="giae095.pdf"/><abstract><title>Abstract</title><sec id="abs1"><title>Background</title><p>Corneocyte surface nanoscale topography (nanotexture) has recently emerged as a potential biomarker for inflammatory skin diseases, such as atopic dermatitis (AD). This assessment method involves quantifying circular nano-size objects (CNOs) in corneocyte nanotexture images, enabling noninvasive analysis via stratum corneum (SC) tape stripping. Current approaches for identifying CNOs rely on computer vision techniques with specific geometric criteria, resulting in inaccuracies due to the susceptibility of nano-imaging techniques to environmental noise and structural occlusion on the corneocyte.</p></sec><sec id="abs2"><title>Results</title><p>This study recruited 45 AD patients and 15 healthy controls, evenly divided into 4 severity groups based on their Eczema Area and Severity Index scores. Subsequently, we collected a dataset of over 1,000 corneocyte nanotexture images using our in-house high-speed dermal atomic force microscope. This dataset was utilized to train state-of-the-art deep learning object detectors for identifying CNOs. Additionally, we implemented a kernel density estimator to analyze the spatial distribution of CNOs, excluding ineffective regions with minimal CNO occurrence, such as ridges and occlusions, thereby enhancing accuracy in density calculations. After fine-tuning, our detection model achieved an overall accuracy of 91.4% in detecting CNOs.</p></sec><sec id="abs3"><title>Conclusions</title><p>By integrating deep learning object detector with spatial analysis algorithms, we developed a precise methodology for calculating CNO density, termed the Effective Corneocyte Topographical Index (ECTI). The ECTI demonstrated exceptional robustness to nano-imaging artifacts and presents substantial potential for advancing AD diagnostics by effectively distinguishing between SC samples of varying AD severity and healthy controls.</p></sec></abstract><kwd-group><kwd>atopic dermatitis (AD)</kwd><kwd>corneocyte surface topography</kwd><kwd>deep learning</kwd><kwd>object detection</kwd><kwd>kernel density estimator (KDE)</kwd><kwd>atomic force microscope (AFM)</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>LEO Foundation</institution><institution-id institution-id-type="DOI">10.13039/501100012331</institution-id></institution-wrap>
</funding-source><award-id>LF-OC-20-000370</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Novo Nordisk Foundation</institution><institution-id institution-id-type="DOI">10.13039/501100009708</institution-id></institution-wrap>
</funding-source><award-id>NNF22OC0076607</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>National Science and Technology Council of Taiwan</institution><institution-id institution-id-type="DOI">10.13039/100020595</institution-id></institution-wrap>
</funding-source><award-id>NSTC 112-2314-B-002-074-MY3</award-id></award-group><award-group award-type="grant"><funding-source>
<institution-wrap><institution>Intelligent Drug Delivery and Sensing using Microcontainers and Nanomechanics (IDUN)</institution></institution-wrap>
</funding-source></award-group></funding-group><counts><page-count count="10"/></counts></article-meta></front><body><sec sec-type="intro" id="sec1"><title>Introduction</title><p>Atopic dermatitis (AD) is a prevalent inflammatory skin disease, affecting approximately 20% of children and 5&#x02013;10% of adults in high-income countries [<xref rid="bib1" ref-type="bibr">1</xref>]. A multinational survey reported that 10&#x02013;20% of adult AD patients experience severe symptoms [<xref rid="bib2" ref-type="bibr">2</xref>]. The increasing severity of AD has been shown to significantly impact quality of life, yet reliable biomarkers for assessing disease severity are still lacking [<xref rid="bib3" ref-type="bibr">3</xref>]. Therefore, finding an accurate measure is crucial for effective disease management and evaluating treatment efficacy.</p><p>The Eczema Area and Severity Index (EASI) [<xref rid="bib4" ref-type="bibr">4</xref>] and SCORing AD (SCORAD) [<xref rid="bib5" ref-type="bibr">5</xref>] scores are the commonly used clinical tools for assessing AD severity, with a preference for the EASI [<xref rid="bib6" ref-type="bibr">6</xref>]. However, the EASI is limited by its moderate interrater reliability and a lack of interpretability data, particularly in defining the severity ranges of mild, moderate, and severe AD [<xref rid="bib7" ref-type="bibr">7</xref>, <xref rid="bib8" ref-type="bibr">8</xref>]. Additionally, the EASI assigns equal weight to both extent and severity, potentially leading to a heterogeneous patient population with the same EASI score [<xref rid="bib9" ref-type="bibr">9</xref>].</p><p>Recently, corneocyte surface nanoscale topography (nanotexture) has emerged as a potential biomarker for evaluating skin diseases, particularly through the quantification of circular nano-size objects (CNOs) in corneocyte nanotexture [<xref rid="bib10" ref-type="bibr">10&#x02013;13</xref>]. CNOs are nano-scale protrusions observed on the corneocyte surface that have been linked to skin barrier impairment [<xref rid="bib14" ref-type="bibr">14</xref>] and AD, although their exact nature and underlying causes remain unidentified [<xref rid="bib12" ref-type="bibr">12</xref>]. This biomarker enables noninvasive <italic toggle="yes">ex vivo</italic> analysis through stratum corneum (SC) tape stripping [<xref rid="bib15" ref-type="bibr">15</xref>], which may serve as an objective and efficient tool for assessing AD severity.</p><p>However, the current method, known as the Dermal Texture Index (DTI), identifies CNOs in corneocyte nanotexture images by utilizing computer vision techniques that rely on specific criteria, such as height, circularity index, and area of CNOs [<xref rid="bib10" ref-type="bibr">10</xref>, <xref rid="bib11" ref-type="bibr">11</xref>]. Consequently, this approach is prone to inaccuracies due to the susceptibility of nano-imaging techniques to environmental noise. Moreover, the DTI calculates CNO density across the entire corneocyte nanotexture image (20 &#x000d7; 20 &#x000b5;m<sup>2</sup>), which may include ineffective regions with minimal CNO occurrence, such as ridges and structural occlusions on the corneocyte surface, potentially compromising the accuracy of density calculations.</p><p>In this study, we used our in-house high-speed dermal atomic force microscope (HS-DAFM) [<xref rid="bib16" ref-type="bibr">16</xref>] to establish an extensive database of corneocyte nanotexture images, capturing various levels of AD severity. The collected data were then leveraged to train state-of-the-art deep learning object detectors for the accurate identification of corneocyte nanotexture features. To address potential inaccuracies and artifacts arising from the nano-imaging process, we further analyzed the spatial distribution of the detected features, aiming to enhance robustness in calculating CNO density. For statistical analyses, this study investigated variations in corneocyte surface topography across different levels of AD severity, as categorized by EASI scores. The objective was to improve current clinical methods used by physicians to assess AD severity, providing a more reliable and quantifiable evaluation tool.</p></sec><sec sec-type="materials|methods" id="sec2"><title>Materials and Methods</title><sec id="sec2-1"><title>Stratum corneum sample collection</title><p>This study included a total of 45 AD patients and 15 healthy controls in Taiwan (&#x02265;18 years). Ethics approval was obtained from the National Taiwan University Hospital (202204089RIND), and all participants provided written informed consent prior to participation. The sample size was estimated using Cochran&#x02019;s formula, based on a 6.7% prevalence of AD in the Taiwanese population [<xref rid="bib17" ref-type="bibr">17</xref>], with an 80% confidence level and a 5% margin of error [<xref rid="bib18" ref-type="bibr">18</xref>, <xref rid="bib19" ref-type="bibr">19</xref>]. The AD patients were evenly divided into 3 severity groups of 15 patients each, based on their EASI scores: G1 (AD mild, EASI = 0.1&#x02013;7.0), G2 (AD moderate, EASI = 7.1&#x02013;21.0), and G3 (AD severe, EASI &#x0003e; 21.0). The healthy controls were categorized as G4 (no AD history). We systematically collected SC samples from both lesional and nonlesional skin areas of each AD patient, ensuring a comprehensive representation of AD severity. No specific instructions were given regarding the interruption of topical treatment to ensure that the collected SC samples closely reflected real-world clinical scenarios. However, we acknowledge the potential influence of topical treatment at the lesional collection sites.</p><p>The SC samples were obtained using a standardized tape-stripping procedure [<xref rid="bib20" ref-type="bibr">20</xref>]. During sampling, we collected 5 consecutive circular adhesive tape strips (D101, 1.54 cm<sup>2</sup>, D-Squame; Clinical &#x00026; Derm) from the volar side of the forearm, approximately 10&#x000a0;cm below the elbow crease. Each tape strip was pressed onto the skin for 10 seconds using a pressure instrument (D500, D-Squame; Clinical &#x00026; Derm) to maintain a constant pressure of 225&#x000a0;g/cm<sup>2</sup>. Subsequently, we gently removed each tape strip with tweezers and stored them individually in sampling vials.</p><p>The initial 2 strips were excluded from analysis to minimize potential contamination or impurities on the skin surface. The third strip underwent RNA analysis [<xref rid="bib21" ref-type="bibr">21</xref>], the fourth strip was used for surface topography imaging with our HS-DAFM, and the fifth strip was analyzed for natural moisturizing factors (NMFs) [<xref rid="bib22" ref-type="bibr">22</xref>]. The SC tapes designated for atomic force microscope (AFM) topography measurement were stored at room temperature, while the remaining tapes were immediately stored at &#x02212;80&#x000b0;C until further analysis. This study focused on analyzing corneocyte surface topography as a potential biomarker for AD severity assessment. Results from RNA and NMF analyses will be detailed in upcoming publications.</p></sec><sec id="sec2-2"><title>Corneocyte surface topography dataset</title><p>To measure corneocyte nanotexture, we utilized an HS-DAFM equipped with an aluminum-coated silicon&#x02013;nitride AFM probe (spring constant of 0.03&#x000a0;N/m, CSC38/Al; MikroMasch) with a tip radius of 8&#x000a0;nm. The SC samples were measured in contact mode at a constant height, with the contact force maintained below 10 nN to ensure consistent measurement quality. The HS-DAFM scanner was calibrated using a piece of DVD data track layer (approximately 1 &#x000d7; 1 cm<sup>2</sup>) as the calibration sample [<xref rid="bib23" ref-type="bibr">23</xref>]. The DVD data tracks are characterized by a fixed period of 740&#x000a0;nm and a defined depth of 160&#x000a0;nm, allowing precise scanner calibration through their measurement.</p><p>For each SC sample, 10 random areas were selected to capture the surface topographical features of corneocytes, resulting in a comprehensive dataset of over 1,000 corneocyte nanotexture images. Each image was acquired at a resolution of 512 &#x000d7; 512 pixels, covering an imaging area of 20 &#x000d7; 20 &#x000b5;m<sup>2</sup>. The scanning range was chosen based on findings from [<xref rid="bib11" ref-type="bibr">11</xref>], which specify the typical dimensions of CNOs (273&#x000a0;nm in height and 305&#x000a0;nm in width), ensuring that the selected area is appropriate for capturing relevant nanoscale features.</p></sec><sec id="sec2-3"><title>Image preprocessing</title><p>Corneocyte nanotexture features are often challenging to discern due to the limited contrast level in AFM imaging [<xref rid="bib24" ref-type="bibr">24</xref>] and their intricate structured backgrounds [<xref rid="bib25" ref-type="bibr">25</xref>]. Therefore, we applied a series of image-processing techniques to enhance the visibility of minute features, such as CNOs, while effectively suppressing environmental noise. This enhancement facilitated the subsequent process of image annotation and CNO detection.</p><p>Initially, we applied Gaussian filtering to smooth the raw images [<xref rid="bib26" ref-type="bibr">26</xref>], followed by subtracting the mean intensity across each row to effectively mitigate striping artifacts in AFM imaging [<xref rid="bib27" ref-type="bibr">27</xref>, <xref rid="bib28" ref-type="bibr">28</xref>]. Subsequently, the images were normalized to a range of 0.0 to 1.0 to ensure consistent intensity levels across all samples. Finally, disk-shaped morphological elements, with diameters of 9 and 15 pixels, were applied as percentile filters, systematically scanning the entire image to enhance local contrast and improve the visibility of subtle features, such as CNOs [<xref rid="bib29" ref-type="bibr">29&#x02013;31</xref>].</p><p>Figure&#x000a0;<xref rid="fig1" ref-type="fig">1</xref> shows the result of the image enhancement algorithms, demonstrating improved visibility of CNOs in a corneocyte nanotexture image captured from an SC sample of an AD patient.</p><fig position="float" id="fig1"><label>Figure 1:</label><caption><p>Demonstration of a corneocyte nanotexture image before and after applying the image enhancement algorithms. (A) Original corneocyte nanotexture image captured using HS-DAFM. (B) Enhanced image revealing clearer CNO contours.</p></caption><graphic xlink:href="giae095fig1" position="float"/></fig></sec><sec id="sec2-4"><title>Training deep learning object detectors for CNO detection</title><p>Object detection is a critical task in computer vision that involves identifying and localizing objects within an image, and it has become a widely used technology in fields ranging from autonomous driving [<xref rid="bib32" ref-type="bibr">32</xref>, <xref rid="bib33" ref-type="bibr">33</xref>] to medical imaging [<xref rid="bib34" ref-type="bibr">34</xref>, <xref rid="bib35" ref-type="bibr">35</xref>]. In this study, we evaluated the performance of 2 state-of-the-art deep learning object detection approaches&#x02014;convolutional neural network (CNN)-based detectors [<xref rid="bib36" ref-type="bibr">36&#x02013;40</xref>] and transformer-based detectors [<xref rid="bib41" ref-type="bibr">41&#x02013;47</xref>]&#x02014;specifically for identifying CNOs in corneocyte nanotexture images.</p><p>Among CNN-based models, the YOLO (You Only Look Once) series [<xref rid="bib38" ref-type="bibr">38&#x02013;40</xref>, <xref rid="bib48" ref-type="bibr">48&#x02013;58</xref>] has emerged as the most popular framework for real-time object detection, renowned for its optimal balance between speed and accuracy [<xref rid="bib59" ref-type="bibr">59&#x02013;61</xref>]. The latest iteration, YOLOv10 [<xref rid="bib58" ref-type="bibr">58</xref>], introduces notable advancements, such as nonmaximum suppression (NMS)&#x02013;free training and large-kernel convolutions, which enhance its efficiency and accuracy, particularly in the detection of small, intricate features [<xref rid="bib62" ref-type="bibr">62</xref>, <xref rid="bib63" ref-type="bibr">63</xref>]. In contrast, transformer-based detectors enable end-to-end object detection [<xref rid="bib64" ref-type="bibr">64</xref>] by employing self-attention mechanisms, which eliminate the need for NMS postprocessing. Building on this framework, RT-DETR (Real-Time Detection Transformer) [<xref rid="bib65" ref-type="bibr">65</xref>, <xref rid="bib66" ref-type="bibr">66</xref>] further implements an efficient hybrid encoder and introduces uncertainty-minimal query selection to improve both accuracy and latency.</p><p>To train the object detectors, we systematically selected a dataset of 300 corneocyte nanotexture images with diverse AD severities. Each image was meticulously labeled, contributing a comprehensive dataset with an average of approximately 250 annotated CNOs per image and over 74,000 annotations in total. The dataset was then randomly split into 3 subsets for training and evaluating the object detectors: an 80% training set, a 10% validation set, and a 10% test set. Additionally, we applied a range of data augmentation techniques [<xref rid="bib67" ref-type="bibr">67</xref>, <xref rid="bib68" ref-type="bibr">68</xref>] to expand the training set 3-fold, including adjustments to brightness (&#x02212;25% to 25%), exposure (&#x02212;15% to 15%), blur (up to 1&#x000a0;pixel), noise (up to 2% of pixels), and Mosaic augmentation [<xref rid="bib48" ref-type="bibr">48</xref>].</p><p>In this study, we focused on fine-tuning YOLOv10 and RT-DETRv2 [<xref rid="bib66" ref-type="bibr">66</xref>] models for CNO detection using our corneocyte nanotexture image dataset. Specifically, we compared the performance of various scales within each model&#x02014;namely, YOLOv10-{N, S, M, B, L, X} and RT-DETRv2-{S, M, L, X}&#x02014;to determine the optimal configuration for CNO detection. All models were trained and evaluated on an NVIDIA Tesla T4 GPU in Google Colab, following the same train-from-scratch settings as in [<xref rid="bib58" ref-type="bibr">58</xref>, <xref rid="bib65" ref-type="bibr">65</xref>], respectively. Due to computational limitations, we adjusted the batch size as necessary. Detailed hyperparameter settings for each model are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S1</xref> and <xref rid="sup1" ref-type="supplementary-material">S2</xref> for further reference.</p></sec><sec id="sec2-5"><title>Spatial analysis using kernel density estimator</title><p>The calculation of CNO density can exhibit significant variability due to the high sensitivity of nano-imaging techniques to environmental noise and structural occlusions on the corneocyte surface. Moreover, regions such as ridges or fringes on the corneocyte tend to have minimal CNO presence, which may compromise the accuracy of density calculations. This inherent variability in CNO distribution poses challenges in obtaining consistent and reliable density estimates.</p><p>To address these issues, we implemented a kernel density estimator (KDE) [<xref rid="bib69" ref-type="bibr">69</xref>, <xref rid="bib70" ref-type="bibr">70</xref>] to generate a continuous, probabilistic density map that captures the spatial distribution of CNOs across the corneocyte surface. KDE provides a flexible framework to estimate densities from sparse and unevenly distributed data points, such as CNO coordinates, by smoothing the distribution over the entire surface. A critical parameter in KDE is the kernel&#x02019;s bandwidth (BW), which determines the smoothness of the density estimate. An overly small BW results in undersmoothing, amplifying minor variations and noise in the data, whereas an excessively large BW oversmooths the density map, potentially obscuring important structural details.</p><p>To optimize KDE performance, we empirically tuned the BW using cross-validation to balance between undersmoothing and oversmoothing [<xref rid="bib71" ref-type="bibr">71</xref>]. This approach ensures that the density map accurately reflects the spatial variation in CNO distribution, while minimizing the influence of noise or occlusion artifacts. As shown in Fig.&#x000a0;<xref rid="fig2" ref-type="fig">2</xref>, the selection of BW has a substantial impact on the KDE output, where smaller BW values emphasize localized variations while larger BW values result in a more homogenized density map. Additionally, we divided the KDE density map into 25 discrete layers to enable a more detailed analysis of CNO distribution across various regions of the corneocyte. This stratified method allowed us to isolate and exclude regions affected by occlusion or artifacts, thereby improving the robustness of the analysis.</p><fig position="float" id="fig2"><label>Figure 2:</label><caption><p>Optimal BW selection for KDE using cross-validation. (A) Corneocyte nanotexture image with detected CNOs marked as green spots. (B) Selected optimal BW = 38. (C) Example of undersmoothing (BW = 10). (D) Example of oversmoothing (BW = 60).</p></caption><graphic xlink:href="giae095fig2" position="float"/></fig><p>For subsequent analyses, we calculated the CNO density on the corneocyte surface by averaging the density values from the central 5 layers of the KDE density map, ensuring a more reliable representation of CNO distribution. In this study, the CNO density calculated using KDE was termed the Effective Corneocyte Topographical Index (ECTI).</p></sec></sec><sec id="sec3"><title>Analyses</title><sec id="sec3-1"><title>Comparative analysis of deep learning object detectors</title><p>In this section, we compare the performance of YOLOv10 and RT-DETRv2 models for CNO detection based on model scale, computational cost, detection accuracy, and inference speed. The standard average precision (AP) metrics [<xref rid="bib72" ref-type="bibr">72</xref>, <xref rid="bib73" ref-type="bibr">73</xref>] were used to evaluate detection accuracy. AP provides a unified score by integrating metrics such as recall, precision, and intersection over union (IoU), ensuring an unbiased performance assessment. AP50 refers to the AP calculated at a fixed IoU threshold of 0.5, whereas AP50-95 represents the mean AP across uniformly sampled IoU thresholds from 0.50 to 0.95, with a step size of 0.05 [<xref rid="bib74" ref-type="bibr">74</xref>]. The evaluation was conducted on a test set of 30 annotated corneocyte nanotexture images. In addition, latency was measured on an NVIDIA Tesla T4 GPU using TensorRT FP16 [<xref rid="bib75" ref-type="bibr">75</xref>], with all test images resized to 512 &#x000d7; 512 pixels to align with the resolution of the corneocyte nanotexture images.</p><p>Table&#x000a0;<xref rid="tbl1" ref-type="table">1</xref> presents the evaluation results of the YOLOv10 and RT-DETRv2 models, including the number of parameters, floating-point operations per second (FLOPS), AP at different IoU thresholds, and latency. Both object detectors achieve high AP50 scores above 83%; however, RT-DETRv2 exhibits lower AP50-95 scores compared to YOLOv10. The results show that YOLOv10 consistently outperforms RT-DETRv2 in detection accuracy across all model scales. Notably, the YOLOv10-L model achieves the highest accuracy, with an AP50 of 91.4% and an AP50-95 of 63.2%, exceeding the best-performing RT-DETRv2 variant (RT-DETRv2-S) with an AP50 of 87.6% and an AP50-95 of 39.6%.</p><table-wrap position="float" id="tbl1"><label>Table 1:</label><caption><p>Performance comparison of YOLOv10 and RT-DETRv2 object detectors across various model scales. The table evaluates the models in terms of the number of parameters (M), FLOPS (G), AP50 (%), AP50-95 (%), and latency (ms).<sup>a</sup></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1">Model</th><th rowspan="1" colspan="1"># Parameter (M)</th><th rowspan="1" colspan="1">FLOPS (G)</th><th rowspan="1" colspan="1">AP50 (%)</th><th rowspan="1" colspan="1">AP50-95 (%)</th><th rowspan="1" colspan="1">Latency (ms)</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">YOLOv10-N</td><td rowspan="1" colspan="1">2.7</td><td rowspan="1" colspan="1">8.2</td><td rowspan="1" colspan="1">89.6</td><td rowspan="1" colspan="1">51.4</td><td rowspan="1" colspan="1">3.33</td></tr><tr><td rowspan="1" colspan="1">YOLOv10-S</td><td rowspan="1" colspan="1">8.0</td><td rowspan="1" colspan="1">24.4</td><td rowspan="1" colspan="1">90.8</td><td rowspan="1" colspan="1">55.5</td><td rowspan="1" colspan="1">4.58</td></tr><tr><td rowspan="1" colspan="1">YOLOv10-M</td><td rowspan="1" colspan="1">16.5</td><td rowspan="1" colspan="1">63.4</td><td rowspan="1" colspan="1">91.3</td><td rowspan="1" colspan="1">59.7</td><td rowspan="1" colspan="1">7.17</td></tr><tr><td rowspan="1" colspan="1">YOLOv10-B</td><td rowspan="1" colspan="1">20.4</td><td rowspan="1" colspan="1">97.7</td><td rowspan="1" colspan="1">91.1</td><td rowspan="1" colspan="1">62.5</td><td rowspan="1" colspan="1">7.58</td></tr><tr><td rowspan="1" colspan="1">YOLOv10-L</td><td rowspan="1" colspan="1">25.7</td><td rowspan="1" colspan="1">126.3</td><td rowspan="1" colspan="1">91.4</td><td rowspan="1" colspan="1">63.2</td><td rowspan="1" colspan="1">9.01</td></tr><tr><td rowspan="1" colspan="1">YOLOv10-X</td><td rowspan="1" colspan="1">31.6</td><td rowspan="1" colspan="1">169.8</td><td rowspan="1" colspan="1">91.2</td><td rowspan="1" colspan="1">62.9</td><td rowspan="1" colspan="1">10.95</td></tr><tr><td rowspan="1" colspan="1">RT-DETRv2-S</td><td rowspan="1" colspan="1">20.0</td><td rowspan="1" colspan="1">60.0</td><td rowspan="1" colspan="1">87.6</td><td rowspan="1" colspan="1">39.6</td><td rowspan="1" colspan="1">5.51</td></tr><tr><td rowspan="1" colspan="1">RT-DETRv2-M</td><td rowspan="1" colspan="1">31.0</td><td rowspan="1" colspan="1">100.0</td><td rowspan="1" colspan="1">84.0</td><td rowspan="1" colspan="1">37.2</td><td rowspan="1" colspan="1">7.48</td></tr><tr><td rowspan="1" colspan="1">RT-DETRv2-L</td><td rowspan="1" colspan="1">42.0</td><td rowspan="1" colspan="1">136.0</td><td rowspan="1" colspan="1">84.3</td><td rowspan="1" colspan="1">33.4</td><td rowspan="1" colspan="1">13.50</td></tr><tr><td rowspan="1" colspan="1">RT-DETRv2-X</td><td rowspan="1" colspan="1">76.0</td><td rowspan="1" colspan="1">259.0</td><td rowspan="1" colspan="1">83.3</td><td rowspan="1" colspan="1">32.0</td><td rowspan="1" colspan="1">21.15</td></tr></tbody></table><table-wrap-foot><fn id="tbl1fn1"><p>
<sup>a</sup>{N, S, M, B, L, X} indicate nano, small, medium, balanced, large, and extra-large models.</p></fn></table-wrap-foot></table-wrap><p>In terms of inference speed, both models are capable of real-time object detection. However, when comparing models of similar scales, such as YOLOv10-B with RT-DETRv2-S and YOLOv10-X with RT-DETRv2-M, RT-DETRv2 generally demonstrates lower computational costs (FLOPS) and reduced latency.</p></sec><sec id="sec3-2"><title>Qualitative results</title><p>Figure&#x000a0;<xref rid="fig3" ref-type="fig">3</xref> presents the qualitative results of applying the fine-tuned YOLOv10-L model to detect CNOs on corneocyte nanotexture images with different AD severity levels (G1, G2, G3, G4). The confidence threshold was set to 0.141, as this value achieved the highest F1 score [<xref rid="bib76" ref-type="bibr">76</xref>] of 0.85, providing an optimal balance between precision and recall. The F1 confidence curve for the fine-tuned YOLOv10-L model is provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref>. The results demonstrate the model&#x02019;s capability to accurately quantify in quantifying CNOs, even in the presence of vibrational noise introduced during topographic imaging.</p><fig position="float" id="fig3"><label>Figure 3:</label><caption><p>CNO detection results using YOLOv10-L model with a confidence threshold of 0.141. (A) Mild AD sample (CNO count = 180). (B) Moderate AD sample (CNO count = 250). (C) Severe AD sample (CNO count = 483). (D) Healthy control (CNO count = 22).</p></caption><graphic xlink:href="giae095fig3" position="float"/></fig><p>Figure&#x000a0;<xref rid="fig4" ref-type="fig">4</xref> presents the analysis results of CNO distribution using KDE, in which the algorithm generates a density map representing the spatial distribution of CNOs. This process effectively excludes regions with ridges or occlusions, thereby improving the accuracy of CNO density calculations.</p><fig position="float" id="fig4"><label>Figure 4:</label><caption><p>Spatial analysis of CNO distribution using KDE. (A) Corneocyte nanotexture image visualizing the presence of prominent ridges. (B) Corneocyte nanotexture image visualizing an area affected by occlusion. The KDE maps illustrate varying CNO densities, with brighter regions indicating higher densities and darker regions representing lower densities.</p></caption><graphic xlink:href="giae095fig4" position="float"/></fig></sec><sec id="sec3-3"><title>Ablation study on KDE</title><p>To evaluate the impact of KDE on the variability of CNO density calculations, we conducted an ablation study, comparing results with and without KDE across different AD severity groups. The coefficient of variation (CV) [<xref rid="bib77" ref-type="bibr">77</xref>] was used as a measure of variability, with lower CV values indicating more stable and consistent density estimates. For each SC sample, the CV was calculated from 10 density estimates derived from its corneocyte nanotexture images. The mean CV for each AD severity group was then determined by averaging the CVs of all samples within the group.</p><p>As shown in Fig.&#x000a0;<xref rid="fig5" ref-type="fig">5</xref>, the application of KDE led to a notable reduction in the CV across most AD groups (G1 to G3), while G4 remained nearly unchanged. Without KDE, the CV values were consistently higher, indicating higher variability in the raw CNO density calculations. Specifically, applying KDE resulted in a reduction of 7.95% in G1 (from 0.440 to 0.405), 18.5% in G2 (from 0.432 to 0.352), 13.0% in G3 (from 0.399 to 0.347), and a slight increase of 1.1% in G4 (from 0.375 to 0.379).</p><fig position="float" id="fig5"><label>Figure 5:</label><caption><p>Effect of KDE on the variability in CNO density calculations across AD severity groups (G1, G2, G3, G4). The gray bars indicate CV values without KDE; the black bars show CV values with KDE applied.</p></caption><graphic xlink:href="giae095fig5" position="float"/></fig><p>The ablation study demonstrates the effectiveness of KDE in generating more robust density estimates by reducing variability, particularly in AD groups (G1 to G3) with higher CNO presence.</p></sec><sec id="sec3-4"><title>Statistical analysis</title><p>The mean ECTI scores, derived from the KDE analyses of 10 corneocyte nanotexture images per SC tape, were used for statistical analyses. Each AD group (G1, G2, G3) contributed a total of 30 data points, comprising 15 from lesional and 15 from nonlesional SC samples. In contrast, the healthy control group (G4) contributed 15 data points exclusively from nonlesional SC samples. All images were preprocessed and CNOs were identified using the fine-tuned YOLOv10-L models.</p><p>Initially, samples from each AD severity group (G1, G2, G3, G4) underwent the Shapiro&#x02013;Wilk normality test [<xref rid="bib78" ref-type="bibr">78</xref>] to assess their data distribution. Given the nonnormal distribution observed in most data groups, the Wilcoxon signed-rank test [<xref rid="bib79" ref-type="bibr">79</xref>] was adopted to determine statistically significant differences between paired samples, focusing on the comparison of lesional and nonlesional SC samples from the same AD patient. In addition, the Wilcoxon rank-sum test [<xref rid="bib80" ref-type="bibr">80</xref>] was applied to identify significant differences between independent sample groups, specifically among the AD severity groups G1, G2, G3, and G4. Samples with missing data or those that could not be paired for comparison were excluded from the analysis.</p><p>Figure&#x000a0;<xref rid="fig6" ref-type="fig">6A</xref> presents the statistical results using box plots, further subdividing each AD severity group into lesional and nonlesional sampled areas. Overall, the plot reveals a clear trend of increasing ECTI scores corresponding to the AD severity. Most AD severity groups exhibit significant differences between lesional and nonlesional SC samples, indicating a higher occurrence of CNOs in the lesional skin areas. Additionally, the healthy controls (G4) consistently demonstrate the lowest ECTI scores compared to other AD severity groups. Figure <xref rid="fig6" ref-type="fig">6B</xref> presents the statistical analysis of nonlesional SC samples across AD severity groups (G1, G2, G3) compared to the healthy control group (G4), demonstrating significant differences between the AD groups and the healthy controls.</p><fig position="float" id="fig6"><label>Figure 6:</label><caption><p>Statistical results of ECTI scores in SC samples from AD patients (<italic toggle="yes">n</italic> = 15 for both lesional and nonlesional skin areas in each group) and healthy controls (<italic toggle="yes">n</italic> = 15). (A) Comparison of ECTI scores between lesional and nonlesional SC samples across AD severity groups. (B) ECTI scores for nonlesional SC samples. (C) CNO density in nonlesional SC samples calculated over the entire imaging area (20 &#x000d7; 20 &#x000b5;m<sup>2</sup>) without KDE. Box plot notations: ns &#x02192; not significant, *<italic toggle="yes">P</italic> &#x02264; 0.05, **<italic toggle="yes">P</italic> &#x02264; 0.01; AD severity groups according to EASI score: G1 &#x02192; mild AD, G2 &#x02192; moderate AD, G3 &#x02192; severe AD, G4 &#x02192; healthy controls.</p></caption><graphic xlink:href="giae095fig6" position="float"/></fig><p>Figure&#x000a0;<xref rid="fig6" ref-type="fig">6C</xref> provides a comparative analysis of CNO density in nonlesional SC samples calculated over the entire imaging area (20 &#x000d7; 20 &#x000b5;m<sup>2</sup>) without using KDE to exclude ineffective regions. The results demonstrate less significant differences between AD severity groups and the healthy control group, particularly being unable to differentiate between mild AD (G1) and healthy controls.</p></sec></sec><sec sec-type="discussion" id="sec4"><title>Discussion</title><p>The findings of this study demonstrated the potential of corneocyte nanotexture as a reliable biomarker for assessing AD severity, particularly through CNO density calculation. By integrating state-of-the-art deep learning object detectors with spatial analysis algorithms, we proposed the ECTI, an accurate and quantifiable measure for evaluating skin barrier impairment [<xref rid="bib14" ref-type="bibr">14</xref>]. The ECTI exhibited remarkable robustness in overcoming the inherent challenges of nano-imaging, such as environmental noise and structural occlusions on the corneocyte surface, further enhancing its applicability in clinical settings.</p><p>Previous studies revealed significant differences in corneocyte nanotexture between healthy and AD skin samples without specifying the clinical scoring of AD severity, resulting in a lack of in-depth analysis for AD severity assessment [<xref rid="bib12" ref-type="bibr">12</xref>]. In our study, we conducted statistical analyses of ECTI scores across different AD severity groups (G1, G2, G3, G4), categorized by their EASI scores. The results revealed a clear trend of increasing ECTI scores with higher AD severity and demonstrated significant differences between AD skin samples of varying severity and healthy controls, in both lesional and nonlesional skin areas. This finding aligns with clinical observations of AD severity, offering clinicians a more objective tool for assessing the skin disease.</p><p>By leveraging deep learning object detectors, we addressed the limitations of the existing DTI method, which is prone to inaccuracies due to its dependence on fixed geometric criteria for CNO identification. To determine the optimal model architecture for CNO detection, we evaluated the performance of 2 state-of-the-art object detectors with various scales: YOLOv10-{N, S, M, B, L, X} and RT-DETRv2-{S, M, L, X}. Both models demonstrated robust performance in CNO detection, with the YOLOv10-L model achieving the highest overall accuracy (AP50) of 91.4%. Although RT-DETRv2 exhibited enhanced computational efficiency at comparable model complexities, the YOLOv10 models were more suitable for this study due to their higher detection accuracy.</p><p>Furthermore, we applied KDE to perform spatial analysis of CNO distribution. Unlike the DTI, which calculates CNO density across the entire corneocyte nanotexture image (20 &#x000d7; 20 <sup>2</sup> &#x000b5;m ) without excluding ineffective regions such as ridges and occlusions, our approach selectively excluded these areas to minimize variance in CNO density calculations. This refinement provided a more precise representation of CNO density, enabling us to effectively distinguish between mild AD (G1) and healthy controls (G4) in nonlesional SC samples.</p><p>Future work could involve expanding the corneocyte nanotexture database to include a wider range of skin diseases and conditions, providing a more comprehensive and interpretable framework for evaluating skin health through corneocyte nanotexture analysis. Additionally, integrating our findings into clinical practice could substantially improve AD severity assessment by offering an objective and quantifiable evaluation method. Clinicians could utilize corneocyte nanotexture analysis as an accessible and effective tool to monitor disease progression, assess treatment efficacy, and personalize therapeutic interventions for routine clinical use.</p><p>This study also acknowledges certain limitations. First, while the sample size in this study is adequate for preliminary analysis, it may not fully capture the variability within the broader population, particularly across diverse ethnic groups and age ranges. Second, the variability in sample collection could lead to inconsistencies. Although a standardized tape-stripping procedure was employed, variations in local eczema severity, exact sampling locations, and individual skin conditions could contribute to discrepancies in the collected SC samples. Moreover, the lack of data on factors such as emollient use, sun exposure, or bathing habits prior to sampling may further affect the results. Finally, as this study focused on AD, the applicability of our approach to other dermatological conditions remains to be validated, necessitating further research to generalize these findings to a wider range of skin diseases.</p></sec><sec sec-type="conclusions" id="sec5"><title>Conclusion</title><p>This study presents a novel methodology that integrates deep learning object detection with spatial analysis to enable robust and accurate CNO density calculation within corneocyte surface topography. The ECTI was introduced as a quantifiable measure for assessing AD severity. Our results revealed significant differences in ECTI scores between SC samples of varying AD severity and healthy controls, in both lesional and nonlesional skin areas, demonstrating its potential as a reliable biomarker for AD assessment. Future work will focus on expanding the corneocyte nanotexture database and exploring the potential of ECTI in broader dermatological applications.</p></sec><sec sec-type="supplementary-material" id="sec7"><title>Additional Files</title><p>
<bold>Supplementary Table S1</bold>. Hyperparameter settings of YOLOv10.</p><p>
<bold>Supplementary Table S2</bold>. Hyperparameter settings of RT-DETRv2.</p><p>
<bold>Supplementary Fig. S1</bold>. Training results of YOLOv10-L on the corneocyte nanotexture dataset. The box loss (box) measures the error in predicted bounding box coordinates, the classification loss (cls) quantifies the error in class predictions, and distribution focal loss (dfl) adjusts the bounding box regression by focusing on more challenging examples to improve precision. &#x0201c;om&#x0201d; denotes evaluation on the training set, and &#x0201c;oo&#x0201d; indicates evaluation on the validation set.</p><p>
<bold>Supplementary Fig. S2</bold>. F1 confidence curve of YOLOv10-L on the corneocyte nanotexture test set. This curve illustrates the relationship between the confidence threshold and the F1 score, with the highest F1 score of 0.85 achieved at a confidence threshold of 0.141. This point indicates the optimal balance between precision and recall for the model.</p><supplementary-material id="sup1" position="float" content-type="local-data"><label>giae095_Supplementary_Figures_and_Tables</label><media xlink:href="giae095_supplementary_figures_and_tables.pdf"/></supplementary-material><supplementary-material id="sup2" position="float" content-type="local-data"><label>giae095_GIGA-D-24-00100_Original_Submission</label><media xlink:href="giae095_giga-d-24-00100_original_submission.pdf"/></supplementary-material><supplementary-material id="sup3" position="float" content-type="local-data"><label>giae095_GIGA-D-24-00100_Revision_1</label><media xlink:href="giae095_giga-d-24-00100_revision_1.pdf"/></supplementary-material><supplementary-material id="sup4" position="float" content-type="local-data"><label>giae095_Response_to_Reviewer_Comments_Original_Submission</label><media xlink:href="giae095_response_to_reviewer_comments_original_submission.pdf"/></supplementary-material><supplementary-material id="sup5" position="float" content-type="local-data"><label>giae095_Reviewer_1_Report_Original_Submission</label><caption><p>Xijian Fan -- 6/23/2024</p></caption><media xlink:href="giae095_reviewer_1_report_original_submission.pdf"/></supplementary-material><supplementary-material id="sup6" position="float" content-type="local-data"><label>giae095_Reviewer_1_Report_Revision_1</label><caption><p>Xijian Fan -- 10/19/2024</p></caption><media xlink:href="giae095_reviewer_1_report_revision_1.pdf"/></supplementary-material><supplementary-material id="sup7" position="float" content-type="local-data"><label>giae095_Reviewer_2_Report_Original_Submission</label><caption><p>Zhicheng Zhang -- 7/5/2024</p></caption><media xlink:href="giae095_reviewer_2_report_original_submission.pdf"/></supplementary-material><supplementary-material id="sup8" position="float" content-type="local-data"><label>giae095_Reviewer_2_Report_Revision_1</label><caption><p>Zhicheng Zhang -- 10/13/2024</p></caption><media xlink:href="giae095_reviewer_2_report_revision_1.pdf"/></supplementary-material></sec><sec id="sec8"><title>Abbreviations</title><p>AD: atopic dermatitis; AFM: atomic force microscope; AP: average precision; BW: bandwidth; CNN: convolutional neural network; CNO: circular nano-size object; CV: coefficient of variation; DTI: Dermal Texture Index; EASI: Eczema Area and Severity Index; ECTI: Effective Corneocyte Topographical Index; FLOPS: floating-point operations per second; HS-DAFM: high-speed dermal atomic force microscope; IoU: intersection over union; KDE: kernel density estimator; NMF: natural moisturizing factor; NMS: nonmaximum suppression; RT-DETR: real-time detection transformer; SC: stratum corneum; SCORAD: SCORing AD; YOLO: You Only Look Once.</p></sec></body><back><sec><title>Author Contributions</title><p>Jen-Hung Wang (Conceptualization [lead], Data curation [lead], Investigation [lead], Methodology [lead], Software [lead], Validation [lead], Visualization [lead], Writing &#x02013; original draft [lead]), Jorge Pereda (Conceptualization [supporting], Methodology [supporting]), Ching-Wen Du (Data curation [equal], Investigation [supporting]), Chia-Yu Chu (Conceptualization [lead], Funding acquisition [supporting], Investigation [supporting], Resources [supporting], Supervision [supporting], Writing &#x02013; review &#x00026; editing [supporting]), Maria Oberl&#x000e4;nder Christensen (Investigation [supporting], Writing &#x02013; review &#x00026; editing [supporting]), Sanja Kezic (Conceptualization [supporting], Investigation [supporting], Writing &#x02013; review &#x00026; editing [supporting]), Ivone Jakasa (Investigation [supporting], Writing &#x02013; review &#x00026; editing [supporting]), Jacob P. Thyssen (Conceptualization [supporting], Writing &#x02013; review &#x00026; editing [supporting]), Sreeja Satheesh (Validation [supporting]), and Edwin En-Te Hwu (Conceptualization [lead], Funding acquisition [lead], Investigation [supporting], Project administration [lead], Resources [lead], Supervision [lead], Writing &#x02013; review &#x00026; editing [supporting])</p></sec><sec><title>Funding</title><p>This project has received funding from the LEO Foundation under the open competition grant agreement No. LF-OC-20-000370, the Novo Nordisk Foundation under the Pioneer Innovator grant agreement No. NNF22OC0076607, the National Science and Technology Council of Taiwan (NSTC 112-2314-B-002-074-MY3), and the Intelligent Drug Delivery and Sensing using Microcontainers and Nanomechanics (IDUN).</p></sec><sec id="sec6"><title>Availability of Source Code and Requirements</title><list list-type="bullet"><list-item><p>Project name: ECTI Atopic Dermatitis</p></list-item><list-item><p>Project homepage: GitHub link: <ext-link xlink:href="https://github.com/JenHungWang/ECTI_Atopic_Dermatitis" ext-link-type="uri">https://github.com/JenHungWang/ECTI_Atopic_Dermatitis</ext-link></p></list-item><list-item><p>Operating system(s): Platform independent</p></list-item><list-item><p>Programming language: Python 3.11.4</p></list-item><list-item><p>Other requirements: Python 3.10+, matplotlib 3.7.2, numpy 1.25.1, opencv-python 4.8.0.74, scipy 1.11.1, scikit-image 0.21.0, scikit-learn 1.3.1, ultralytics 8.2.95, customtkinter 5.2.1</p></list-item><list-item><p>License: PSF, BSD, Apache, AGPL-3.0</p></list-item><list-item><p>Workflowhub: <ext-link xlink:href="https://doi.org/10.48546/workflowhub" ext-link-type="uri">https://doi.org/10.48546/workflowhub.workflow.1161.1</ext-link></p></list-item><list-item><p>ECTI is registered as a software application on SciCrunch (<ext-link xlink:href="https://scicrunch.org/resolver/RRID" ext-link-type="uri">RRID</ext-link>: SCR_025706) and biotools (biotools:ecti_atopic_dermatitis)</p></list-item></list></sec><sec sec-type="data-availability"><title>Data Availability</title><p>The corneocyte nanotexture dataset, along with the annotations used to train YOLOv10 and RT-DETRv2 object detection models, is available in the GitHub repository [<xref rid="bib81" ref-type="bibr">81</xref>]. Fine-tuned models and source code can also be downloaded from the same repository. Workflows are archived in WorkflowHub.eu [<xref rid="bib83_156_270524" ref-type="bibr">82</xref>] and all additional supporting data and materials are accessible via the <italic toggle="yes">GigaScience</italic> database, GigaDB [<xref rid="bib82" ref-type="bibr">83</xref>].</p></sec><sec sec-type="COI-statement"><title>Competing Interests</title><p>The authors declare that they have no competing interests.</p></sec><ref-list id="ref1"><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Langan</surname>
<given-names>SM</given-names>
</string-name>, <string-name><surname>Irvine</surname><given-names>AD</given-names></string-name>, <string-name><surname>Weidinger</surname><given-names>S</given-names></string-name></person-group>. <article-title>Atopic dermatitis</article-title>. <source>Lancet</source>. <year>2020</year>;<volume>396</volume>(<issue>10247</issue>):<fpage>345</fpage>&#x02013;<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1016/S0140-6736(20)31286-1</pub-id>.<pub-id pub-id-type="pmid">32738956</pub-id>
</mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Barbarot</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Auziere</surname><given-names>S</given-names></string-name>, <string-name><surname>Gadkari</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Epidemiology of atopic dermatitis in adults: results from an international survey</article-title>. <source>Allergy</source>. <year>2018</year>;<volume>73</volume>:<fpage>1284</fpage>&#x02013;<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1111/all.13401</pub-id>.<pub-id pub-id-type="pmid">29319189</pub-id>
</mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Drucker</surname>
<given-names>AM</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>AR</given-names></string-name>, <string-name><surname>Li</surname><given-names>WQ</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The burden of atopic dermatitis: summary of a report for the National Eczema Association</article-title>. <source>J Invest Dermatol</source>. <year>2017</year>;<volume>137</volume>(<issue>1</issue>):<fpage>26</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.jid.2016.07.012</pub-id>.<pub-id pub-id-type="pmid">27616422</pub-id>
</mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hanifin</surname>
<given-names>JM</given-names>
</string-name>, <string-name><surname>Thurston</surname><given-names>M</given-names></string-name>, <string-name><surname>Omoto</surname><given-names>M</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Eczema Area and Severity Index (EASI): assessment of reliability in atopic dermatitis</article-title>. <source>Exp Dermatol</source>. <year>2001</year>;<volume>10</volume>:<fpage>11</fpage>&#x02013;<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1034/j.1600-0625.2001.100102.x</pub-id>.<pub-id pub-id-type="pmid">11168575</pub-id>
</mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kunz</surname>
<given-names>B</given-names>
</string-name>, <string-name><surname>Oranje</surname><given-names>AP</given-names></string-name>, <string-name><surname>Labr&#x000e8;ze</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Clinical validation and guidelines for the SCORAD Index: consensus report of the European Task Force on Atopic Dermatitis</article-title>. <source>Dermatology</source>. <year>1997</year>;<volume>195</volume>:<fpage>10</fpage>&#x02013;<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1159/000245677</pub-id>.</mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhao</surname>
<given-names>CY</given-names>
</string-name>, <string-name><surname>Tran</surname><given-names>AQT</given-names></string-name>, <string-name><surname>Lazo-Dizon</surname><given-names>JP</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A pilot comparison study of four clinician-rated atopic dermatitis severity scales</article-title>. <source>Br J Dermatol</source>. <year>2015</year>;<volume>173</volume>:<fpage>488</fpage>&#x02013;<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1111/bjd.13846</pub-id>.<pub-id pub-id-type="pmid">25891151</pub-id>
</mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schmitt</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Langan</surname><given-names>S</given-names></string-name>, <string-name><surname>Deckert</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Assessment of clinical signs of atopic dermatitis: a systematic review and recommendation</article-title>. <source>J Allergy Clin Immunol</source>. <year>2013</year>;<volume>132</volume>:<fpage>1337</fpage>&#x02013;<lpage>47</lpage>. <pub-id pub-id-type="doi">10.1016/j.jaci.2013.07.008</pub-id>.<pub-id pub-id-type="pmid">24035157</pub-id>
</mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Thomas</surname>
<given-names>KS</given-names>
</string-name>
</person-group>. <article-title>EASI does it: a comparison of four eczema severity scales</article-title>. <source>Br J Dermatol</source>. <year>2015</year>;<volume>173</volume>:<fpage>316</fpage>&#x02013;<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1111/bjd.13967</pub-id>.<pub-id pub-id-type="pmid">26346073</pub-id>
</mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hanifin</surname>
<given-names>JM</given-names>
</string-name>, <string-name><surname>Baghoomian</surname><given-names>W</given-names></string-name>, <string-name><surname>Grinich</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Eczema Area and Severity index&#x02014;a practical guide</article-title>. <source>Dermatitis</source>. <year>2022</year>;<volume>33</volume>:<fpage>187</fpage>&#x02013;<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1097/DER.0000000000000895</pub-id>.<pub-id pub-id-type="pmid">35594457</pub-id>
</mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Riethmuller</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>McAleer</surname><given-names>MA</given-names></string-name>, <string-name><surname>Koppes</surname><given-names>SA</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Filaggrin breakdown products determine corneocyte conformation in patients with atopic dermatitis</article-title>. <source>J Allergy Clin Immunol</source>. <year>2015</year>;<volume>136</volume>:<fpage>1573</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1016/j.jaci.2015.04.042</pub-id>.<pub-id pub-id-type="pmid">26071937</pub-id>
</mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Franz</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Beutel</surname><given-names>M</given-names></string-name>, <string-name><surname>Gevers</surname><given-names>K</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Nanoscale alterations of corneocytes indicate skin disease</article-title>. <source>Skin Res Technol</source>. <year>2016</year>;<volume>22</volume>:<fpage>174</fpage>&#x02013;<lpage>80</lpage>. <pub-id pub-id-type="doi">10.1111/srt.12247</pub-id>.<pub-id pub-id-type="pmid">26100642</pub-id>
</mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Engebretsen</surname>
<given-names>KA</given-names>
</string-name>, <string-name><surname>Bandier</surname><given-names>J</given-names></string-name>, <string-name><surname>Kezic</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Concentration of filaggrin monomers, its metabolites and corneocyte surface texture in individuals with a history of atopic dermatitis and controls</article-title>. <source>J Eur Acad Dermatol Venereol</source>. <year>2018</year>;<volume>32</volume>:<fpage>796</fpage>&#x02013;<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1111/jdv.14801</pub-id>.<pub-id pub-id-type="pmid">29360238</pub-id>
</mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Riethm&#x000fc;ller</surname>
<given-names>C</given-names>
</string-name>
</person-group>. <article-title>Assessing the skin barrier via corneocyte morphometry</article-title>. <source>Exp Dermatol</source>. <year>2018</year>;<volume>27</volume>:<fpage>923</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1111/exd.13741</pub-id>.<pub-id pub-id-type="pmid">30019542</pub-id>
</mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>de&#x000a0;Boer</surname>
<given-names>FL</given-names>
</string-name>, <string-name><surname>van&#x000a0;der&#x000a0;Molen</surname><given-names>HF</given-names></string-name>, <string-name><surname>Kezic</surname><given-names>S</given-names></string-name></person-group>. <article-title>Epidermal biomarkers of the skin barrier in atopic and contact dermatitis</article-title>. <source>Contact Dermatitis</source>. <year>2023</year>;<volume>89</volume>:<fpage>221</fpage>&#x02013;<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1111/cod.14391</pub-id>.<pub-id pub-id-type="pmid">37571977</pub-id>
</mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lademann</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Jacobi</surname><given-names>U</given-names></string-name>, <string-name><surname>Surber</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The tape stripping procedure&#x02014;evaluation of some critical parameters</article-title>. <source>Eur J Pharm Biopharm</source>. <year>2009</year>;<volume>72</volume>:<fpage>317</fpage>&#x02013;<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1016/j.ejpb.2008.08.008</pub-id>.<pub-id pub-id-type="pmid">18775778</pub-id>
</mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liao</surname>
<given-names>HS</given-names>
</string-name>, <string-name><surname>Akhtar</surname><given-names>I</given-names></string-name>, <string-name><surname>Werner</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Open-source controller for low-cost and high-speed atomic force microscopy imaging of skin corneocyte nanotextures</article-title>. <source>HardwareX</source>. <year>2022</year>;<volume>12</volume>:<fpage>e00341</fpage>. <pub-id pub-id-type="doi">10.1016/j.ohx.2022.e00341</pub-id>.<pub-id pub-id-type="pmid">35936941</pub-id>
</mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chan</surname>
<given-names>TC</given-names>
</string-name>, <string-name><surname>Wu</surname><given-names>NL</given-names></string-name>, <string-name><surname>Wong</surname><given-names>LS</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Taiwanese Dermatological Association consensus for the management of atopic dermatitis: a 2020 update</article-title>. <source>J Formos Med Assoc</source>. <year>2021</year>;<volume>120</volume>(<issue>1</issue>):<fpage>429</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/j.jfma.2020.06.008</pub-id>.<pub-id pub-id-type="pmid">32564976</pub-id>
</mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hajian-Tilaki</surname>
<given-names>K</given-names>
</string-name>
</person-group>. <article-title>Sample size estimation in diagnostic test studies of biomedical informatics</article-title>. <source>J Biomed Inform</source>. <year>2014</year>;<volume>48</volume>:<fpage>193</fpage>&#x02013;<lpage>204</lpage>. <pub-id pub-id-type="doi">10.1016/j.jbi.2014.02.013</pub-id>.<pub-id pub-id-type="pmid">24582925</pub-id>
</mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Heinisch</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Cochran</surname><given-names>WG</given-names></string-name></person-group>. <article-title>Sampling techniques, 2. Aufl. John Wiley and Sons, New York, London 1963. Preis s</article-title>. <source>Biom Z</source>. <year>1965</year>;<volume>7</volume>(<issue>3</issue>):<fpage>203</fpage>. <pub-id pub-id-type="doi">10.1002/bimj.19650070312.</pub-id></mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Dapic</surname>
<given-names>I</given-names>
</string-name>, <string-name><surname>Jakasa</surname><given-names>I</given-names></string-name>, <string-name><surname>Yau</surname><given-names>NLH</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Evaluation of an HPLC method for the determination of natural moisturizing factors in the human stratum corneum</article-title>. <source>Anal Lett</source>. <year>2013</year>;<volume>46</volume>:<fpage>2133</fpage>&#x02013;<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1080/00032719.2013.789881</pub-id>.</mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Inoue</surname>
<given-names>T</given-names>
</string-name>, <string-name><surname>Kuwano</surname><given-names>T</given-names></string-name>, <string-name><surname>Uehara</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Non-invasive human skin transcriptome analysis using mRNA in skin surface lipids</article-title>. <source>Commun Biol</source>. <year>2022</year>;<volume>5</volume>:<fpage>215</fpage>. <pub-id pub-id-type="doi">10.1111/jdv.18173</pub-id>.<pub-id pub-id-type="pmid">35264722</pub-id>
</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kezic</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Kammeyer</surname><given-names>A</given-names></string-name>, <string-name><surname>Calkoen</surname><given-names>F</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Natural moisturizing factor components in the stratum corneum as biomarkers of filaggrin genotype: evaluation of minimally invasive methods</article-title>. <source>Br J Dermatol</source>. <year>2009</year>;<volume>161</volume>:<fpage>1098</fpage>&#x02013;<lpage>104</lpage>. <pub-id pub-id-type="doi">10.1111/j.1365-2133.2009.09342.x</pub-id>.<pub-id pub-id-type="pmid">19857209</pub-id>
</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hwu</surname>
<given-names>EET</given-names>
</string-name>, <string-name><surname>Boisen</surname><given-names>A</given-names></string-name></person-group>. <article-title>Hacking CD/DVD/blu-ray for biosensing</article-title>. <source>ACS Sensors</source>. <year>2018</year>;<volume>3</volume>(<issue>7</issue>):<fpage>1222</fpage>&#x02013;<lpage>32</lpage>. <pub-id pub-id-type="doi">10.1021/acssensors.8b00340</pub-id>.<pub-id pub-id-type="pmid">29978699</pub-id>
</mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kienberger</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Pastushenko</surname><given-names>VP</given-names></string-name>, <string-name><surname>Kada</surname><given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Improving the contrast of topographical AFM images by a simple averaging filter</article-title>. <source>Ultramicroscopy</source>. <year>2006</year>;<volume>106</volume>:<fpage>822</fpage>&#x02013;<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/j.ultramic.2005.11.013</pub-id>.<pub-id pub-id-type="pmid">16675120</pub-id>
</mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kimori</surname>
<given-names>Y</given-names>
</string-name>
</person-group>. <article-title>Mathematical morphology-based approach to the enhancement of morphological features in medical images</article-title>. <source>J Clin Bioinform</source>. <year>2011</year>;<volume>1</volume>:<fpage>33</fpage>. <pub-id pub-id-type="doi">10.1186/2043-9113-1-33</pub-id>.</mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Gedraite</surname>
<given-names>ES</given-names>
</string-name>, <string-name><surname>Hadad</surname><given-names>M</given-names></string-name></person-group>. <article-title>Investigation on the effect of a gaussian blur in image filtering and segmentation</article-title>. In: <source>Proceedings ELMAR-2011</source>. <publisher-loc>Zadar,&#x000a0;Croatia</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2011</year>:<fpage>393</fpage>&#x02013;<lpage>396</lpage>. <ext-link xlink:href="https://ieeexplore.ieee.org/document/6044249" ext-link-type="uri">https://ieeexplore.ieee.org/document/6044249</ext-link>.</mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Eaton</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>West</surname><given-names>P</given-names></string-name></person-group>. <source>Atomic force microscopy</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2010</year>. <pub-id pub-id-type="doi">10.1093/acprof:oso/9780199570454.001.0001</pub-id>.</mixed-citation></ref><ref id="bib28"><label>28.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kubo</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Umeda</surname><given-names>K</given-names></string-name>, <string-name><surname>Kodera</surname><given-names>N</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Removing the parachuting artifact using two-way scanning data in high-speed atomic force microscopy</article-title>. <source>Biophysics and Physicobiology</source>. <year>2023</year>;<volume>20</volume>(<issue>1</issue>):<fpage>e200006</fpage>. <pub-id pub-id-type="doi">10.2142/biophysico.bppb-v20.0006</pub-id>.<pub-id pub-id-type="pmid">37234854</pub-id>
</mixed-citation></ref><ref id="bib29"><label>29.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Toet</surname>
<given-names>A</given-names>
</string-name>
</person-group>. <article-title>Adaptive multi-scale contrast enhancement through non-linear pyramid recombination</article-title>. <source>Pattern Recognit Lett</source>. <year>1990</year>;<volume>11</volume>(<issue>11</issue>):<fpage>735</fpage>&#x02013;<lpage>42</lpage>. <pub-id pub-id-type="doi">10.1016/0167-8655(90)90092-G</pub-id>.</mixed-citation></ref><ref id="bib30"><label>30.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Haralick</surname>
<given-names>RM</given-names>
</string-name>, <string-name><surname>Sternberg</surname><given-names>SR</given-names></string-name>, <string-name><surname>Zhuang</surname><given-names>X</given-names></string-name></person-group>. <article-title>Image analysis using mathematical morphology</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1987</year>;<volume>PAMI-9</volume>:<fpage>532</fpage>&#x02013;<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.1987.4767941</pub-id>.</mixed-citation></ref><ref id="bib31"><label>31.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Oh</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Hwang</surname><given-names>H</given-names></string-name></person-group>. <article-title>Feature enhancement of medical images using morphology-based homomorphic filter and differential evolution algorithm</article-title>. <source>Int J Control Autom Syst</source>. <year>2010</year>;<volume>8</volume>:<fpage>857</fpage>&#x02013;<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1007/s12555-010-0418-y</pub-id>.</mixed-citation></ref><ref id="bib32"><label>32.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jia</surname>
<given-names>X</given-names>
</string-name>, <string-name><surname>Tong</surname><given-names>Y</given-names></string-name>, <string-name><surname>Qiao</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Fast and accurate object detector for autonomous driving based on improved YOLOv5</article-title>. <source>Sci Rep</source>. <year>2023</year>;<volume>13</volume>(<issue>1</issue>):<fpage>9711</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-023-36868-w</pub-id>.<pub-id pub-id-type="pmid">37322088</pub-id>
</mixed-citation></ref><ref id="bib33"><label>33.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Bogdoll</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Nitsche</surname><given-names>M</given-names></string-name>, <string-name><surname>Zollner</surname><given-names>JM</given-names></string-name></person-group>. <article-title>Anomaly detection in autonomous driving: a survey</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source>. <publisher-loc>New Orleans, LA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2022</year>:<fpage>4487</fpage>&#x02013;<lpage>4498</lpage>. <pub-id pub-id-type="doi">10.1109/CVPRW56347.2022.00495</pub-id>.</mixed-citation></ref><ref id="bib34"><label>34.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sobek</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Medina&#x000a0;Inojosa</surname><given-names>JR</given-names></string-name>, <string-name><surname>Medina&#x000a0;Inojosa</surname><given-names>BJ</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>MedYOLO: a medical image object detection framework</article-title>. <source>J Imaging Inform Med</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.1007/s10278-024-01138-2</pub-id>.</mixed-citation></ref><ref id="bib35"><label>35.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shou</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Meng</surname><given-names>T</given-names></string-name>, <string-name><surname>Ai</surname><given-names>W</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Object detection in medical images based on hierarchical transformer and mask mechanism</article-title>. <source>Comput Intell Neurosci</source>. <year>2022</year>;<volume>2022</volume>:<fpage>1</fpage>&#x02013;<lpage>12</lpage>. <pub-id pub-id-type="doi">10.1155/2022/5863782</pub-id>.</mixed-citation></ref><ref id="bib36"><label>36.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Girshick</surname>
<given-names>R</given-names>
</string-name>
</person-group>. <article-title>Fast R-CNN</article-title>. In: <source>2015 IEEE International Conference on Computer Vision (ICCV)</source>. <publisher-loc>Santiago, Chile</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2015</year>:<fpage>1440</fpage>&#x02013;<lpage>1448</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV.2015.169</pub-id>.</mixed-citation></ref><ref id="bib37"><label>37.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>He</surname>
<given-names>K</given-names>
</string-name>, <string-name><surname>Gkioxari</surname><given-names>G</given-names></string-name>, <string-name><surname>Dollar</surname><given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Mask R-CNN</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2020</year>;<volume>42</volume>(<issue>2</issue>):<fpage>386</fpage>&#x02013;<lpage>97</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2844175</pub-id>.<pub-id pub-id-type="pmid">29994331</pub-id>
</mixed-citation></ref><ref id="bib38"><label>38.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Redmon</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Divvala</surname><given-names>S</given-names></string-name>, <string-name><surname>Girshick</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>You only look once: unified, real-time object detection</article-title>. In: <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Las Vegas, NV, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2016</year>:<fpage>779</fpage>&#x02013;<lpage>788</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>.</mixed-citation></ref><ref id="bib39"><label>39.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Redmon</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Farhadi</surname><given-names>A</given-names></string-name></person-group>. <article-title>YOLO9000: Better, Faster, Stronger</article-title>. In: <source>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Honolulu, HI, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2017</year>:<fpage>6517</fpage>&#x02013;<lpage>6525</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR.2017.690</pub-id>.</mixed-citation></ref><ref id="bib40"><label>40.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Redmon</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Farhadi</surname><given-names>A</given-names></string-name></person-group>. <article-title>YOLOv3: an incremental improvement</article-title>. <source>Arxiv</source>. <year>2018</year>. <pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id>.</mixed-citation></ref><ref id="bib41"><label>41.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Carion</surname>
<given-names>N</given-names>
</string-name>, <string-name><surname>Massa</surname><given-names>F</given-names></string-name>, <string-name><surname>Synnaeve</surname><given-names>G</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>End-to-end object detection with transformers</article-title>. In: <source>Computer Vision &#x02013; ECCV 2020: 16th European Conference</source>. <publisher-loc>Glasgow, UK</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>; <year>2020</year>:<fpage>213</fpage>&#x02013;<lpage>229</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-030-58452-8_13</pub-id>.</mixed-citation></ref><ref id="bib42"><label>42.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhu</surname>
<given-names>X</given-names>
</string-name>, <string-name><surname>Su</surname><given-names>W</given-names></string-name>, <string-name><surname>Lu</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Deformable DETR: Deformable Transformers for End-to-End Object Detection</article-title>. In: <source>International Conference on Learning Representations</source>. <year>2021</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2010.04159</pub-id>.</mixed-citation></ref><ref id="bib43"><label>43.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Zhang</surname>
<given-names>H</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>F</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</article-title>. In: <source>The Eleventh International Conference on Learning Representations</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2203.03605</pub-id>.</mixed-citation></ref><ref id="bib44"><label>44.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Li</surname>
<given-names>F</given-names>
</string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</article-title>. In: <source>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>New Orleans, LA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2022</year>:<fpage>13609</fpage>&#x02013;<lpage>13617</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01325</pub-id>.</mixed-citation></ref><ref id="bib45"><label>45.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Liu</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>F</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR</article-title>. <source>International Conference on Learning Representations</source>. <year>2022</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2201.12329</pub-id>.</mixed-citation></ref><ref id="bib46"><label>46.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Meng</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>Fan</surname><given-names>Z</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Conditional DETR for fast training convergence</article-title>. In: <source>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source>. <publisher-loc>Montreal, QC, Canada</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2021</year>:<fpage>3631</fpage>&#x02013;<lpage>3640</lpage>. <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00363</pub-id>.</mixed-citation></ref><ref id="bib47"><label>47.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Yang</surname><given-names>T</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Anchor DETR: query design for transformer-based detector</article-title>. <source>Proc AAAI Conf Artificial Intell</source>. <year>2022</year>;<volume>36</volume>(<issue>3</issue>):<fpage>2567</fpage>&#x02013;<lpage>75</lpage>. <pub-id pub-id-type="doi">10.1609/aaai.v36i3.20158</pub-id>.</mixed-citation></ref><ref id="bib48"><label>48.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bochkovskiy</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Wang</surname><given-names>CY</given-names></string-name>, <string-name><surname>Liao</surname><given-names>HYM</given-names></string-name></person-group>. <article-title>YOLOv4: optimal speed and accuracy of object detection</article-title>. <source>arXiv</source>. <year>2020</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2004.10934.</pub-id></mixed-citation></ref><ref id="bib49"><label>49.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ge</surname>
<given-names>Z</given-names>
</string-name>, <string-name><surname>Liu</surname><given-names>S</given-names></string-name>, <string-name><surname>Wang</surname><given-names>F</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOX: exceeding YOLO Series in 2021</article-title>. <source>arXiv</source>. <year>2021</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2107.08430.</pub-id></mixed-citation></ref><ref id="bib50"><label>50.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>CY</given-names>
</string-name>, <string-name><surname>Bochkovskiy</surname><given-names>A</given-names></string-name>, <string-name><surname>Liao</surname><given-names>HYM</given-names></string-name></person-group>. <article-title>Scaled-YOLOv4: Scaling Cross Stage Partial Network</article-title>. In: <source>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Nashville, TN, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2021</year>:<fpage>13024</fpage>&#x02013;<lpage>13033</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01283</pub-id>.</mixed-citation></ref><ref id="bib51"><label>51.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Yuan</surname><given-names>X</given-names></string-name>, <string-name><surname>Wu</surname><given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLO-MS: rethinking multi-scale representation learning for real-time object detection</article-title>. <source>arXiv</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2308.05480.</pub-id></mixed-citation></ref><ref id="bib52"><label>52.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Huang</surname>
<given-names>L</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>W</given-names></string-name>, <string-name><surname>Shen</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOCS: object detection based on dense channel compression for feature spatial solidification</article-title>. <source>arXiv</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2305.04170.</pub-id></mixed-citation></ref><ref id="bib53"><label>53.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Li</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>L</given-names></string-name>, <string-name><surname>Geng</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOv6 v3.0: a full-scale reloading</article-title>. <source>arXiv</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2301.05586.</pub-id>Accessed 20 September 2024.</mixed-citation></ref><ref id="bib54"><label>54.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>He</surname><given-names>W</given-names></string-name>, <string-name><surname>Nie</surname><given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism</article-title>. <source>Thirty-seventh Conference on Neural Information Processing Systems</source>. <year>2023</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2309.11331</pub-id></mixed-citation></ref><ref id="bib55"><label>55.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>CY</given-names>
</string-name>, <string-name><surname>Bochkovskiy</surname><given-names>A</given-names></string-name>, <string-name><surname>Liao</surname><given-names>HYM</given-names></string-name></person-group>. <article-title>YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors</article-title>. In: <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).</source><publisher-loc>Vancouver, BC, Canada</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2023</year>:<fpage>7464</fpage>&#x02013;<lpage>7475</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00721</pub-id>.</mixed-citation></ref><ref id="bib56"><label>56.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Varghese</surname>
<given-names>R</given-names>
</string-name>, <string-name><surname>Sambath</surname><given-names>M</given-names></string-name></person-group>. <article-title>YOLOv8: a novel object detection algorithm with enhanced performance and robustness</article-title>. In: <source>2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS)</source>. <publisher-loc>Chennai, India</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2024</year>:<fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/ADICS58448.2024.10533619</pub-id>.</mixed-citation></ref><ref id="bib57"><label>57.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>CY</given-names>
</string-name>, <string-name><surname>Yeh</surname><given-names>IH</given-names></string-name>, <string-name><surname>Liao</surname><given-names>HYM</given-names></string-name></person-group>. <article-title>YOLOv9: learning what you want to learn using programmable gradient information</article-title>. <source>arXiv</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2402.13616.</pub-id>Accessed 20 September 2024.</mixed-citation></ref><ref id="bib58"><label>58.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>A</given-names>
</string-name>, <string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <string-name><surname>Liu</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOv10: Real-Time End-to-End Object Detection</article-title>. <source>The Thirty-eighth Annual Conference on Neural Information Processing Systems</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2405.14458</pub-id></mixed-citation></ref><ref id="bib59"><label>59.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Li</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Li</surname><given-names>L</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOv6: a single-stage object detection framework for industrial applications</article-title>. <source>arXiv</source>. <year>2022</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2209.02976</pub-id>.</mixed-citation></ref><ref id="bib60"><label>60.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Rahman</surname>
<given-names>S</given-names>
</string-name>, <string-name><surname>Rony</surname><given-names>JH</given-names></string-name>, <string-name><surname>Uddin</surname><given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>Real-time obstacle detection with YOLOv8 in a WSN using UAV aerial photography</article-title>. <source>J Imaging</source>. <year>2023</year>;<volume>9</volume>(<issue>10</issue>):<fpage>216</fpage>. <pub-id pub-id-type="doi">10.3390/jimaging9100216</pub-id>.<pub-id pub-id-type="pmid">37888323</pub-id>
</mixed-citation></ref><ref id="bib61"><label>61.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Khare</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Gandhi</surname><given-names>S</given-names></string-name>, <string-name><surname>Rahalkar</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>YOLOv8-based visual detection of road hazards: potholes, sewer covers, and manholes</article-title>. In: <source>2023 IEEE Pune Section International Conference (PuneCon)</source>. <publisher-loc>Pune, India</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2023</year>:<fpage>1</fpage>&#x02013;<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/PuneCon58714.2023.10449999</pub-id>.</mixed-citation></ref><ref id="bib62"><label>62.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>CY</given-names>
</string-name>, <string-name><surname>Liao</surname><given-names>HYM</given-names></string-name></person-group>. <article-title>YOLOv1 to YOLOv10: the fastest and most accurate real-time object detection systems</article-title>. <source>APSIPA Transactions on Signal and Information Processing</source>. <year>2024</year>;<volume>13</volume>(<issue>1</issue>):<fpage>e29</fpage>. <pub-id pub-id-type="doi">10.1561/116.20240058</pub-id>.</mixed-citation></ref><ref id="bib63"><label>63.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hussain</surname>
<given-names>M</given-names>
</string-name>
</person-group>. <article-title>YOLOv5, YOLOv8 and YOLOv10: the go-to detectors for real-time vision</article-title>. <source>arXiv</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2407.02988.</pub-id></mixed-citation></ref><ref id="bib64"><label>64.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Sun</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Jiang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Xie</surname><given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>What makes for end-to-end object detection?</article-title>In: <source>Proceedings of the 38th International Conference on Machine Learning</source>. <publisher-name>PMLR</publisher-name>; <year>2021</year>;<volume>139</volume>:<fpage>9934</fpage>&#x02013;<lpage>9944</lpage>. <ext-link xlink:href="https://proceedings.mlr.press/v139/sun21b.html" ext-link-type="uri">https://proceedings.mlr.press/v139/sun21b.html</ext-link>.</mixed-citation></ref><ref id="bib65"><label>65.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhao</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Lv</surname><given-names>W</given-names></string-name>, <string-name><surname>Xu</surname><given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>DETRs Beat YOLOs on Real-time Object Detection</article-title>. In: <source>2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>. <publisher-loc>Seattle, WA, USA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2024</year>;<fpage>16965</fpage>&#x02013;<lpage>16974</lpage>. <pub-id pub-id-type="doi">10.1109/CVPR52733.2024.01605</pub-id>.</mixed-citation></ref><ref id="bib66"><label>66.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lv</surname>
<given-names>W</given-names>
</string-name>, <string-name><surname>Zhao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Chang</surname><given-names>Q</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>RT-DETRv2: improved baseline with bag-of-freebies for real-time detection transformer</article-title>. <source>arXiv</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.48550/arXiv.2407.17140</pub-id>.</mixed-citation></ref><ref id="bib67"><label>67.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Yoon</surname><given-names>S</given-names></string-name>, <string-name><surname>Fuentes</surname><given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A comprehensive survey of image augmentation techniques for deep learning</article-title>. <source>Pattern Recognit</source>. <year>2023</year>;<volume>137</volume>:<fpage>109347</fpage>. <pub-id pub-id-type="doi">10.1016/j.patcog.2023.109347</pub-id>.</mixed-citation></ref><ref id="bib68"><label>68.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shorten</surname>
<given-names>C</given-names>
</string-name>, <string-name><surname>Khoshgoftaar</surname><given-names>TM</given-names></string-name></person-group>. <article-title>A survey on image data augmentation for deep learning</article-title>. <source>J Big Data</source>. <year>2019</year>;<volume>6</volume>(<issue>1</issue>):<fpage>60</fpage>. <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id>.</mixed-citation></ref><ref id="bib69"><label>69.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Chen</surname>
<given-names>YC</given-names>
</string-name>
</person-group>. <article-title>A tutorial on kernel density estimation and recent advances</article-title>. <source>Biostat Epidemiol</source>. <year>2017</year>;<volume>1</volume>(<issue>1</issue>):<fpage>161</fpage>&#x02013;<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1080/24709360.2017.1396742</pub-id>.</mixed-citation></ref><ref id="bib70"><label>70.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>W&#x00119;glarczyk</surname>
<given-names>S</given-names>
</string-name>
</person-group>. <article-title>Kernel density estimation and its application</article-title>. <source>ITM Web of Conferences</source>. <year>2018</year>;<volume>23</volume>:<fpage>00037</fpage>. <pub-id pub-id-type="doi">10.1051/itmconf/20182300037</pub-id>.</mixed-citation></ref><ref id="bib71"><label>71.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Heidenreich</surname>
<given-names>NB</given-names>
</string-name>, <string-name><surname>Schindler</surname><given-names>A</given-names></string-name>, <string-name><surname>Sperlich</surname><given-names>S</given-names></string-name></person-group>. <article-title>Bandwidth selection for kernel density estimation: a review of fully automatic selectors</article-title>. <source>Adv Stat Anal</source>. <year>2013</year>;<volume>97</volume>(<issue>4</issue>):<fpage>403</fpage>&#x02013;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1007/s10182-013-0216-y</pub-id>.</mixed-citation></ref><ref id="bib72"><label>72.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Everingham</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Eslami</surname><given-names>SMA</given-names></string-name>, <string-name><surname>Van&#x000a0;Gool</surname><given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Pascal visual object classes challenge: a retrospective</article-title>. <source>Int J Comput Vis</source>. <year>2014</year>;<volume>111</volume>(<issue>1</issue>):<fpage>98</fpage>&#x02013;<lpage>136</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-014-0733-5</pub-id>.</mixed-citation></ref><ref id="bib73"><label>73.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Russakovsky</surname>
<given-names>O</given-names>
</string-name>, <string-name><surname>Deng</surname><given-names>J</given-names></string-name>, <string-name><surname>Su</surname><given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>ImageNet large scale visual recognition challenge</article-title>. <source>Int J Comput Vis</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>&#x02013;<lpage>52</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>.</mixed-citation></ref><ref id="bib74"><label>74.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Everingham</surname>
<given-names>M</given-names>
</string-name>, <string-name><surname>Van&#x000a0;Gool</surname><given-names>L</given-names></string-name>, <string-name><surname>Williams</surname><given-names>CKI</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>The Pascal visual object classes (VOC) challenge</article-title>. <source>Int J Comput Vis</source>. <year>2009</year>;<volume>88</volume>(<issue>2</issue>):<fpage>303</fpage>&#x02013;<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id>.</mixed-citation></ref><ref id="bib75"><label>75.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Zhou</surname>
<given-names>Y</given-names>
</string-name>, <string-name><surname>Yang</surname><given-names>K</given-names></string-name></person-group>. <article-title>Exploring TensorRT to Improve Real-Time Inference for Deep Learning</article-title>. In: <source>2022 IEEE 24th Int Conf on High Performance Computing &#x00026; Communications; 8th Int Conf on Data Science &#x00026; Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud &#x00026; Big Data Systems &#x00026; Application (HPCC/DSS/SmartCity/DependSys)</source>. <publisher-loc>Hainan, China</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2022</year>:<fpage>2011</fpage>&#x02013;<lpage>2018</lpage>. <pub-id pub-id-type="doi">10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00299</pub-id>.</mixed-citation></ref><ref id="bib76"><label>76.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ganguly</surname>
<given-names>P</given-names>
</string-name>, <string-name><surname>Methani</surname><given-names>NS</given-names></string-name>, <string-name><surname>Khapra</surname><given-names>MM</given-names></string-name>, <etal>et al.</etal></person-group>
<article-title>A systematic evaluation of object detection networks for scientific plots</article-title>. <source>Proc AAAI Conf Artificial Intell</source>. <year>2021</year>;<volume>35</volume>(<issue>2</issue>):<fpage>1379</fpage>&#x02013;<lpage>87</lpage>. <pub-id pub-id-type="doi">10.1609/aaai.v35i2.16227</pub-id>.</mixed-citation></ref><ref id="bib77"><label>77.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Reed</surname>
<given-names>GF</given-names>
</string-name>, <string-name><surname>Lynn</surname><given-names>F</given-names></string-name>, <string-name><surname>Meade</surname><given-names>BD</given-names></string-name></person-group>. <article-title>Use of coefficient of variation in assessing variability of quantitative assays</article-title>. <source>Clin Vaccine Immunol</source>20<year>03</year>;<volume>10</volume>(<issue>6</issue>):<fpage>1162</fpage>. <pub-id pub-id-type="doi">10.1128/CDLI.10.6.1162.2003.</pub-id></mixed-citation></ref><ref id="bib78"><label>78.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shapiro</surname>
<given-names>SS</given-names>
</string-name>, <string-name><surname>Wilk</surname><given-names>MB</given-names></string-name></person-group>. <article-title>An analysis of variance test for normality (complete samples)</article-title>. <source>Biometrika</source>. <year>1965</year>;<volume>52</volume>(<issue>3&#x02013;4</issue>):<fpage>591</fpage>&#x02013;<lpage>611</lpage>. <pub-id pub-id-type="doi">10.1093/biomet/52.3-4.591</pub-id>.</mixed-citation></ref><ref id="bib79"><label>79.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Rey</surname>
<given-names>D</given-names>
</string-name>, <string-name><surname>Neuh&#x000e4;user</surname><given-names>M</given-names></string-name></person-group>. <source>Wilcoxon-signed-rank test. Berlin: Springer</source>. <year>2011</year>;<fpage>1658</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-642-04898-2_616</pub-id>.</mixed-citation></ref><ref id="bib80"><label>80.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fay</surname>
<given-names>MP</given-names>
</string-name>, <string-name><surname>Proschan</surname><given-names>MA</given-names></string-name></person-group>. <article-title>Wilcoxon-Mann-Whitney or t-test? On assumptions for hypothesis tests and multiple interpretations of decision rules</article-title>. <source>Stat Surveys</source>. <year>2010</year>;<volume>4</volume>:<fpage>1</fpage>&#x02013;<lpage>39</lpage>. <pub-id pub-id-type="doi">10.1214/09-SS051</pub-id>.</mixed-citation></ref><ref id="bib81"><label>81.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>
<given-names>JH</given-names>
</string-name>, <string-name><surname>Pereda</surname><given-names>J</given-names></string-name>, <string-name><surname>Hwu</surname><given-names>EET</given-names></string-name></person-group>. <article-title>Source code: stratum corneum nanotexture feature detection using deep learning and spatial analysis</article-title>. <year>2024</year>. <ext-link xlink:href="https://github.com/JenHungWang/ECTI_Atopic_Dermatitis" ext-link-type="uri">https://github.com/JenHungWang/ECTI_Atopic_Dermatitis</ext-link>. <comment>Accessed 3 September 2024</comment>.</mixed-citation></ref><ref id="bib83_156_270524"><label>82.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Wang</surname>, <given-names>J.-H</given-names></string-name>
</person-group>. <article-title>ECTI Atopic Dermatitis</article-title>. <source>WorkflowHub</source>. <year>2024</year>. <pub-id pub-id-type="doi">10.48546/WORKFLOWHUB.WORKFLOW.1161.1</pub-id></mixed-citation></ref><ref id="bib82"><label>83.</label><mixed-citation publication-type="data">
<person-group person-group-type="curator">
<string-name>
<surname>Wang</surname>
<given-names>J</given-names>
</string-name>, <string-name><surname>Pereda</surname><given-names>J</given-names></string-name>, <string-name><surname>Du</surname><given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>
<data-title>Supporting data for &#x0201c;Stratum Corneum Nanotexture Feature Detection Using Deep Learning and Spatial Analysis: A Noninvasive Tool for Skin Barrier Assessment</data-title>.&#x0201d; <comment>GigaScience Database.</comment><year>2024</year>. <pub-id pub-id-type="doi">10.5524/102604.</pub-id></mixed-citation></ref></ref-list></back></article>